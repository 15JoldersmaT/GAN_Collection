{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"448a7b3f-379a-44aa-ce32-074e44ff03a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 294/4000, Step 59, d_loss: 0.33722397685050964, g_loss: 6.031556606292725\n","Epoch 294/4000, Step 60, d_loss: 0.35650718212127686, g_loss: 5.974877834320068\n","Epoch 294/4000, Step 61, d_loss: 0.34572717547416687, g_loss: 5.981827259063721\n","Epoch 294/4000, Step 62, d_loss: 0.3607417643070221, g_loss: 5.617284297943115\n","Epoch 294/4000, Step 63, d_loss: 0.3385123908519745, g_loss: 5.418952465057373\n","Epoch 294/4000, Step 64, d_loss: 0.35168954730033875, g_loss: 7.775112628936768\n","Epoch 294/4000, Step 65, d_loss: 0.344204306602478, g_loss: 5.913851261138916\n","Epoch 294/4000, Step 66, d_loss: 0.34097716212272644, g_loss: 7.43634033203125\n","Epoch 294/4000, Step 67, d_loss: 0.3353872299194336, g_loss: 5.875293254852295\n","Epoch 294/4000, Step 68, d_loss: 0.36092859506607056, g_loss: 6.274741172790527\n","Epoch 294/4000, Step 69, d_loss: 0.3426917791366577, g_loss: 6.116741180419922\n","Epoch 294/4000, Step 70, d_loss: 0.33633410930633545, g_loss: 6.066377639770508\n","Epoch 294/4000, Step 71, d_loss: 0.3644879162311554, g_loss: 5.378096103668213\n","Epoch 294/4000, Step 72, d_loss: 0.33875274658203125, g_loss: 6.439863204956055\n","Epoch 294/4000, Step 73, d_loss: 0.3631748557090759, g_loss: 7.286146640777588\n","Epoch 294/4000, Step 74, d_loss: 0.3433135449886322, g_loss: 6.078183650970459\n","Epoch 294/4000, Step 75, d_loss: 0.33382511138916016, g_loss: 8.986842155456543\n","Epoch 294/4000, Step 76, d_loss: 0.3463889956474304, g_loss: 5.3216142654418945\n","Epoch 294/4000, Step 77, d_loss: 0.3426327407360077, g_loss: 4.983407020568848\n","Epoch 294/4000, Step 78, d_loss: 0.3534835875034332, g_loss: 5.394879341125488\n","Epoch 294/4000, Step 79, d_loss: 0.3313571512699127, g_loss: 7.239385604858398\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 295/4000, Step 1, d_loss: 0.3320144712924957, g_loss: 5.171365261077881\n","Epoch 295/4000, Step 2, d_loss: 0.3679729700088501, g_loss: 5.163074493408203\n","Epoch 295/4000, Step 3, d_loss: 0.3465331494808197, g_loss: 5.719781875610352\n","Epoch 295/4000, Step 4, d_loss: 0.35886552929878235, g_loss: 6.979805946350098\n","Epoch 295/4000, Step 5, d_loss: 0.3494453728199005, g_loss: 5.639061450958252\n","Epoch 295/4000, Step 6, d_loss: 0.339245080947876, g_loss: 5.077415943145752\n","Epoch 295/4000, Step 7, d_loss: 0.35094285011291504, g_loss: 5.48389196395874\n","Epoch 295/4000, Step 8, d_loss: 0.33670133352279663, g_loss: 5.686141014099121\n","Epoch 295/4000, Step 9, d_loss: 0.33467432856559753, g_loss: 5.5676679611206055\n","Epoch 295/4000, Step 10, d_loss: 0.34247612953186035, g_loss: 5.537205219268799\n","Epoch 295/4000, Step 11, d_loss: 0.3376278877258301, g_loss: 5.583505630493164\n","Epoch 295/4000, Step 12, d_loss: 0.3486722409725189, g_loss: 7.248896598815918\n","Epoch 295/4000, Step 13, d_loss: 0.34247153997421265, g_loss: 5.695597171783447\n","Epoch 295/4000, Step 14, d_loss: 0.32970577478408813, g_loss: 8.191060066223145\n","Epoch 295/4000, Step 15, d_loss: 0.3320511281490326, g_loss: 5.746103286743164\n","Epoch 295/4000, Step 16, d_loss: 0.34619277715682983, g_loss: 5.856836318969727\n","Epoch 295/4000, Step 17, d_loss: 0.3305870592594147, g_loss: 4.849667549133301\n","Epoch 295/4000, Step 18, d_loss: 0.34022724628448486, g_loss: 5.085265159606934\n","Epoch 295/4000, Step 19, d_loss: 0.33912456035614014, g_loss: 5.293204307556152\n","Epoch 295/4000, Step 20, d_loss: 0.3351573646068573, g_loss: 7.300649642944336\n","Epoch 295/4000, Step 21, d_loss: 0.352446049451828, g_loss: 6.26693868637085\n","Epoch 295/4000, Step 22, d_loss: 0.3469492495059967, g_loss: 6.389806747436523\n","Epoch 295/4000, Step 23, d_loss: 0.3463541865348816, g_loss: 5.880794048309326\n","Epoch 295/4000, Step 24, d_loss: 0.3358362019062042, g_loss: 6.6798095703125\n","Epoch 295/4000, Step 25, d_loss: 0.37174296379089355, g_loss: 5.7271809577941895\n","Epoch 295/4000, Step 26, d_loss: 0.3334350287914276, g_loss: 5.980290412902832\n","Epoch 295/4000, Step 27, d_loss: 0.340604305267334, g_loss: 6.062325954437256\n","Epoch 295/4000, Step 28, d_loss: 0.33949774503707886, g_loss: 5.875814914703369\n","Epoch 295/4000, Step 29, d_loss: 0.3405526876449585, g_loss: 5.586304187774658\n","Epoch 295/4000, Step 30, d_loss: 0.3406316637992859, g_loss: 5.028911113739014\n","Epoch 295/4000, Step 31, d_loss: 0.36117684841156006, g_loss: 5.224579811096191\n","Epoch 295/4000, Step 32, d_loss: 0.34607037901878357, g_loss: 4.747296333312988\n","Epoch 295/4000, Step 33, d_loss: 0.33908823132514954, g_loss: 6.100196838378906\n","Epoch 295/4000, Step 34, d_loss: 0.3335483968257904, g_loss: 5.797920227050781\n","Epoch 295/4000, Step 35, d_loss: 0.33372581005096436, g_loss: 8.074204444885254\n","Epoch 295/4000, Step 36, d_loss: 0.3732869327068329, g_loss: 6.470713138580322\n","Epoch 295/4000, Step 37, d_loss: 0.33429425954818726, g_loss: 5.548114776611328\n","Epoch 295/4000, Step 38, d_loss: 0.33799731731414795, g_loss: 6.3352861404418945\n","Epoch 295/4000, Step 39, d_loss: 0.3372415006160736, g_loss: 5.791799545288086\n","Epoch 295/4000, Step 40, d_loss: 0.33933258056640625, g_loss: 5.776449203491211\n","Epoch 295/4000, Step 41, d_loss: 0.3381142318248749, g_loss: 6.564053058624268\n","Epoch 295/4000, Step 42, d_loss: 0.33574438095092773, g_loss: 5.4052534103393555\n","Epoch 295/4000, Step 43, d_loss: 0.33402177691459656, g_loss: 6.247012615203857\n","Epoch 295/4000, Step 44, d_loss: 0.33985471725463867, g_loss: 7.4964375495910645\n","Epoch 295/4000, Step 45, d_loss: 0.34437474608421326, g_loss: 6.333138942718506\n","Epoch 295/4000, Step 46, d_loss: 0.33653196692466736, g_loss: 6.455814838409424\n","Epoch 295/4000, Step 47, d_loss: 0.33487820625305176, g_loss: 6.056518077850342\n","Epoch 295/4000, Step 48, d_loss: 0.3620162904262543, g_loss: 6.4381608963012695\n","Epoch 295/4000, Step 49, d_loss: 0.3438341021537781, g_loss: 5.4115729331970215\n","Epoch 295/4000, Step 50, d_loss: 0.3501584827899933, g_loss: 5.2839837074279785\n","Epoch 295/4000, Step 51, d_loss: 0.36031290888786316, g_loss: 5.456610202789307\n","Epoch 295/4000, Step 52, d_loss: 0.3368428349494934, g_loss: 5.159940719604492\n","Epoch 295/4000, Step 53, d_loss: 0.3504319190979004, g_loss: 7.547791957855225\n","Epoch 295/4000, Step 54, d_loss: 0.3359828293323517, g_loss: 6.147594451904297\n","Epoch 295/4000, Step 55, d_loss: 0.33793479204177856, g_loss: 6.216803550720215\n","Epoch 295/4000, Step 56, d_loss: 0.36109665036201477, g_loss: 9.708296775817871\n","Epoch 295/4000, Step 57, d_loss: 0.34106287360191345, g_loss: 5.655389308929443\n","Epoch 295/4000, Step 58, d_loss: 0.33369484543800354, g_loss: 5.429104804992676\n","Epoch 295/4000, Step 59, d_loss: 0.33900758624076843, g_loss: 5.3898491859436035\n","Epoch 295/4000, Step 60, d_loss: 0.34249183535575867, g_loss: 5.236377239227295\n","Epoch 295/4000, Step 61, d_loss: 0.33501705527305603, g_loss: 4.834881782531738\n","Epoch 295/4000, Step 62, d_loss: 0.34925776720046997, g_loss: 4.741701126098633\n","Epoch 295/4000, Step 63, d_loss: 0.35615038871765137, g_loss: 4.926671028137207\n","Epoch 295/4000, Step 64, d_loss: 0.34199953079223633, g_loss: 10.132645606994629\n","Epoch 295/4000, Step 65, d_loss: 0.33627331256866455, g_loss: 6.57039213180542\n","Epoch 295/4000, Step 66, d_loss: 0.34467166662216187, g_loss: 5.852479934692383\n","Epoch 295/4000, Step 67, d_loss: 0.3402678072452545, g_loss: 6.074601173400879\n","Epoch 295/4000, Step 68, d_loss: 0.34047824144363403, g_loss: 6.234352111816406\n","Epoch 295/4000, Step 69, d_loss: 0.3509235680103302, g_loss: 5.562604904174805\n","Epoch 295/4000, Step 70, d_loss: 0.3455811142921448, g_loss: 5.9423017501831055\n","Epoch 295/4000, Step 71, d_loss: 0.35057392716407776, g_loss: 7.245656967163086\n","Epoch 295/4000, Step 72, d_loss: 0.3382577896118164, g_loss: 8.284075736999512\n","Epoch 295/4000, Step 73, d_loss: 0.35414832830429077, g_loss: 4.304214954376221\n","Epoch 295/4000, Step 74, d_loss: 0.3634571135044098, g_loss: 4.397510528564453\n","Epoch 295/4000, Step 75, d_loss: 0.3573583960533142, g_loss: 5.287456035614014\n","Epoch 295/4000, Step 76, d_loss: 0.3582283854484558, g_loss: 6.175696849822998\n","Epoch 295/4000, Step 77, d_loss: 0.3361869156360626, g_loss: 6.332963943481445\n","Epoch 295/4000, Step 78, d_loss: 0.3500969707965851, g_loss: 6.642884254455566\n","Epoch 295/4000, Step 79, d_loss: 0.46940311789512634, g_loss: 5.997445106506348\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 296/4000, Step 1, d_loss: 0.35506629943847656, g_loss: 7.613839149475098\n","Epoch 296/4000, Step 2, d_loss: 0.38289880752563477, g_loss: 4.616513729095459\n","Epoch 296/4000, Step 3, d_loss: 0.385916531085968, g_loss: 7.446604251861572\n","Epoch 296/4000, Step 4, d_loss: 0.3682578504085541, g_loss: 6.200689315795898\n","Epoch 296/4000, Step 5, d_loss: 0.356669157743454, g_loss: 5.906064987182617\n","Epoch 296/4000, Step 6, d_loss: 0.34186962246894836, g_loss: 6.763603687286377\n","Epoch 296/4000, Step 7, d_loss: 0.3347098231315613, g_loss: 7.527562618255615\n","Epoch 296/4000, Step 8, d_loss: 0.4645969867706299, g_loss: 6.051790714263916\n","Epoch 296/4000, Step 9, d_loss: 0.4498814344406128, g_loss: 5.319530010223389\n","Epoch 296/4000, Step 10, d_loss: 0.36217403411865234, g_loss: 3.2047927379608154\n","Epoch 296/4000, Step 11, d_loss: 0.5170745253562927, g_loss: 2.512662172317505\n","Epoch 296/4000, Step 12, d_loss: 0.5730135440826416, g_loss: 3.883455753326416\n","Epoch 296/4000, Step 13, d_loss: 0.4681577980518341, g_loss: 4.526153564453125\n","Epoch 296/4000, Step 14, d_loss: 0.3779376447200775, g_loss: 5.812615871429443\n","Epoch 296/4000, Step 15, d_loss: 0.38683581352233887, g_loss: 5.950290679931641\n","Epoch 296/4000, Step 16, d_loss: 0.40243202447891235, g_loss: 6.242384433746338\n","Epoch 296/4000, Step 17, d_loss: 0.4007797837257385, g_loss: 5.878562927246094\n","Epoch 296/4000, Step 18, d_loss: 0.36455804109573364, g_loss: 4.985718727111816\n","Epoch 296/4000, Step 19, d_loss: 0.35957205295562744, g_loss: 5.730600833892822\n","Epoch 296/4000, Step 20, d_loss: 0.3544654846191406, g_loss: 4.128679275512695\n","Epoch 296/4000, Step 21, d_loss: 0.3642871379852295, g_loss: 4.1344194412231445\n","Epoch 296/4000, Step 22, d_loss: 0.37520039081573486, g_loss: 4.8874382972717285\n","Epoch 296/4000, Step 23, d_loss: 0.3701685070991516, g_loss: 3.787318468093872\n","Epoch 296/4000, Step 24, d_loss: 0.3509798049926758, g_loss: 4.23218297958374\n","Epoch 296/4000, Step 25, d_loss: 0.3514014780521393, g_loss: 4.204160690307617\n","Epoch 296/4000, Step 26, d_loss: 0.36675164103507996, g_loss: 6.468392372131348\n","Epoch 296/4000, Step 27, d_loss: 0.40709900856018066, g_loss: 4.164702892303467\n","Epoch 296/4000, Step 28, d_loss: 0.35384199023246765, g_loss: 7.270342826843262\n","Epoch 296/4000, Step 29, d_loss: 0.355354905128479, g_loss: 4.641665458679199\n","Epoch 296/4000, Step 30, d_loss: 0.3747059106826782, g_loss: 4.311982154846191\n","Epoch 296/4000, Step 31, d_loss: 0.3702407479286194, g_loss: 5.245518684387207\n","Epoch 296/4000, Step 32, d_loss: 0.3570095896720886, g_loss: 5.445178508758545\n","Epoch 296/4000, Step 33, d_loss: 0.3710308074951172, g_loss: 5.760256290435791\n","Epoch 296/4000, Step 34, d_loss: 0.3451153337955475, g_loss: 6.797839641571045\n","Epoch 296/4000, Step 35, d_loss: 0.33037179708480835, g_loss: 6.262258529663086\n","Epoch 296/4000, Step 36, d_loss: 0.34136515855789185, g_loss: 6.649988174438477\n","Epoch 296/4000, Step 37, d_loss: 0.3861359655857086, g_loss: 5.22279691696167\n","Epoch 296/4000, Step 38, d_loss: 0.3399856388568878, g_loss: 7.526350498199463\n","Epoch 296/4000, Step 39, d_loss: 0.330493688583374, g_loss: 6.771394729614258\n","Epoch 296/4000, Step 40, d_loss: 0.3588721752166748, g_loss: 5.386093616485596\n","Epoch 296/4000, Step 41, d_loss: 0.3392380475997925, g_loss: 8.430390357971191\n","Epoch 296/4000, Step 42, d_loss: 0.34133800864219666, g_loss: 5.368083953857422\n","Epoch 296/4000, Step 43, d_loss: 0.34251877665519714, g_loss: 5.19478702545166\n","Epoch 296/4000, Step 44, d_loss: 0.33948639035224915, g_loss: 6.078019142150879\n","Epoch 296/4000, Step 45, d_loss: 0.3481374979019165, g_loss: 5.390992641448975\n","Epoch 296/4000, Step 46, d_loss: 0.33815276622772217, g_loss: 5.678711891174316\n","Epoch 296/4000, Step 47, d_loss: 0.33835741877555847, g_loss: 4.46121883392334\n","Epoch 296/4000, Step 48, d_loss: 0.33815819025039673, g_loss: 6.294879913330078\n","Epoch 296/4000, Step 49, d_loss: 0.33653461933135986, g_loss: 5.983786582946777\n","Epoch 296/4000, Step 50, d_loss: 0.3344074785709381, g_loss: 5.793760299682617\n","Epoch 296/4000, Step 51, d_loss: 0.34285709261894226, g_loss: 6.133110523223877\n","Epoch 296/4000, Step 52, d_loss: 0.3339201509952545, g_loss: 6.019678592681885\n","Epoch 296/4000, Step 53, d_loss: 0.33499494194984436, g_loss: 7.000543117523193\n","Epoch 296/4000, Step 54, d_loss: 0.3298119306564331, g_loss: 6.568573951721191\n","Epoch 296/4000, Step 55, d_loss: 0.34180110692977905, g_loss: 6.491301536560059\n","Epoch 296/4000, Step 56, d_loss: 0.3361740708351135, g_loss: 6.879417419433594\n","Epoch 296/4000, Step 57, d_loss: 0.34189021587371826, g_loss: 8.490153312683105\n","Epoch 296/4000, Step 58, d_loss: 0.3328801095485687, g_loss: 6.123023986816406\n","Epoch 296/4000, Step 59, d_loss: 0.33315420150756836, g_loss: 6.091139793395996\n","Epoch 296/4000, Step 60, d_loss: 0.33606618642807007, g_loss: 5.8287458419799805\n","Epoch 296/4000, Step 61, d_loss: 0.3492550849914551, g_loss: 5.877612590789795\n","Epoch 296/4000, Step 62, d_loss: 0.3414191007614136, g_loss: 5.923613548278809\n","Epoch 296/4000, Step 63, d_loss: 0.3341984748840332, g_loss: 6.2417802810668945\n","Epoch 296/4000, Step 64, d_loss: 0.33730781078338623, g_loss: 6.182177543640137\n","Epoch 296/4000, Step 65, d_loss: 0.33839136362075806, g_loss: 5.895118713378906\n","Epoch 296/4000, Step 66, d_loss: 0.33659860491752625, g_loss: 6.677779197692871\n","Epoch 296/4000, Step 67, d_loss: 0.3571973443031311, g_loss: 5.589370250701904\n","Epoch 296/4000, Step 68, d_loss: 0.3372593820095062, g_loss: 7.436672687530518\n","Epoch 296/4000, Step 69, d_loss: 0.35334959626197815, g_loss: 6.588344097137451\n","Epoch 296/4000, Step 70, d_loss: 0.3318886160850525, g_loss: 6.90311861038208\n","Epoch 296/4000, Step 71, d_loss: 0.33122140169143677, g_loss: 6.335610389709473\n","Epoch 296/4000, Step 72, d_loss: 0.3341221511363983, g_loss: 5.872680187225342\n","Epoch 296/4000, Step 73, d_loss: 0.34163063764572144, g_loss: 5.782159328460693\n","Epoch 296/4000, Step 74, d_loss: 0.3802056312561035, g_loss: 5.484169960021973\n","Epoch 296/4000, Step 75, d_loss: 0.3375653922557831, g_loss: 5.3184967041015625\n","Epoch 296/4000, Step 76, d_loss: 0.34138166904449463, g_loss: 4.543323993682861\n","Epoch 296/4000, Step 77, d_loss: 0.3458603620529175, g_loss: 6.00509786605835\n","Epoch 296/4000, Step 78, d_loss: 0.36436954140663147, g_loss: 5.238076686859131\n","Epoch 296/4000, Step 79, d_loss: 0.39131274819374084, g_loss: 3.9575607776641846\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 297/4000, Step 1, d_loss: 0.35728710889816284, g_loss: 4.2909979820251465\n","Epoch 297/4000, Step 2, d_loss: 0.36547964811325073, g_loss: 5.271259307861328\n","Epoch 297/4000, Step 3, d_loss: 0.3461720645427704, g_loss: 4.747986316680908\n","Epoch 297/4000, Step 4, d_loss: 0.3450252413749695, g_loss: 5.314391613006592\n","Epoch 297/4000, Step 5, d_loss: 0.3346758484840393, g_loss: 6.354707717895508\n","Epoch 297/4000, Step 6, d_loss: 0.3516833484172821, g_loss: 5.945618152618408\n","Epoch 297/4000, Step 7, d_loss: 0.3670710325241089, g_loss: 6.061074256896973\n","Epoch 297/4000, Step 8, d_loss: 0.34761714935302734, g_loss: 7.988826751708984\n","Epoch 297/4000, Step 9, d_loss: 0.33656924962997437, g_loss: 4.2690558433532715\n","Epoch 297/4000, Step 10, d_loss: 0.34262385964393616, g_loss: 5.005118370056152\n","Epoch 297/4000, Step 11, d_loss: 0.34394025802612305, g_loss: 6.625432968139648\n","Epoch 297/4000, Step 12, d_loss: 0.33604881167411804, g_loss: 5.038567066192627\n","Epoch 297/4000, Step 13, d_loss: 0.33924204111099243, g_loss: 4.989964485168457\n","Epoch 297/4000, Step 14, d_loss: 0.3310790956020355, g_loss: 5.817151069641113\n","Epoch 297/4000, Step 15, d_loss: 0.328806608915329, g_loss: 6.412351131439209\n","Epoch 297/4000, Step 16, d_loss: 0.35749536752700806, g_loss: 5.785336494445801\n","Epoch 297/4000, Step 17, d_loss: 0.3488931357860565, g_loss: 6.88071346282959\n","Epoch 297/4000, Step 18, d_loss: 0.3381902873516083, g_loss: 5.039445400238037\n","Epoch 297/4000, Step 19, d_loss: 0.3389688730239868, g_loss: 4.914153575897217\n","Epoch 297/4000, Step 20, d_loss: 0.34255450963974, g_loss: 4.823544025421143\n","Epoch 297/4000, Step 21, d_loss: 0.35209283232688904, g_loss: 5.38651180267334\n","Epoch 297/4000, Step 22, d_loss: 0.3408471941947937, g_loss: 5.1042399406433105\n","Epoch 297/4000, Step 23, d_loss: 0.3490765392780304, g_loss: 6.123752117156982\n","Epoch 297/4000, Step 24, d_loss: 0.3313983380794525, g_loss: 5.40101432800293\n","Epoch 297/4000, Step 25, d_loss: 0.3379104435443878, g_loss: 6.480416297912598\n","Epoch 297/4000, Step 26, d_loss: 0.35246896743774414, g_loss: 5.803935527801514\n","Epoch 297/4000, Step 27, d_loss: 0.3343697488307953, g_loss: 8.968461036682129\n","Epoch 297/4000, Step 28, d_loss: 0.3329888880252838, g_loss: 5.509744167327881\n","Epoch 297/4000, Step 29, d_loss: 0.3320615291595459, g_loss: 5.41644287109375\n","Epoch 297/4000, Step 30, d_loss: 0.3405747711658478, g_loss: 5.2940354347229\n","Epoch 297/4000, Step 31, d_loss: 0.353026807308197, g_loss: 5.962976932525635\n","Epoch 297/4000, Step 32, d_loss: 0.34646865725517273, g_loss: 4.74833869934082\n","Epoch 297/4000, Step 33, d_loss: 0.34516090154647827, g_loss: 5.084076881408691\n","Epoch 297/4000, Step 34, d_loss: 0.34837406873703003, g_loss: 5.518310546875\n","Epoch 297/4000, Step 35, d_loss: 0.3377862274646759, g_loss: 5.481409549713135\n","Epoch 297/4000, Step 36, d_loss: 0.3420986831188202, g_loss: 6.171960830688477\n","Epoch 297/4000, Step 37, d_loss: 0.34341660141944885, g_loss: 5.600308895111084\n","Epoch 297/4000, Step 38, d_loss: 0.33433353900909424, g_loss: 6.054039478302002\n","Epoch 297/4000, Step 39, d_loss: 0.330852746963501, g_loss: 6.018789291381836\n","Epoch 297/4000, Step 40, d_loss: 0.341115802526474, g_loss: 6.246169090270996\n","Epoch 297/4000, Step 41, d_loss: 0.41503503918647766, g_loss: 5.631159782409668\n","Epoch 297/4000, Step 42, d_loss: 0.3404301702976227, g_loss: 4.721607208251953\n","Epoch 297/4000, Step 43, d_loss: 0.340230792760849, g_loss: 4.47601842880249\n","Epoch 297/4000, Step 44, d_loss: 0.37493881583213806, g_loss: 9.03609848022461\n","Epoch 297/4000, Step 45, d_loss: 0.38388457894325256, g_loss: 5.164900779724121\n","Epoch 297/4000, Step 46, d_loss: 0.3707479238510132, g_loss: 5.078753471374512\n","Epoch 297/4000, Step 47, d_loss: 0.3641194999217987, g_loss: 5.539154529571533\n","Epoch 297/4000, Step 48, d_loss: 0.34603649377822876, g_loss: 5.5000691413879395\n","Epoch 297/4000, Step 49, d_loss: 0.3305729925632477, g_loss: 6.048636436462402\n","Epoch 297/4000, Step 50, d_loss: 0.3532353937625885, g_loss: 6.243589878082275\n","Epoch 297/4000, Step 51, d_loss: 0.3457475006580353, g_loss: 5.9586687088012695\n","Epoch 297/4000, Step 52, d_loss: 0.34795141220092773, g_loss: 7.716732501983643\n","Epoch 297/4000, Step 53, d_loss: 0.33828791975975037, g_loss: 5.846558094024658\n","Epoch 297/4000, Step 54, d_loss: 0.3373079001903534, g_loss: 5.5697102546691895\n","Epoch 297/4000, Step 55, d_loss: 0.3369137644767761, g_loss: 5.366276741027832\n","Epoch 297/4000, Step 56, d_loss: 0.34284690022468567, g_loss: 5.4691972732543945\n","Epoch 297/4000, Step 57, d_loss: 0.34089070558547974, g_loss: 5.862480163574219\n","Epoch 297/4000, Step 58, d_loss: 0.34250882267951965, g_loss: 6.124234199523926\n","Epoch 297/4000, Step 59, d_loss: 0.3407815396785736, g_loss: 5.96533727645874\n","Epoch 297/4000, Step 60, d_loss: 0.33831846714019775, g_loss: 5.378085136413574\n","Epoch 297/4000, Step 61, d_loss: 0.33583158254623413, g_loss: 6.177120685577393\n","Epoch 297/4000, Step 62, d_loss: 0.35356658697128296, g_loss: 5.102892875671387\n","Epoch 297/4000, Step 63, d_loss: 0.34800657629966736, g_loss: 5.883800029754639\n","Epoch 297/4000, Step 64, d_loss: 0.34092357754707336, g_loss: 6.009649276733398\n","Epoch 297/4000, Step 65, d_loss: 0.3519957959651947, g_loss: 5.842174530029297\n","Epoch 297/4000, Step 66, d_loss: 0.33955445885658264, g_loss: 5.345091819763184\n","Epoch 297/4000, Step 67, d_loss: 0.34430932998657227, g_loss: 6.27445650100708\n","Epoch 297/4000, Step 68, d_loss: 0.3357195258140564, g_loss: 5.990009784698486\n","Epoch 297/4000, Step 69, d_loss: 0.3516654074192047, g_loss: 7.0356903076171875\n","Epoch 297/4000, Step 70, d_loss: 0.338174045085907, g_loss: 5.691824436187744\n","Epoch 297/4000, Step 71, d_loss: 0.33529722690582275, g_loss: 5.858692646026611\n","Epoch 297/4000, Step 72, d_loss: 0.34156563878059387, g_loss: 6.291292190551758\n","Epoch 297/4000, Step 73, d_loss: 0.3383500874042511, g_loss: 7.900601387023926\n","Epoch 297/4000, Step 74, d_loss: 0.34237584471702576, g_loss: 5.216463088989258\n","Epoch 297/4000, Step 75, d_loss: 0.3378986120223999, g_loss: 5.917659759521484\n","Epoch 297/4000, Step 76, d_loss: 0.3378816246986389, g_loss: 6.210086345672607\n","Epoch 297/4000, Step 77, d_loss: 0.3443290889263153, g_loss: 5.411545753479004\n","Epoch 297/4000, Step 78, d_loss: 0.3386605381965637, g_loss: 5.550182819366455\n","Epoch 297/4000, Step 79, d_loss: 1.6447397470474243, g_loss: 4.36679744720459\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 298/4000, Step 1, d_loss: 0.4804890751838684, g_loss: 1.8736727237701416\n","Epoch 298/4000, Step 2, d_loss: 0.6057860851287842, g_loss: 1.9956424236297607\n","Epoch 298/4000, Step 3, d_loss: 0.7134450674057007, g_loss: 4.112838268280029\n","Epoch 298/4000, Step 4, d_loss: 0.5420869588851929, g_loss: 5.1182732582092285\n","Epoch 298/4000, Step 5, d_loss: 0.5021427869796753, g_loss: 5.252501487731934\n","Epoch 298/4000, Step 6, d_loss: 0.5349534153938293, g_loss: 5.430665493011475\n","Epoch 298/4000, Step 7, d_loss: 0.9202399253845215, g_loss: 5.317543029785156\n","Epoch 298/4000, Step 8, d_loss: 0.4920644462108612, g_loss: 3.1597537994384766\n","Epoch 298/4000, Step 9, d_loss: 0.47036808729171753, g_loss: 3.3550221920013428\n","Epoch 298/4000, Step 10, d_loss: 0.5633471012115479, g_loss: 2.828521966934204\n","Epoch 298/4000, Step 11, d_loss: 0.47796210646629333, g_loss: 2.942878007888794\n","Epoch 298/4000, Step 12, d_loss: 1.1054431200027466, g_loss: 4.532373905181885\n","Epoch 298/4000, Step 13, d_loss: 0.4955178201198578, g_loss: 3.6983964443206787\n","Epoch 298/4000, Step 14, d_loss: 0.6543195247650146, g_loss: 5.444724082946777\n","Epoch 298/4000, Step 15, d_loss: 0.4376404583454132, g_loss: 6.479041576385498\n","Epoch 298/4000, Step 16, d_loss: 0.47790223360061646, g_loss: 8.008171081542969\n","Epoch 298/4000, Step 17, d_loss: 0.3754623234272003, g_loss: 7.385116100311279\n","Epoch 298/4000, Step 18, d_loss: 0.4369121491909027, g_loss: 7.065916538238525\n","Epoch 298/4000, Step 19, d_loss: 0.35569706559181213, g_loss: 8.197870254516602\n","Epoch 298/4000, Step 20, d_loss: 0.3701133728027344, g_loss: 5.726825714111328\n","Epoch 298/4000, Step 21, d_loss: 0.4662645757198334, g_loss: 4.525359630584717\n","Epoch 298/4000, Step 22, d_loss: 0.4725419878959656, g_loss: 3.220344305038452\n","Epoch 298/4000, Step 23, d_loss: 0.43017077445983887, g_loss: 6.2467241287231445\n","Epoch 298/4000, Step 24, d_loss: 0.45878952741622925, g_loss: 7.494350433349609\n","Epoch 298/4000, Step 25, d_loss: 0.4473542273044586, g_loss: 6.359227180480957\n","Epoch 298/4000, Step 26, d_loss: 0.45337432622909546, g_loss: 1.803640604019165\n","Epoch 298/4000, Step 27, d_loss: 0.5072355270385742, g_loss: 5.636262893676758\n","Epoch 298/4000, Step 28, d_loss: 0.40470343828201294, g_loss: 5.026725769042969\n","Epoch 298/4000, Step 29, d_loss: 0.3599884808063507, g_loss: 5.760303974151611\n","Epoch 298/4000, Step 30, d_loss: 0.38440534472465515, g_loss: 7.622762203216553\n","Epoch 298/4000, Step 31, d_loss: 0.44437575340270996, g_loss: 4.853737831115723\n","Epoch 298/4000, Step 32, d_loss: 0.4729785919189453, g_loss: 4.845629692077637\n","Epoch 298/4000, Step 33, d_loss: 0.3630203902721405, g_loss: 1.7146878242492676\n","Epoch 298/4000, Step 34, d_loss: 0.6164649128913879, g_loss: 4.774261951446533\n","Epoch 298/4000, Step 35, d_loss: 0.4244668185710907, g_loss: 7.017348289489746\n","Epoch 298/4000, Step 36, d_loss: 0.4033074975013733, g_loss: 5.410112380981445\n","Epoch 298/4000, Step 37, d_loss: 0.3835728168487549, g_loss: 7.153390407562256\n","Epoch 298/4000, Step 38, d_loss: 0.396702378988266, g_loss: 6.122035980224609\n","Epoch 298/4000, Step 39, d_loss: 0.35731324553489685, g_loss: 7.144087791442871\n","Epoch 298/4000, Step 40, d_loss: 0.37774375081062317, g_loss: 7.383772850036621\n","Epoch 298/4000, Step 41, d_loss: 0.3469504714012146, g_loss: 5.449612140655518\n","Epoch 298/4000, Step 42, d_loss: 0.3751305341720581, g_loss: 6.348780632019043\n","Epoch 298/4000, Step 43, d_loss: 0.38157057762145996, g_loss: 7.526433944702148\n","Epoch 298/4000, Step 44, d_loss: 0.3853003978729248, g_loss: 6.934149265289307\n","Epoch 298/4000, Step 45, d_loss: 0.3455590009689331, g_loss: 6.850420951843262\n","Epoch 298/4000, Step 46, d_loss: 0.3522527813911438, g_loss: 7.56207275390625\n","Epoch 298/4000, Step 47, d_loss: 0.3556521534919739, g_loss: 6.935169696807861\n","Epoch 298/4000, Step 48, d_loss: 0.3500632345676422, g_loss: 7.9335198402404785\n","Epoch 298/4000, Step 49, d_loss: 0.34711509943008423, g_loss: 5.900198936462402\n","Epoch 298/4000, Step 50, d_loss: 0.3699682354927063, g_loss: 6.739164352416992\n","Epoch 298/4000, Step 51, d_loss: 0.38095608353614807, g_loss: 6.853912830352783\n","Epoch 298/4000, Step 52, d_loss: 0.3467402756214142, g_loss: 6.844548225402832\n","Epoch 298/4000, Step 53, d_loss: 0.3632466793060303, g_loss: 5.041433811187744\n","Epoch 298/4000, Step 54, d_loss: 0.3486427664756775, g_loss: 7.872992992401123\n","Epoch 298/4000, Step 55, d_loss: 0.34299376606941223, g_loss: 6.091050148010254\n","Epoch 298/4000, Step 56, d_loss: 0.3393225371837616, g_loss: 6.57045316696167\n","Epoch 298/4000, Step 57, d_loss: 0.3431512117385864, g_loss: 6.867712020874023\n","Epoch 298/4000, Step 58, d_loss: 0.3601429760456085, g_loss: 5.394900321960449\n","Epoch 298/4000, Step 59, d_loss: 0.3441011309623718, g_loss: 6.006678104400635\n","Epoch 298/4000, Step 60, d_loss: 0.33737820386886597, g_loss: 6.6516947746276855\n","Epoch 298/4000, Step 61, d_loss: 0.37012824416160583, g_loss: 7.019365310668945\n","Epoch 298/4000, Step 62, d_loss: 0.3483429253101349, g_loss: 5.204026222229004\n","Epoch 298/4000, Step 63, d_loss: 0.38724011182785034, g_loss: 6.664968967437744\n","Epoch 298/4000, Step 64, d_loss: 0.3521955907344818, g_loss: 7.482454776763916\n","Epoch 298/4000, Step 65, d_loss: 0.3530178964138031, g_loss: 5.937922954559326\n","Epoch 298/4000, Step 66, d_loss: 0.35591426491737366, g_loss: 3.54284930229187\n","Epoch 298/4000, Step 67, d_loss: 0.3542238473892212, g_loss: 5.490410327911377\n","Epoch 298/4000, Step 68, d_loss: 0.3334975838661194, g_loss: 6.336368560791016\n","Epoch 298/4000, Step 69, d_loss: 0.35040855407714844, g_loss: 4.513672828674316\n","Epoch 298/4000, Step 70, d_loss: 0.3583869934082031, g_loss: 5.5664191246032715\n","Epoch 298/4000, Step 71, d_loss: 0.3414013981819153, g_loss: 4.128815650939941\n","Epoch 298/4000, Step 72, d_loss: 0.3379267156124115, g_loss: 5.428425312042236\n","Epoch 298/4000, Step 73, d_loss: 0.33711230754852295, g_loss: 5.877471446990967\n","Epoch 298/4000, Step 74, d_loss: 0.3580421209335327, g_loss: 3.2394487857818604\n","Epoch 298/4000, Step 75, d_loss: 0.34975093603134155, g_loss: 5.072723865509033\n","Epoch 298/4000, Step 76, d_loss: 0.34200820326805115, g_loss: 4.795664310455322\n","Epoch 298/4000, Step 77, d_loss: 0.4160080552101135, g_loss: 3.3406362533569336\n","Epoch 298/4000, Step 78, d_loss: 0.37723615765571594, g_loss: 5.62044095993042\n","Epoch 298/4000, Step 79, d_loss: 0.39837446808815, g_loss: 3.9461112022399902\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 299/4000, Step 1, d_loss: 0.3494323790073395, g_loss: 6.2499847412109375\n","Epoch 299/4000, Step 2, d_loss: 0.3439549505710602, g_loss: 4.068346977233887\n","Epoch 299/4000, Step 3, d_loss: 0.36357468366622925, g_loss: 6.192168235778809\n","Epoch 299/4000, Step 4, d_loss: 0.4029152989387512, g_loss: 4.372509002685547\n","Epoch 299/4000, Step 5, d_loss: 0.3840208053588867, g_loss: 6.624751091003418\n","Epoch 299/4000, Step 6, d_loss: 0.33667048811912537, g_loss: 3.7259504795074463\n","Epoch 299/4000, Step 7, d_loss: 0.370357871055603, g_loss: 3.7789721488952637\n","Epoch 299/4000, Step 8, d_loss: 0.3456897437572479, g_loss: 3.513333320617676\n","Epoch 299/4000, Step 9, d_loss: 0.36874401569366455, g_loss: 3.168121576309204\n","Epoch 299/4000, Step 10, d_loss: 0.3772777318954468, g_loss: 3.190835952758789\n","Epoch 299/4000, Step 11, d_loss: 0.49799293279647827, g_loss: 5.531209945678711\n","Epoch 299/4000, Step 12, d_loss: 0.5234006643295288, g_loss: 6.353317737579346\n","Epoch 299/4000, Step 13, d_loss: 0.5452358722686768, g_loss: 4.9856977462768555\n","Epoch 299/4000, Step 14, d_loss: 0.740053653717041, g_loss: 3.4635276794433594\n","Epoch 299/4000, Step 15, d_loss: 1.2288504838943481, g_loss: 3.3667731285095215\n","Epoch 299/4000, Step 16, d_loss: 0.974657416343689, g_loss: 4.026367664337158\n","Epoch 299/4000, Step 17, d_loss: 0.673831582069397, g_loss: 2.4316883087158203\n","Epoch 299/4000, Step 18, d_loss: 0.6267930269241333, g_loss: 2.5708000659942627\n","Epoch 299/4000, Step 19, d_loss: 0.4293544888496399, g_loss: 6.4292378425598145\n","Epoch 299/4000, Step 20, d_loss: 0.5832111239433289, g_loss: 6.331267356872559\n","Epoch 299/4000, Step 21, d_loss: 0.5526145100593567, g_loss: 6.316431999206543\n","Epoch 299/4000, Step 22, d_loss: 0.5538305044174194, g_loss: 8.005548477172852\n","Epoch 299/4000, Step 23, d_loss: 0.457290917634964, g_loss: 7.46121883392334\n","Epoch 299/4000, Step 24, d_loss: 0.45616614818573, g_loss: 6.018599987030029\n","Epoch 299/4000, Step 25, d_loss: 0.520990252494812, g_loss: 9.930882453918457\n","Epoch 299/4000, Step 26, d_loss: 0.5215780138969421, g_loss: 8.400588989257812\n","Epoch 299/4000, Step 27, d_loss: 0.6231521964073181, g_loss: 9.167930603027344\n","Epoch 299/4000, Step 28, d_loss: 0.545800507068634, g_loss: 9.07201862335205\n","Epoch 299/4000, Step 29, d_loss: 0.4454270899295807, g_loss: 9.269634246826172\n","Epoch 299/4000, Step 30, d_loss: 0.39639464020729065, g_loss: 7.9994916915893555\n","Epoch 299/4000, Step 31, d_loss: 0.444232314825058, g_loss: 6.276991367340088\n","Epoch 299/4000, Step 32, d_loss: 0.41440293192863464, g_loss: 5.7709245681762695\n","Epoch 299/4000, Step 33, d_loss: 0.5196022987365723, g_loss: 3.3454134464263916\n","Epoch 299/4000, Step 34, d_loss: 0.45246613025665283, g_loss: 2.0452914237976074\n","Epoch 299/4000, Step 35, d_loss: 1.3087711334228516, g_loss: 6.907259464263916\n","Epoch 299/4000, Step 36, d_loss: 0.7078972458839417, g_loss: 9.027755737304688\n","Epoch 299/4000, Step 37, d_loss: 1.180387020111084, g_loss: 9.760979652404785\n","Epoch 299/4000, Step 38, d_loss: 0.4907945990562439, g_loss: 5.530491352081299\n","Epoch 299/4000, Step 39, d_loss: 0.4423525333404541, g_loss: 3.555687427520752\n","Epoch 299/4000, Step 40, d_loss: 0.506301999092102, g_loss: 2.8069071769714355\n","Epoch 299/4000, Step 41, d_loss: 0.6301584243774414, g_loss: 2.9161763191223145\n","Epoch 299/4000, Step 42, d_loss: 0.7400294542312622, g_loss: 3.7080559730529785\n","Epoch 299/4000, Step 43, d_loss: 0.600544810295105, g_loss: 5.933473587036133\n","Epoch 299/4000, Step 44, d_loss: 0.6388764977455139, g_loss: 7.667809009552002\n","Epoch 299/4000, Step 45, d_loss: 0.5727903246879578, g_loss: 8.591325759887695\n","Epoch 299/4000, Step 46, d_loss: 0.45419272780418396, g_loss: 10.042268753051758\n","Epoch 299/4000, Step 47, d_loss: 0.35684099793434143, g_loss: 12.280402183532715\n","Epoch 299/4000, Step 48, d_loss: 0.39514315128326416, g_loss: 12.734009742736816\n","Epoch 299/4000, Step 49, d_loss: 0.3788038492202759, g_loss: 12.467538833618164\n","Epoch 299/4000, Step 50, d_loss: 0.5282804369926453, g_loss: 11.114739418029785\n","Epoch 299/4000, Step 51, d_loss: 0.3936655521392822, g_loss: 10.834287643432617\n","Epoch 299/4000, Step 52, d_loss: 0.4019683599472046, g_loss: 10.134546279907227\n","Epoch 299/4000, Step 53, d_loss: 0.41977041959762573, g_loss: 11.2344331741333\n","Epoch 299/4000, Step 54, d_loss: 0.4286884367465973, g_loss: 11.023774147033691\n","Epoch 299/4000, Step 55, d_loss: 0.4021114706993103, g_loss: 9.924732208251953\n","Epoch 299/4000, Step 56, d_loss: 0.4063739478588104, g_loss: 11.198744773864746\n","Epoch 299/4000, Step 57, d_loss: 0.43218693137168884, g_loss: 11.824219703674316\n","Epoch 299/4000, Step 58, d_loss: 0.39269891381263733, g_loss: 11.560588836669922\n","Epoch 299/4000, Step 59, d_loss: 0.34977468848228455, g_loss: 11.245667457580566\n","Epoch 299/4000, Step 60, d_loss: 0.3510676622390747, g_loss: 10.42315673828125\n","Epoch 299/4000, Step 61, d_loss: 0.357754111289978, g_loss: 10.932432174682617\n","Epoch 299/4000, Step 62, d_loss: 0.36107107996940613, g_loss: 11.014615058898926\n","Epoch 299/4000, Step 63, d_loss: 0.3613966107368469, g_loss: 11.267060279846191\n","Epoch 299/4000, Step 64, d_loss: 0.35622894763946533, g_loss: 10.5157470703125\n","Epoch 299/4000, Step 65, d_loss: 0.42405280470848083, g_loss: 9.76915454864502\n","Epoch 299/4000, Step 66, d_loss: 0.3538525402545929, g_loss: 9.930243492126465\n","Epoch 299/4000, Step 67, d_loss: 0.39123010635375977, g_loss: 7.406558036804199\n","Epoch 299/4000, Step 68, d_loss: 0.36104312539100647, g_loss: 7.667919158935547\n","Epoch 299/4000, Step 69, d_loss: 0.3492615222930908, g_loss: 8.321718215942383\n","Epoch 299/4000, Step 70, d_loss: 0.3678284287452698, g_loss: 7.707993507385254\n","Epoch 299/4000, Step 71, d_loss: 0.36767396330833435, g_loss: 7.649351119995117\n","Epoch 299/4000, Step 72, d_loss: 0.35109052062034607, g_loss: 8.538925170898438\n","Epoch 299/4000, Step 73, d_loss: 0.3698931038379669, g_loss: 7.772429943084717\n","Epoch 299/4000, Step 74, d_loss: 0.3690528869628906, g_loss: 7.624597549438477\n","Epoch 299/4000, Step 75, d_loss: 0.441080242395401, g_loss: 6.799434185028076\n","Epoch 299/4000, Step 76, d_loss: 0.3536781966686249, g_loss: 7.224703311920166\n","Epoch 299/4000, Step 77, d_loss: 0.35310453176498413, g_loss: 5.72658109664917\n","Epoch 299/4000, Step 78, d_loss: 0.3916480243206024, g_loss: 6.912477493286133\n","Epoch 299/4000, Step 79, d_loss: 0.3884485960006714, g_loss: 5.0796356201171875\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 300/4000, Step 1, d_loss: 0.43798181414604187, g_loss: 6.55012321472168\n","Epoch 300/4000, Step 2, d_loss: 0.37767162919044495, g_loss: 4.429758071899414\n","Epoch 300/4000, Step 3, d_loss: 0.3608110547065735, g_loss: 5.411055564880371\n","Epoch 300/4000, Step 4, d_loss: 0.3578289747238159, g_loss: 6.234421253204346\n","Epoch 300/4000, Step 5, d_loss: 0.3668709099292755, g_loss: 5.203116416931152\n","Epoch 300/4000, Step 6, d_loss: 0.42555147409439087, g_loss: 5.582027435302734\n","Epoch 300/4000, Step 7, d_loss: 0.3563981056213379, g_loss: 6.203791618347168\n","Epoch 300/4000, Step 8, d_loss: 0.36126118898391724, g_loss: 5.445758819580078\n","Epoch 300/4000, Step 9, d_loss: 0.34919798374176025, g_loss: 5.862813949584961\n","Epoch 300/4000, Step 10, d_loss: 0.3815292716026306, g_loss: 4.14061975479126\n","Epoch 300/4000, Step 11, d_loss: 0.35348081588745117, g_loss: 4.428877353668213\n","Epoch 300/4000, Step 12, d_loss: 0.37710610032081604, g_loss: 5.450671672821045\n","Epoch 300/4000, Step 13, d_loss: 0.35614001750946045, g_loss: 5.314218521118164\n","Epoch 300/4000, Step 14, d_loss: 0.3633039891719818, g_loss: 5.050240993499756\n","Epoch 300/4000, Step 15, d_loss: 0.3411741256713867, g_loss: 4.90593147277832\n","Epoch 300/4000, Step 16, d_loss: 0.3487299084663391, g_loss: 4.2902512550354\n","Epoch 300/4000, Step 17, d_loss: 0.372688353061676, g_loss: 4.064713001251221\n","Epoch 300/4000, Step 18, d_loss: 0.3879053592681885, g_loss: 3.5852527618408203\n","Epoch 300/4000, Step 19, d_loss: 0.40438827872276306, g_loss: 4.783912181854248\n","Epoch 300/4000, Step 20, d_loss: 0.42517513036727905, g_loss: 6.256082534790039\n","Epoch 300/4000, Step 21, d_loss: 0.36183106899261475, g_loss: 7.111905097961426\n","Epoch 300/4000, Step 22, d_loss: 0.3576783537864685, g_loss: 8.142901420593262\n","Epoch 300/4000, Step 23, d_loss: 0.3416079878807068, g_loss: 7.179244518280029\n","Epoch 300/4000, Step 24, d_loss: 0.33889293670654297, g_loss: 6.212805271148682\n","Epoch 300/4000, Step 25, d_loss: 0.37526005506515503, g_loss: 5.575927257537842\n","Epoch 300/4000, Step 26, d_loss: 0.35405734181404114, g_loss: 5.572973728179932\n","Epoch 300/4000, Step 27, d_loss: 0.356181263923645, g_loss: 6.745987415313721\n","Epoch 300/4000, Step 28, d_loss: 0.3581910729408264, g_loss: 5.214354991912842\n","Epoch 300/4000, Step 29, d_loss: 0.3426136076450348, g_loss: 7.148982524871826\n","Epoch 300/4000, Step 30, d_loss: 0.3553939461708069, g_loss: 5.010092258453369\n","Epoch 300/4000, Step 31, d_loss: 0.3956439793109894, g_loss: 6.17274808883667\n","Epoch 300/4000, Step 32, d_loss: 0.3556685745716095, g_loss: 6.668593406677246\n","Epoch 300/4000, Step 33, d_loss: 0.34570175409317017, g_loss: 6.322970390319824\n","Epoch 300/4000, Step 34, d_loss: 0.3647001087665558, g_loss: 6.0547027587890625\n","Epoch 300/4000, Step 35, d_loss: 0.34546753764152527, g_loss: 5.97356653213501\n","Epoch 300/4000, Step 36, d_loss: 0.3441680073738098, g_loss: 5.398931980133057\n","Epoch 300/4000, Step 37, d_loss: 0.34082406759262085, g_loss: 5.838109970092773\n","Epoch 300/4000, Step 38, d_loss: 0.35451388359069824, g_loss: 5.13626766204834\n","Epoch 300/4000, Step 39, d_loss: 0.3640877902507782, g_loss: 6.648080825805664\n","Epoch 300/4000, Step 40, d_loss: 0.3546936511993408, g_loss: 7.708264350891113\n","Epoch 300/4000, Step 41, d_loss: 0.3692193925380707, g_loss: 6.144261837005615\n","Epoch 300/4000, Step 42, d_loss: 0.3403204679489136, g_loss: 4.966795444488525\n","Epoch 300/4000, Step 43, d_loss: 0.3414875268936157, g_loss: 6.205836296081543\n","Epoch 300/4000, Step 44, d_loss: 0.34933000802993774, g_loss: 6.673557758331299\n","Epoch 300/4000, Step 45, d_loss: 0.35864561796188354, g_loss: 6.365922451019287\n","Epoch 300/4000, Step 46, d_loss: 0.3353561460971832, g_loss: 5.632893085479736\n","Epoch 300/4000, Step 47, d_loss: 0.3400351107120514, g_loss: 5.765443801879883\n","Epoch 300/4000, Step 48, d_loss: 0.3413473069667816, g_loss: 4.62868595123291\n","Epoch 300/4000, Step 49, d_loss: 0.3425964117050171, g_loss: 7.229809761047363\n","Epoch 300/4000, Step 50, d_loss: 0.3506779074668884, g_loss: 4.998857498168945\n","Epoch 300/4000, Step 51, d_loss: 0.3548978269100189, g_loss: 5.827515602111816\n","Epoch 300/4000, Step 52, d_loss: 0.34462177753448486, g_loss: 5.172123908996582\n","Epoch 300/4000, Step 53, d_loss: 0.3567716181278229, g_loss: 4.317813396453857\n","Epoch 300/4000, Step 54, d_loss: 0.3401414453983307, g_loss: 6.957006931304932\n","Epoch 300/4000, Step 55, d_loss: 0.3527299165725708, g_loss: 5.093685150146484\n","Epoch 300/4000, Step 56, d_loss: 0.34510210156440735, g_loss: 5.3530168533325195\n","Epoch 300/4000, Step 57, d_loss: 0.35866039991378784, g_loss: 5.775369167327881\n","Epoch 300/4000, Step 58, d_loss: 0.3413318693637848, g_loss: 4.926792621612549\n","Epoch 300/4000, Step 59, d_loss: 0.33936789631843567, g_loss: 5.217807769775391\n","Epoch 300/4000, Step 60, d_loss: 0.33776384592056274, g_loss: 4.882433891296387\n","Epoch 300/4000, Step 61, d_loss: 0.3569534718990326, g_loss: 4.277973651885986\n","Epoch 300/4000, Step 62, d_loss: 0.3729722499847412, g_loss: 4.728603363037109\n","Epoch 300/4000, Step 63, d_loss: 0.3621133863925934, g_loss: 3.9319748878479004\n","Epoch 300/4000, Step 64, d_loss: 0.36221036314964294, g_loss: 5.180790424346924\n","Epoch 300/4000, Step 65, d_loss: 0.34808099269866943, g_loss: 6.102743148803711\n","Epoch 300/4000, Step 66, d_loss: 0.3590863347053528, g_loss: 5.682493686676025\n","Epoch 300/4000, Step 67, d_loss: 0.3714979887008667, g_loss: 5.173220634460449\n","Epoch 300/4000, Step 68, d_loss: 0.4639213979244232, g_loss: 5.652541160583496\n","Epoch 300/4000, Step 69, d_loss: 0.35793495178222656, g_loss: 2.4752261638641357\n","Epoch 300/4000, Step 70, d_loss: 0.39718097448349, g_loss: 3.2867891788482666\n","Epoch 300/4000, Step 71, d_loss: 0.40677911043167114, g_loss: 2.8802294731140137\n","Epoch 300/4000, Step 72, d_loss: 0.41269925236701965, g_loss: 3.3158316612243652\n","Epoch 300/4000, Step 73, d_loss: 0.40637850761413574, g_loss: 6.960615634918213\n","Epoch 300/4000, Step 74, d_loss: 0.37753817439079285, g_loss: 6.3342509269714355\n","Epoch 300/4000, Step 75, d_loss: 0.37559229135513306, g_loss: 5.419671535491943\n","Epoch 300/4000, Step 76, d_loss: 0.3850296437740326, g_loss: 8.407782554626465\n","Epoch 300/4000, Step 77, d_loss: 0.3912324905395508, g_loss: 5.103819847106934\n","Epoch 300/4000, Step 78, d_loss: 0.41935500502586365, g_loss: 3.3269810676574707\n","Epoch 300/4000, Step 79, d_loss: 1.907822608947754, g_loss: 2.997252941131592\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 301/4000, Step 1, d_loss: 0.4829389452934265, g_loss: 2.3838086128234863\n","Epoch 301/4000, Step 2, d_loss: 0.7848593592643738, g_loss: 1.3885136842727661\n","Epoch 301/4000, Step 3, d_loss: 0.9098302125930786, g_loss: 2.236107349395752\n","Epoch 301/4000, Step 4, d_loss: 0.7034302949905396, g_loss: 5.682908535003662\n","Epoch 301/4000, Step 5, d_loss: 0.5942438840866089, g_loss: 5.461657524108887\n","Epoch 301/4000, Step 6, d_loss: 0.719054639339447, g_loss: 6.372694969177246\n","Epoch 301/4000, Step 7, d_loss: 0.6857913136482239, g_loss: 3.9024031162261963\n","Epoch 301/4000, Step 8, d_loss: 0.5291967988014221, g_loss: 2.30513072013855\n","Epoch 301/4000, Step 9, d_loss: 0.4962044954299927, g_loss: 3.9951047897338867\n","Epoch 301/4000, Step 10, d_loss: 0.6584778428077698, g_loss: 2.0015883445739746\n","Epoch 301/4000, Step 11, d_loss: 0.5510803461074829, g_loss: 2.5560312271118164\n","Epoch 301/4000, Step 12, d_loss: 0.9201751351356506, g_loss: 3.9177844524383545\n","Epoch 301/4000, Step 13, d_loss: 0.5126037001609802, g_loss: 5.38123083114624\n","Epoch 301/4000, Step 14, d_loss: 0.6438151001930237, g_loss: 2.779127597808838\n","Epoch 301/4000, Step 15, d_loss: 0.6484450697898865, g_loss: 0.4037678837776184\n","Epoch 301/4000, Step 16, d_loss: 0.41455960273742676, g_loss: 0.1796017289161682\n","Epoch 301/4000, Step 17, d_loss: 1.5575125217437744, g_loss: 5.392210483551025\n","Epoch 301/4000, Step 18, d_loss: 0.4014338552951813, g_loss: 4.788104057312012\n","Epoch 301/4000, Step 19, d_loss: 0.5526570081710815, g_loss: 5.681870937347412\n","Epoch 301/4000, Step 20, d_loss: 0.6382845640182495, g_loss: 5.970330715179443\n","Epoch 301/4000, Step 21, d_loss: 0.6645849943161011, g_loss: 2.260019302368164\n","Epoch 301/4000, Step 22, d_loss: 0.40866225957870483, g_loss: 4.585917949676514\n","Epoch 301/4000, Step 23, d_loss: 0.4454958140850067, g_loss: 1.3522436618804932\n","Epoch 301/4000, Step 24, d_loss: 0.47148630023002625, g_loss: 4.063838481903076\n","Epoch 301/4000, Step 25, d_loss: 0.43921422958374023, g_loss: 3.410374402999878\n","Epoch 301/4000, Step 26, d_loss: 0.44565320014953613, g_loss: 4.604604244232178\n","Epoch 301/4000, Step 27, d_loss: 0.4174394905567169, g_loss: 7.64871883392334\n","Epoch 301/4000, Step 28, d_loss: 0.42579537630081177, g_loss: 5.411896705627441\n","Epoch 301/4000, Step 29, d_loss: 0.4712742567062378, g_loss: 6.305952072143555\n","Epoch 301/4000, Step 30, d_loss: 0.3972902297973633, g_loss: 6.85129451751709\n","Epoch 301/4000, Step 31, d_loss: 0.35945799946784973, g_loss: 5.807229042053223\n","Epoch 301/4000, Step 32, d_loss: 0.4013626277446747, g_loss: 4.7853546142578125\n","Epoch 301/4000, Step 33, d_loss: 0.41130945086479187, g_loss: 6.600410461425781\n","Epoch 301/4000, Step 34, d_loss: 0.4123329818248749, g_loss: 6.845032215118408\n","Epoch 301/4000, Step 35, d_loss: 0.4149107038974762, g_loss: 6.72555685043335\n","Epoch 301/4000, Step 36, d_loss: 0.43382108211517334, g_loss: 7.4379096031188965\n","Epoch 301/4000, Step 37, d_loss: 0.39075809717178345, g_loss: 4.35824728012085\n","Epoch 301/4000, Step 38, d_loss: 0.4287354350090027, g_loss: 4.632863521575928\n","Epoch 301/4000, Step 39, d_loss: 0.3894289433956146, g_loss: 6.639608860015869\n","Epoch 301/4000, Step 40, d_loss: 0.3921471834182739, g_loss: 4.480929851531982\n","Epoch 301/4000, Step 41, d_loss: 0.36889415979385376, g_loss: 4.969803333282471\n","Epoch 301/4000, Step 42, d_loss: 0.3881695568561554, g_loss: 3.8610661029815674\n","Epoch 301/4000, Step 43, d_loss: 0.423870325088501, g_loss: 4.231905937194824\n","Epoch 301/4000, Step 44, d_loss: 0.3952024579048157, g_loss: 5.242051124572754\n","Epoch 301/4000, Step 45, d_loss: 0.35496729612350464, g_loss: 4.381237983703613\n","Epoch 301/4000, Step 46, d_loss: 0.35355278849601746, g_loss: 6.559619903564453\n","Epoch 301/4000, Step 47, d_loss: 0.3672639727592468, g_loss: 4.53262186050415\n","Epoch 301/4000, Step 48, d_loss: 0.36869946122169495, g_loss: 4.550630569458008\n","Epoch 301/4000, Step 49, d_loss: 0.3679308295249939, g_loss: 3.2835886478424072\n","Epoch 301/4000, Step 50, d_loss: 0.38284146785736084, g_loss: 6.2289628982543945\n","Epoch 301/4000, Step 51, d_loss: 0.35273632407188416, g_loss: 6.306078910827637\n","Epoch 301/4000, Step 52, d_loss: 0.35651153326034546, g_loss: 5.3120808601379395\n","Epoch 301/4000, Step 53, d_loss: 0.37752196192741394, g_loss: 5.335295677185059\n","Epoch 301/4000, Step 54, d_loss: 0.3777349293231964, g_loss: 3.7214407920837402\n","Epoch 301/4000, Step 55, d_loss: 0.38130077719688416, g_loss: 4.2291765213012695\n","Epoch 301/4000, Step 56, d_loss: 0.3614167273044586, g_loss: 4.46452522277832\n","Epoch 301/4000, Step 57, d_loss: 0.373245507478714, g_loss: 3.839586019515991\n","Epoch 301/4000, Step 58, d_loss: 0.42043283581733704, g_loss: 4.311589241027832\n","Epoch 301/4000, Step 59, d_loss: 0.36761632561683655, g_loss: 5.1334228515625\n","Epoch 301/4000, Step 60, d_loss: 0.35915857553482056, g_loss: 5.859485626220703\n","Epoch 301/4000, Step 61, d_loss: 0.35390275716781616, g_loss: 6.225492477416992\n","Epoch 301/4000, Step 62, d_loss: 0.3698171079158783, g_loss: 4.927860260009766\n","Epoch 301/4000, Step 63, d_loss: 0.3651486933231354, g_loss: 8.08690071105957\n","Epoch 301/4000, Step 64, d_loss: 0.35696154832839966, g_loss: 4.4055280685424805\n","Epoch 301/4000, Step 65, d_loss: 0.36757344007492065, g_loss: 4.696526050567627\n","Epoch 301/4000, Step 66, d_loss: 0.3665253818035126, g_loss: 3.661165714263916\n","Epoch 301/4000, Step 67, d_loss: 0.3421648442745209, g_loss: 6.3557448387146\n","Epoch 301/4000, Step 68, d_loss: 0.3586467206478119, g_loss: 4.444275379180908\n","Epoch 301/4000, Step 69, d_loss: 0.3489867150783539, g_loss: 5.149312973022461\n","Epoch 301/4000, Step 70, d_loss: 0.35277339816093445, g_loss: 7.761615753173828\n","Epoch 301/4000, Step 71, d_loss: 0.35594630241394043, g_loss: 5.156022548675537\n","Epoch 301/4000, Step 72, d_loss: 0.3439472019672394, g_loss: 5.596248626708984\n","Epoch 301/4000, Step 73, d_loss: 0.3719679117202759, g_loss: 6.004642963409424\n","Epoch 301/4000, Step 74, d_loss: 0.3945920765399933, g_loss: 5.932124137878418\n","Epoch 301/4000, Step 75, d_loss: 0.35085049271583557, g_loss: 4.713654518127441\n","Epoch 301/4000, Step 76, d_loss: 0.3654044270515442, g_loss: 4.70348596572876\n","Epoch 301/4000, Step 77, d_loss: 0.3730608820915222, g_loss: 3.95851993560791\n","Epoch 301/4000, Step 78, d_loss: 0.4234776198863983, g_loss: 3.842369794845581\n","Epoch 301/4000, Step 79, d_loss: 0.40252217650413513, g_loss: 4.242326736450195\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 302/4000, Step 1, d_loss: 0.35731232166290283, g_loss: 3.3392953872680664\n","Epoch 302/4000, Step 2, d_loss: 0.36315950751304626, g_loss: 4.8253960609436035\n","Epoch 302/4000, Step 3, d_loss: 0.3407687246799469, g_loss: 4.576231002807617\n","Epoch 302/4000, Step 4, d_loss: 0.35774901509284973, g_loss: 4.9620490074157715\n","Epoch 302/4000, Step 5, d_loss: 0.3787650465965271, g_loss: 9.055742263793945\n","Epoch 302/4000, Step 6, d_loss: 0.3525931239128113, g_loss: 6.019914627075195\n","Epoch 302/4000, Step 7, d_loss: 0.4042457938194275, g_loss: 5.688321590423584\n","Epoch 302/4000, Step 8, d_loss: 0.3678085505962372, g_loss: 4.79875373840332\n","Epoch 302/4000, Step 9, d_loss: 0.3550572395324707, g_loss: 4.493171215057373\n","Epoch 302/4000, Step 10, d_loss: 0.3780878484249115, g_loss: 4.437134742736816\n","Epoch 302/4000, Step 11, d_loss: 0.41060230135917664, g_loss: 3.6337459087371826\n","Epoch 302/4000, Step 12, d_loss: 0.3534232974052429, g_loss: 4.668378829956055\n","Epoch 302/4000, Step 13, d_loss: 0.34999364614486694, g_loss: 5.0640549659729\n","Epoch 302/4000, Step 14, d_loss: 0.42143121361732483, g_loss: 5.558984756469727\n","Epoch 302/4000, Step 15, d_loss: 0.3578210771083832, g_loss: 5.029458522796631\n","Epoch 302/4000, Step 16, d_loss: 0.3566219210624695, g_loss: 4.542813301086426\n","Epoch 302/4000, Step 17, d_loss: 0.3430769741535187, g_loss: 6.907899856567383\n","Epoch 302/4000, Step 18, d_loss: 0.33958858251571655, g_loss: 4.534017562866211\n","Epoch 302/4000, Step 19, d_loss: 0.3492858111858368, g_loss: 4.057278633117676\n","Epoch 302/4000, Step 20, d_loss: 0.36164891719818115, g_loss: 5.395634174346924\n","Epoch 302/4000, Step 21, d_loss: 0.36407339572906494, g_loss: 3.964951753616333\n","Epoch 302/4000, Step 22, d_loss: 0.36397063732147217, g_loss: 2.848353147506714\n","Epoch 302/4000, Step 23, d_loss: 0.36267226934432983, g_loss: 3.7173221111297607\n","Epoch 302/4000, Step 24, d_loss: 0.3777449131011963, g_loss: 4.41767692565918\n","Epoch 302/4000, Step 25, d_loss: 0.35772356390953064, g_loss: 3.4032557010650635\n","Epoch 302/4000, Step 26, d_loss: 0.3985561430454254, g_loss: 3.1960978507995605\n","Epoch 302/4000, Step 27, d_loss: 0.355338454246521, g_loss: 3.773186683654785\n","Epoch 302/4000, Step 28, d_loss: 0.5754677653312683, g_loss: 4.124858379364014\n","Epoch 302/4000, Step 29, d_loss: 0.3867863118648529, g_loss: 2.6212844848632812\n","Epoch 302/4000, Step 30, d_loss: 0.4566018581390381, g_loss: 2.935408592224121\n","Epoch 302/4000, Step 31, d_loss: 0.41923996806144714, g_loss: 5.040781021118164\n","Epoch 302/4000, Step 32, d_loss: 0.4125145971775055, g_loss: 4.312713146209717\n","Epoch 302/4000, Step 33, d_loss: 0.36271539330482483, g_loss: 5.17648983001709\n","Epoch 302/4000, Step 34, d_loss: 0.3470700979232788, g_loss: 5.26453161239624\n","Epoch 302/4000, Step 35, d_loss: 0.3746766448020935, g_loss: 5.977281093597412\n","Epoch 302/4000, Step 36, d_loss: 0.37070155143737793, g_loss: 4.988489627838135\n","Epoch 302/4000, Step 37, d_loss: 0.3932620584964752, g_loss: 5.285913467407227\n","Epoch 302/4000, Step 38, d_loss: 0.3475584089756012, g_loss: 5.3101983070373535\n","Epoch 302/4000, Step 39, d_loss: 0.3464154899120331, g_loss: 5.394668102264404\n","Epoch 302/4000, Step 40, d_loss: 0.36093175411224365, g_loss: 5.2258806228637695\n","Epoch 302/4000, Step 41, d_loss: 0.3568290174007416, g_loss: 5.380057334899902\n","Epoch 302/4000, Step 42, d_loss: 0.3492773473262787, g_loss: 5.5633416175842285\n","Epoch 302/4000, Step 43, d_loss: 0.3466194272041321, g_loss: 5.709198951721191\n","Epoch 302/4000, Step 44, d_loss: 0.35169610381126404, g_loss: 5.022211074829102\n","Epoch 302/4000, Step 45, d_loss: 0.36248087882995605, g_loss: 5.963565349578857\n","Epoch 302/4000, Step 46, d_loss: 0.3549453020095825, g_loss: 5.92946195602417\n","Epoch 302/4000, Step 47, d_loss: 0.3658100366592407, g_loss: 5.2735748291015625\n","Epoch 302/4000, Step 48, d_loss: 0.35221078991889954, g_loss: 5.587230682373047\n","Epoch 302/4000, Step 49, d_loss: 0.3734543025493622, g_loss: 6.570321083068848\n","Epoch 302/4000, Step 50, d_loss: 0.3546946048736572, g_loss: 8.559331893920898\n","Epoch 302/4000, Step 51, d_loss: 0.33682310581207275, g_loss: 5.237313747406006\n","Epoch 302/4000, Step 52, d_loss: 0.3349669873714447, g_loss: 6.0435919761657715\n","Epoch 302/4000, Step 53, d_loss: 0.35924506187438965, g_loss: 6.131547451019287\n","Epoch 302/4000, Step 54, d_loss: 0.35041797161102295, g_loss: 4.871434688568115\n","Epoch 302/4000, Step 55, d_loss: 0.342673659324646, g_loss: 5.716132164001465\n","Epoch 302/4000, Step 56, d_loss: 0.3502032160758972, g_loss: 4.6616950035095215\n","Epoch 302/4000, Step 57, d_loss: 0.3668062388896942, g_loss: 4.770575523376465\n","Epoch 302/4000, Step 58, d_loss: 0.3549777865409851, g_loss: 6.307244300842285\n","Epoch 302/4000, Step 59, d_loss: 0.3595872223377228, g_loss: 6.555402755737305\n","Epoch 302/4000, Step 60, d_loss: 0.3421502709388733, g_loss: 6.721953868865967\n","Epoch 302/4000, Step 61, d_loss: 0.3510771095752716, g_loss: 6.690999984741211\n","Epoch 302/4000, Step 62, d_loss: 0.41464781761169434, g_loss: 6.341455936431885\n","Epoch 302/4000, Step 63, d_loss: 0.346206396818161, g_loss: 5.345821380615234\n","Epoch 302/4000, Step 64, d_loss: 0.34957870841026306, g_loss: 6.959286689758301\n","Epoch 302/4000, Step 65, d_loss: 0.33871784806251526, g_loss: 5.084704875946045\n","Epoch 302/4000, Step 66, d_loss: 0.37717801332473755, g_loss: 5.579750061035156\n","Epoch 302/4000, Step 67, d_loss: 0.37174341082572937, g_loss: 4.352597713470459\n","Epoch 302/4000, Step 68, d_loss: 0.3651539981365204, g_loss: 4.207683563232422\n","Epoch 302/4000, Step 69, d_loss: 0.34581026434898376, g_loss: 6.466196537017822\n","Epoch 302/4000, Step 70, d_loss: 0.3469046354293823, g_loss: 4.938354969024658\n","Epoch 302/4000, Step 71, d_loss: 0.3465717136859894, g_loss: 5.540546894073486\n","Epoch 302/4000, Step 72, d_loss: 0.3429999351501465, g_loss: 5.452916622161865\n","Epoch 302/4000, Step 73, d_loss: 0.3381613492965698, g_loss: 4.989878177642822\n","Epoch 302/4000, Step 74, d_loss: 0.3460117280483246, g_loss: 5.819000720977783\n","Epoch 302/4000, Step 75, d_loss: 0.33594179153442383, g_loss: 5.191104888916016\n","Epoch 302/4000, Step 76, d_loss: 0.34602364897727966, g_loss: 4.567712306976318\n","Epoch 302/4000, Step 77, d_loss: 0.3408714532852173, g_loss: 5.038798809051514\n","Epoch 302/4000, Step 78, d_loss: 0.3373246490955353, g_loss: 4.617875576019287\n","Epoch 302/4000, Step 79, d_loss: 0.33603978157043457, g_loss: 5.449638366699219\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 303/4000, Step 1, d_loss: 0.3692198395729065, g_loss: 5.576456069946289\n","Epoch 303/4000, Step 2, d_loss: 0.36996275186538696, g_loss: 6.323416233062744\n","Epoch 303/4000, Step 3, d_loss: 0.3395300805568695, g_loss: 6.490830421447754\n","Epoch 303/4000, Step 4, d_loss: 0.3383641839027405, g_loss: 5.559506893157959\n","Epoch 303/4000, Step 5, d_loss: 0.3370650112628937, g_loss: 6.238867282867432\n","Epoch 303/4000, Step 6, d_loss: 0.336487740278244, g_loss: 7.587810516357422\n","Epoch 303/4000, Step 7, d_loss: 0.3631250858306885, g_loss: 5.938511848449707\n","Epoch 303/4000, Step 8, d_loss: 0.333002507686615, g_loss: 6.432106018066406\n","Epoch 303/4000, Step 9, d_loss: 0.3380470871925354, g_loss: 7.972219467163086\n","Epoch 303/4000, Step 10, d_loss: 0.33737537264823914, g_loss: 5.8494768142700195\n","Epoch 303/4000, Step 11, d_loss: 0.34815874695777893, g_loss: 4.773248195648193\n","Epoch 303/4000, Step 12, d_loss: 0.3374042809009552, g_loss: 4.62428092956543\n","Epoch 303/4000, Step 13, d_loss: 0.35373586416244507, g_loss: 3.8840291500091553\n","Epoch 303/4000, Step 14, d_loss: 0.36324042081832886, g_loss: 5.418625831604004\n","Epoch 303/4000, Step 15, d_loss: 0.36135333776474, g_loss: 7.659781455993652\n","Epoch 303/4000, Step 16, d_loss: 0.35014668107032776, g_loss: 6.122359275817871\n","Epoch 303/4000, Step 17, d_loss: 0.3481441140174866, g_loss: 7.9465789794921875\n","Epoch 303/4000, Step 18, d_loss: 0.34732043743133545, g_loss: 6.861973285675049\n","Epoch 303/4000, Step 19, d_loss: 0.3384491503238678, g_loss: 6.466818332672119\n","Epoch 303/4000, Step 20, d_loss: 0.3388076722621918, g_loss: 7.717247009277344\n","Epoch 303/4000, Step 21, d_loss: 0.3362589180469513, g_loss: 8.06162166595459\n","Epoch 303/4000, Step 22, d_loss: 0.3682706952095032, g_loss: 6.429433822631836\n","Epoch 303/4000, Step 23, d_loss: 0.3479948043823242, g_loss: 5.372403621673584\n","Epoch 303/4000, Step 24, d_loss: 0.3416686952114105, g_loss: 4.3545026779174805\n","Epoch 303/4000, Step 25, d_loss: 0.3651587963104248, g_loss: 5.886294364929199\n","Epoch 303/4000, Step 26, d_loss: 0.350725382566452, g_loss: 4.8979926109313965\n","Epoch 303/4000, Step 27, d_loss: 0.3441026508808136, g_loss: 4.8648552894592285\n","Epoch 303/4000, Step 28, d_loss: 0.33829018473625183, g_loss: 7.459080696105957\n","Epoch 303/4000, Step 29, d_loss: 0.3540181517601013, g_loss: 5.692326545715332\n","Epoch 303/4000, Step 30, d_loss: 0.3391297161579132, g_loss: 5.03510046005249\n","Epoch 303/4000, Step 31, d_loss: 0.3442017436027527, g_loss: 5.81233024597168\n","Epoch 303/4000, Step 32, d_loss: 0.3356280028820038, g_loss: 5.778878211975098\n","Epoch 303/4000, Step 33, d_loss: 0.33366623520851135, g_loss: 6.02991247177124\n","Epoch 303/4000, Step 34, d_loss: 0.34739261865615845, g_loss: 6.384552955627441\n","Epoch 303/4000, Step 35, d_loss: 0.34222501516342163, g_loss: 6.353100776672363\n","Epoch 303/4000, Step 36, d_loss: 0.34364545345306396, g_loss: 4.54718017578125\n","Epoch 303/4000, Step 37, d_loss: 0.34972551465034485, g_loss: 4.245283603668213\n","Epoch 303/4000, Step 38, d_loss: 0.34420275688171387, g_loss: 6.215709686279297\n","Epoch 303/4000, Step 39, d_loss: 0.3431899845600128, g_loss: 5.944948673248291\n","Epoch 303/4000, Step 40, d_loss: 0.34549710154533386, g_loss: 5.578086853027344\n","Epoch 303/4000, Step 41, d_loss: 0.35304248332977295, g_loss: 7.108615875244141\n","Epoch 303/4000, Step 42, d_loss: 0.34312352538108826, g_loss: 4.976686954498291\n","Epoch 303/4000, Step 43, d_loss: 0.3411440849304199, g_loss: 5.963230133056641\n","Epoch 303/4000, Step 44, d_loss: 0.3567664325237274, g_loss: 8.31683349609375\n","Epoch 303/4000, Step 45, d_loss: 0.35860052704811096, g_loss: 6.563409805297852\n","Epoch 303/4000, Step 46, d_loss: 0.36169523000717163, g_loss: 6.35572624206543\n","Epoch 303/4000, Step 47, d_loss: 0.35016441345214844, g_loss: 5.850831031799316\n","Epoch 303/4000, Step 48, d_loss: 0.36824044585227966, g_loss: 5.312185287475586\n","Epoch 303/4000, Step 49, d_loss: 0.34388551115989685, g_loss: 7.721245765686035\n","Epoch 303/4000, Step 50, d_loss: 0.35232627391815186, g_loss: 4.1306047439575195\n","Epoch 303/4000, Step 51, d_loss: 0.3663543164730072, g_loss: 4.096676349639893\n","Epoch 303/4000, Step 52, d_loss: 0.37479788064956665, g_loss: 4.565188884735107\n","Epoch 303/4000, Step 53, d_loss: 0.3448277413845062, g_loss: 4.227184295654297\n","Epoch 303/4000, Step 54, d_loss: 0.3522666096687317, g_loss: 5.196809768676758\n","Epoch 303/4000, Step 55, d_loss: 0.35542669892311096, g_loss: 5.625483989715576\n","Epoch 303/4000, Step 56, d_loss: 0.3470074236392975, g_loss: 5.689541816711426\n","Epoch 303/4000, Step 57, d_loss: 0.3432799279689789, g_loss: 6.881872177124023\n","Epoch 303/4000, Step 58, d_loss: 0.3538069427013397, g_loss: 5.354245185852051\n","Epoch 303/4000, Step 59, d_loss: 0.3663347363471985, g_loss: 5.0004658699035645\n","Epoch 303/4000, Step 60, d_loss: 0.3459463119506836, g_loss: 4.81812047958374\n","Epoch 303/4000, Step 61, d_loss: 0.3552306294441223, g_loss: 7.440884113311768\n","Epoch 303/4000, Step 62, d_loss: 0.3342684209346771, g_loss: 4.525680065155029\n","Epoch 303/4000, Step 63, d_loss: 0.3422262668609619, g_loss: 6.100875377655029\n","Epoch 303/4000, Step 64, d_loss: 0.3587665259838104, g_loss: 4.857999801635742\n","Epoch 303/4000, Step 65, d_loss: 0.3497233986854553, g_loss: 5.671360969543457\n","Epoch 303/4000, Step 66, d_loss: 0.3501889109611511, g_loss: 5.052096843719482\n","Epoch 303/4000, Step 67, d_loss: 0.3705677390098572, g_loss: 6.619465351104736\n","Epoch 303/4000, Step 68, d_loss: 0.33909788727760315, g_loss: 5.592188835144043\n","Epoch 303/4000, Step 69, d_loss: 0.3505135774612427, g_loss: 4.933111190795898\n","Epoch 303/4000, Step 70, d_loss: 0.3627336025238037, g_loss: 3.6775362491607666\n","Epoch 303/4000, Step 71, d_loss: 0.34523338079452515, g_loss: 5.719486236572266\n","Epoch 303/4000, Step 72, d_loss: 0.3532327711582184, g_loss: 6.3386616706848145\n","Epoch 303/4000, Step 73, d_loss: 0.3376726806163788, g_loss: 5.161107540130615\n","Epoch 303/4000, Step 74, d_loss: 0.34249094128608704, g_loss: 8.086038589477539\n","Epoch 303/4000, Step 75, d_loss: 0.34741920232772827, g_loss: 6.214387893676758\n","Epoch 303/4000, Step 76, d_loss: 0.344524621963501, g_loss: 5.888926982879639\n","Epoch 303/4000, Step 77, d_loss: 0.33637866377830505, g_loss: 5.5277299880981445\n","Epoch 303/4000, Step 78, d_loss: 0.35122430324554443, g_loss: 4.5451459884643555\n","Epoch 303/4000, Step 79, d_loss: 1.2426114082336426, g_loss: 3.310549020767212\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 304/4000, Step 1, d_loss: 0.4502374529838562, g_loss: 1.388968586921692\n","Epoch 304/4000, Step 2, d_loss: 0.7377597689628601, g_loss: 1.5975059270858765\n","Epoch 304/4000, Step 3, d_loss: 1.7209205627441406, g_loss: 3.72502064704895\n","Epoch 304/4000, Step 4, d_loss: 0.5954440832138062, g_loss: 5.44038200378418\n","Epoch 304/4000, Step 5, d_loss: 0.7195248603820801, g_loss: 7.002574443817139\n","Epoch 304/4000, Step 6, d_loss: 0.9516924023628235, g_loss: 9.297003746032715\n","Epoch 304/4000, Step 7, d_loss: 0.8770151138305664, g_loss: 4.300373554229736\n","Epoch 304/4000, Step 8, d_loss: 0.4080163240432739, g_loss: 1.8169766664505005\n","Epoch 304/4000, Step 9, d_loss: 0.49237942695617676, g_loss: 1.7638795375823975\n","Epoch 304/4000, Step 10, d_loss: 0.8225013613700867, g_loss: 6.533478736877441\n","Epoch 304/4000, Step 11, d_loss: 1.023937463760376, g_loss: 0.09250776469707489\n","Epoch 304/4000, Step 12, d_loss: 0.7529270648956299, g_loss: 0.060585036873817444\n","Epoch 304/4000, Step 13, d_loss: 2.813880443572998, g_loss: 8.367667198181152\n","Epoch 304/4000, Step 14, d_loss: 0.5968518257141113, g_loss: 11.330812454223633\n","Epoch 304/4000, Step 15, d_loss: 0.910313069820404, g_loss: 8.963785171508789\n","Epoch 304/4000, Step 16, d_loss: 1.0006033182144165, g_loss: 11.88839054107666\n","Epoch 304/4000, Step 17, d_loss: 0.9286041855812073, g_loss: 7.639349460601807\n","Epoch 304/4000, Step 18, d_loss: 0.4830867052078247, g_loss: 5.0537590980529785\n","Epoch 304/4000, Step 19, d_loss: 0.4549484848976135, g_loss: 3.120983600616455\n","Epoch 304/4000, Step 20, d_loss: 0.5486913323402405, g_loss: 2.8250415325164795\n","Epoch 304/4000, Step 21, d_loss: 0.8249250054359436, g_loss: 2.3732173442840576\n","Epoch 304/4000, Step 22, d_loss: 1.0086348056793213, g_loss: 4.207331657409668\n","Epoch 304/4000, Step 23, d_loss: 0.5507691502571106, g_loss: 7.444557189941406\n","Epoch 304/4000, Step 24, d_loss: 0.5558480620384216, g_loss: 7.320423126220703\n","Epoch 304/4000, Step 25, d_loss: 0.45875489711761475, g_loss: 8.13726806640625\n","Epoch 304/4000, Step 26, d_loss: 0.8937234282493591, g_loss: 8.049988746643066\n","Epoch 304/4000, Step 27, d_loss: 0.45802828669548035, g_loss: 6.055818557739258\n","Epoch 304/4000, Step 28, d_loss: 0.3625975549221039, g_loss: 5.931936264038086\n","Epoch 304/4000, Step 29, d_loss: 0.43683725595474243, g_loss: 3.8970706462860107\n","Epoch 304/4000, Step 30, d_loss: 0.41976723074913025, g_loss: 4.5390496253967285\n","Epoch 304/4000, Step 31, d_loss: 0.48164477944374084, g_loss: 3.0881569385528564\n","Epoch 304/4000, Step 32, d_loss: 0.5145689249038696, g_loss: 4.059593200683594\n","Epoch 304/4000, Step 33, d_loss: 0.4356611967086792, g_loss: 6.643593788146973\n","Epoch 304/4000, Step 34, d_loss: 0.4689790606498718, g_loss: 4.645642280578613\n","Epoch 304/4000, Step 35, d_loss: 0.43226945400238037, g_loss: 4.6820831298828125\n","Epoch 304/4000, Step 36, d_loss: 0.4015232026576996, g_loss: 5.722438335418701\n","Epoch 304/4000, Step 37, d_loss: 0.410085529088974, g_loss: 5.3138837814331055\n","Epoch 304/4000, Step 38, d_loss: 0.402366042137146, g_loss: 5.813989162445068\n","Epoch 304/4000, Step 39, d_loss: 0.9260151386260986, g_loss: 4.57814884185791\n","Epoch 304/4000, Step 40, d_loss: 0.3938922882080078, g_loss: 4.310568332672119\n","Epoch 304/4000, Step 41, d_loss: 0.43139252066612244, g_loss: 4.958290100097656\n","Epoch 304/4000, Step 42, d_loss: 0.4597557783126831, g_loss: 4.560585021972656\n","Epoch 304/4000, Step 43, d_loss: 0.46676120162010193, g_loss: 5.697526931762695\n","Epoch 304/4000, Step 44, d_loss: 0.4529888331890106, g_loss: 4.02224588394165\n","Epoch 304/4000, Step 45, d_loss: 0.4979197382926941, g_loss: 4.461364269256592\n","Epoch 304/4000, Step 46, d_loss: 0.4003412127494812, g_loss: 4.514537811279297\n","Epoch 304/4000, Step 47, d_loss: 0.6011595129966736, g_loss: 6.096076011657715\n","Epoch 304/4000, Step 48, d_loss: 0.39051729440689087, g_loss: 5.732706546783447\n","Epoch 304/4000, Step 49, d_loss: 0.369039922952652, g_loss: 2.6615726947784424\n","Epoch 304/4000, Step 50, d_loss: 0.41150885820388794, g_loss: 4.60029411315918\n","Epoch 304/4000, Step 51, d_loss: 0.4772452414035797, g_loss: 2.6704599857330322\n","Epoch 304/4000, Step 52, d_loss: 0.43265199661254883, g_loss: 6.312141418457031\n","Epoch 304/4000, Step 53, d_loss: 0.4788397252559662, g_loss: 6.705868244171143\n","Epoch 304/4000, Step 54, d_loss: 0.42536231875419617, g_loss: 6.617516040802002\n","Epoch 304/4000, Step 55, d_loss: 0.4424188435077667, g_loss: 4.558177947998047\n","Epoch 304/4000, Step 56, d_loss: 0.41033288836479187, g_loss: 7.027303218841553\n","Epoch 304/4000, Step 57, d_loss: 0.4273384213447571, g_loss: 6.9767961502075195\n","Epoch 304/4000, Step 58, d_loss: 0.3955078423023224, g_loss: 6.480745792388916\n","Epoch 304/4000, Step 59, d_loss: 0.37441274523735046, g_loss: 6.996139049530029\n","Epoch 304/4000, Step 60, d_loss: 0.37975919246673584, g_loss: 7.056005954742432\n","Epoch 304/4000, Step 61, d_loss: 0.3740960359573364, g_loss: 7.043691158294678\n","Epoch 304/4000, Step 62, d_loss: 0.3584301769733429, g_loss: 5.354938507080078\n","Epoch 304/4000, Step 63, d_loss: 0.35809987783432007, g_loss: 5.361989974975586\n","Epoch 304/4000, Step 64, d_loss: 0.37491631507873535, g_loss: 4.666354656219482\n","Epoch 304/4000, Step 65, d_loss: 0.39152657985687256, g_loss: 5.9882965087890625\n","Epoch 304/4000, Step 66, d_loss: 0.4015865623950958, g_loss: 4.410659313201904\n","Epoch 304/4000, Step 67, d_loss: 0.4007488489151001, g_loss: 5.382444381713867\n","Epoch 304/4000, Step 68, d_loss: 0.38576987385749817, g_loss: 4.530939102172852\n","Epoch 304/4000, Step 69, d_loss: 0.3919697105884552, g_loss: 4.729028224945068\n","Epoch 304/4000, Step 70, d_loss: 0.3647543787956238, g_loss: 4.86912727355957\n","Epoch 304/4000, Step 71, d_loss: 0.3582637906074524, g_loss: 5.754803657531738\n","Epoch 304/4000, Step 72, d_loss: 0.3386586904525757, g_loss: 5.793112754821777\n","Epoch 304/4000, Step 73, d_loss: 0.3420853614807129, g_loss: 5.675027847290039\n","Epoch 304/4000, Step 74, d_loss: 0.36810290813446045, g_loss: 5.285902976989746\n","Epoch 304/4000, Step 75, d_loss: 0.35859283804893494, g_loss: 5.35667085647583\n","Epoch 304/4000, Step 76, d_loss: 0.3448815941810608, g_loss: 5.5086541175842285\n","Epoch 304/4000, Step 77, d_loss: 0.3455599546432495, g_loss: 5.964456558227539\n","Epoch 304/4000, Step 78, d_loss: 0.42590615153312683, g_loss: 5.4859299659729\n","Epoch 304/4000, Step 79, d_loss: 1.037405014038086, g_loss: 4.664501667022705\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 305/4000, Step 1, d_loss: 0.443310409784317, g_loss: 2.282029390335083\n","Epoch 305/4000, Step 2, d_loss: 0.6220611929893494, g_loss: 2.2049131393432617\n","Epoch 305/4000, Step 3, d_loss: 0.7405571341514587, g_loss: 2.7987141609191895\n","Epoch 305/4000, Step 4, d_loss: 0.6466045379638672, g_loss: 2.9715867042541504\n","Epoch 305/4000, Step 5, d_loss: 0.566740095615387, g_loss: 4.135601043701172\n","Epoch 305/4000, Step 6, d_loss: 0.4760066270828247, g_loss: 4.96159029006958\n","Epoch 305/4000, Step 7, d_loss: 0.45316585898399353, g_loss: 6.326064586639404\n","Epoch 305/4000, Step 8, d_loss: 0.3860686123371124, g_loss: 6.61566162109375\n","Epoch 305/4000, Step 9, d_loss: 0.41363459825515747, g_loss: 7.065106391906738\n","Epoch 305/4000, Step 10, d_loss: 0.4194784164428711, g_loss: 8.830705642700195\n","Epoch 305/4000, Step 11, d_loss: 0.3604941666126251, g_loss: 8.787084579467773\n","Epoch 305/4000, Step 12, d_loss: 0.3495837152004242, g_loss: 7.996871471405029\n","Epoch 305/4000, Step 13, d_loss: 0.34400829672813416, g_loss: 7.919090747833252\n","Epoch 305/4000, Step 14, d_loss: 0.39076921343803406, g_loss: 9.861517906188965\n","Epoch 305/4000, Step 15, d_loss: 0.38093051314353943, g_loss: 8.812906265258789\n","Epoch 305/4000, Step 16, d_loss: 0.4077010154724121, g_loss: 9.506970405578613\n","Epoch 305/4000, Step 17, d_loss: 0.3898795247077942, g_loss: 7.761566638946533\n","Epoch 305/4000, Step 18, d_loss: 0.3525157868862152, g_loss: 7.265314102172852\n","Epoch 305/4000, Step 19, d_loss: 0.3754644989967346, g_loss: 10.108221054077148\n","Epoch 305/4000, Step 20, d_loss: 0.3618420362472534, g_loss: 5.650940895080566\n","Epoch 305/4000, Step 21, d_loss: 0.365083783864975, g_loss: 7.208987712860107\n","Epoch 305/4000, Step 22, d_loss: 0.3609415590763092, g_loss: 7.085782051086426\n","Epoch 305/4000, Step 23, d_loss: 0.35051628947257996, g_loss: 6.643397808074951\n","Epoch 305/4000, Step 24, d_loss: 0.35594984889030457, g_loss: 6.181663513183594\n","Epoch 305/4000, Step 25, d_loss: 0.3431217074394226, g_loss: 8.526304244995117\n","Epoch 305/4000, Step 26, d_loss: 0.3625113368034363, g_loss: 5.870369911193848\n","Epoch 305/4000, Step 27, d_loss: 0.3805583417415619, g_loss: 6.028499126434326\n","Epoch 305/4000, Step 28, d_loss: 0.34883564710617065, g_loss: 6.6639227867126465\n","Epoch 305/4000, Step 29, d_loss: 0.3671003580093384, g_loss: 6.13435173034668\n","Epoch 305/4000, Step 30, d_loss: 0.35038504004478455, g_loss: 5.626358985900879\n","Epoch 305/4000, Step 31, d_loss: 0.33669906854629517, g_loss: 7.217838764190674\n","Epoch 305/4000, Step 32, d_loss: 0.3352002799510956, g_loss: 7.318480491638184\n","Epoch 305/4000, Step 33, d_loss: 0.3390997052192688, g_loss: 5.888911247253418\n","Epoch 305/4000, Step 34, d_loss: 0.36280351877212524, g_loss: 6.6101226806640625\n","Epoch 305/4000, Step 35, d_loss: 0.35471436381340027, g_loss: 7.801163196563721\n","Epoch 305/4000, Step 36, d_loss: 0.337756872177124, g_loss: 7.264614582061768\n","Epoch 305/4000, Step 37, d_loss: 0.33532950282096863, g_loss: 5.849602222442627\n","Epoch 305/4000, Step 38, d_loss: 0.34006017446517944, g_loss: 5.077200889587402\n","Epoch 305/4000, Step 39, d_loss: 0.34655502438545227, g_loss: 5.989006042480469\n","Epoch 305/4000, Step 40, d_loss: 0.33627912402153015, g_loss: 6.003824234008789\n","Epoch 305/4000, Step 41, d_loss: 0.3408944606781006, g_loss: 3.3323476314544678\n","Epoch 305/4000, Step 42, d_loss: 0.4125598967075348, g_loss: 8.044809341430664\n","Epoch 305/4000, Step 43, d_loss: 0.3627138137817383, g_loss: 5.465266227722168\n","Epoch 305/4000, Step 44, d_loss: 0.34825077652931213, g_loss: 3.9487669467926025\n","Epoch 305/4000, Step 45, d_loss: 0.34245234727859497, g_loss: 6.040292739868164\n","Epoch 305/4000, Step 46, d_loss: 0.3575459420681, g_loss: 3.69872784614563\n","Epoch 305/4000, Step 47, d_loss: 0.3448941707611084, g_loss: 5.204806804656982\n","Epoch 305/4000, Step 48, d_loss: 0.35489600896835327, g_loss: 5.056929588317871\n","Epoch 305/4000, Step 49, d_loss: 0.3431704640388489, g_loss: 4.8442912101745605\n","Epoch 305/4000, Step 50, d_loss: 0.34230971336364746, g_loss: 5.093708515167236\n","Epoch 305/4000, Step 51, d_loss: 0.3435908854007721, g_loss: 4.915144443511963\n","Epoch 305/4000, Step 52, d_loss: 0.3457435071468353, g_loss: 5.367427825927734\n","Epoch 305/4000, Step 53, d_loss: 0.3360552489757538, g_loss: 6.274804592132568\n","Epoch 305/4000, Step 54, d_loss: 0.3397393524646759, g_loss: 5.483945369720459\n","Epoch 305/4000, Step 55, d_loss: 0.3623878061771393, g_loss: 5.363549709320068\n","Epoch 305/4000, Step 56, d_loss: 0.34570226073265076, g_loss: 5.038456916809082\n","Epoch 305/4000, Step 57, d_loss: 0.37209153175354004, g_loss: 8.846240997314453\n","Epoch 305/4000, Step 58, d_loss: 0.35130712389945984, g_loss: 5.928674221038818\n","Epoch 305/4000, Step 59, d_loss: 0.344903826713562, g_loss: 4.8534345626831055\n","Epoch 305/4000, Step 60, d_loss: 0.3340108394622803, g_loss: 4.427703380584717\n","Epoch 305/4000, Step 61, d_loss: 0.36062559485435486, g_loss: 4.38567590713501\n","Epoch 305/4000, Step 62, d_loss: 0.3457688093185425, g_loss: 3.745512008666992\n","Epoch 305/4000, Step 63, d_loss: 0.3630063533782959, g_loss: 4.380252361297607\n","Epoch 305/4000, Step 64, d_loss: 0.34833434224128723, g_loss: 4.754830837249756\n","Epoch 305/4000, Step 65, d_loss: 0.3419400751590729, g_loss: 4.138950347900391\n","Epoch 305/4000, Step 66, d_loss: 0.35239559412002563, g_loss: 5.129332542419434\n","Epoch 305/4000, Step 67, d_loss: 0.3343013525009155, g_loss: 7.513874053955078\n","Epoch 305/4000, Step 68, d_loss: 0.34345322847366333, g_loss: 4.663123607635498\n","Epoch 305/4000, Step 69, d_loss: 0.3370349705219269, g_loss: 5.919330596923828\n","Epoch 305/4000, Step 70, d_loss: 0.344289094209671, g_loss: 5.701212406158447\n","Epoch 305/4000, Step 71, d_loss: 0.40722566843032837, g_loss: 7.397828578948975\n","Epoch 305/4000, Step 72, d_loss: 0.3371916711330414, g_loss: 5.5455121994018555\n","Epoch 305/4000, Step 73, d_loss: 0.35295313596725464, g_loss: 5.880940914154053\n","Epoch 305/4000, Step 74, d_loss: 0.37472739815711975, g_loss: 4.353799819946289\n","Epoch 305/4000, Step 75, d_loss: 0.33912578225135803, g_loss: 4.066186428070068\n","Epoch 305/4000, Step 76, d_loss: 0.34047332406044006, g_loss: 4.213690757751465\n","Epoch 305/4000, Step 77, d_loss: 0.3389820456504822, g_loss: 3.767839193344116\n","Epoch 305/4000, Step 78, d_loss: 0.33652007579803467, g_loss: 4.014163494110107\n","Epoch 305/4000, Step 79, d_loss: 0.5754743814468384, g_loss: 3.428927183151245\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 306/4000, Step 1, d_loss: 0.35403892397880554, g_loss: 4.302453994750977\n","Epoch 306/4000, Step 2, d_loss: 0.37315884232521057, g_loss: 3.217672824859619\n","Epoch 306/4000, Step 3, d_loss: 0.3946038782596588, g_loss: 4.121213436126709\n","Epoch 306/4000, Step 4, d_loss: 0.3816772401332855, g_loss: 4.116849899291992\n","Epoch 306/4000, Step 5, d_loss: 0.37874510884284973, g_loss: 3.535277843475342\n","Epoch 306/4000, Step 6, d_loss: 0.4620937407016754, g_loss: 5.952991485595703\n","Epoch 306/4000, Step 7, d_loss: 0.3828694224357605, g_loss: 5.294892311096191\n","Epoch 306/4000, Step 8, d_loss: 0.40023165941238403, g_loss: 6.166558265686035\n","Epoch 306/4000, Step 9, d_loss: 0.4003082811832428, g_loss: 4.704972743988037\n","Epoch 306/4000, Step 10, d_loss: 0.3547183871269226, g_loss: 6.347586154937744\n","Epoch 306/4000, Step 11, d_loss: 0.37361204624176025, g_loss: 7.063724994659424\n","Epoch 306/4000, Step 12, d_loss: 0.34473514556884766, g_loss: 5.000808238983154\n","Epoch 306/4000, Step 13, d_loss: 0.365374356508255, g_loss: 6.127527713775635\n","Epoch 306/4000, Step 14, d_loss: 0.36179205775260925, g_loss: 4.741905689239502\n","Epoch 306/4000, Step 15, d_loss: 0.3773019015789032, g_loss: 5.657165050506592\n","Epoch 306/4000, Step 16, d_loss: 0.37475138902664185, g_loss: 5.441205978393555\n","Epoch 306/4000, Step 17, d_loss: 0.3690973222255707, g_loss: 4.7078375816345215\n","Epoch 306/4000, Step 18, d_loss: 0.3625282645225525, g_loss: 5.521912097930908\n","Epoch 306/4000, Step 19, d_loss: 0.3594766855239868, g_loss: 5.559952735900879\n","Epoch 306/4000, Step 20, d_loss: 0.3468233644962311, g_loss: 4.127146244049072\n","Epoch 306/4000, Step 21, d_loss: 0.3452981412410736, g_loss: 5.231701850891113\n","Epoch 306/4000, Step 22, d_loss: 0.34494900703430176, g_loss: 5.127695083618164\n","Epoch 306/4000, Step 23, d_loss: 0.3486558496952057, g_loss: 5.524214744567871\n","Epoch 306/4000, Step 24, d_loss: 0.3381500542163849, g_loss: 8.77114200592041\n","Epoch 306/4000, Step 25, d_loss: 0.3398210108280182, g_loss: 5.960575580596924\n","Epoch 306/4000, Step 26, d_loss: 0.365843266248703, g_loss: 6.2710957527160645\n","Epoch 306/4000, Step 27, d_loss: 0.34416463971138, g_loss: 5.348113536834717\n","Epoch 306/4000, Step 28, d_loss: 0.344772070646286, g_loss: 5.988800048828125\n","Epoch 306/4000, Step 29, d_loss: 0.35708722472190857, g_loss: 4.976319313049316\n","Epoch 306/4000, Step 30, d_loss: 0.34402063488960266, g_loss: 5.308737754821777\n","Epoch 306/4000, Step 31, d_loss: 0.336434006690979, g_loss: 6.313165664672852\n","Epoch 306/4000, Step 32, d_loss: 0.3564640283584595, g_loss: 4.517228126525879\n","Epoch 306/4000, Step 33, d_loss: 0.36916404962539673, g_loss: 5.396124839782715\n","Epoch 306/4000, Step 34, d_loss: 0.34848514199256897, g_loss: 4.687126636505127\n","Epoch 306/4000, Step 35, d_loss: 0.3518441915512085, g_loss: 5.342488765716553\n","Epoch 306/4000, Step 36, d_loss: 0.3346662223339081, g_loss: 5.111956596374512\n","Epoch 306/4000, Step 37, d_loss: 0.3558202087879181, g_loss: 5.175771713256836\n","Epoch 306/4000, Step 38, d_loss: 0.34879758954048157, g_loss: 5.6687798500061035\n","Epoch 306/4000, Step 39, d_loss: 0.35580962896347046, g_loss: 7.818385601043701\n","Epoch 306/4000, Step 40, d_loss: 0.33880338072776794, g_loss: 6.807338237762451\n","Epoch 306/4000, Step 41, d_loss: 0.35280731320381165, g_loss: 5.659677982330322\n","Epoch 306/4000, Step 42, d_loss: 0.33614134788513184, g_loss: 6.500428199768066\n","Epoch 306/4000, Step 43, d_loss: 0.3487855792045593, g_loss: 8.41494369506836\n","Epoch 306/4000, Step 44, d_loss: 0.3371409773826599, g_loss: 5.893310546875\n","Epoch 306/4000, Step 45, d_loss: 0.33437323570251465, g_loss: 6.26584529876709\n","Epoch 306/4000, Step 46, d_loss: 0.33914464712142944, g_loss: 6.449840068817139\n","Epoch 306/4000, Step 47, d_loss: 0.3678297698497772, g_loss: 6.4510369300842285\n","Epoch 306/4000, Step 48, d_loss: 0.34136199951171875, g_loss: 5.821409702301025\n","Epoch 306/4000, Step 49, d_loss: 0.33830684423446655, g_loss: 6.247603416442871\n","Epoch 306/4000, Step 50, d_loss: 0.3448754549026489, g_loss: 6.411253929138184\n","Epoch 306/4000, Step 51, d_loss: 0.3637455999851227, g_loss: 5.3903584480285645\n","Epoch 306/4000, Step 52, d_loss: 0.35120341181755066, g_loss: 7.954730033874512\n","Epoch 306/4000, Step 53, d_loss: 0.34164369106292725, g_loss: 5.057280540466309\n","Epoch 306/4000, Step 54, d_loss: 0.3462528884410858, g_loss: 6.180578708648682\n","Epoch 306/4000, Step 55, d_loss: 0.34662458300590515, g_loss: 6.748772621154785\n","Epoch 306/4000, Step 56, d_loss: 0.34971195459365845, g_loss: 5.657691478729248\n","Epoch 306/4000, Step 57, d_loss: 0.3453722894191742, g_loss: 5.307780742645264\n","Epoch 306/4000, Step 58, d_loss: 0.3505169451236725, g_loss: 5.575848579406738\n","Epoch 306/4000, Step 59, d_loss: 0.33322277665138245, g_loss: 8.22630786895752\n","Epoch 306/4000, Step 60, d_loss: 0.34091025590896606, g_loss: 5.883091449737549\n","Epoch 306/4000, Step 61, d_loss: 0.3461771011352539, g_loss: 5.6664934158325195\n","Epoch 306/4000, Step 62, d_loss: 0.3364385664463043, g_loss: 5.793565273284912\n","Epoch 306/4000, Step 63, d_loss: 0.3458784222602844, g_loss: 5.427957534790039\n","Epoch 306/4000, Step 64, d_loss: 0.34416744112968445, g_loss: 5.022571563720703\n","Epoch 306/4000, Step 65, d_loss: 0.34380868077278137, g_loss: 5.9604973793029785\n","Epoch 306/4000, Step 66, d_loss: 0.3446180820465088, g_loss: 4.798430442810059\n","Epoch 306/4000, Step 67, d_loss: 0.3402940034866333, g_loss: 4.954186916351318\n","Epoch 306/4000, Step 68, d_loss: 0.3363604247570038, g_loss: 5.12593412399292\n","Epoch 306/4000, Step 69, d_loss: 0.3339889943599701, g_loss: 5.807287693023682\n","Epoch 306/4000, Step 70, d_loss: 0.3379497230052948, g_loss: 5.028487205505371\n","Epoch 306/4000, Step 71, d_loss: 0.3483976721763611, g_loss: 4.961209774017334\n","Epoch 306/4000, Step 72, d_loss: 0.3450213372707367, g_loss: 5.342370986938477\n","Epoch 306/4000, Step 73, d_loss: 0.34190303087234497, g_loss: 5.45607852935791\n","Epoch 306/4000, Step 74, d_loss: 0.3353821337223053, g_loss: 4.829897880554199\n","Epoch 306/4000, Step 75, d_loss: 0.3380242586135864, g_loss: 5.342041492462158\n","Epoch 306/4000, Step 76, d_loss: 0.3515576422214508, g_loss: 4.717708587646484\n","Epoch 306/4000, Step 77, d_loss: 0.3407256007194519, g_loss: 5.300864219665527\n","Epoch 306/4000, Step 78, d_loss: 0.33736151456832886, g_loss: 5.050043106079102\n","Epoch 306/4000, Step 79, d_loss: 0.8752380013465881, g_loss: 3.9578664302825928\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 307/4000, Step 1, d_loss: 0.391370952129364, g_loss: 3.007638692855835\n","Epoch 307/4000, Step 2, d_loss: 0.47372129559516907, g_loss: 3.2006819248199463\n","Epoch 307/4000, Step 3, d_loss: 0.46534866094589233, g_loss: 2.6687095165252686\n","Epoch 307/4000, Step 4, d_loss: 0.49175748229026794, g_loss: 2.3404462337493896\n","Epoch 307/4000, Step 5, d_loss: 0.5621278285980225, g_loss: 2.627570867538452\n","Epoch 307/4000, Step 6, d_loss: 0.4214007258415222, g_loss: 4.437013626098633\n","Epoch 307/4000, Step 7, d_loss: 0.4058911204338074, g_loss: 4.829404830932617\n","Epoch 307/4000, Step 8, d_loss: 0.3783506453037262, g_loss: 6.366490364074707\n","Epoch 307/4000, Step 9, d_loss: 0.36713096499443054, g_loss: 6.232639312744141\n","Epoch 307/4000, Step 10, d_loss: 0.40092527866363525, g_loss: 7.007783889770508\n","Epoch 307/4000, Step 11, d_loss: 0.40362951159477234, g_loss: 7.200719356536865\n","Epoch 307/4000, Step 12, d_loss: 0.3581143617630005, g_loss: 8.572184562683105\n","Epoch 307/4000, Step 13, d_loss: 0.34718596935272217, g_loss: 5.311087608337402\n","Epoch 307/4000, Step 14, d_loss: 0.36368638277053833, g_loss: 5.72883415222168\n","Epoch 307/4000, Step 15, d_loss: 0.3626266419887543, g_loss: 5.8487749099731445\n","Epoch 307/4000, Step 16, d_loss: 0.3639228641986847, g_loss: 5.265288352966309\n","Epoch 307/4000, Step 17, d_loss: 0.37511149048805237, g_loss: 4.485456466674805\n","Epoch 307/4000, Step 18, d_loss: 0.39087557792663574, g_loss: 6.083471775054932\n","Epoch 307/4000, Step 19, d_loss: 0.37135326862335205, g_loss: 4.818361759185791\n","Epoch 307/4000, Step 20, d_loss: 0.35478049516677856, g_loss: 5.655324459075928\n","Epoch 307/4000, Step 21, d_loss: 0.3701639771461487, g_loss: 5.036738872528076\n","Epoch 307/4000, Step 22, d_loss: 0.3425493538379669, g_loss: 5.82022762298584\n","Epoch 307/4000, Step 23, d_loss: 0.3418388366699219, g_loss: 6.2228593826293945\n","Epoch 307/4000, Step 24, d_loss: 0.3382963538169861, g_loss: 6.164364337921143\n","Epoch 307/4000, Step 25, d_loss: 0.3512577414512634, g_loss: 6.336583614349365\n","Epoch 307/4000, Step 26, d_loss: 0.3419572710990906, g_loss: 5.982076644897461\n","Epoch 307/4000, Step 27, d_loss: 0.3402562737464905, g_loss: 6.35839319229126\n","Epoch 307/4000, Step 28, d_loss: 0.34736889600753784, g_loss: 6.460540771484375\n","Epoch 307/4000, Step 29, d_loss: 0.3389187157154083, g_loss: 5.4763264656066895\n","Epoch 307/4000, Step 30, d_loss: 0.34270429611206055, g_loss: 8.111525535583496\n","Epoch 307/4000, Step 31, d_loss: 0.3433668315410614, g_loss: 5.803859233856201\n","Epoch 307/4000, Step 32, d_loss: 0.34505486488342285, g_loss: 6.003595352172852\n","Epoch 307/4000, Step 33, d_loss: 0.3397878110408783, g_loss: 5.604165077209473\n","Epoch 307/4000, Step 34, d_loss: 0.3431398570537567, g_loss: 5.696079254150391\n","Epoch 307/4000, Step 35, d_loss: 0.33328327536582947, g_loss: 5.364587783813477\n","Epoch 307/4000, Step 36, d_loss: 0.34522297978401184, g_loss: 5.626725673675537\n","Epoch 307/4000, Step 37, d_loss: 0.3387393355369568, g_loss: 9.005107879638672\n","Epoch 307/4000, Step 38, d_loss: 0.33638015389442444, g_loss: 5.695001602172852\n","Epoch 307/4000, Step 39, d_loss: 0.35432326793670654, g_loss: 4.848732948303223\n","Epoch 307/4000, Step 40, d_loss: 0.3457343578338623, g_loss: 5.79028844833374\n","Epoch 307/4000, Step 41, d_loss: 0.3355516493320465, g_loss: 5.918984413146973\n","Epoch 307/4000, Step 42, d_loss: 0.3469066023826599, g_loss: 5.0669426918029785\n","Epoch 307/4000, Step 43, d_loss: 0.3392984867095947, g_loss: 4.730498313903809\n","Epoch 307/4000, Step 44, d_loss: 0.3432875871658325, g_loss: 5.601546764373779\n","Epoch 307/4000, Step 45, d_loss: 0.34247690439224243, g_loss: 5.949648380279541\n","Epoch 307/4000, Step 46, d_loss: 0.3351956605911255, g_loss: 5.870599746704102\n","Epoch 307/4000, Step 47, d_loss: 0.33557558059692383, g_loss: 5.427064895629883\n","Epoch 307/4000, Step 48, d_loss: 0.3412828743457794, g_loss: 5.4818243980407715\n","Epoch 307/4000, Step 49, d_loss: 0.33926597237586975, g_loss: 6.797662734985352\n","Epoch 307/4000, Step 50, d_loss: 0.3321656584739685, g_loss: 5.037179470062256\n","Epoch 307/4000, Step 51, d_loss: 0.33387577533721924, g_loss: 7.312318325042725\n","Epoch 307/4000, Step 52, d_loss: 0.33601194620132446, g_loss: 6.380946636199951\n","Epoch 307/4000, Step 53, d_loss: 0.33384281396865845, g_loss: 7.243265628814697\n","Epoch 307/4000, Step 54, d_loss: 0.33920177817344666, g_loss: 5.3837456703186035\n","Epoch 307/4000, Step 55, d_loss: 0.33604830503463745, g_loss: 6.431099891662598\n","Epoch 307/4000, Step 56, d_loss: 0.3481687307357788, g_loss: 5.841856002807617\n","Epoch 307/4000, Step 57, d_loss: 0.3417854309082031, g_loss: 5.963650226593018\n","Epoch 307/4000, Step 58, d_loss: 0.33622658252716064, g_loss: 5.704742431640625\n","Epoch 307/4000, Step 59, d_loss: 0.34594300389289856, g_loss: 5.57805061340332\n","Epoch 307/4000, Step 60, d_loss: 0.3396598696708679, g_loss: 4.564574241638184\n","Epoch 307/4000, Step 61, d_loss: 0.3357633352279663, g_loss: 5.514270305633545\n","Epoch 307/4000, Step 62, d_loss: 0.3439081907272339, g_loss: 6.334070682525635\n","Epoch 307/4000, Step 63, d_loss: 0.3373858332633972, g_loss: 5.891696929931641\n","Epoch 307/4000, Step 64, d_loss: 0.3381311893463135, g_loss: 6.2086029052734375\n","Epoch 307/4000, Step 65, d_loss: 0.34089410305023193, g_loss: 6.114181995391846\n","Epoch 307/4000, Step 66, d_loss: 0.3386503756046295, g_loss: 6.013218879699707\n","Epoch 307/4000, Step 67, d_loss: 0.33804070949554443, g_loss: 5.9288763999938965\n","Epoch 307/4000, Step 68, d_loss: 0.336490660905838, g_loss: 5.699904441833496\n","Epoch 307/4000, Step 69, d_loss: 0.3317027688026428, g_loss: 5.655076503753662\n","Epoch 307/4000, Step 70, d_loss: 0.3380669057369232, g_loss: 7.00427770614624\n","Epoch 307/4000, Step 71, d_loss: 0.34170758724212646, g_loss: 6.128173351287842\n","Epoch 307/4000, Step 72, d_loss: 0.33399951457977295, g_loss: 6.236855506896973\n","Epoch 307/4000, Step 73, d_loss: 0.34134936332702637, g_loss: 5.463437557220459\n","Epoch 307/4000, Step 74, d_loss: 0.35670745372772217, g_loss: 5.813345432281494\n","Epoch 307/4000, Step 75, d_loss: 0.33655598759651184, g_loss: 5.774595260620117\n","Epoch 307/4000, Step 76, d_loss: 0.3373284935951233, g_loss: 5.436300754547119\n","Epoch 307/4000, Step 77, d_loss: 0.33444395661354065, g_loss: 6.013345718383789\n","Epoch 307/4000, Step 78, d_loss: 0.3419097065925598, g_loss: 4.834981441497803\n","Epoch 307/4000, Step 79, d_loss: 0.3854256272315979, g_loss: 4.862302780151367\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 308/4000, Step 1, d_loss: 0.36387214064598083, g_loss: 6.020148277282715\n","Epoch 308/4000, Step 2, d_loss: 0.3744939863681793, g_loss: 3.013472080230713\n","Epoch 308/4000, Step 3, d_loss: 0.35906508564949036, g_loss: 6.359694957733154\n","Epoch 308/4000, Step 4, d_loss: 0.39254093170166016, g_loss: 4.711675643920898\n","Epoch 308/4000, Step 5, d_loss: 0.3500938415527344, g_loss: 4.368428707122803\n","Epoch 308/4000, Step 6, d_loss: 0.3486837148666382, g_loss: 4.682981967926025\n","Epoch 308/4000, Step 7, d_loss: 0.3467957675457001, g_loss: 5.009012699127197\n","Epoch 308/4000, Step 8, d_loss: 0.3448758125305176, g_loss: 6.94295072555542\n","Epoch 308/4000, Step 9, d_loss: 0.34440627694129944, g_loss: 4.7712202072143555\n","Epoch 308/4000, Step 10, d_loss: 0.34929656982421875, g_loss: 5.895862102508545\n","Epoch 308/4000, Step 11, d_loss: 0.33442410826683044, g_loss: 6.792670726776123\n","Epoch 308/4000, Step 12, d_loss: 0.3470887839794159, g_loss: 8.407422065734863\n","Epoch 308/4000, Step 13, d_loss: 0.35320013761520386, g_loss: 5.457554817199707\n","Epoch 308/4000, Step 14, d_loss: 0.35206812620162964, g_loss: 7.738958835601807\n","Epoch 308/4000, Step 15, d_loss: 0.3728417158126831, g_loss: 5.9837646484375\n","Epoch 308/4000, Step 16, d_loss: 0.34956255555152893, g_loss: 4.685159683227539\n","Epoch 308/4000, Step 17, d_loss: 0.3416752517223358, g_loss: 4.238490581512451\n","Epoch 308/4000, Step 18, d_loss: 0.3475869297981262, g_loss: 6.546595573425293\n","Epoch 308/4000, Step 19, d_loss: 0.36297643184661865, g_loss: 5.096005916595459\n","Epoch 308/4000, Step 20, d_loss: 0.33850330114364624, g_loss: 3.51149582862854\n","Epoch 308/4000, Step 21, d_loss: 0.3371451199054718, g_loss: 6.446797847747803\n","Epoch 308/4000, Step 22, d_loss: 0.3421093225479126, g_loss: 5.74539041519165\n","Epoch 308/4000, Step 23, d_loss: 0.3424932658672333, g_loss: 5.11750602722168\n","Epoch 308/4000, Step 24, d_loss: 0.3500019609928131, g_loss: 5.890172004699707\n","Epoch 308/4000, Step 25, d_loss: 0.3369379937648773, g_loss: 8.888818740844727\n","Epoch 308/4000, Step 26, d_loss: 0.33226674795150757, g_loss: 3.7884511947631836\n","Epoch 308/4000, Step 27, d_loss: 0.34938156604766846, g_loss: 5.476324558258057\n","Epoch 308/4000, Step 28, d_loss: 0.3519284427165985, g_loss: 4.772563934326172\n","Epoch 308/4000, Step 29, d_loss: 0.34636297821998596, g_loss: 4.526673316955566\n","Epoch 308/4000, Step 30, d_loss: 0.33785614371299744, g_loss: 4.911684513092041\n","Epoch 308/4000, Step 31, d_loss: 0.34171727299690247, g_loss: 5.020413875579834\n","Epoch 308/4000, Step 32, d_loss: 0.3585037589073181, g_loss: 5.254533767700195\n","Epoch 308/4000, Step 33, d_loss: 0.34766891598701477, g_loss: 4.951576232910156\n","Epoch 308/4000, Step 34, d_loss: 0.3521340489387512, g_loss: 5.10734748840332\n","Epoch 308/4000, Step 35, d_loss: 0.3333282768726349, g_loss: 4.980262279510498\n","Epoch 308/4000, Step 36, d_loss: 0.33629319071769714, g_loss: 8.597273826599121\n","Epoch 308/4000, Step 37, d_loss: 0.3396165370941162, g_loss: 5.4308695793151855\n","Epoch 308/4000, Step 38, d_loss: 0.34621721506118774, g_loss: 4.56233024597168\n","Epoch 308/4000, Step 39, d_loss: 0.3477080166339874, g_loss: 4.934232711791992\n","Epoch 308/4000, Step 40, d_loss: 0.33761513233184814, g_loss: 4.104126930236816\n","Epoch 308/4000, Step 41, d_loss: 0.34545278549194336, g_loss: 9.824544906616211\n","Epoch 308/4000, Step 42, d_loss: 0.3401036560535431, g_loss: 4.640785217285156\n","Epoch 308/4000, Step 43, d_loss: 0.35117053985595703, g_loss: 5.599915504455566\n","Epoch 308/4000, Step 44, d_loss: 0.33905383944511414, g_loss: 5.960281848907471\n","Epoch 308/4000, Step 45, d_loss: 0.35096868872642517, g_loss: 7.299782752990723\n","Epoch 308/4000, Step 46, d_loss: 0.3421209454536438, g_loss: 5.190095901489258\n","Epoch 308/4000, Step 47, d_loss: 0.3394513428211212, g_loss: 9.317224502563477\n","Epoch 308/4000, Step 48, d_loss: 0.3390478789806366, g_loss: 5.3033246994018555\n","Epoch 308/4000, Step 49, d_loss: 0.3322235941886902, g_loss: 4.9309306144714355\n","Epoch 308/4000, Step 50, d_loss: 0.3354956805706024, g_loss: 5.641936302185059\n","Epoch 308/4000, Step 51, d_loss: 0.3713861107826233, g_loss: 8.405511856079102\n","Epoch 308/4000, Step 52, d_loss: 0.35312533378601074, g_loss: 5.889726161956787\n","Epoch 308/4000, Step 53, d_loss: 0.3458814024925232, g_loss: 5.446934700012207\n","Epoch 308/4000, Step 54, d_loss: 0.33441078662872314, g_loss: 8.147055625915527\n","Epoch 308/4000, Step 55, d_loss: 0.336544007062912, g_loss: 5.440708637237549\n","Epoch 308/4000, Step 56, d_loss: 0.3486633896827698, g_loss: 5.9333109855651855\n","Epoch 308/4000, Step 57, d_loss: 0.3418789803981781, g_loss: 7.134671688079834\n","Epoch 308/4000, Step 58, d_loss: 0.33929967880249023, g_loss: 7.60421895980835\n","Epoch 308/4000, Step 59, d_loss: 0.3329122066497803, g_loss: 5.5165581703186035\n","Epoch 308/4000, Step 60, d_loss: 0.34994420409202576, g_loss: 5.212945461273193\n","Epoch 308/4000, Step 61, d_loss: 0.34812089800834656, g_loss: 5.302301406860352\n","Epoch 308/4000, Step 62, d_loss: 0.3426680862903595, g_loss: 6.442917346954346\n","Epoch 308/4000, Step 63, d_loss: 0.34065473079681396, g_loss: 4.978804111480713\n","Epoch 308/4000, Step 64, d_loss: 0.3424398899078369, g_loss: 7.315676689147949\n","Epoch 308/4000, Step 65, d_loss: 0.33842921257019043, g_loss: 6.328989028930664\n","Epoch 308/4000, Step 66, d_loss: 0.33397457003593445, g_loss: 6.2173542976379395\n","Epoch 308/4000, Step 67, d_loss: 0.3451347351074219, g_loss: 4.989912509918213\n","Epoch 308/4000, Step 68, d_loss: 0.38499879837036133, g_loss: 5.216975212097168\n","Epoch 308/4000, Step 69, d_loss: 0.3527306020259857, g_loss: 8.173921585083008\n","Epoch 308/4000, Step 70, d_loss: 0.3458539843559265, g_loss: 6.943438529968262\n","Epoch 308/4000, Step 71, d_loss: 0.34674251079559326, g_loss: 4.144468307495117\n","Epoch 308/4000, Step 72, d_loss: 0.3448166847229004, g_loss: 4.912613868713379\n","Epoch 308/4000, Step 73, d_loss: 0.3531152307987213, g_loss: 4.979473114013672\n","Epoch 308/4000, Step 74, d_loss: 0.3628171682357788, g_loss: 5.018248081207275\n","Epoch 308/4000, Step 75, d_loss: 0.3438672423362732, g_loss: 4.8013596534729\n","Epoch 308/4000, Step 76, d_loss: 0.33536478877067566, g_loss: 6.038556098937988\n","Epoch 308/4000, Step 77, d_loss: 0.3337130546569824, g_loss: 7.915284156799316\n","Epoch 308/4000, Step 78, d_loss: 0.3375794291496277, g_loss: 5.815398216247559\n","Epoch 308/4000, Step 79, d_loss: 0.8875082731246948, g_loss: 4.314352512359619\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 309/4000, Step 1, d_loss: 0.3549138009548187, g_loss: 3.9672114849090576\n","Epoch 309/4000, Step 2, d_loss: 0.45493969321250916, g_loss: 3.1411702632904053\n","Epoch 309/4000, Step 3, d_loss: 0.48601752519607544, g_loss: 3.509770631790161\n","Epoch 309/4000, Step 4, d_loss: 0.4556896388530731, g_loss: 3.701986789703369\n","Epoch 309/4000, Step 5, d_loss: 0.4997558891773224, g_loss: 3.4825170040130615\n","Epoch 309/4000, Step 6, d_loss: 0.41390323638916016, g_loss: 5.136645317077637\n","Epoch 309/4000, Step 7, d_loss: 0.3872140347957611, g_loss: 7.638274669647217\n","Epoch 309/4000, Step 8, d_loss: 0.3574025630950928, g_loss: 7.9505720138549805\n","Epoch 309/4000, Step 9, d_loss: 0.3463052213191986, g_loss: 9.20154094696045\n","Epoch 309/4000, Step 10, d_loss: 0.4174973964691162, g_loss: 7.214155673980713\n","Epoch 309/4000, Step 11, d_loss: 0.3941822052001953, g_loss: 7.353656768798828\n","Epoch 309/4000, Step 12, d_loss: 0.3484959006309509, g_loss: 7.616441249847412\n","Epoch 309/4000, Step 13, d_loss: 0.3872540593147278, g_loss: 6.700337886810303\n","Epoch 309/4000, Step 14, d_loss: 0.33926647901535034, g_loss: 5.924293518066406\n","Epoch 309/4000, Step 15, d_loss: 0.3565303683280945, g_loss: 5.872452259063721\n","Epoch 309/4000, Step 16, d_loss: 0.3731256127357483, g_loss: 5.282834529876709\n","Epoch 309/4000, Step 17, d_loss: 0.37296953797340393, g_loss: 5.068629741668701\n","Epoch 309/4000, Step 18, d_loss: 0.37586310505867004, g_loss: 5.341643333435059\n","Epoch 309/4000, Step 19, d_loss: 0.40009593963623047, g_loss: 6.293186664581299\n","Epoch 309/4000, Step 20, d_loss: 0.3550451993942261, g_loss: 6.837912082672119\n","Epoch 309/4000, Step 21, d_loss: 0.3591586947441101, g_loss: 8.029696464538574\n","Epoch 309/4000, Step 22, d_loss: 0.3400192856788635, g_loss: 8.660483360290527\n","Epoch 309/4000, Step 23, d_loss: 0.3404823839664459, g_loss: 7.204250812530518\n","Epoch 309/4000, Step 24, d_loss: 0.46070465445518494, g_loss: 8.66668701171875\n","Epoch 309/4000, Step 25, d_loss: 0.3896731436252594, g_loss: 6.609654903411865\n","Epoch 309/4000, Step 26, d_loss: 0.33943119645118713, g_loss: 5.709662914276123\n","Epoch 309/4000, Step 27, d_loss: 0.35100701451301575, g_loss: 5.340656757354736\n","Epoch 309/4000, Step 28, d_loss: 0.36034300923347473, g_loss: 7.484984874725342\n","Epoch 309/4000, Step 29, d_loss: 0.36099669337272644, g_loss: 5.849339485168457\n","Epoch 309/4000, Step 30, d_loss: 0.381210595369339, g_loss: 7.2189040184021\n","Epoch 309/4000, Step 31, d_loss: 0.36346250772476196, g_loss: 5.974870681762695\n","Epoch 309/4000, Step 32, d_loss: 0.3491693139076233, g_loss: 7.637848854064941\n","Epoch 309/4000, Step 33, d_loss: 0.35002270340919495, g_loss: 6.350976943969727\n","Epoch 309/4000, Step 34, d_loss: 0.3363592028617859, g_loss: 7.246547698974609\n","Epoch 309/4000, Step 35, d_loss: 0.3365629315376282, g_loss: 9.078400611877441\n","Epoch 309/4000, Step 36, d_loss: 0.34354889392852783, g_loss: 7.48895788192749\n","Epoch 309/4000, Step 37, d_loss: 0.3774246573448181, g_loss: 6.169193267822266\n","Epoch 309/4000, Step 38, d_loss: 0.3918555974960327, g_loss: 6.035399436950684\n","Epoch 309/4000, Step 39, d_loss: 0.3379499614238739, g_loss: 7.08705472946167\n","Epoch 309/4000, Step 40, d_loss: 0.340157687664032, g_loss: 5.847580432891846\n","Epoch 309/4000, Step 41, d_loss: 0.34991008043289185, g_loss: 4.719803810119629\n","Epoch 309/4000, Step 42, d_loss: 0.36400362849235535, g_loss: 5.858591556549072\n","Epoch 309/4000, Step 43, d_loss: 0.3523311913013458, g_loss: 9.675292015075684\n","Epoch 309/4000, Step 44, d_loss: 0.3573467433452606, g_loss: 4.653388977050781\n","Epoch 309/4000, Step 45, d_loss: 0.3539998531341553, g_loss: 5.819339275360107\n","Epoch 309/4000, Step 46, d_loss: 0.3521121144294739, g_loss: 5.79116678237915\n","Epoch 309/4000, Step 47, d_loss: 0.33633482456207275, g_loss: 6.577542304992676\n","Epoch 309/4000, Step 48, d_loss: 0.34841328859329224, g_loss: 7.171080589294434\n","Epoch 309/4000, Step 49, d_loss: 0.3372085988521576, g_loss: 6.481489181518555\n","Epoch 309/4000, Step 50, d_loss: 0.3390853703022003, g_loss: 6.522405624389648\n","Epoch 309/4000, Step 51, d_loss: 0.3358239233493805, g_loss: 6.588506698608398\n","Epoch 309/4000, Step 52, d_loss: 0.3295561373233795, g_loss: 6.415788173675537\n","Epoch 309/4000, Step 53, d_loss: 0.33540210127830505, g_loss: 8.213728904724121\n","Epoch 309/4000, Step 54, d_loss: 0.350132018327713, g_loss: 5.731232643127441\n","Epoch 309/4000, Step 55, d_loss: 0.33915916085243225, g_loss: 5.745299339294434\n","Epoch 309/4000, Step 56, d_loss: 0.3340078294277191, g_loss: 6.249690055847168\n","Epoch 309/4000, Step 57, d_loss: 0.34017330408096313, g_loss: 5.607808589935303\n","Epoch 309/4000, Step 58, d_loss: 0.341470867395401, g_loss: 5.234005928039551\n","Epoch 309/4000, Step 59, d_loss: 0.3341497480869293, g_loss: 4.687385559082031\n","Epoch 309/4000, Step 60, d_loss: 0.3464539349079132, g_loss: 5.131182670593262\n","Epoch 309/4000, Step 61, d_loss: 0.3398595452308655, g_loss: 5.252778053283691\n","Epoch 309/4000, Step 62, d_loss: 0.3401262164115906, g_loss: 5.442448616027832\n","Epoch 309/4000, Step 63, d_loss: 0.33975592255592346, g_loss: 5.464073657989502\n","Epoch 309/4000, Step 64, d_loss: 0.3336380422115326, g_loss: 7.041560649871826\n","Epoch 309/4000, Step 65, d_loss: 0.3644232451915741, g_loss: 7.245145320892334\n","Epoch 309/4000, Step 66, d_loss: 0.3347129225730896, g_loss: 7.099270343780518\n","Epoch 309/4000, Step 67, d_loss: 0.3431466519832611, g_loss: 4.810971736907959\n","Epoch 309/4000, Step 68, d_loss: 0.33959484100341797, g_loss: 5.088322639465332\n","Epoch 309/4000, Step 69, d_loss: 0.3617384135723114, g_loss: 5.139886379241943\n","Epoch 309/4000, Step 70, d_loss: 0.3323172926902771, g_loss: 5.265531539916992\n","Epoch 309/4000, Step 71, d_loss: 0.34142065048217773, g_loss: 4.342642784118652\n","Epoch 309/4000, Step 72, d_loss: 0.34970372915267944, g_loss: 5.171226501464844\n","Epoch 309/4000, Step 73, d_loss: 0.33784976601600647, g_loss: 4.992121696472168\n","Epoch 309/4000, Step 74, d_loss: 0.33811742067337036, g_loss: 8.042853355407715\n","Epoch 309/4000, Step 75, d_loss: 0.3511350154876709, g_loss: 7.000063896179199\n","Epoch 309/4000, Step 76, d_loss: 0.3478764295578003, g_loss: 5.258653163909912\n","Epoch 309/4000, Step 77, d_loss: 0.3933100402355194, g_loss: 6.426908493041992\n","Epoch 309/4000, Step 78, d_loss: 0.33386778831481934, g_loss: 5.794360160827637\n","Epoch 309/4000, Step 79, d_loss: 0.34907376766204834, g_loss: 5.591777324676514\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 310/4000, Step 1, d_loss: 0.33836179971694946, g_loss: 4.895824909210205\n","Epoch 310/4000, Step 2, d_loss: 0.35154110193252563, g_loss: 5.02191162109375\n","Epoch 310/4000, Step 3, d_loss: 0.35106831789016724, g_loss: 5.174869537353516\n","Epoch 310/4000, Step 4, d_loss: 0.3434043824672699, g_loss: 4.757319927215576\n","Epoch 310/4000, Step 5, d_loss: 0.33728572726249695, g_loss: 4.4603681564331055\n","Epoch 310/4000, Step 6, d_loss: 0.35361069440841675, g_loss: 5.212717533111572\n","Epoch 310/4000, Step 7, d_loss: 0.3380226790904999, g_loss: 6.895459175109863\n","Epoch 310/4000, Step 8, d_loss: 0.3582221567630768, g_loss: 5.219578742980957\n","Epoch 310/4000, Step 9, d_loss: 0.3740575313568115, g_loss: 6.374429225921631\n","Epoch 310/4000, Step 10, d_loss: 0.33539894223213196, g_loss: 4.976707935333252\n","Epoch 310/4000, Step 11, d_loss: 0.33331698179244995, g_loss: 4.772604465484619\n","Epoch 310/4000, Step 12, d_loss: 0.3312639892101288, g_loss: 6.566020965576172\n","Epoch 310/4000, Step 13, d_loss: 0.33937227725982666, g_loss: 7.291827201843262\n","Epoch 310/4000, Step 14, d_loss: 0.3339903652667999, g_loss: 6.7407612800598145\n","Epoch 310/4000, Step 15, d_loss: 0.34469273686408997, g_loss: 6.608640193939209\n","Epoch 310/4000, Step 16, d_loss: 0.33639904856681824, g_loss: 6.496127128601074\n","Epoch 310/4000, Step 17, d_loss: 0.33257001638412476, g_loss: 7.168282985687256\n","Epoch 310/4000, Step 18, d_loss: 0.3341710567474365, g_loss: 6.240413665771484\n","Epoch 310/4000, Step 19, d_loss: 0.336436927318573, g_loss: 6.331607818603516\n","Epoch 310/4000, Step 20, d_loss: 0.3410171866416931, g_loss: 5.541724681854248\n","Epoch 310/4000, Step 21, d_loss: 0.33559122681617737, g_loss: 5.820074081420898\n","Epoch 310/4000, Step 22, d_loss: 0.3371332883834839, g_loss: 7.125490188598633\n","Epoch 310/4000, Step 23, d_loss: 0.3328741192817688, g_loss: 5.777539253234863\n","Epoch 310/4000, Step 24, d_loss: 0.3307613432407379, g_loss: 6.797240257263184\n","Epoch 310/4000, Step 25, d_loss: 0.35503479838371277, g_loss: 6.700767517089844\n","Epoch 310/4000, Step 26, d_loss: 0.34851306676864624, g_loss: 6.345508575439453\n","Epoch 310/4000, Step 27, d_loss: 0.3323145806789398, g_loss: 5.940398693084717\n","Epoch 310/4000, Step 28, d_loss: 0.33505889773368835, g_loss: 5.991368293762207\n","Epoch 310/4000, Step 29, d_loss: 0.33789485692977905, g_loss: 5.975229263305664\n","Epoch 310/4000, Step 30, d_loss: 0.3398423194885254, g_loss: 8.04524040222168\n","Epoch 310/4000, Step 31, d_loss: 0.3382173776626587, g_loss: 5.332180023193359\n","Epoch 310/4000, Step 32, d_loss: 0.337919145822525, g_loss: 5.383362770080566\n","Epoch 310/4000, Step 33, d_loss: 0.3486415445804596, g_loss: 7.7270917892456055\n","Epoch 310/4000, Step 34, d_loss: 0.3388453423976898, g_loss: 5.930411338806152\n","Epoch 310/4000, Step 35, d_loss: 0.33710917830467224, g_loss: 5.954562664031982\n","Epoch 310/4000, Step 36, d_loss: 0.3456180691719055, g_loss: 5.48241662979126\n","Epoch 310/4000, Step 37, d_loss: 0.3330244719982147, g_loss: 5.817116737365723\n","Epoch 310/4000, Step 38, d_loss: 0.3430114686489105, g_loss: 6.114800453186035\n","Epoch 310/4000, Step 39, d_loss: 0.33860814571380615, g_loss: 5.579901218414307\n","Epoch 310/4000, Step 40, d_loss: 0.3509560227394104, g_loss: 9.921656608581543\n","Epoch 310/4000, Step 41, d_loss: 0.3387654423713684, g_loss: 6.662317752838135\n","Epoch 310/4000, Step 42, d_loss: 0.33612480759620667, g_loss: 4.608646869659424\n","Epoch 310/4000, Step 43, d_loss: 0.3443661630153656, g_loss: 6.3723554611206055\n","Epoch 310/4000, Step 44, d_loss: 0.33612146973609924, g_loss: 5.349830627441406\n","Epoch 310/4000, Step 45, d_loss: 0.35403358936309814, g_loss: 4.98535680770874\n","Epoch 310/4000, Step 46, d_loss: 0.3386485278606415, g_loss: 5.31489896774292\n","Epoch 310/4000, Step 47, d_loss: 0.33778345584869385, g_loss: 5.163926124572754\n","Epoch 310/4000, Step 48, d_loss: 0.3404322564601898, g_loss: 4.851184844970703\n","Epoch 310/4000, Step 49, d_loss: 0.335422158241272, g_loss: 5.646912574768066\n","Epoch 310/4000, Step 50, d_loss: 0.3362935781478882, g_loss: 5.251952171325684\n","Epoch 310/4000, Step 51, d_loss: 0.3444865942001343, g_loss: 4.96173620223999\n","Epoch 310/4000, Step 52, d_loss: 0.338268905878067, g_loss: 4.804566860198975\n","Epoch 310/4000, Step 53, d_loss: 0.3850152790546417, g_loss: 5.96975564956665\n","Epoch 310/4000, Step 54, d_loss: 0.3333192467689514, g_loss: 4.575766086578369\n","Epoch 310/4000, Step 55, d_loss: 0.3447568416595459, g_loss: 8.39400863647461\n","Epoch 310/4000, Step 56, d_loss: 0.355056494474411, g_loss: 8.562701225280762\n","Epoch 310/4000, Step 57, d_loss: 0.368703693151474, g_loss: 4.954090118408203\n","Epoch 310/4000, Step 58, d_loss: 0.3445640504360199, g_loss: 6.780452728271484\n","Epoch 310/4000, Step 59, d_loss: 0.34047505259513855, g_loss: 5.8522868156433105\n","Epoch 310/4000, Step 60, d_loss: 0.34092503786087036, g_loss: 6.852800369262695\n","Epoch 310/4000, Step 61, d_loss: 0.36136889457702637, g_loss: 6.31505823135376\n","Epoch 310/4000, Step 62, d_loss: 0.3323051333427429, g_loss: 7.550607204437256\n","Epoch 310/4000, Step 63, d_loss: 0.33848679065704346, g_loss: 6.497438430786133\n","Epoch 310/4000, Step 64, d_loss: 0.345477819442749, g_loss: 4.8936638832092285\n","Epoch 310/4000, Step 65, d_loss: 0.33622118830680847, g_loss: 5.16565465927124\n","Epoch 310/4000, Step 66, d_loss: 0.3463323712348938, g_loss: 6.954036712646484\n","Epoch 310/4000, Step 67, d_loss: 0.3482683598995209, g_loss: 5.045162677764893\n","Epoch 310/4000, Step 68, d_loss: 0.3461330831050873, g_loss: 8.236193656921387\n","Epoch 310/4000, Step 69, d_loss: 0.3418115973472595, g_loss: 5.849600791931152\n","Epoch 310/4000, Step 70, d_loss: 0.34527456760406494, g_loss: 5.218316555023193\n","Epoch 310/4000, Step 71, d_loss: 0.3401821255683899, g_loss: 5.6698174476623535\n","Epoch 310/4000, Step 72, d_loss: 0.3363342881202698, g_loss: 5.108896255493164\n","Epoch 310/4000, Step 73, d_loss: 0.3387546241283417, g_loss: 7.564934253692627\n","Epoch 310/4000, Step 74, d_loss: 0.3437480926513672, g_loss: 5.827708721160889\n","Epoch 310/4000, Step 75, d_loss: 0.34466928243637085, g_loss: 6.104987144470215\n","Epoch 310/4000, Step 76, d_loss: 0.34962281584739685, g_loss: 4.509430408477783\n","Epoch 310/4000, Step 77, d_loss: 0.33884647488594055, g_loss: 5.152431011199951\n","Epoch 310/4000, Step 78, d_loss: 0.3361063599586487, g_loss: 6.143887996673584\n","Epoch 310/4000, Step 79, d_loss: 0.8687939643859863, g_loss: 5.646683692932129\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 311/4000, Step 1, d_loss: 0.38019731640815735, g_loss: 3.918768882751465\n","Epoch 311/4000, Step 2, d_loss: 0.4464988708496094, g_loss: 3.3902342319488525\n","Epoch 311/4000, Step 3, d_loss: 0.46968093514442444, g_loss: 5.629896640777588\n","Epoch 311/4000, Step 4, d_loss: 0.5244233012199402, g_loss: 5.480735778808594\n","Epoch 311/4000, Step 5, d_loss: 0.48447129130363464, g_loss: 4.5842509269714355\n","Epoch 311/4000, Step 6, d_loss: 0.46591222286224365, g_loss: 4.5052361488342285\n","Epoch 311/4000, Step 7, d_loss: 0.45375820994377136, g_loss: 4.442131519317627\n","Epoch 311/4000, Step 8, d_loss: 0.41484442353248596, g_loss: 4.012874126434326\n","Epoch 311/4000, Step 9, d_loss: 0.44081351161003113, g_loss: 5.768247127532959\n","Epoch 311/4000, Step 10, d_loss: 0.3953157961368561, g_loss: 6.614874362945557\n","Epoch 311/4000, Step 11, d_loss: 0.37943941354751587, g_loss: 6.297131061553955\n","Epoch 311/4000, Step 12, d_loss: 0.38175123929977417, g_loss: 6.831484317779541\n","Epoch 311/4000, Step 13, d_loss: 0.4668189287185669, g_loss: 4.342220306396484\n","Epoch 311/4000, Step 14, d_loss: 0.3422478437423706, g_loss: 4.091607093811035\n","Epoch 311/4000, Step 15, d_loss: 0.35391858220100403, g_loss: 5.7939887046813965\n","Epoch 311/4000, Step 16, d_loss: 0.4018009901046753, g_loss: 6.399515151977539\n","Epoch 311/4000, Step 17, d_loss: 0.3985878527164459, g_loss: 7.338962554931641\n","Epoch 311/4000, Step 18, d_loss: 0.4227859079837799, g_loss: 4.187875270843506\n","Epoch 311/4000, Step 19, d_loss: 0.4008846879005432, g_loss: 2.9758214950561523\n","Epoch 311/4000, Step 20, d_loss: 0.3706487715244293, g_loss: 4.910232067108154\n","Epoch 311/4000, Step 21, d_loss: 0.36060377955436707, g_loss: 4.907904148101807\n","Epoch 311/4000, Step 22, d_loss: 0.3648090958595276, g_loss: 4.842175483703613\n","Epoch 311/4000, Step 23, d_loss: 0.3546268343925476, g_loss: 5.222790718078613\n","Epoch 311/4000, Step 24, d_loss: 0.3647010922431946, g_loss: 4.124826908111572\n","Epoch 311/4000, Step 25, d_loss: 0.35032495856285095, g_loss: 8.59296703338623\n","Epoch 311/4000, Step 26, d_loss: 0.35872045159339905, g_loss: 5.348165988922119\n","Epoch 311/4000, Step 27, d_loss: 0.3813212513923645, g_loss: 5.714667320251465\n","Epoch 311/4000, Step 28, d_loss: 0.3572089374065399, g_loss: 8.861370086669922\n","Epoch 311/4000, Step 29, d_loss: 0.3614664077758789, g_loss: 4.742554187774658\n","Epoch 311/4000, Step 30, d_loss: 0.3480542004108429, g_loss: 5.870070934295654\n","Epoch 311/4000, Step 31, d_loss: 0.3683899939060211, g_loss: 6.01533317565918\n","Epoch 311/4000, Step 32, d_loss: 0.33506104350090027, g_loss: 5.138172149658203\n","Epoch 311/4000, Step 33, d_loss: 0.34491264820098877, g_loss: 3.8252882957458496\n","Epoch 311/4000, Step 34, d_loss: 0.37607628107070923, g_loss: 3.346773147583008\n","Epoch 311/4000, Step 35, d_loss: 0.3417389690876007, g_loss: 5.047215461730957\n","Epoch 311/4000, Step 36, d_loss: 0.37248778343200684, g_loss: 5.905655384063721\n","Epoch 311/4000, Step 37, d_loss: 0.40264225006103516, g_loss: 6.220705032348633\n","Epoch 311/4000, Step 38, d_loss: 0.35170960426330566, g_loss: 7.213083744049072\n","Epoch 311/4000, Step 39, d_loss: 0.3510403633117676, g_loss: 4.603221893310547\n","Epoch 311/4000, Step 40, d_loss: 0.3854803740978241, g_loss: 5.7691473960876465\n","Epoch 311/4000, Step 41, d_loss: 0.34282156825065613, g_loss: 3.993252754211426\n","Epoch 311/4000, Step 42, d_loss: 0.36049434542655945, g_loss: 5.555662631988525\n","Epoch 311/4000, Step 43, d_loss: 0.3414449691772461, g_loss: 4.347673416137695\n","Epoch 311/4000, Step 44, d_loss: 0.33824193477630615, g_loss: 3.766413450241089\n","Epoch 311/4000, Step 45, d_loss: 0.34800487756729126, g_loss: 3.2749266624450684\n","Epoch 311/4000, Step 46, d_loss: 0.34702152013778687, g_loss: 5.1909565925598145\n","Epoch 311/4000, Step 47, d_loss: 0.34221193194389343, g_loss: 6.019543170928955\n","Epoch 311/4000, Step 48, d_loss: 0.3762240707874298, g_loss: 5.4101643562316895\n","Epoch 311/4000, Step 49, d_loss: 0.34667932987213135, g_loss: 5.7518720626831055\n","Epoch 311/4000, Step 50, d_loss: 0.3352663516998291, g_loss: 6.626810073852539\n","Epoch 311/4000, Step 51, d_loss: 0.35999658703804016, g_loss: 5.210257530212402\n","Epoch 311/4000, Step 52, d_loss: 0.3420751094818115, g_loss: 3.818162441253662\n","Epoch 311/4000, Step 53, d_loss: 0.3423871099948883, g_loss: 3.693892002105713\n","Epoch 311/4000, Step 54, d_loss: 0.34013649821281433, g_loss: 5.455531597137451\n","Epoch 311/4000, Step 55, d_loss: 0.3614644408226013, g_loss: 7.179240703582764\n","Epoch 311/4000, Step 56, d_loss: 0.3453010618686676, g_loss: 5.787913799285889\n","Epoch 311/4000, Step 57, d_loss: 0.3523606061935425, g_loss: 3.9382574558258057\n","Epoch 311/4000, Step 58, d_loss: 0.35708168148994446, g_loss: 5.3512492179870605\n","Epoch 311/4000, Step 59, d_loss: 0.34414076805114746, g_loss: 5.306144714355469\n","Epoch 311/4000, Step 60, d_loss: 0.34307536482810974, g_loss: 4.787583351135254\n","Epoch 311/4000, Step 61, d_loss: 0.3620634078979492, g_loss: 6.993467807769775\n","Epoch 311/4000, Step 62, d_loss: 0.34547916054725647, g_loss: 5.737669944763184\n","Epoch 311/4000, Step 63, d_loss: 0.34745728969573975, g_loss: 3.780785083770752\n","Epoch 311/4000, Step 64, d_loss: 0.3714744746685028, g_loss: 6.915841102600098\n","Epoch 311/4000, Step 65, d_loss: 0.34785357117652893, g_loss: 3.284912586212158\n","Epoch 311/4000, Step 66, d_loss: 0.3894549012184143, g_loss: 5.381205081939697\n","Epoch 311/4000, Step 67, d_loss: 0.36063605546951294, g_loss: 5.642234802246094\n","Epoch 311/4000, Step 68, d_loss: 0.3506879210472107, g_loss: 6.602858066558838\n","Epoch 311/4000, Step 69, d_loss: 0.3777261972427368, g_loss: 9.592838287353516\n","Epoch 311/4000, Step 70, d_loss: 0.41637536883354187, g_loss: 6.908802032470703\n","Epoch 311/4000, Step 71, d_loss: 0.34374144673347473, g_loss: 5.060005187988281\n","Epoch 311/4000, Step 72, d_loss: 0.35496729612350464, g_loss: 5.071291923522949\n","Epoch 311/4000, Step 73, d_loss: 0.3530050218105316, g_loss: 6.859448432922363\n","Epoch 311/4000, Step 74, d_loss: 0.35631316900253296, g_loss: 6.519813537597656\n","Epoch 311/4000, Step 75, d_loss: 0.3693155348300934, g_loss: 6.5985636711120605\n","Epoch 311/4000, Step 76, d_loss: 0.3539310395717621, g_loss: 6.796706676483154\n","Epoch 311/4000, Step 77, d_loss: 0.3467952013015747, g_loss: 6.471321105957031\n","Epoch 311/4000, Step 78, d_loss: 0.3421690762042999, g_loss: 5.218445777893066\n","Epoch 311/4000, Step 79, d_loss: 0.3516627252101898, g_loss: 4.935117244720459\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 312/4000, Step 1, d_loss: 0.3365599811077118, g_loss: 6.019737243652344\n","Epoch 312/4000, Step 2, d_loss: 0.3566782772541046, g_loss: 6.157751083374023\n","Epoch 312/4000, Step 3, d_loss: 0.3483249545097351, g_loss: 6.383161544799805\n","Epoch 312/4000, Step 4, d_loss: 0.3592885434627533, g_loss: 6.327261447906494\n","Epoch 312/4000, Step 5, d_loss: 0.33980482816696167, g_loss: 8.370638847351074\n","Epoch 312/4000, Step 6, d_loss: 0.35159486532211304, g_loss: 7.932002067565918\n","Epoch 312/4000, Step 7, d_loss: 0.3401060700416565, g_loss: 5.287271022796631\n","Epoch 312/4000, Step 8, d_loss: 0.33400940895080566, g_loss: 4.80244779586792\n","Epoch 312/4000, Step 9, d_loss: 0.3366270363330841, g_loss: 6.940328121185303\n","Epoch 312/4000, Step 10, d_loss: 0.33363884687423706, g_loss: 7.357353210449219\n","Epoch 312/4000, Step 11, d_loss: 0.3353712558746338, g_loss: 6.8322014808654785\n","Epoch 312/4000, Step 12, d_loss: 0.3368174731731415, g_loss: 5.347620964050293\n","Epoch 312/4000, Step 13, d_loss: 0.33556145429611206, g_loss: 5.139822483062744\n","Epoch 312/4000, Step 14, d_loss: 0.34022456407546997, g_loss: 8.483809471130371\n","Epoch 312/4000, Step 15, d_loss: 0.339150995016098, g_loss: 5.95718240737915\n","Epoch 312/4000, Step 16, d_loss: 0.3615870773792267, g_loss: 6.968803405761719\n","Epoch 312/4000, Step 17, d_loss: 0.34519490599632263, g_loss: 5.136002540588379\n","Epoch 312/4000, Step 18, d_loss: 0.3372644782066345, g_loss: 5.898099422454834\n","Epoch 312/4000, Step 19, d_loss: 0.3333892524242401, g_loss: 6.954163074493408\n","Epoch 312/4000, Step 20, d_loss: 0.33950918912887573, g_loss: 7.084947109222412\n","Epoch 312/4000, Step 21, d_loss: 0.35223278403282166, g_loss: 5.833674430847168\n","Epoch 312/4000, Step 22, d_loss: 0.3375891447067261, g_loss: 5.337522506713867\n","Epoch 312/4000, Step 23, d_loss: 0.3401603102684021, g_loss: 5.652055263519287\n","Epoch 312/4000, Step 24, d_loss: 0.3371540904045105, g_loss: 6.264924049377441\n","Epoch 312/4000, Step 25, d_loss: 0.3389957845211029, g_loss: 4.723485469818115\n","Epoch 312/4000, Step 26, d_loss: 0.3391815423965454, g_loss: 4.122589111328125\n","Epoch 312/4000, Step 27, d_loss: 0.34318941831588745, g_loss: 5.092695713043213\n","Epoch 312/4000, Step 28, d_loss: 0.33840131759643555, g_loss: 5.22767448425293\n","Epoch 312/4000, Step 29, d_loss: 0.3502519428730011, g_loss: 3.716686725616455\n","Epoch 312/4000, Step 30, d_loss: 0.33503544330596924, g_loss: 4.6407599449157715\n","Epoch 312/4000, Step 31, d_loss: 0.3412129878997803, g_loss: 5.996888637542725\n","Epoch 312/4000, Step 32, d_loss: 0.3859175145626068, g_loss: 4.933905601501465\n","Epoch 312/4000, Step 33, d_loss: 0.3304598927497864, g_loss: 4.284531116485596\n","Epoch 312/4000, Step 34, d_loss: 0.3705631494522095, g_loss: 4.12032413482666\n","Epoch 312/4000, Step 35, d_loss: 0.3306809067726135, g_loss: 5.14846134185791\n","Epoch 312/4000, Step 36, d_loss: 0.35453128814697266, g_loss: 6.525145053863525\n","Epoch 312/4000, Step 37, d_loss: 0.3335949182510376, g_loss: 9.053434371948242\n","Epoch 312/4000, Step 38, d_loss: 0.338024377822876, g_loss: 5.6697468757629395\n","Epoch 312/4000, Step 39, d_loss: 0.3363492786884308, g_loss: 5.2114973068237305\n","Epoch 312/4000, Step 40, d_loss: 0.3342229425907135, g_loss: 7.728378772735596\n","Epoch 312/4000, Step 41, d_loss: 0.33657434582710266, g_loss: 6.277937412261963\n","Epoch 312/4000, Step 42, d_loss: 0.36347079277038574, g_loss: 5.336609840393066\n","Epoch 312/4000, Step 43, d_loss: 0.33571165800094604, g_loss: 7.112767696380615\n","Epoch 312/4000, Step 44, d_loss: 0.3410414159297943, g_loss: 7.542991638183594\n","Epoch 312/4000, Step 45, d_loss: 0.3406892418861389, g_loss: 6.497506618499756\n","Epoch 312/4000, Step 46, d_loss: 0.37841469049453735, g_loss: 5.2331862449646\n","Epoch 312/4000, Step 47, d_loss: 0.34032967686653137, g_loss: 6.302789688110352\n","Epoch 312/4000, Step 48, d_loss: 0.3461328148841858, g_loss: 3.9759106636047363\n","Epoch 312/4000, Step 49, d_loss: 0.3497328460216522, g_loss: 6.195699214935303\n","Epoch 312/4000, Step 50, d_loss: 0.3340924382209778, g_loss: 6.422266006469727\n","Epoch 312/4000, Step 51, d_loss: 0.33490511775016785, g_loss: 7.053916931152344\n","Epoch 312/4000, Step 52, d_loss: 0.34654784202575684, g_loss: 6.293564796447754\n","Epoch 312/4000, Step 53, d_loss: 0.37008005380630493, g_loss: 6.775102615356445\n","Epoch 312/4000, Step 54, d_loss: 0.36805641651153564, g_loss: 7.958683490753174\n","Epoch 312/4000, Step 55, d_loss: 0.3639753460884094, g_loss: 4.3657426834106445\n","Epoch 312/4000, Step 56, d_loss: 0.33753371238708496, g_loss: 3.079864978790283\n","Epoch 312/4000, Step 57, d_loss: 0.3380594551563263, g_loss: 7.723546028137207\n","Epoch 312/4000, Step 58, d_loss: 0.3602519929409027, g_loss: 3.6606156826019287\n","Epoch 312/4000, Step 59, d_loss: 0.36898136138916016, g_loss: 5.635555744171143\n","Epoch 312/4000, Step 60, d_loss: 0.35208672285079956, g_loss: 6.493613243103027\n","Epoch 312/4000, Step 61, d_loss: 0.34506916999816895, g_loss: 5.097235679626465\n","Epoch 312/4000, Step 62, d_loss: 0.33728528022766113, g_loss: 5.7426958084106445\n","Epoch 312/4000, Step 63, d_loss: 0.34054264426231384, g_loss: 7.863239765167236\n","Epoch 312/4000, Step 64, d_loss: 0.3331788182258606, g_loss: 6.373288154602051\n","Epoch 312/4000, Step 65, d_loss: 0.34389880299568176, g_loss: 4.544571399688721\n","Epoch 312/4000, Step 66, d_loss: 0.33342844247817993, g_loss: 5.394636154174805\n","Epoch 312/4000, Step 67, d_loss: 0.3381996154785156, g_loss: 6.169590473175049\n","Epoch 312/4000, Step 68, d_loss: 0.3400464355945587, g_loss: 4.621968746185303\n","Epoch 312/4000, Step 69, d_loss: 0.3370973467826843, g_loss: 6.96052885055542\n","Epoch 312/4000, Step 70, d_loss: 0.36921945214271545, g_loss: 3.8738903999328613\n","Epoch 312/4000, Step 71, d_loss: 0.32990995049476624, g_loss: 5.689493656158447\n","Epoch 312/4000, Step 72, d_loss: 0.35255640745162964, g_loss: 6.032593250274658\n","Epoch 312/4000, Step 73, d_loss: 0.3387410342693329, g_loss: 7.9606242179870605\n","Epoch 312/4000, Step 74, d_loss: 0.3329768180847168, g_loss: 6.540565490722656\n","Epoch 312/4000, Step 75, d_loss: 0.3384232223033905, g_loss: 5.749521255493164\n","Epoch 312/4000, Step 76, d_loss: 0.3451654016971588, g_loss: 4.806709289550781\n","Epoch 312/4000, Step 77, d_loss: 0.33167028427124023, g_loss: 4.808106422424316\n","Epoch 312/4000, Step 78, d_loss: 0.33239567279815674, g_loss: 5.994690418243408\n","Epoch 312/4000, Step 79, d_loss: 1.1763962507247925, g_loss: 6.003133296966553\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 313/4000, Step 1, d_loss: 0.37446004152297974, g_loss: 2.749730110168457\n","Epoch 313/4000, Step 2, d_loss: 0.5318769812583923, g_loss: 4.102025985717773\n","Epoch 313/4000, Step 3, d_loss: 0.5148940682411194, g_loss: 3.8430371284484863\n","Epoch 313/4000, Step 4, d_loss: 0.5297836065292358, g_loss: 3.571869373321533\n","Epoch 313/4000, Step 5, d_loss: 0.5275798439979553, g_loss: 4.325082778930664\n","Epoch 313/4000, Step 6, d_loss: 0.45479294657707214, g_loss: 3.0620179176330566\n","Epoch 313/4000, Step 7, d_loss: 0.44673100113868713, g_loss: 3.9944210052490234\n","Epoch 313/4000, Step 8, d_loss: 0.5057402849197388, g_loss: 8.137335777282715\n","Epoch 313/4000, Step 9, d_loss: 0.4679359495639801, g_loss: 3.8559226989746094\n","Epoch 313/4000, Step 10, d_loss: 0.3660133481025696, g_loss: 7.083245754241943\n","Epoch 313/4000, Step 11, d_loss: 0.3657824695110321, g_loss: 7.473033428192139\n","Epoch 313/4000, Step 12, d_loss: 0.3587167263031006, g_loss: 4.512300968170166\n","Epoch 313/4000, Step 13, d_loss: 0.36124783754348755, g_loss: 9.15920352935791\n","Epoch 313/4000, Step 14, d_loss: 0.40712758898735046, g_loss: 5.10190486907959\n","Epoch 313/4000, Step 15, d_loss: 0.38973701000213623, g_loss: 4.603862762451172\n","Epoch 313/4000, Step 16, d_loss: 0.4409487545490265, g_loss: 7.429708003997803\n","Epoch 313/4000, Step 17, d_loss: 0.3994106352329254, g_loss: 3.9676365852355957\n","Epoch 313/4000, Step 18, d_loss: 0.3849502503871918, g_loss: 7.775801658630371\n","Epoch 313/4000, Step 19, d_loss: 0.36470404267311096, g_loss: 3.78594708442688\n","Epoch 313/4000, Step 20, d_loss: 0.3870992064476013, g_loss: 8.204610824584961\n","Epoch 313/4000, Step 21, d_loss: 0.3743087649345398, g_loss: 3.0304980278015137\n","Epoch 313/4000, Step 22, d_loss: 0.406801700592041, g_loss: 7.313527584075928\n","Epoch 313/4000, Step 23, d_loss: 0.36898887157440186, g_loss: 5.314032554626465\n","Epoch 313/4000, Step 24, d_loss: 0.37852567434310913, g_loss: 5.158253192901611\n","Epoch 313/4000, Step 25, d_loss: 0.4555228650569916, g_loss: 4.531086444854736\n","Epoch 313/4000, Step 26, d_loss: 0.36453357338905334, g_loss: 4.000964164733887\n","Epoch 313/4000, Step 27, d_loss: 0.3816385269165039, g_loss: 5.038745403289795\n","Epoch 313/4000, Step 28, d_loss: 0.35603925585746765, g_loss: 2.873391628265381\n","Epoch 313/4000, Step 29, d_loss: 0.3561399281024933, g_loss: 2.2219717502593994\n","Epoch 313/4000, Step 30, d_loss: 0.3861396312713623, g_loss: 2.105452060699463\n","Epoch 313/4000, Step 31, d_loss: 0.36716458201408386, g_loss: 4.445443153381348\n","Epoch 313/4000, Step 32, d_loss: 0.3845154643058777, g_loss: 5.962199687957764\n","Epoch 313/4000, Step 33, d_loss: 0.3843853175640106, g_loss: 5.569277286529541\n","Epoch 313/4000, Step 34, d_loss: 0.4676690101623535, g_loss: 2.6442127227783203\n","Epoch 313/4000, Step 35, d_loss: 0.4079968333244324, g_loss: 2.967169761657715\n","Epoch 313/4000, Step 36, d_loss: 0.4064256548881531, g_loss: 5.906912326812744\n","Epoch 313/4000, Step 37, d_loss: 0.359494149684906, g_loss: 3.6596736907958984\n","Epoch 313/4000, Step 38, d_loss: 0.364737331867218, g_loss: 3.610912561416626\n","Epoch 313/4000, Step 39, d_loss: 0.33301040530204773, g_loss: 5.33123254776001\n","Epoch 313/4000, Step 40, d_loss: 0.338188111782074, g_loss: 5.283796787261963\n","Epoch 313/4000, Step 41, d_loss: 0.34002062678337097, g_loss: 3.499084711074829\n","Epoch 313/4000, Step 42, d_loss: 0.35881635546684265, g_loss: 4.865938663482666\n","Epoch 313/4000, Step 43, d_loss: 0.43060439825057983, g_loss: 3.2024874687194824\n","Epoch 313/4000, Step 44, d_loss: 0.36984407901763916, g_loss: 3.9385836124420166\n","Epoch 313/4000, Step 45, d_loss: 0.35536253452301025, g_loss: 3.947490692138672\n","Epoch 313/4000, Step 46, d_loss: 0.34241917729377747, g_loss: 7.036069393157959\n","Epoch 313/4000, Step 47, d_loss: 0.38652488589286804, g_loss: 6.853091716766357\n","Epoch 313/4000, Step 48, d_loss: 0.3546980917453766, g_loss: 7.181661605834961\n","Epoch 313/4000, Step 49, d_loss: 0.3448556363582611, g_loss: 7.1954426765441895\n","Epoch 313/4000, Step 50, d_loss: 0.38902658224105835, g_loss: 7.323133945465088\n","Epoch 313/4000, Step 51, d_loss: 0.3613903820514679, g_loss: 5.3064703941345215\n","Epoch 313/4000, Step 52, d_loss: 0.34773507714271545, g_loss: 7.673929691314697\n","Epoch 313/4000, Step 53, d_loss: 0.3488771915435791, g_loss: 6.670858383178711\n","Epoch 313/4000, Step 54, d_loss: 0.3607560396194458, g_loss: 3.386692523956299\n","Epoch 313/4000, Step 55, d_loss: 0.36772122979164124, g_loss: 7.8236002922058105\n","Epoch 313/4000, Step 56, d_loss: 0.3361368775367737, g_loss: 7.294685363769531\n","Epoch 313/4000, Step 57, d_loss: 0.3548884093761444, g_loss: 3.9962029457092285\n","Epoch 313/4000, Step 58, d_loss: 0.38154730200767517, g_loss: 5.9417901039123535\n","Epoch 313/4000, Step 59, d_loss: 0.35989484190940857, g_loss: 4.250859260559082\n","Epoch 313/4000, Step 60, d_loss: 0.33382293581962585, g_loss: 4.199039936065674\n","Epoch 313/4000, Step 61, d_loss: 0.33904892206192017, g_loss: 5.582179546356201\n","Epoch 313/4000, Step 62, d_loss: 0.3338252305984497, g_loss: 4.068848133087158\n","Epoch 313/4000, Step 63, d_loss: 0.33855295181274414, g_loss: 8.94482135772705\n","Epoch 313/4000, Step 64, d_loss: 0.36336126923561096, g_loss: 4.782582759857178\n","Epoch 313/4000, Step 65, d_loss: 0.34872472286224365, g_loss: 4.062439441680908\n","Epoch 313/4000, Step 66, d_loss: 0.33716464042663574, g_loss: 6.254918575286865\n","Epoch 313/4000, Step 67, d_loss: 0.34235039353370667, g_loss: 8.17488956451416\n","Epoch 313/4000, Step 68, d_loss: 0.33739572763442993, g_loss: 5.648717880249023\n","Epoch 313/4000, Step 69, d_loss: 0.33881521224975586, g_loss: 3.9724581241607666\n","Epoch 313/4000, Step 70, d_loss: 0.3537871241569519, g_loss: 4.306199073791504\n","Epoch 313/4000, Step 71, d_loss: 0.3430273234844208, g_loss: 6.992397785186768\n","Epoch 313/4000, Step 72, d_loss: 0.3546077609062195, g_loss: 5.678196430206299\n","Epoch 313/4000, Step 73, d_loss: 0.4678494930267334, g_loss: 4.309856414794922\n","Epoch 313/4000, Step 74, d_loss: 0.3729221820831299, g_loss: 8.09188175201416\n","Epoch 313/4000, Step 75, d_loss: 0.380886435508728, g_loss: 3.9470179080963135\n","Epoch 313/4000, Step 76, d_loss: 0.3638053238391876, g_loss: 3.9031736850738525\n","Epoch 313/4000, Step 77, d_loss: 0.3406352996826172, g_loss: 4.350513935089111\n","Epoch 313/4000, Step 78, d_loss: 0.34479841589927673, g_loss: 5.931221008300781\n","Epoch 313/4000, Step 79, d_loss: 0.339781254529953, g_loss: 5.504075050354004\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 314/4000, Step 1, d_loss: 0.34768301248550415, g_loss: 6.368689060211182\n","Epoch 314/4000, Step 2, d_loss: 0.33853816986083984, g_loss: 6.612677097320557\n","Epoch 314/4000, Step 3, d_loss: 0.3414890766143799, g_loss: 7.569231033325195\n","Epoch 314/4000, Step 4, d_loss: 0.33805230259895325, g_loss: 5.575637340545654\n","Epoch 314/4000, Step 5, d_loss: 0.3421507477760315, g_loss: 9.220124244689941\n","Epoch 314/4000, Step 6, d_loss: 0.34383347630500793, g_loss: 9.362693786621094\n","Epoch 314/4000, Step 7, d_loss: 0.3463592231273651, g_loss: 6.141764163970947\n","Epoch 314/4000, Step 8, d_loss: 0.33984822034835815, g_loss: 5.818576812744141\n","Epoch 314/4000, Step 9, d_loss: 0.360550194978714, g_loss: 7.200362682342529\n","Epoch 314/4000, Step 10, d_loss: 0.3520345687866211, g_loss: 5.737789154052734\n","Epoch 314/4000, Step 11, d_loss: 0.3544889986515045, g_loss: 9.507283210754395\n","Epoch 314/4000, Step 12, d_loss: 0.3390134274959564, g_loss: 5.003389358520508\n","Epoch 314/4000, Step 13, d_loss: 0.36666905879974365, g_loss: 6.764793872833252\n","Epoch 314/4000, Step 14, d_loss: 0.3477574288845062, g_loss: 5.625099182128906\n","Epoch 314/4000, Step 15, d_loss: 0.34898361563682556, g_loss: 10.570375442504883\n","Epoch 314/4000, Step 16, d_loss: 0.3469449579715729, g_loss: 5.758925437927246\n","Epoch 314/4000, Step 17, d_loss: 0.3367120325565338, g_loss: 9.926941871643066\n","Epoch 314/4000, Step 18, d_loss: 0.34083807468414307, g_loss: 6.506162166595459\n","Epoch 314/4000, Step 19, d_loss: 0.35057303309440613, g_loss: 7.016363143920898\n","Epoch 314/4000, Step 20, d_loss: 0.34645235538482666, g_loss: 5.713988304138184\n","Epoch 314/4000, Step 21, d_loss: 0.3394072651863098, g_loss: 5.700339317321777\n","Epoch 314/4000, Step 22, d_loss: 0.3599017262458801, g_loss: 5.721765518188477\n","Epoch 314/4000, Step 23, d_loss: 0.37560486793518066, g_loss: 5.42034387588501\n","Epoch 314/4000, Step 24, d_loss: 0.345414400100708, g_loss: 6.306650161743164\n","Epoch 314/4000, Step 25, d_loss: 0.3526131212711334, g_loss: 8.547715187072754\n","Epoch 314/4000, Step 26, d_loss: 0.3325740694999695, g_loss: 4.952703475952148\n","Epoch 314/4000, Step 27, d_loss: 0.3337751030921936, g_loss: 7.542391300201416\n","Epoch 314/4000, Step 28, d_loss: 0.33981063961982727, g_loss: 5.5231194496154785\n","Epoch 314/4000, Step 29, d_loss: 0.33913326263427734, g_loss: 6.729323863983154\n","Epoch 314/4000, Step 30, d_loss: 0.3552759289741516, g_loss: 6.554404258728027\n","Epoch 314/4000, Step 31, d_loss: 0.343946635723114, g_loss: 7.392439842224121\n","Epoch 314/4000, Step 32, d_loss: 0.33344292640686035, g_loss: 6.129424571990967\n","Epoch 314/4000, Step 33, d_loss: 0.3688143789768219, g_loss: 6.860141754150391\n","Epoch 314/4000, Step 34, d_loss: 0.37167948484420776, g_loss: 5.609454154968262\n","Epoch 314/4000, Step 35, d_loss: 0.36474066972732544, g_loss: 5.074263095855713\n","Epoch 314/4000, Step 36, d_loss: 0.3571101427078247, g_loss: 5.560550689697266\n","Epoch 314/4000, Step 37, d_loss: 0.3524337708950043, g_loss: 7.37704610824585\n","Epoch 314/4000, Step 38, d_loss: 0.34003669023513794, g_loss: 5.819700241088867\n","Epoch 314/4000, Step 39, d_loss: 0.34058505296707153, g_loss: 6.320670127868652\n","Epoch 314/4000, Step 40, d_loss: 0.3756742775440216, g_loss: 6.767815589904785\n","Epoch 314/4000, Step 41, d_loss: 0.34321659803390503, g_loss: 6.456857204437256\n","Epoch 314/4000, Step 42, d_loss: 0.34461867809295654, g_loss: 6.6872663497924805\n","Epoch 314/4000, Step 43, d_loss: 0.351696252822876, g_loss: 6.82302188873291\n","Epoch 314/4000, Step 44, d_loss: 0.3479420840740204, g_loss: 5.313540935516357\n","Epoch 314/4000, Step 45, d_loss: 0.36495137214660645, g_loss: 4.964607238769531\n","Epoch 314/4000, Step 46, d_loss: 0.3858868181705475, g_loss: 5.270021915435791\n","Epoch 314/4000, Step 47, d_loss: 0.37724941968917847, g_loss: 5.1871747970581055\n","Epoch 314/4000, Step 48, d_loss: 0.3301253616809845, g_loss: 5.464705944061279\n","Epoch 314/4000, Step 49, d_loss: 0.341197669506073, g_loss: 6.324129581451416\n","Epoch 314/4000, Step 50, d_loss: 0.35846054553985596, g_loss: 5.727583885192871\n","Epoch 314/4000, Step 51, d_loss: 0.3556211590766907, g_loss: 6.293841361999512\n","Epoch 314/4000, Step 52, d_loss: 0.3345411419868469, g_loss: 6.481696605682373\n","Epoch 314/4000, Step 53, d_loss: 0.33661696314811707, g_loss: 5.4347639083862305\n","Epoch 314/4000, Step 54, d_loss: 0.3480876088142395, g_loss: 5.747073173522949\n","Epoch 314/4000, Step 55, d_loss: 0.34335142374038696, g_loss: 5.067239284515381\n","Epoch 314/4000, Step 56, d_loss: 0.336921364068985, g_loss: 6.898804664611816\n","Epoch 314/4000, Step 57, d_loss: 0.3447374701499939, g_loss: 6.367162704467773\n","Epoch 314/4000, Step 58, d_loss: 0.3975470960140228, g_loss: 4.465444087982178\n","Epoch 314/4000, Step 59, d_loss: 0.3540496528148651, g_loss: 5.531904697418213\n","Epoch 314/4000, Step 60, d_loss: 0.3359421193599701, g_loss: 3.925797700881958\n","Epoch 314/4000, Step 61, d_loss: 0.333636611700058, g_loss: 4.68660306930542\n","Epoch 314/4000, Step 62, d_loss: 0.3521868884563446, g_loss: 4.815134525299072\n","Epoch 314/4000, Step 63, d_loss: 0.3622487187385559, g_loss: 4.731033802032471\n","Epoch 314/4000, Step 64, d_loss: 0.34253767132759094, g_loss: 5.0710906982421875\n","Epoch 314/4000, Step 65, d_loss: 0.3494657874107361, g_loss: 5.739613056182861\n","Epoch 314/4000, Step 66, d_loss: 0.33636605739593506, g_loss: 6.2611565589904785\n","Epoch 314/4000, Step 67, d_loss: 0.33791470527648926, g_loss: 5.317785739898682\n","Epoch 314/4000, Step 68, d_loss: 0.34857264161109924, g_loss: 5.84315299987793\n","Epoch 314/4000, Step 69, d_loss: 0.3452357351779938, g_loss: 4.648055076599121\n","Epoch 314/4000, Step 70, d_loss: 0.34754040837287903, g_loss: 5.305722713470459\n","Epoch 314/4000, Step 71, d_loss: 0.34202948212623596, g_loss: 4.582263946533203\n","Epoch 314/4000, Step 72, d_loss: 0.3369951844215393, g_loss: 5.671511173248291\n","Epoch 314/4000, Step 73, d_loss: 0.4144599139690399, g_loss: 5.1652445793151855\n","Epoch 314/4000, Step 74, d_loss: 0.34799209237098694, g_loss: 3.457974672317505\n","Epoch 314/4000, Step 75, d_loss: 0.3615107238292694, g_loss: 4.470358371734619\n","Epoch 314/4000, Step 76, d_loss: 0.3839529752731323, g_loss: 3.80692720413208\n","Epoch 314/4000, Step 77, d_loss: 0.3567584455013275, g_loss: 4.9092302322387695\n","Epoch 314/4000, Step 78, d_loss: 0.36841318011283875, g_loss: 5.652715682983398\n","Epoch 314/4000, Step 79, d_loss: 0.33836784958839417, g_loss: 5.0903801918029785\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 315/4000, Step 1, d_loss: 0.3413064181804657, g_loss: 5.962460994720459\n","Epoch 315/4000, Step 2, d_loss: 0.3451897203922272, g_loss: 4.92341947555542\n","Epoch 315/4000, Step 3, d_loss: 0.3436112105846405, g_loss: 6.734732627868652\n","Epoch 315/4000, Step 4, d_loss: 0.34044909477233887, g_loss: 6.541332721710205\n","Epoch 315/4000, Step 5, d_loss: 0.33580631017684937, g_loss: 5.474660873413086\n","Epoch 315/4000, Step 6, d_loss: 0.3370445966720581, g_loss: 5.734971523284912\n","Epoch 315/4000, Step 7, d_loss: 0.3484368920326233, g_loss: 9.12570571899414\n","Epoch 315/4000, Step 8, d_loss: 0.3538854122161865, g_loss: 5.016801834106445\n","Epoch 315/4000, Step 9, d_loss: 0.3525393307209015, g_loss: 5.949514389038086\n","Epoch 315/4000, Step 10, d_loss: 0.3299131393432617, g_loss: 6.889420986175537\n","Epoch 315/4000, Step 11, d_loss: 0.33600857853889465, g_loss: 6.932210445404053\n","Epoch 315/4000, Step 12, d_loss: 0.33046814799308777, g_loss: 6.819786071777344\n","Epoch 315/4000, Step 13, d_loss: 0.3422139286994934, g_loss: 6.758543491363525\n","Epoch 315/4000, Step 14, d_loss: 0.3521524667739868, g_loss: 4.736693382263184\n","Epoch 315/4000, Step 15, d_loss: 0.35253581404685974, g_loss: 5.92486047744751\n","Epoch 315/4000, Step 16, d_loss: 0.3342914283275604, g_loss: 5.2117018699646\n","Epoch 315/4000, Step 17, d_loss: 0.33752137422561646, g_loss: 5.194633960723877\n","Epoch 315/4000, Step 18, d_loss: 0.33931154012680054, g_loss: 9.928529739379883\n","Epoch 315/4000, Step 19, d_loss: 0.35210853815078735, g_loss: 5.898797035217285\n","Epoch 315/4000, Step 20, d_loss: 0.34062913060188293, g_loss: 6.336942195892334\n","Epoch 315/4000, Step 21, d_loss: 0.34121108055114746, g_loss: 6.858716011047363\n","Epoch 315/4000, Step 22, d_loss: 0.3398216664791107, g_loss: 6.489077568054199\n","Epoch 315/4000, Step 23, d_loss: 0.333262175321579, g_loss: 4.991107940673828\n","Epoch 315/4000, Step 24, d_loss: 0.36261746287345886, g_loss: 5.498083114624023\n","Epoch 315/4000, Step 25, d_loss: 0.3655773997306824, g_loss: 4.530210018157959\n","Epoch 315/4000, Step 26, d_loss: 0.34891173243522644, g_loss: 4.286493301391602\n","Epoch 315/4000, Step 27, d_loss: 0.36395537853240967, g_loss: 7.007137298583984\n","Epoch 315/4000, Step 28, d_loss: 0.36802321672439575, g_loss: 8.612488746643066\n","Epoch 315/4000, Step 29, d_loss: 0.36654192209243774, g_loss: 5.488288879394531\n","Epoch 315/4000, Step 30, d_loss: 0.36204472184181213, g_loss: 5.373007774353027\n","Epoch 315/4000, Step 31, d_loss: 0.3551573157310486, g_loss: 4.676436901092529\n","Epoch 315/4000, Step 32, d_loss: 0.33458542823791504, g_loss: 6.276352882385254\n","Epoch 315/4000, Step 33, d_loss: 0.3346962630748749, g_loss: 5.150552749633789\n","Epoch 315/4000, Step 34, d_loss: 0.3575965464115143, g_loss: 7.024835109710693\n","Epoch 315/4000, Step 35, d_loss: 0.35060548782348633, g_loss: 8.51611614227295\n","Epoch 315/4000, Step 36, d_loss: 0.3571144938468933, g_loss: 6.285955429077148\n","Epoch 315/4000, Step 37, d_loss: 0.4031549394130707, g_loss: 6.639293193817139\n","Epoch 315/4000, Step 38, d_loss: 0.3464880883693695, g_loss: 4.001040458679199\n","Epoch 315/4000, Step 39, d_loss: 0.3953290581703186, g_loss: 3.235931873321533\n","Epoch 315/4000, Step 40, d_loss: 0.38142672181129456, g_loss: 7.662971019744873\n","Epoch 315/4000, Step 41, d_loss: 0.37744197249412537, g_loss: 4.943098545074463\n","Epoch 315/4000, Step 42, d_loss: 0.348570317029953, g_loss: 5.575549125671387\n","Epoch 315/4000, Step 43, d_loss: 0.3387064039707184, g_loss: 5.457241535186768\n","Epoch 315/4000, Step 44, d_loss: 0.3379231095314026, g_loss: 7.229294300079346\n","Epoch 315/4000, Step 45, d_loss: 0.34661170840263367, g_loss: 5.2245259284973145\n","Epoch 315/4000, Step 46, d_loss: 0.3486051559448242, g_loss: 7.026973247528076\n","Epoch 315/4000, Step 47, d_loss: 0.33410996198654175, g_loss: 6.291628360748291\n","Epoch 315/4000, Step 48, d_loss: 0.3313327133655548, g_loss: 6.239691257476807\n","Epoch 315/4000, Step 49, d_loss: 0.335041880607605, g_loss: 5.893466472625732\n","Epoch 315/4000, Step 50, d_loss: 0.35052749514579773, g_loss: 8.048498153686523\n","Epoch 315/4000, Step 51, d_loss: 0.3454590439796448, g_loss: 5.899176120758057\n","Epoch 315/4000, Step 52, d_loss: 0.3388863801956177, g_loss: 5.621663570404053\n","Epoch 315/4000, Step 53, d_loss: 0.3571825921535492, g_loss: 4.301253795623779\n","Epoch 315/4000, Step 54, d_loss: 0.3380739688873291, g_loss: 4.837543487548828\n","Epoch 315/4000, Step 55, d_loss: 0.3406844437122345, g_loss: 5.23670768737793\n","Epoch 315/4000, Step 56, d_loss: 0.34173157811164856, g_loss: 5.033893585205078\n","Epoch 315/4000, Step 57, d_loss: 0.34480050206184387, g_loss: 5.615163326263428\n","Epoch 315/4000, Step 58, d_loss: 0.3421994745731354, g_loss: 4.85773229598999\n","Epoch 315/4000, Step 59, d_loss: 0.34410521388053894, g_loss: 5.007924556732178\n","Epoch 315/4000, Step 60, d_loss: 0.33558914065361023, g_loss: 6.190125942230225\n","Epoch 315/4000, Step 61, d_loss: 0.3355919122695923, g_loss: 5.585708141326904\n","Epoch 315/4000, Step 62, d_loss: 0.3354678153991699, g_loss: 5.236401557922363\n","Epoch 315/4000, Step 63, d_loss: 0.3324047327041626, g_loss: 5.464732646942139\n","Epoch 315/4000, Step 64, d_loss: 0.34386390447616577, g_loss: 4.841646194458008\n","Epoch 315/4000, Step 65, d_loss: 0.34561997652053833, g_loss: 5.13036584854126\n","Epoch 315/4000, Step 66, d_loss: 0.3330286741256714, g_loss: 5.970767974853516\n","Epoch 315/4000, Step 67, d_loss: 0.3491561710834503, g_loss: 5.534719944000244\n","Epoch 315/4000, Step 68, d_loss: 0.3410486876964569, g_loss: 8.840646743774414\n","Epoch 315/4000, Step 69, d_loss: 0.3358807861804962, g_loss: 5.747900485992432\n","Epoch 315/4000, Step 70, d_loss: 0.3351740539073944, g_loss: 6.943426609039307\n","Epoch 315/4000, Step 71, d_loss: 0.3528815805912018, g_loss: 6.286398410797119\n","Epoch 315/4000, Step 72, d_loss: 0.3480827808380127, g_loss: 6.8515143394470215\n","Epoch 315/4000, Step 73, d_loss: 0.3490881323814392, g_loss: 6.1822357177734375\n","Epoch 315/4000, Step 74, d_loss: 0.3389827609062195, g_loss: 7.64590311050415\n","Epoch 315/4000, Step 75, d_loss: 0.33859583735466003, g_loss: 5.4886908531188965\n","Epoch 315/4000, Step 76, d_loss: 0.333751916885376, g_loss: 5.93339729309082\n","Epoch 315/4000, Step 77, d_loss: 0.3418632745742798, g_loss: 8.341135025024414\n","Epoch 315/4000, Step 78, d_loss: 0.3366851806640625, g_loss: 5.676555633544922\n","Epoch 315/4000, Step 79, d_loss: 0.702187716960907, g_loss: 4.813203811645508\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 316/4000, Step 1, d_loss: 0.360721230506897, g_loss: 4.690201759338379\n","Epoch 316/4000, Step 2, d_loss: 0.3814857006072998, g_loss: 3.9245872497558594\n","Epoch 316/4000, Step 3, d_loss: 0.48242390155792236, g_loss: 2.81227445602417\n","Epoch 316/4000, Step 4, d_loss: 0.4744068384170532, g_loss: 3.838068962097168\n","Epoch 316/4000, Step 5, d_loss: 0.40758946537971497, g_loss: 4.784933567047119\n","Epoch 316/4000, Step 6, d_loss: 0.3458123505115509, g_loss: 3.7435383796691895\n","Epoch 316/4000, Step 7, d_loss: 0.34303751587867737, g_loss: 6.549102783203125\n","Epoch 316/4000, Step 8, d_loss: 0.3531145453453064, g_loss: 5.020222187042236\n","Epoch 316/4000, Step 9, d_loss: 0.41895174980163574, g_loss: 6.479323863983154\n","Epoch 316/4000, Step 10, d_loss: 0.3671448528766632, g_loss: 6.459618091583252\n","Epoch 316/4000, Step 11, d_loss: 0.349485844373703, g_loss: 4.191932201385498\n","Epoch 316/4000, Step 12, d_loss: 0.3639066815376282, g_loss: 5.409985542297363\n","Epoch 316/4000, Step 13, d_loss: 0.3513448238372803, g_loss: 4.249149322509766\n","Epoch 316/4000, Step 14, d_loss: 0.3545066714286804, g_loss: 3.3527259826660156\n","Epoch 316/4000, Step 15, d_loss: 0.36974212527275085, g_loss: 5.766391754150391\n","Epoch 316/4000, Step 16, d_loss: 0.3551635146141052, g_loss: 6.977519512176514\n","Epoch 316/4000, Step 17, d_loss: 0.3853324055671692, g_loss: 6.64021635055542\n","Epoch 316/4000, Step 18, d_loss: 0.3810034692287445, g_loss: 5.864219665527344\n","Epoch 316/4000, Step 19, d_loss: 0.33953139185905457, g_loss: 5.938165187835693\n","Epoch 316/4000, Step 20, d_loss: 0.3464454114437103, g_loss: 6.850233554840088\n","Epoch 316/4000, Step 21, d_loss: 0.3601875305175781, g_loss: 6.945382118225098\n","Epoch 316/4000, Step 22, d_loss: 0.34961315989494324, g_loss: 5.136430740356445\n","Epoch 316/4000, Step 23, d_loss: 0.3377819359302521, g_loss: 6.250123023986816\n","Epoch 316/4000, Step 24, d_loss: 0.3392397463321686, g_loss: 6.000311851501465\n","Epoch 316/4000, Step 25, d_loss: 0.34369203448295593, g_loss: 6.345691680908203\n","Epoch 316/4000, Step 26, d_loss: 0.3593045771121979, g_loss: 5.158941268920898\n","Epoch 316/4000, Step 27, d_loss: 0.346937894821167, g_loss: 5.840122699737549\n","Epoch 316/4000, Step 28, d_loss: 0.34209999442100525, g_loss: 5.916681289672852\n","Epoch 316/4000, Step 29, d_loss: 0.3474839925765991, g_loss: 6.286334991455078\n","Epoch 316/4000, Step 30, d_loss: 0.337047278881073, g_loss: 6.287146091461182\n","Epoch 316/4000, Step 31, d_loss: 0.3298463821411133, g_loss: 5.849727630615234\n","Epoch 316/4000, Step 32, d_loss: 0.3357604444026947, g_loss: 6.279428958892822\n","Epoch 316/4000, Step 33, d_loss: 0.3347441852092743, g_loss: 6.43010950088501\n","Epoch 316/4000, Step 34, d_loss: 0.3400508165359497, g_loss: 6.865376949310303\n","Epoch 316/4000, Step 35, d_loss: 0.3343503773212433, g_loss: 6.594624996185303\n","Epoch 316/4000, Step 36, d_loss: 0.33641448616981506, g_loss: 6.645954132080078\n","Epoch 316/4000, Step 37, d_loss: 0.33580583333969116, g_loss: 7.798750400543213\n","Epoch 316/4000, Step 38, d_loss: 0.3339521884918213, g_loss: 6.2818098068237305\n","Epoch 316/4000, Step 39, d_loss: 0.3420751690864563, g_loss: 7.190161228179932\n","Epoch 316/4000, Step 40, d_loss: 0.3371184766292572, g_loss: 6.158658027648926\n","Epoch 316/4000, Step 41, d_loss: 0.3377581238746643, g_loss: 5.3272786140441895\n","Epoch 316/4000, Step 42, d_loss: 0.34158632159233093, g_loss: 9.09700870513916\n","Epoch 316/4000, Step 43, d_loss: 0.33490172028541565, g_loss: 6.45703125\n","Epoch 316/4000, Step 44, d_loss: 0.33079952001571655, g_loss: 5.851694107055664\n","Epoch 316/4000, Step 45, d_loss: 0.3431546092033386, g_loss: 5.279463768005371\n","Epoch 316/4000, Step 46, d_loss: 0.33652544021606445, g_loss: 6.708410263061523\n","Epoch 316/4000, Step 47, d_loss: 0.3392629623413086, g_loss: 5.25004768371582\n","Epoch 316/4000, Step 48, d_loss: 0.33133625984191895, g_loss: 8.189455032348633\n","Epoch 316/4000, Step 49, d_loss: 0.39379769563674927, g_loss: 6.002133369445801\n","Epoch 316/4000, Step 50, d_loss: 0.33441147208213806, g_loss: 6.153851509094238\n","Epoch 316/4000, Step 51, d_loss: 0.33849257230758667, g_loss: 5.292553901672363\n","Epoch 316/4000, Step 52, d_loss: 0.3300589621067047, g_loss: 5.794485092163086\n","Epoch 316/4000, Step 53, d_loss: 0.350126177072525, g_loss: 5.684904098510742\n","Epoch 316/4000, Step 54, d_loss: 0.3384791612625122, g_loss: 5.477000713348389\n","Epoch 316/4000, Step 55, d_loss: 0.33429154753685, g_loss: 7.843025207519531\n","Epoch 316/4000, Step 56, d_loss: 0.3402979075908661, g_loss: 5.359221935272217\n","Epoch 316/4000, Step 57, d_loss: 0.34060952067375183, g_loss: 5.154834747314453\n","Epoch 316/4000, Step 58, d_loss: 0.33432620763778687, g_loss: 5.15777063369751\n","Epoch 316/4000, Step 59, d_loss: 0.34505629539489746, g_loss: 5.292361736297607\n","Epoch 316/4000, Step 60, d_loss: 0.3381975591182709, g_loss: 5.13714599609375\n","Epoch 316/4000, Step 61, d_loss: 0.35511407256126404, g_loss: 5.207834720611572\n","Epoch 316/4000, Step 62, d_loss: 0.3382660448551178, g_loss: 6.297464370727539\n","Epoch 316/4000, Step 63, d_loss: 0.33627378940582275, g_loss: 5.6142473220825195\n","Epoch 316/4000, Step 64, d_loss: 0.3374573886394501, g_loss: 5.888128280639648\n","Epoch 316/4000, Step 65, d_loss: 0.3350500464439392, g_loss: 5.638368129730225\n","Epoch 316/4000, Step 66, d_loss: 0.3358841836452484, g_loss: 7.523087501525879\n","Epoch 316/4000, Step 67, d_loss: 0.36094313859939575, g_loss: 5.211461544036865\n","Epoch 316/4000, Step 68, d_loss: 0.3384539484977722, g_loss: 5.56247615814209\n","Epoch 316/4000, Step 69, d_loss: 0.33352166414260864, g_loss: 4.825106620788574\n","Epoch 316/4000, Step 70, d_loss: 0.35105299949645996, g_loss: 4.3958353996276855\n","Epoch 316/4000, Step 71, d_loss: 0.3493930697441101, g_loss: 4.906375885009766\n","Epoch 316/4000, Step 72, d_loss: 0.36020126938819885, g_loss: 5.219942092895508\n","Epoch 316/4000, Step 73, d_loss: 0.3395371735095978, g_loss: 6.586931228637695\n","Epoch 316/4000, Step 74, d_loss: 0.33868902921676636, g_loss: 6.472620010375977\n","Epoch 316/4000, Step 75, d_loss: 0.35181334614753723, g_loss: 8.92780590057373\n","Epoch 316/4000, Step 76, d_loss: 0.3643719553947449, g_loss: 5.964798450469971\n","Epoch 316/4000, Step 77, d_loss: 0.33235451579093933, g_loss: 5.9768524169921875\n","Epoch 316/4000, Step 78, d_loss: 0.3328843414783478, g_loss: 4.9832377433776855\n","Epoch 316/4000, Step 79, d_loss: 0.49403953552246094, g_loss: 3.9098684787750244\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 317/4000, Step 1, d_loss: 0.37194007635116577, g_loss: 3.469388484954834\n","Epoch 317/4000, Step 2, d_loss: 0.40311089158058167, g_loss: 3.8259854316711426\n","Epoch 317/4000, Step 3, d_loss: 0.4156970977783203, g_loss: 2.960843086242676\n","Epoch 317/4000, Step 4, d_loss: 0.37482109665870667, g_loss: 3.967024326324463\n","Epoch 317/4000, Step 5, d_loss: 0.40189269185066223, g_loss: 4.037275314331055\n","Epoch 317/4000, Step 6, d_loss: 0.3812010586261749, g_loss: 7.415281772613525\n","Epoch 317/4000, Step 7, d_loss: 0.39829903841018677, g_loss: 6.084291458129883\n","Epoch 317/4000, Step 8, d_loss: 0.36672255396842957, g_loss: 6.0855231285095215\n","Epoch 317/4000, Step 9, d_loss: 0.341109961271286, g_loss: 5.297685146331787\n","Epoch 317/4000, Step 10, d_loss: 0.33734050393104553, g_loss: 5.816538333892822\n","Epoch 317/4000, Step 11, d_loss: 0.3357003629207611, g_loss: 5.704596042633057\n","Epoch 317/4000, Step 12, d_loss: 0.3560852110385895, g_loss: 6.014583587646484\n","Epoch 317/4000, Step 13, d_loss: 0.3723585605621338, g_loss: 6.731136798858643\n","Epoch 317/4000, Step 14, d_loss: 0.3893454074859619, g_loss: 4.856055736541748\n","Epoch 317/4000, Step 15, d_loss: 0.4216732978820801, g_loss: 6.137544631958008\n","Epoch 317/4000, Step 16, d_loss: 0.36102572083473206, g_loss: 7.9869818687438965\n","Epoch 317/4000, Step 17, d_loss: 0.36944815516471863, g_loss: 4.417640686035156\n","Epoch 317/4000, Step 18, d_loss: 0.36080822348594666, g_loss: 4.038015842437744\n","Epoch 317/4000, Step 19, d_loss: 0.36843422055244446, g_loss: 4.796933650970459\n","Epoch 317/4000, Step 20, d_loss: 0.3773876428604126, g_loss: 3.881723642349243\n","Epoch 317/4000, Step 21, d_loss: 0.36176472902297974, g_loss: 4.24461555480957\n","Epoch 317/4000, Step 22, d_loss: 0.36362648010253906, g_loss: 5.916199207305908\n","Epoch 317/4000, Step 23, d_loss: 0.3574616312980652, g_loss: 6.903707027435303\n","Epoch 317/4000, Step 24, d_loss: 0.3381631672382355, g_loss: 6.165215969085693\n","Epoch 317/4000, Step 25, d_loss: 0.344852089881897, g_loss: 4.865564346313477\n","Epoch 317/4000, Step 26, d_loss: 0.34108567237854004, g_loss: 5.487452030181885\n","Epoch 317/4000, Step 27, d_loss: 0.3368184268474579, g_loss: 5.847478866577148\n","Epoch 317/4000, Step 28, d_loss: 0.3406650722026825, g_loss: 5.411144733428955\n","Epoch 317/4000, Step 29, d_loss: 0.3412889540195465, g_loss: 6.074976444244385\n","Epoch 317/4000, Step 30, d_loss: 0.3647414743900299, g_loss: 5.188787460327148\n","Epoch 317/4000, Step 31, d_loss: 0.34050071239471436, g_loss: 6.224656581878662\n","Epoch 317/4000, Step 32, d_loss: 0.386522114276886, g_loss: 5.841561794281006\n","Epoch 317/4000, Step 33, d_loss: 0.33376607298851013, g_loss: 6.895140647888184\n","Epoch 317/4000, Step 34, d_loss: 0.3362870514392853, g_loss: 6.683359146118164\n","Epoch 317/4000, Step 35, d_loss: 0.34416595101356506, g_loss: 4.719640254974365\n","Epoch 317/4000, Step 36, d_loss: 0.34590601921081543, g_loss: 4.296076774597168\n","Epoch 317/4000, Step 37, d_loss: 0.34272244572639465, g_loss: 3.6370432376861572\n","Epoch 317/4000, Step 38, d_loss: 0.3419901430606842, g_loss: 5.2110185623168945\n","Epoch 317/4000, Step 39, d_loss: 0.34598565101623535, g_loss: 5.250229358673096\n","Epoch 317/4000, Step 40, d_loss: 0.336680144071579, g_loss: 6.262087821960449\n","Epoch 317/4000, Step 41, d_loss: 0.3386909067630768, g_loss: 7.286303997039795\n","Epoch 317/4000, Step 42, d_loss: 0.331872820854187, g_loss: 5.6595258712768555\n","Epoch 317/4000, Step 43, d_loss: 0.3288379907608032, g_loss: 6.074224948883057\n","Epoch 317/4000, Step 44, d_loss: 0.3318818509578705, g_loss: 9.846406936645508\n","Epoch 317/4000, Step 45, d_loss: 0.33367031812667847, g_loss: 6.679596424102783\n","Epoch 317/4000, Step 46, d_loss: 0.3382677733898163, g_loss: 5.466793060302734\n","Epoch 317/4000, Step 47, d_loss: 0.33109304308891296, g_loss: 6.060507297515869\n","Epoch 317/4000, Step 48, d_loss: 0.3402211666107178, g_loss: 6.307731628417969\n","Epoch 317/4000, Step 49, d_loss: 0.33631372451782227, g_loss: 7.40574836730957\n","Epoch 317/4000, Step 50, d_loss: 0.3360748291015625, g_loss: 6.444689750671387\n","Epoch 317/4000, Step 51, d_loss: 0.3390931189060211, g_loss: 6.82263708114624\n","Epoch 317/4000, Step 52, d_loss: 0.33258262276649475, g_loss: 6.932648658752441\n","Epoch 317/4000, Step 53, d_loss: 0.3388214707374573, g_loss: 6.317718029022217\n","Epoch 317/4000, Step 54, d_loss: 0.3351273536682129, g_loss: 7.264153003692627\n","Epoch 317/4000, Step 55, d_loss: 0.340912789106369, g_loss: 5.689425945281982\n","Epoch 317/4000, Step 56, d_loss: 0.3321857154369354, g_loss: 5.185666561126709\n","Epoch 317/4000, Step 57, d_loss: 0.33326661586761475, g_loss: 6.619290351867676\n","Epoch 317/4000, Step 58, d_loss: 0.3334544003009796, g_loss: 5.695346355438232\n","Epoch 317/4000, Step 59, d_loss: 0.33069559931755066, g_loss: 8.150201797485352\n","Epoch 317/4000, Step 60, d_loss: 0.3307788670063019, g_loss: 6.30100154876709\n","Epoch 317/4000, Step 61, d_loss: 0.3376213312149048, g_loss: 8.170064926147461\n","Epoch 317/4000, Step 62, d_loss: 0.3339884579181671, g_loss: 5.841220855712891\n","Epoch 317/4000, Step 63, d_loss: 0.35672619938850403, g_loss: 5.186594486236572\n","Epoch 317/4000, Step 64, d_loss: 0.3281075954437256, g_loss: 5.879075050354004\n","Epoch 317/4000, Step 65, d_loss: 0.3340698182582855, g_loss: 6.403388023376465\n","Epoch 317/4000, Step 66, d_loss: 0.3355650305747986, g_loss: 4.395702838897705\n","Epoch 317/4000, Step 67, d_loss: 0.3523881435394287, g_loss: 5.94569206237793\n","Epoch 317/4000, Step 68, d_loss: 0.3565678596496582, g_loss: 5.520222187042236\n","Epoch 317/4000, Step 69, d_loss: 0.3414328396320343, g_loss: 7.2651448249816895\n","Epoch 317/4000, Step 70, d_loss: 0.33493077754974365, g_loss: 6.0574140548706055\n","Epoch 317/4000, Step 71, d_loss: 0.3406525254249573, g_loss: 6.73565149307251\n","Epoch 317/4000, Step 72, d_loss: 0.3431642949581146, g_loss: 6.680487632751465\n","Epoch 317/4000, Step 73, d_loss: 0.33510711789131165, g_loss: 6.541286468505859\n","Epoch 317/4000, Step 74, d_loss: 0.37725234031677246, g_loss: 4.8633270263671875\n","Epoch 317/4000, Step 75, d_loss: 0.34099021553993225, g_loss: 4.759117126464844\n","Epoch 317/4000, Step 76, d_loss: 0.34312018752098083, g_loss: 4.316631317138672\n","Epoch 317/4000, Step 77, d_loss: 0.354938805103302, g_loss: 4.924618721008301\n","Epoch 317/4000, Step 78, d_loss: 0.3648512065410614, g_loss: 4.355534553527832\n","Epoch 317/4000, Step 79, d_loss: 0.569691002368927, g_loss: 4.002683162689209\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 318/4000, Step 1, d_loss: 0.3734067678451538, g_loss: 3.0523197650909424\n","Epoch 318/4000, Step 2, d_loss: 0.4180316925048828, g_loss: 2.9208946228027344\n","Epoch 318/4000, Step 3, d_loss: 0.40236303210258484, g_loss: 4.900965690612793\n","Epoch 318/4000, Step 4, d_loss: 0.42125236988067627, g_loss: 4.7643842697143555\n","Epoch 318/4000, Step 5, d_loss: 0.469096302986145, g_loss: 4.737926959991455\n","Epoch 318/4000, Step 6, d_loss: 0.4460504353046417, g_loss: 4.29949426651001\n","Epoch 318/4000, Step 7, d_loss: 0.41609975695610046, g_loss: 4.538086891174316\n","Epoch 318/4000, Step 8, d_loss: 0.3575720191001892, g_loss: 3.862448215484619\n","Epoch 318/4000, Step 9, d_loss: 0.3523593246936798, g_loss: 4.155999660491943\n","Epoch 318/4000, Step 10, d_loss: 0.37555763125419617, g_loss: 5.140185356140137\n","Epoch 318/4000, Step 11, d_loss: 0.3763919472694397, g_loss: 4.667796611785889\n","Epoch 318/4000, Step 12, d_loss: 0.3963351547718048, g_loss: 5.330623626708984\n","Epoch 318/4000, Step 13, d_loss: 0.3833172917366028, g_loss: 5.506371974945068\n","Epoch 318/4000, Step 14, d_loss: 0.3623337149620056, g_loss: 3.975292921066284\n","Epoch 318/4000, Step 15, d_loss: 0.39367175102233887, g_loss: 3.7503530979156494\n","Epoch 318/4000, Step 16, d_loss: 0.3959846794605255, g_loss: 5.377628326416016\n","Epoch 318/4000, Step 17, d_loss: 0.35485947132110596, g_loss: 6.112541198730469\n","Epoch 318/4000, Step 18, d_loss: 0.3730267584323883, g_loss: 3.933628559112549\n","Epoch 318/4000, Step 19, d_loss: 0.4123128652572632, g_loss: 3.9937655925750732\n","Epoch 318/4000, Step 20, d_loss: 0.3744506537914276, g_loss: 5.270595073699951\n","Epoch 318/4000, Step 21, d_loss: 0.3618681728839874, g_loss: 5.068349361419678\n","Epoch 318/4000, Step 22, d_loss: 0.34960389137268066, g_loss: 3.6575217247009277\n","Epoch 318/4000, Step 23, d_loss: 0.3677712082862854, g_loss: 5.310100078582764\n","Epoch 318/4000, Step 24, d_loss: 0.3677055835723877, g_loss: 7.361835956573486\n","Epoch 318/4000, Step 25, d_loss: 0.3814815282821655, g_loss: 6.378921985626221\n","Epoch 318/4000, Step 26, d_loss: 0.3857031762599945, g_loss: 5.907112121582031\n","Epoch 318/4000, Step 27, d_loss: 0.3459511995315552, g_loss: 6.233757972717285\n","Epoch 318/4000, Step 28, d_loss: 0.3659568428993225, g_loss: 4.94435977935791\n","Epoch 318/4000, Step 29, d_loss: 0.3383687734603882, g_loss: 5.316993236541748\n","Epoch 318/4000, Step 30, d_loss: 0.36324959993362427, g_loss: 7.018118381500244\n","Epoch 318/4000, Step 31, d_loss: 0.342444509267807, g_loss: 7.040950298309326\n","Epoch 318/4000, Step 32, d_loss: 0.35629454255104065, g_loss: 6.424498558044434\n","Epoch 318/4000, Step 33, d_loss: 0.3504093289375305, g_loss: 5.538671970367432\n","Epoch 318/4000, Step 34, d_loss: 0.34753215312957764, g_loss: 7.819087028503418\n","Epoch 318/4000, Step 35, d_loss: 0.35406363010406494, g_loss: 7.744826316833496\n","Epoch 318/4000, Step 36, d_loss: 0.34287357330322266, g_loss: 5.350316524505615\n","Epoch 318/4000, Step 37, d_loss: 0.33603355288505554, g_loss: 6.491858005523682\n","Epoch 318/4000, Step 38, d_loss: 0.3372788429260254, g_loss: 6.547583103179932\n","Epoch 318/4000, Step 39, d_loss: 0.343031644821167, g_loss: 6.397116184234619\n","Epoch 318/4000, Step 40, d_loss: 0.33719179034233093, g_loss: 7.599576473236084\n","Epoch 318/4000, Step 41, d_loss: 0.34444913268089294, g_loss: 5.768420219421387\n","Epoch 318/4000, Step 42, d_loss: 0.3721873462200165, g_loss: 5.032649040222168\n","Epoch 318/4000, Step 43, d_loss: 0.35063421726226807, g_loss: 5.796148777008057\n","Epoch 318/4000, Step 44, d_loss: 0.3597531020641327, g_loss: 4.354278087615967\n","Epoch 318/4000, Step 45, d_loss: 0.35518431663513184, g_loss: 5.3247294425964355\n","Epoch 318/4000, Step 46, d_loss: 0.3652450740337372, g_loss: 4.446434497833252\n","Epoch 318/4000, Step 47, d_loss: 0.3638364374637604, g_loss: 5.48052453994751\n","Epoch 318/4000, Step 48, d_loss: 0.35278359055519104, g_loss: 6.099210262298584\n","Epoch 318/4000, Step 49, d_loss: 0.33775755763053894, g_loss: 5.362527847290039\n","Epoch 318/4000, Step 50, d_loss: 0.3395618200302124, g_loss: 6.6898016929626465\n","Epoch 318/4000, Step 51, d_loss: 0.348342627286911, g_loss: 5.800346374511719\n","Epoch 318/4000, Step 52, d_loss: 0.3382628262042999, g_loss: 5.890409469604492\n","Epoch 318/4000, Step 53, d_loss: 0.3353544771671295, g_loss: 5.933533668518066\n","Epoch 318/4000, Step 54, d_loss: 0.33778107166290283, g_loss: 6.699594497680664\n","Epoch 318/4000, Step 55, d_loss: 0.34687191247940063, g_loss: 6.0430989265441895\n","Epoch 318/4000, Step 56, d_loss: 0.3464371860027313, g_loss: 6.468011379241943\n","Epoch 318/4000, Step 57, d_loss: 0.36899906396865845, g_loss: 7.0397419929504395\n","Epoch 318/4000, Step 58, d_loss: 0.3323540985584259, g_loss: 6.6200079917907715\n","Epoch 318/4000, Step 59, d_loss: 0.33224600553512573, g_loss: 7.491708278656006\n","Epoch 318/4000, Step 60, d_loss: 0.3327098488807678, g_loss: 6.251989364624023\n","Epoch 318/4000, Step 61, d_loss: 0.33750152587890625, g_loss: 6.505246639251709\n","Epoch 318/4000, Step 62, d_loss: 0.3443983793258667, g_loss: 6.274501800537109\n","Epoch 318/4000, Step 63, d_loss: 0.34126466512680054, g_loss: 5.769229412078857\n","Epoch 318/4000, Step 64, d_loss: 0.3418278098106384, g_loss: 6.514645576477051\n","Epoch 318/4000, Step 65, d_loss: 0.34628334641456604, g_loss: 6.374515533447266\n","Epoch 318/4000, Step 66, d_loss: 0.3698558211326599, g_loss: 6.805838108062744\n","Epoch 318/4000, Step 67, d_loss: 0.3313315510749817, g_loss: 7.741131782531738\n","Epoch 318/4000, Step 68, d_loss: 0.3411288559436798, g_loss: 6.561281204223633\n","Epoch 318/4000, Step 69, d_loss: 0.34259867668151855, g_loss: 6.800955295562744\n","Epoch 318/4000, Step 70, d_loss: 0.3376585841178894, g_loss: 7.295477390289307\n","Epoch 318/4000, Step 71, d_loss: 0.34912195801734924, g_loss: 9.194157600402832\n","Epoch 318/4000, Step 72, d_loss: 0.3646141290664673, g_loss: 6.91853141784668\n","Epoch 318/4000, Step 73, d_loss: 0.33311909437179565, g_loss: 10.69328498840332\n","Epoch 318/4000, Step 74, d_loss: 0.34407925605773926, g_loss: 5.915621757507324\n","Epoch 318/4000, Step 75, d_loss: 0.33353087306022644, g_loss: 6.645984172821045\n","Epoch 318/4000, Step 76, d_loss: 0.3648827373981476, g_loss: 6.4764275550842285\n","Epoch 318/4000, Step 77, d_loss: 0.3649607300758362, g_loss: 5.190269470214844\n","Epoch 318/4000, Step 78, d_loss: 0.36305904388427734, g_loss: 7.477118492126465\n","Epoch 318/4000, Step 79, d_loss: 0.6290387511253357, g_loss: 7.032011032104492\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 319/4000, Step 1, d_loss: 0.3458126187324524, g_loss: 4.7468581199646\n","Epoch 319/4000, Step 2, d_loss: 0.3887604773044586, g_loss: 4.136842250823975\n","Epoch 319/4000, Step 3, d_loss: 0.4013266861438751, g_loss: 5.703779697418213\n","Epoch 319/4000, Step 4, d_loss: 0.4107886850833893, g_loss: 5.1998982429504395\n","Epoch 319/4000, Step 5, d_loss: 0.40816640853881836, g_loss: 5.288868427276611\n","Epoch 319/4000, Step 6, d_loss: 0.4095335900783539, g_loss: 5.748453140258789\n","Epoch 319/4000, Step 7, d_loss: 0.43512430787086487, g_loss: 5.999007225036621\n","Epoch 319/4000, Step 8, d_loss: 0.39027655124664307, g_loss: 4.785080909729004\n","Epoch 319/4000, Step 9, d_loss: 0.35187003016471863, g_loss: 5.795603275299072\n","Epoch 319/4000, Step 10, d_loss: 0.35425636172294617, g_loss: 5.230907917022705\n","Epoch 319/4000, Step 11, d_loss: 0.3635832369327545, g_loss: 5.9062066078186035\n","Epoch 319/4000, Step 12, d_loss: 0.3583028018474579, g_loss: 7.447513103485107\n","Epoch 319/4000, Step 13, d_loss: 0.3736802637577057, g_loss: 4.943420886993408\n","Epoch 319/4000, Step 14, d_loss: 0.39060503244400024, g_loss: 4.277058124542236\n","Epoch 319/4000, Step 15, d_loss: 0.3493328094482422, g_loss: 4.000612735748291\n","Epoch 319/4000, Step 16, d_loss: 0.3724186420440674, g_loss: 6.424283027648926\n","Epoch 319/4000, Step 17, d_loss: 0.36602434515953064, g_loss: 4.8467230796813965\n","Epoch 319/4000, Step 18, d_loss: 0.3638555109500885, g_loss: 5.67703104019165\n","Epoch 319/4000, Step 19, d_loss: 0.37972626090049744, g_loss: 6.798778057098389\n","Epoch 319/4000, Step 20, d_loss: 0.3487470746040344, g_loss: 4.830287456512451\n","Epoch 319/4000, Step 21, d_loss: 0.3573996424674988, g_loss: 5.239274501800537\n","Epoch 319/4000, Step 22, d_loss: 0.3465452492237091, g_loss: 6.297954559326172\n","Epoch 319/4000, Step 23, d_loss: 0.34499308466911316, g_loss: 5.5830302238464355\n","Epoch 319/4000, Step 24, d_loss: 0.34030404686927795, g_loss: 6.508701801300049\n","Epoch 319/4000, Step 25, d_loss: 0.33851879835128784, g_loss: 5.928912162780762\n","Epoch 319/4000, Step 26, d_loss: 0.3342375159263611, g_loss: 5.783178329467773\n","Epoch 319/4000, Step 27, d_loss: 0.34070542454719543, g_loss: 5.900189399719238\n","Epoch 319/4000, Step 28, d_loss: 0.3521948754787445, g_loss: 5.62440824508667\n","Epoch 319/4000, Step 29, d_loss: 0.3367164134979248, g_loss: 6.188711166381836\n","Epoch 319/4000, Step 30, d_loss: 0.34268930554389954, g_loss: 5.289917945861816\n","Epoch 319/4000, Step 31, d_loss: 0.35997405648231506, g_loss: 5.552153587341309\n","Epoch 319/4000, Step 32, d_loss: 0.3395420014858246, g_loss: 5.736692428588867\n","Epoch 319/4000, Step 33, d_loss: 0.3366400897502899, g_loss: 4.5746355056762695\n","Epoch 319/4000, Step 34, d_loss: 0.34422898292541504, g_loss: 5.752729892730713\n","Epoch 319/4000, Step 35, d_loss: 0.34109023213386536, g_loss: 5.430799961090088\n","Epoch 319/4000, Step 36, d_loss: 0.3486408591270447, g_loss: 6.051425457000732\n","Epoch 319/4000, Step 37, d_loss: 0.35684677958488464, g_loss: 5.5298285484313965\n","Epoch 319/4000, Step 38, d_loss: 0.34120431542396545, g_loss: 6.620795726776123\n","Epoch 319/4000, Step 39, d_loss: 0.36835917830467224, g_loss: 6.2695770263671875\n","Epoch 319/4000, Step 40, d_loss: 0.33503666520118713, g_loss: 6.27910852432251\n","Epoch 319/4000, Step 41, d_loss: 0.3330805003643036, g_loss: 5.913316249847412\n","Epoch 319/4000, Step 42, d_loss: 0.33605244755744934, g_loss: 7.328392028808594\n","Epoch 319/4000, Step 43, d_loss: 0.3330989480018616, g_loss: 6.39168119430542\n","Epoch 319/4000, Step 44, d_loss: 0.36353665590286255, g_loss: 6.241316318511963\n","Epoch 319/4000, Step 45, d_loss: 0.333824098110199, g_loss: 5.539910793304443\n","Epoch 319/4000, Step 46, d_loss: 0.34265100955963135, g_loss: 5.361476421356201\n","Epoch 319/4000, Step 47, d_loss: 0.33539319038391113, g_loss: 5.670403957366943\n","Epoch 319/4000, Step 48, d_loss: 0.335657000541687, g_loss: 6.118567943572998\n","Epoch 319/4000, Step 49, d_loss: 0.33238303661346436, g_loss: 5.226870059967041\n","Epoch 319/4000, Step 50, d_loss: 0.3388086259365082, g_loss: 6.025997161865234\n","Epoch 319/4000, Step 51, d_loss: 0.33277130126953125, g_loss: 5.216775417327881\n","Epoch 319/4000, Step 52, d_loss: 0.3375853896141052, g_loss: 7.423954963684082\n","Epoch 319/4000, Step 53, d_loss: 0.3393809199333191, g_loss: 7.295807838439941\n","Epoch 319/4000, Step 54, d_loss: 0.4000338613986969, g_loss: 6.42153263092041\n","Epoch 319/4000, Step 55, d_loss: 0.32961517572402954, g_loss: 5.937283992767334\n","Epoch 319/4000, Step 56, d_loss: 0.33559492230415344, g_loss: 5.778141021728516\n","Epoch 319/4000, Step 57, d_loss: 0.34107545018196106, g_loss: 5.817349433898926\n","Epoch 319/4000, Step 58, d_loss: 0.34386909008026123, g_loss: 5.455300807952881\n","Epoch 319/4000, Step 59, d_loss: 0.3516574501991272, g_loss: 5.41682243347168\n","Epoch 319/4000, Step 60, d_loss: 0.33717960119247437, g_loss: 5.68314790725708\n","Epoch 319/4000, Step 61, d_loss: 0.3447535037994385, g_loss: 5.927433013916016\n","Epoch 319/4000, Step 62, d_loss: 0.34208306670188904, g_loss: 5.531739711761475\n","Epoch 319/4000, Step 63, d_loss: 0.3406476378440857, g_loss: 5.468024253845215\n","Epoch 319/4000, Step 64, d_loss: 0.33854570984840393, g_loss: 5.865344524383545\n","Epoch 319/4000, Step 65, d_loss: 0.35385051369667053, g_loss: 5.630206108093262\n","Epoch 319/4000, Step 66, d_loss: 0.34506887197494507, g_loss: 4.976841449737549\n","Epoch 319/4000, Step 67, d_loss: 0.3705374300479889, g_loss: 4.787242412567139\n","Epoch 319/4000, Step 68, d_loss: 0.34941229224205017, g_loss: 4.781510829925537\n","Epoch 319/4000, Step 69, d_loss: 0.3489459455013275, g_loss: 6.914556980133057\n","Epoch 319/4000, Step 70, d_loss: 0.3406582176685333, g_loss: 5.18085241317749\n","Epoch 319/4000, Step 71, d_loss: 0.3324812948703766, g_loss: 4.921618938446045\n","Epoch 319/4000, Step 72, d_loss: 0.3394460678100586, g_loss: 5.252974033355713\n","Epoch 319/4000, Step 73, d_loss: 0.34639254212379456, g_loss: 5.627544403076172\n","Epoch 319/4000, Step 74, d_loss: 0.34652405977249146, g_loss: 5.525818824768066\n","Epoch 319/4000, Step 75, d_loss: 0.3328651189804077, g_loss: 6.181031703948975\n","Epoch 319/4000, Step 76, d_loss: 0.338957279920578, g_loss: 5.340516567230225\n","Epoch 319/4000, Step 77, d_loss: 0.33425888419151306, g_loss: 9.309253692626953\n","Epoch 319/4000, Step 78, d_loss: 0.3526245653629303, g_loss: 6.3645100593566895\n","Epoch 319/4000, Step 79, d_loss: 0.3400057256221771, g_loss: 5.2916059494018555\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 320/4000, Step 1, d_loss: 0.36599260568618774, g_loss: 4.391826629638672\n","Epoch 320/4000, Step 2, d_loss: 0.33953362703323364, g_loss: 5.556675434112549\n","Epoch 320/4000, Step 3, d_loss: 0.35770758986473083, g_loss: 5.3794403076171875\n","Epoch 320/4000, Step 4, d_loss: 0.3464664816856384, g_loss: 5.518866062164307\n","Epoch 320/4000, Step 5, d_loss: 0.34698817133903503, g_loss: 5.558204650878906\n","Epoch 320/4000, Step 6, d_loss: 0.33857476711273193, g_loss: 5.158842086791992\n","Epoch 320/4000, Step 7, d_loss: 0.3372107446193695, g_loss: 6.866803169250488\n","Epoch 320/4000, Step 8, d_loss: 0.3410245478153229, g_loss: 8.353131294250488\n","Epoch 320/4000, Step 9, d_loss: 0.33571046590805054, g_loss: 5.75953483581543\n","Epoch 320/4000, Step 10, d_loss: 0.3377205431461334, g_loss: 5.364720821380615\n","Epoch 320/4000, Step 11, d_loss: 0.330240398645401, g_loss: 5.682563781738281\n","Epoch 320/4000, Step 12, d_loss: 0.33492404222488403, g_loss: 5.8927459716796875\n","Epoch 320/4000, Step 13, d_loss: 0.34233370423316956, g_loss: 5.253810882568359\n","Epoch 320/4000, Step 14, d_loss: 0.3314860761165619, g_loss: 5.779142379760742\n","Epoch 320/4000, Step 15, d_loss: 0.34113261103630066, g_loss: 6.644168376922607\n","Epoch 320/4000, Step 16, d_loss: 0.3320195972919464, g_loss: 5.932260990142822\n","Epoch 320/4000, Step 17, d_loss: 0.33530157804489136, g_loss: 5.972543716430664\n","Epoch 320/4000, Step 18, d_loss: 0.3376362919807434, g_loss: 5.790689945220947\n","Epoch 320/4000, Step 19, d_loss: 0.34827685356140137, g_loss: 5.5407209396362305\n","Epoch 320/4000, Step 20, d_loss: 0.3402774930000305, g_loss: 6.362880706787109\n","Epoch 320/4000, Step 21, d_loss: 0.3434871435165405, g_loss: 6.098766803741455\n","Epoch 320/4000, Step 22, d_loss: 0.33000221848487854, g_loss: 7.940624713897705\n","Epoch 320/4000, Step 23, d_loss: 0.3390463590621948, g_loss: 7.203652381896973\n","Epoch 320/4000, Step 24, d_loss: 0.33800098299980164, g_loss: 5.443452835083008\n","Epoch 320/4000, Step 25, d_loss: 0.35944172739982605, g_loss: 6.64285135269165\n","Epoch 320/4000, Step 26, d_loss: 0.3408164083957672, g_loss: 5.812777519226074\n","Epoch 320/4000, Step 27, d_loss: 0.33341550827026367, g_loss: 5.508171081542969\n","Epoch 320/4000, Step 28, d_loss: 0.33392220735549927, g_loss: 6.277540683746338\n","Epoch 320/4000, Step 29, d_loss: 0.3397856056690216, g_loss: 5.573789596557617\n","Epoch 320/4000, Step 30, d_loss: 0.3365066945552826, g_loss: 5.280882835388184\n","Epoch 320/4000, Step 31, d_loss: 0.34568366408348083, g_loss: 5.784107685089111\n","Epoch 320/4000, Step 32, d_loss: 0.335422545671463, g_loss: 6.539943218231201\n","Epoch 320/4000, Step 33, d_loss: 0.32969778776168823, g_loss: 5.944608688354492\n","Epoch 320/4000, Step 34, d_loss: 0.3367393910884857, g_loss: 5.720047473907471\n","Epoch 320/4000, Step 35, d_loss: 0.34577807784080505, g_loss: 5.4866132736206055\n","Epoch 320/4000, Step 36, d_loss: 0.33356064558029175, g_loss: 9.217239379882812\n","Epoch 320/4000, Step 37, d_loss: 0.3326495587825775, g_loss: 5.756536483764648\n","Epoch 320/4000, Step 38, d_loss: 0.33522793650627136, g_loss: 8.972762107849121\n","Epoch 320/4000, Step 39, d_loss: 0.33365386724472046, g_loss: 5.120217323303223\n","Epoch 320/4000, Step 40, d_loss: 0.3313908576965332, g_loss: 5.674022674560547\n","Epoch 320/4000, Step 41, d_loss: 0.33328479528427124, g_loss: 6.183794975280762\n","Epoch 320/4000, Step 42, d_loss: 0.33637163043022156, g_loss: 5.864863872528076\n","Epoch 320/4000, Step 43, d_loss: 0.33538514375686646, g_loss: 8.610654830932617\n","Epoch 320/4000, Step 44, d_loss: 0.3344586193561554, g_loss: 9.219294548034668\n","Epoch 320/4000, Step 45, d_loss: 0.33412638306617737, g_loss: 6.287505626678467\n","Epoch 320/4000, Step 46, d_loss: 0.35532346367836, g_loss: 5.889827251434326\n","Epoch 320/4000, Step 47, d_loss: 0.33239418268203735, g_loss: 5.454061985015869\n","Epoch 320/4000, Step 48, d_loss: 0.3312196731567383, g_loss: 5.325622081756592\n","Epoch 320/4000, Step 49, d_loss: 0.3299659788608551, g_loss: 5.384020805358887\n","Epoch 320/4000, Step 50, d_loss: 0.34167444705963135, g_loss: 5.7737717628479\n","Epoch 320/4000, Step 51, d_loss: 0.3379584848880768, g_loss: 8.1448392868042\n","Epoch 320/4000, Step 52, d_loss: 0.3370814025402069, g_loss: 6.158466339111328\n","Epoch 320/4000, Step 53, d_loss: 0.32987651228904724, g_loss: 6.042494773864746\n","Epoch 320/4000, Step 54, d_loss: 0.347234308719635, g_loss: 5.6277174949646\n","Epoch 320/4000, Step 55, d_loss: 0.3311176002025604, g_loss: 7.241339683532715\n","Epoch 320/4000, Step 56, d_loss: 0.34652432799339294, g_loss: 6.053088188171387\n","Epoch 320/4000, Step 57, d_loss: 0.3579939305782318, g_loss: 5.647618293762207\n","Epoch 320/4000, Step 58, d_loss: 0.33843424916267395, g_loss: 5.855135917663574\n","Epoch 320/4000, Step 59, d_loss: 0.3365507423877716, g_loss: 5.298811912536621\n","Epoch 320/4000, Step 60, d_loss: 0.33264172077178955, g_loss: 5.711949348449707\n","Epoch 320/4000, Step 61, d_loss: 0.3338370621204376, g_loss: 5.745377540588379\n","Epoch 320/4000, Step 62, d_loss: 0.3404572606086731, g_loss: 6.261922836303711\n","Epoch 320/4000, Step 63, d_loss: 0.34061530232429504, g_loss: 4.928651809692383\n","Epoch 320/4000, Step 64, d_loss: 0.33468276262283325, g_loss: 7.075313091278076\n","Epoch 320/4000, Step 65, d_loss: 0.34115883708000183, g_loss: 4.96263313293457\n","Epoch 320/4000, Step 66, d_loss: 0.36047741770744324, g_loss: 4.54747200012207\n","Epoch 320/4000, Step 67, d_loss: 0.34198978543281555, g_loss: 5.616346836090088\n","Epoch 320/4000, Step 68, d_loss: 0.330322265625, g_loss: 4.936141014099121\n","Epoch 320/4000, Step 69, d_loss: 0.34426748752593994, g_loss: 5.266928672790527\n","Epoch 320/4000, Step 70, d_loss: 0.33023253083229065, g_loss: 6.296116828918457\n","Epoch 320/4000, Step 71, d_loss: 0.33304962515830994, g_loss: 6.040339469909668\n","Epoch 320/4000, Step 72, d_loss: 0.3496212959289551, g_loss: 5.708774566650391\n","Epoch 320/4000, Step 73, d_loss: 0.33178725838661194, g_loss: 5.57430362701416\n","Epoch 320/4000, Step 74, d_loss: 0.33601436018943787, g_loss: 8.430977821350098\n","Epoch 320/4000, Step 75, d_loss: 0.3420376777648926, g_loss: 5.859447002410889\n","Epoch 320/4000, Step 76, d_loss: 0.33379215002059937, g_loss: 4.7158732414245605\n","Epoch 320/4000, Step 77, d_loss: 0.3459169566631317, g_loss: 6.229635715484619\n","Epoch 320/4000, Step 78, d_loss: 0.3494509756565094, g_loss: 4.898070812225342\n","Epoch 320/4000, Step 79, d_loss: 1.3267672061920166, g_loss: 3.8782119750976562\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 321/4000, Step 1, d_loss: 0.4267530143260956, g_loss: 3.4959797859191895\n","Epoch 321/4000, Step 2, d_loss: 0.4440789818763733, g_loss: 3.415985345840454\n","Epoch 321/4000, Step 3, d_loss: 0.47415027022361755, g_loss: 3.648508310317993\n","Epoch 321/4000, Step 4, d_loss: 0.46761617064476013, g_loss: 4.2914323806762695\n","Epoch 321/4000, Step 5, d_loss: 0.47286126017570496, g_loss: 3.2088277339935303\n","Epoch 321/4000, Step 6, d_loss: 0.5327690839767456, g_loss: 3.785956382751465\n","Epoch 321/4000, Step 7, d_loss: 0.6093682050704956, g_loss: 3.7671053409576416\n","Epoch 321/4000, Step 8, d_loss: 0.41164782643318176, g_loss: 4.515694618225098\n","Epoch 321/4000, Step 9, d_loss: 0.36578014492988586, g_loss: 4.219879150390625\n","Epoch 321/4000, Step 10, d_loss: 0.3449406027793884, g_loss: 5.021915912628174\n","Epoch 321/4000, Step 11, d_loss: 0.3614642024040222, g_loss: 3.61781644821167\n","Epoch 321/4000, Step 12, d_loss: 0.39347460865974426, g_loss: 7.330053329467773\n","Epoch 321/4000, Step 13, d_loss: 0.4276914894580841, g_loss: 2.4774158000946045\n","Epoch 321/4000, Step 14, d_loss: 0.39630061388015747, g_loss: 2.539034366607666\n","Epoch 321/4000, Step 15, d_loss: 0.4021836519241333, g_loss: 3.7034506797790527\n","Epoch 321/4000, Step 16, d_loss: 0.40554919838905334, g_loss: 2.0164167881011963\n","Epoch 321/4000, Step 17, d_loss: 0.3784799575805664, g_loss: 4.917902946472168\n","Epoch 321/4000, Step 18, d_loss: 0.5326489806175232, g_loss: 6.059591293334961\n","Epoch 321/4000, Step 19, d_loss: 0.3560585677623749, g_loss: 2.937927484512329\n","Epoch 321/4000, Step 20, d_loss: 0.388966828584671, g_loss: 4.392261028289795\n","Epoch 321/4000, Step 21, d_loss: 0.3898150324821472, g_loss: 3.3326573371887207\n","Epoch 321/4000, Step 22, d_loss: 0.46053794026374817, g_loss: 5.204668998718262\n","Epoch 321/4000, Step 23, d_loss: 0.3636482357978821, g_loss: 2.6789228916168213\n","Epoch 321/4000, Step 24, d_loss: 0.3838425874710083, g_loss: 3.077883005142212\n","Epoch 321/4000, Step 25, d_loss: 0.416831910610199, g_loss: 4.688855171203613\n","Epoch 321/4000, Step 26, d_loss: 0.36291223764419556, g_loss: 3.5572872161865234\n","Epoch 321/4000, Step 27, d_loss: 0.36192891001701355, g_loss: 3.490370035171509\n","Epoch 321/4000, Step 28, d_loss: 0.35426297783851624, g_loss: 5.0298357009887695\n","Epoch 321/4000, Step 29, d_loss: 0.3421599864959717, g_loss: 4.650496482849121\n","Epoch 321/4000, Step 30, d_loss: 0.3438270688056946, g_loss: 5.535243988037109\n","Epoch 321/4000, Step 31, d_loss: 0.35804668068885803, g_loss: 5.619351387023926\n","Epoch 321/4000, Step 32, d_loss: 0.3884333670139313, g_loss: 3.753788471221924\n","Epoch 321/4000, Step 33, d_loss: 0.37958383560180664, g_loss: 4.509928226470947\n","Epoch 321/4000, Step 34, d_loss: 0.3798079490661621, g_loss: 3.678480625152588\n","Epoch 321/4000, Step 35, d_loss: 0.381887286901474, g_loss: 4.615779876708984\n","Epoch 321/4000, Step 36, d_loss: 0.35184407234191895, g_loss: 3.9380764961242676\n","Epoch 321/4000, Step 37, d_loss: 0.35193541646003723, g_loss: 5.85246467590332\n","Epoch 321/4000, Step 38, d_loss: 0.33921676874160767, g_loss: 4.668026924133301\n","Epoch 321/4000, Step 39, d_loss: 0.3360203504562378, g_loss: 5.029709339141846\n","Epoch 321/4000, Step 40, d_loss: 0.3415469825267792, g_loss: 5.361528396606445\n","Epoch 321/4000, Step 41, d_loss: 0.4158484935760498, g_loss: 5.727808475494385\n","Epoch 321/4000, Step 42, d_loss: 0.338457852602005, g_loss: 5.351235389709473\n","Epoch 321/4000, Step 43, d_loss: 0.35928675532341003, g_loss: 4.3108906745910645\n","Epoch 321/4000, Step 44, d_loss: 0.3729183077812195, g_loss: 5.07560396194458\n","Epoch 321/4000, Step 45, d_loss: 0.3345724046230316, g_loss: 5.352802276611328\n","Epoch 321/4000, Step 46, d_loss: 0.33864304423332214, g_loss: 6.353975772857666\n","Epoch 321/4000, Step 47, d_loss: 0.336811363697052, g_loss: 5.044898509979248\n","Epoch 321/4000, Step 48, d_loss: 0.333422988653183, g_loss: 6.329033374786377\n","Epoch 321/4000, Step 49, d_loss: 0.3398008942604065, g_loss: 6.584074020385742\n","Epoch 321/4000, Step 50, d_loss: 0.3610074818134308, g_loss: 8.15783405303955\n","Epoch 321/4000, Step 51, d_loss: 0.3363721966743469, g_loss: 6.168497085571289\n","Epoch 321/4000, Step 52, d_loss: 0.33851367235183716, g_loss: 6.390350818634033\n","Epoch 321/4000, Step 53, d_loss: 0.34481361508369446, g_loss: 6.797745704650879\n","Epoch 321/4000, Step 54, d_loss: 0.3466832637786865, g_loss: 6.583385467529297\n","Epoch 321/4000, Step 55, d_loss: 0.34210413694381714, g_loss: 7.150193691253662\n","Epoch 321/4000, Step 56, d_loss: 0.33814263343811035, g_loss: 8.8572998046875\n","Epoch 321/4000, Step 57, d_loss: 0.38465416431427, g_loss: 7.038449287414551\n","Epoch 321/4000, Step 58, d_loss: 0.33435875177383423, g_loss: 6.739156246185303\n","Epoch 321/4000, Step 59, d_loss: 0.34940141439437866, g_loss: 4.822459697723389\n","Epoch 321/4000, Step 60, d_loss: 0.3449748456478119, g_loss: 5.4926581382751465\n","Epoch 321/4000, Step 61, d_loss: 0.3421357572078705, g_loss: 7.030532360076904\n","Epoch 321/4000, Step 62, d_loss: 0.33810552954673767, g_loss: 6.655325889587402\n","Epoch 321/4000, Step 63, d_loss: 0.3589613735675812, g_loss: 4.9803361892700195\n","Epoch 321/4000, Step 64, d_loss: 0.3518558442592621, g_loss: 7.301287651062012\n","Epoch 321/4000, Step 65, d_loss: 0.3456955552101135, g_loss: 6.687925338745117\n","Epoch 321/4000, Step 66, d_loss: 0.33402034640312195, g_loss: 6.077441692352295\n","Epoch 321/4000, Step 67, d_loss: 0.3585381805896759, g_loss: 5.69949197769165\n","Epoch 321/4000, Step 68, d_loss: 0.34356194734573364, g_loss: 5.745599746704102\n","Epoch 321/4000, Step 69, d_loss: 0.4745946228504181, g_loss: 5.747256278991699\n","Epoch 321/4000, Step 70, d_loss: 0.3541266918182373, g_loss: 5.147690773010254\n","Epoch 321/4000, Step 71, d_loss: 0.38722604513168335, g_loss: 5.092069149017334\n","Epoch 321/4000, Step 72, d_loss: 0.39952823519706726, g_loss: 6.440540313720703\n","Epoch 321/4000, Step 73, d_loss: 0.4262036383152008, g_loss: 4.546884536743164\n","Epoch 321/4000, Step 74, d_loss: 0.36529889702796936, g_loss: 5.025223731994629\n","Epoch 321/4000, Step 75, d_loss: 0.34154850244522095, g_loss: 6.490188121795654\n","Epoch 321/4000, Step 76, d_loss: 0.3391934335231781, g_loss: 6.098207950592041\n","Epoch 321/4000, Step 77, d_loss: 0.3551480770111084, g_loss: 5.983922004699707\n","Epoch 321/4000, Step 78, d_loss: 0.3741717040538788, g_loss: 6.9998297691345215\n","Epoch 321/4000, Step 79, d_loss: 0.3929471969604492, g_loss: 5.650947570800781\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 322/4000, Step 1, d_loss: 0.33586931228637695, g_loss: 5.546907901763916\n","Epoch 322/4000, Step 2, d_loss: 0.34469690918922424, g_loss: 4.94605827331543\n","Epoch 322/4000, Step 3, d_loss: 0.3901093006134033, g_loss: 4.8503098487854\n","Epoch 322/4000, Step 4, d_loss: 0.3911169469356537, g_loss: 4.289092540740967\n","Epoch 322/4000, Step 5, d_loss: 0.3593239188194275, g_loss: 5.222931385040283\n","Epoch 322/4000, Step 6, d_loss: 0.35581931471824646, g_loss: 5.038522243499756\n","Epoch 322/4000, Step 7, d_loss: 0.3394862115383148, g_loss: 5.152573585510254\n","Epoch 322/4000, Step 8, d_loss: 0.35297760367393494, g_loss: 6.203047275543213\n","Epoch 322/4000, Step 9, d_loss: 0.36503979563713074, g_loss: 6.726123332977295\n","Epoch 322/4000, Step 10, d_loss: 0.37911805510520935, g_loss: 6.97309684753418\n","Epoch 322/4000, Step 11, d_loss: 0.3421919643878937, g_loss: 5.167812347412109\n","Epoch 322/4000, Step 12, d_loss: 0.3318978250026703, g_loss: 4.885137557983398\n","Epoch 322/4000, Step 13, d_loss: 0.34590110182762146, g_loss: 5.337535858154297\n","Epoch 322/4000, Step 14, d_loss: 0.35016825795173645, g_loss: 5.280314922332764\n","Epoch 322/4000, Step 15, d_loss: 0.3723354637622833, g_loss: 5.3143815994262695\n","Epoch 322/4000, Step 16, d_loss: 0.36011800169944763, g_loss: 5.357133388519287\n","Epoch 322/4000, Step 17, d_loss: 0.3451305031776428, g_loss: 6.830765724182129\n","Epoch 322/4000, Step 18, d_loss: 0.34374621510505676, g_loss: 6.924441337585449\n","Epoch 322/4000, Step 19, d_loss: 0.33922144770622253, g_loss: 6.6662468910217285\n","Epoch 322/4000, Step 20, d_loss: 0.33756014704704285, g_loss: 6.800082206726074\n","Epoch 322/4000, Step 21, d_loss: 0.34330278635025024, g_loss: 7.701010227203369\n","Epoch 322/4000, Step 22, d_loss: 0.3809633255004883, g_loss: 7.846780300140381\n","Epoch 322/4000, Step 23, d_loss: 0.3470650911331177, g_loss: 6.252166748046875\n","Epoch 322/4000, Step 24, d_loss: 0.33339107036590576, g_loss: 7.379748344421387\n","Epoch 322/4000, Step 25, d_loss: 0.35249873995780945, g_loss: 5.067127227783203\n","Epoch 322/4000, Step 26, d_loss: 0.35245051980018616, g_loss: 5.226710319519043\n","Epoch 322/4000, Step 27, d_loss: 0.357992023229599, g_loss: 7.3678998947143555\n","Epoch 322/4000, Step 28, d_loss: 0.35950809717178345, g_loss: 7.597817420959473\n","Epoch 322/4000, Step 29, d_loss: 0.35065731406211853, g_loss: 6.199464321136475\n","Epoch 322/4000, Step 30, d_loss: 0.344730019569397, g_loss: 7.208271503448486\n","Epoch 322/4000, Step 31, d_loss: 0.32935193181037903, g_loss: 7.395866870880127\n","Epoch 322/4000, Step 32, d_loss: 0.3354145288467407, g_loss: 7.683366298675537\n","Epoch 322/4000, Step 33, d_loss: 0.3430982828140259, g_loss: 7.326567649841309\n","Epoch 322/4000, Step 34, d_loss: 0.35405832529067993, g_loss: 6.9744720458984375\n","Epoch 322/4000, Step 35, d_loss: 0.33162105083465576, g_loss: 6.770905017852783\n","Epoch 322/4000, Step 36, d_loss: 0.3319132328033447, g_loss: 6.6422953605651855\n","Epoch 322/4000, Step 37, d_loss: 0.34005698561668396, g_loss: 6.6886067390441895\n","Epoch 322/4000, Step 38, d_loss: 0.3393699526786804, g_loss: 6.98821496963501\n","Epoch 322/4000, Step 39, d_loss: 0.3483852744102478, g_loss: 6.269025802612305\n","Epoch 322/4000, Step 40, d_loss: 0.33655044436454773, g_loss: 6.300769329071045\n","Epoch 322/4000, Step 41, d_loss: 0.3321461081504822, g_loss: 6.212244033813477\n","Epoch 322/4000, Step 42, d_loss: 0.32974380254745483, g_loss: 7.581668853759766\n","Epoch 322/4000, Step 43, d_loss: 0.3601757884025574, g_loss: 7.366539478302002\n","Epoch 322/4000, Step 44, d_loss: 0.3309274911880493, g_loss: 6.332982063293457\n","Epoch 322/4000, Step 45, d_loss: 0.3300521969795227, g_loss: 6.563770771026611\n","Epoch 322/4000, Step 46, d_loss: 0.33202555775642395, g_loss: 8.16055679321289\n","Epoch 322/4000, Step 47, d_loss: 0.33779752254486084, g_loss: 8.998659133911133\n","Epoch 322/4000, Step 48, d_loss: 0.3345138430595398, g_loss: 7.131464958190918\n","Epoch 322/4000, Step 49, d_loss: 0.3461141884326935, g_loss: 7.846248626708984\n","Epoch 322/4000, Step 50, d_loss: 0.33158010244369507, g_loss: 6.906822681427002\n","Epoch 322/4000, Step 51, d_loss: 0.3277760148048401, g_loss: 6.203518867492676\n","Epoch 322/4000, Step 52, d_loss: 0.3324087858200073, g_loss: 6.955721855163574\n","Epoch 322/4000, Step 53, d_loss: 0.3487105965614319, g_loss: 6.920647144317627\n","Epoch 322/4000, Step 54, d_loss: 0.3439840078353882, g_loss: 7.039684295654297\n","Epoch 322/4000, Step 55, d_loss: 0.3343627452850342, g_loss: 6.059723377227783\n","Epoch 322/4000, Step 56, d_loss: 0.3566496968269348, g_loss: 9.641288757324219\n","Epoch 322/4000, Step 57, d_loss: 0.34684088826179504, g_loss: 9.266096115112305\n","Epoch 322/4000, Step 58, d_loss: 0.34342899918556213, g_loss: 7.6800007820129395\n","Epoch 322/4000, Step 59, d_loss: 0.35576215386390686, g_loss: 7.6018242835998535\n","Epoch 322/4000, Step 60, d_loss: 0.32942989468574524, g_loss: 7.004523754119873\n","Epoch 322/4000, Step 61, d_loss: 0.3315894603729248, g_loss: 7.014536380767822\n","Epoch 322/4000, Step 62, d_loss: 0.34122931957244873, g_loss: 6.69536828994751\n","Epoch 322/4000, Step 63, d_loss: 0.34149169921875, g_loss: 6.455461502075195\n","Epoch 322/4000, Step 64, d_loss: 0.34139853715896606, g_loss: 7.355240821838379\n","Epoch 322/4000, Step 65, d_loss: 0.33455437421798706, g_loss: 6.96651029586792\n","Epoch 322/4000, Step 66, d_loss: 0.33376219868659973, g_loss: 7.530241012573242\n","Epoch 322/4000, Step 67, d_loss: 0.3421846032142639, g_loss: 6.652636528015137\n","Epoch 322/4000, Step 68, d_loss: 0.34856683015823364, g_loss: 6.654703617095947\n","Epoch 322/4000, Step 69, d_loss: 0.3370020091533661, g_loss: 6.413531303405762\n","Epoch 322/4000, Step 70, d_loss: 0.3340938091278076, g_loss: 5.691213130950928\n","Epoch 322/4000, Step 71, d_loss: 0.3320600390434265, g_loss: 6.348146438598633\n","Epoch 322/4000, Step 72, d_loss: 0.33279943466186523, g_loss: 6.247624397277832\n","Epoch 322/4000, Step 73, d_loss: 0.3292773365974426, g_loss: 5.76647424697876\n","Epoch 322/4000, Step 74, d_loss: 0.32844266295433044, g_loss: 6.32243537902832\n","Epoch 322/4000, Step 75, d_loss: 0.3529171347618103, g_loss: 6.121232986450195\n","Epoch 322/4000, Step 76, d_loss: 0.33411458134651184, g_loss: 6.165152072906494\n","Epoch 322/4000, Step 77, d_loss: 0.335309237241745, g_loss: 7.213584899902344\n","Epoch 322/4000, Step 78, d_loss: 0.32946592569351196, g_loss: 6.785222053527832\n","Epoch 322/4000, Step 79, d_loss: 0.32976198196411133, g_loss: 6.983266830444336\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 323/4000, Step 1, d_loss: 0.3510516583919525, g_loss: 6.942317008972168\n","Epoch 323/4000, Step 2, d_loss: 0.3391591012477875, g_loss: 6.4012250900268555\n","Epoch 323/4000, Step 3, d_loss: 0.3655478358268738, g_loss: 6.21605920791626\n","Epoch 323/4000, Step 4, d_loss: 0.3305118680000305, g_loss: 5.856218338012695\n","Epoch 323/4000, Step 5, d_loss: 0.34122133255004883, g_loss: 7.077266693115234\n","Epoch 323/4000, Step 6, d_loss: 0.34931647777557373, g_loss: 6.0714030265808105\n","Epoch 323/4000, Step 7, d_loss: 0.3640635907649994, g_loss: 5.270040035247803\n","Epoch 323/4000, Step 8, d_loss: 0.3460733890533447, g_loss: 6.06667947769165\n","Epoch 323/4000, Step 9, d_loss: 0.3353623151779175, g_loss: 5.140964984893799\n","Epoch 323/4000, Step 10, d_loss: 0.3301498293876648, g_loss: 6.151174545288086\n","Epoch 323/4000, Step 11, d_loss: 0.34293508529663086, g_loss: 6.157008171081543\n","Epoch 323/4000, Step 12, d_loss: 0.35405242443084717, g_loss: 6.665525913238525\n","Epoch 323/4000, Step 13, d_loss: 0.3482055068016052, g_loss: 5.903402805328369\n","Epoch 323/4000, Step 14, d_loss: 0.32898378372192383, g_loss: 7.866117477416992\n","Epoch 323/4000, Step 15, d_loss: 0.3324359953403473, g_loss: 5.166633129119873\n","Epoch 323/4000, Step 16, d_loss: 0.3656483292579651, g_loss: 5.538722038269043\n","Epoch 323/4000, Step 17, d_loss: 0.35205966234207153, g_loss: 5.4943366050720215\n","Epoch 323/4000, Step 18, d_loss: 0.3405766189098358, g_loss: 5.994818210601807\n","Epoch 323/4000, Step 19, d_loss: 0.3347645699977875, g_loss: 7.379812240600586\n","Epoch 323/4000, Step 20, d_loss: 0.33440107107162476, g_loss: 6.19246244430542\n","Epoch 323/4000, Step 21, d_loss: 0.33794745802879333, g_loss: 6.445103645324707\n","Epoch 323/4000, Step 22, d_loss: 0.3310982286930084, g_loss: 5.943783760070801\n","Epoch 323/4000, Step 23, d_loss: 0.3367046117782593, g_loss: 7.8727216720581055\n","Epoch 323/4000, Step 24, d_loss: 0.3297601640224457, g_loss: 7.880961894989014\n","Epoch 323/4000, Step 25, d_loss: 0.33253127336502075, g_loss: 5.753643035888672\n","Epoch 323/4000, Step 26, d_loss: 0.3348231315612793, g_loss: 8.420629501342773\n","Epoch 323/4000, Step 27, d_loss: 0.3306633234024048, g_loss: 6.0794596672058105\n","Epoch 323/4000, Step 28, d_loss: 0.33416181802749634, g_loss: 7.826413631439209\n","Epoch 323/4000, Step 29, d_loss: 0.3393744230270386, g_loss: 5.384287357330322\n","Epoch 323/4000, Step 30, d_loss: 0.3336455821990967, g_loss: 5.8136887550354\n","Epoch 323/4000, Step 31, d_loss: 0.3557302951812744, g_loss: 6.4736480712890625\n","Epoch 323/4000, Step 32, d_loss: 0.33228185772895813, g_loss: 6.204868793487549\n","Epoch 323/4000, Step 33, d_loss: 0.33847394585609436, g_loss: 6.088215351104736\n","Epoch 323/4000, Step 34, d_loss: 0.33222630620002747, g_loss: 5.84916353225708\n","Epoch 323/4000, Step 35, d_loss: 0.3413380980491638, g_loss: 5.036149978637695\n","Epoch 323/4000, Step 36, d_loss: 0.33349737524986267, g_loss: 5.587018966674805\n","Epoch 323/4000, Step 37, d_loss: 0.33315137028694153, g_loss: 5.973044395446777\n","Epoch 323/4000, Step 38, d_loss: 0.32887932658195496, g_loss: 5.531334400177002\n","Epoch 323/4000, Step 39, d_loss: 0.3316190242767334, g_loss: 6.130793571472168\n","Epoch 323/4000, Step 40, d_loss: 0.3412884771823883, g_loss: 5.65115213394165\n","Epoch 323/4000, Step 41, d_loss: 0.3324914872646332, g_loss: 6.100590705871582\n","Epoch 323/4000, Step 42, d_loss: 0.33640220761299133, g_loss: 6.4707841873168945\n","Epoch 323/4000, Step 43, d_loss: 0.3283710181713104, g_loss: 7.320645332336426\n","Epoch 323/4000, Step 44, d_loss: 0.3316967785358429, g_loss: 5.443113327026367\n","Epoch 323/4000, Step 45, d_loss: 0.3294751048088074, g_loss: 6.328648090362549\n","Epoch 323/4000, Step 46, d_loss: 0.3336696922779083, g_loss: 6.229145050048828\n","Epoch 323/4000, Step 47, d_loss: 0.32929927110671997, g_loss: 6.073709487915039\n","Epoch 323/4000, Step 48, d_loss: 0.33505529165267944, g_loss: 6.151246547698975\n","Epoch 323/4000, Step 49, d_loss: 0.3362809121608734, g_loss: 5.708371639251709\n","Epoch 323/4000, Step 50, d_loss: 0.35173332691192627, g_loss: 5.156430721282959\n","Epoch 323/4000, Step 51, d_loss: 0.33547061681747437, g_loss: 5.6872944831848145\n","Epoch 323/4000, Step 52, d_loss: 0.33095690608024597, g_loss: 5.9658660888671875\n","Epoch 323/4000, Step 53, d_loss: 0.3309403657913208, g_loss: 6.358397483825684\n","Epoch 323/4000, Step 54, d_loss: 0.3364318013191223, g_loss: 5.766329765319824\n","Epoch 323/4000, Step 55, d_loss: 0.33634066581726074, g_loss: 5.407765865325928\n","Epoch 323/4000, Step 56, d_loss: 0.33591511845588684, g_loss: 6.006404399871826\n","Epoch 323/4000, Step 57, d_loss: 0.3406301438808441, g_loss: 5.35635232925415\n","Epoch 323/4000, Step 58, d_loss: 0.3369683027267456, g_loss: 4.787841796875\n","Epoch 323/4000, Step 59, d_loss: 0.3410260081291199, g_loss: 5.448852062225342\n","Epoch 323/4000, Step 60, d_loss: 0.3341197967529297, g_loss: 5.513410568237305\n","Epoch 323/4000, Step 61, d_loss: 0.3462027907371521, g_loss: 7.664844036102295\n","Epoch 323/4000, Step 62, d_loss: 0.33573004603385925, g_loss: 5.949705123901367\n","Epoch 323/4000, Step 63, d_loss: 0.3530772626399994, g_loss: 6.580631256103516\n","Epoch 323/4000, Step 64, d_loss: 0.3518100678920746, g_loss: 6.407578468322754\n","Epoch 323/4000, Step 65, d_loss: 0.3339695930480957, g_loss: 5.540101051330566\n","Epoch 323/4000, Step 66, d_loss: 0.3338591456413269, g_loss: 5.728343963623047\n","Epoch 323/4000, Step 67, d_loss: 0.3479290306568146, g_loss: 5.153955936431885\n","Epoch 323/4000, Step 68, d_loss: 0.3422723710536957, g_loss: 4.840094566345215\n","Epoch 323/4000, Step 69, d_loss: 0.33271217346191406, g_loss: 5.6741719245910645\n","Epoch 323/4000, Step 70, d_loss: 0.35019034147262573, g_loss: 5.165519714355469\n","Epoch 323/4000, Step 71, d_loss: 0.328081876039505, g_loss: 5.318546772003174\n","Epoch 323/4000, Step 72, d_loss: 0.3316485285758972, g_loss: 6.360404968261719\n","Epoch 323/4000, Step 73, d_loss: 0.3513621985912323, g_loss: 7.262397289276123\n","Epoch 323/4000, Step 74, d_loss: 0.3382374346256256, g_loss: 7.529510498046875\n","Epoch 323/4000, Step 75, d_loss: 0.32836028933525085, g_loss: 5.937760353088379\n","Epoch 323/4000, Step 76, d_loss: 0.3382946252822876, g_loss: 6.2093963623046875\n","Epoch 323/4000, Step 77, d_loss: 0.33534112572669983, g_loss: 6.061306476593018\n","Epoch 323/4000, Step 78, d_loss: 0.34430959820747375, g_loss: 5.895493030548096\n","Epoch 323/4000, Step 79, d_loss: 0.5662214159965515, g_loss: 4.633553504943848\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 324/4000, Step 1, d_loss: 0.3737408518791199, g_loss: 4.022832870483398\n","Epoch 324/4000, Step 2, d_loss: 0.3841620683670044, g_loss: 4.084568977355957\n","Epoch 324/4000, Step 3, d_loss: 0.42687225341796875, g_loss: 4.388728618621826\n","Epoch 324/4000, Step 4, d_loss: 0.40013357996940613, g_loss: 4.013518810272217\n","Epoch 324/4000, Step 5, d_loss: 0.40029630064964294, g_loss: 4.139161586761475\n","Epoch 324/4000, Step 6, d_loss: 0.41479384899139404, g_loss: 5.542092323303223\n","Epoch 324/4000, Step 7, d_loss: 0.36696505546569824, g_loss: 4.65562105178833\n","Epoch 324/4000, Step 8, d_loss: 0.35118368268013, g_loss: 4.604097366333008\n","Epoch 324/4000, Step 9, d_loss: 0.34158289432525635, g_loss: 7.440141201019287\n","Epoch 324/4000, Step 10, d_loss: 0.342044860124588, g_loss: 6.166906833648682\n","Epoch 324/4000, Step 11, d_loss: 0.3490103483200073, g_loss: 5.9806718826293945\n","Epoch 324/4000, Step 12, d_loss: 0.3635511100292206, g_loss: 6.221153259277344\n","Epoch 324/4000, Step 13, d_loss: 0.3486258089542389, g_loss: 8.932357788085938\n","Epoch 324/4000, Step 14, d_loss: 0.3695567846298218, g_loss: 6.117091178894043\n","Epoch 324/4000, Step 15, d_loss: 0.3509555459022522, g_loss: 6.04551362991333\n","Epoch 324/4000, Step 16, d_loss: 0.3384092152118683, g_loss: 6.420556545257568\n","Epoch 324/4000, Step 17, d_loss: 0.33555909991264343, g_loss: 5.942034721374512\n","Epoch 324/4000, Step 18, d_loss: 0.34222882986068726, g_loss: 6.728509426116943\n","Epoch 324/4000, Step 19, d_loss: 0.3541397750377655, g_loss: 4.622461795806885\n","Epoch 324/4000, Step 20, d_loss: 0.3416321575641632, g_loss: 5.654942035675049\n","Epoch 324/4000, Step 21, d_loss: 0.3527507483959198, g_loss: 4.778378486633301\n","Epoch 324/4000, Step 22, d_loss: 0.33696871995925903, g_loss: 4.742900371551514\n","Epoch 324/4000, Step 23, d_loss: 0.34469184279441833, g_loss: 6.335665225982666\n","Epoch 324/4000, Step 24, d_loss: 0.3542496860027313, g_loss: 4.771752834320068\n","Epoch 324/4000, Step 25, d_loss: 0.3326120674610138, g_loss: 5.813927173614502\n","Epoch 324/4000, Step 26, d_loss: 0.3345254361629486, g_loss: 5.761120319366455\n","Epoch 324/4000, Step 27, d_loss: 0.3346134126186371, g_loss: 6.111732482910156\n","Epoch 324/4000, Step 28, d_loss: 0.3372463881969452, g_loss: 7.006579875946045\n","Epoch 324/4000, Step 29, d_loss: 0.40263110399246216, g_loss: 6.7725300788879395\n","Epoch 324/4000, Step 30, d_loss: 0.3354126811027527, g_loss: 6.781449317932129\n","Epoch 324/4000, Step 31, d_loss: 0.3420563340187073, g_loss: 5.617161273956299\n","Epoch 324/4000, Step 32, d_loss: 0.3522731363773346, g_loss: 5.816155910491943\n","Epoch 324/4000, Step 33, d_loss: 0.34216731786727905, g_loss: 4.781458377838135\n","Epoch 324/4000, Step 34, d_loss: 0.35400497913360596, g_loss: 5.462078094482422\n","Epoch 324/4000, Step 35, d_loss: 0.33320683240890503, g_loss: 5.209394454956055\n","Epoch 324/4000, Step 36, d_loss: 0.35522499680519104, g_loss: 4.866179466247559\n","Epoch 324/4000, Step 37, d_loss: 0.33890846371650696, g_loss: 4.964725494384766\n","Epoch 324/4000, Step 38, d_loss: 0.3360407054424286, g_loss: 6.370400905609131\n","Epoch 324/4000, Step 39, d_loss: 0.3461875915527344, g_loss: 6.134613990783691\n","Epoch 324/4000, Step 40, d_loss: 0.3524516522884369, g_loss: 5.425649642944336\n","Epoch 324/4000, Step 41, d_loss: 0.341694712638855, g_loss: 4.868139743804932\n","Epoch 324/4000, Step 42, d_loss: 0.3430631458759308, g_loss: 5.630556583404541\n","Epoch 324/4000, Step 43, d_loss: 0.33316707611083984, g_loss: 5.551312446594238\n","Epoch 324/4000, Step 44, d_loss: 0.3409995436668396, g_loss: 5.225585460662842\n","Epoch 324/4000, Step 45, d_loss: 0.3409377634525299, g_loss: 6.150839805603027\n","Epoch 324/4000, Step 46, d_loss: 0.34648585319519043, g_loss: 5.831825256347656\n","Epoch 324/4000, Step 47, d_loss: 0.35918742418289185, g_loss: 7.911075592041016\n","Epoch 324/4000, Step 48, d_loss: 0.35719171166419983, g_loss: 5.270135402679443\n","Epoch 324/4000, Step 49, d_loss: 0.33849388360977173, g_loss: 4.779788970947266\n","Epoch 324/4000, Step 50, d_loss: 0.3350122570991516, g_loss: 5.926764011383057\n","Epoch 324/4000, Step 51, d_loss: 0.3510756492614746, g_loss: 8.052206039428711\n","Epoch 324/4000, Step 52, d_loss: 0.3414119482040405, g_loss: 5.781753063201904\n","Epoch 324/4000, Step 53, d_loss: 0.3526201844215393, g_loss: 5.309858322143555\n","Epoch 324/4000, Step 54, d_loss: 0.3572600185871124, g_loss: 5.0258259773254395\n","Epoch 324/4000, Step 55, d_loss: 0.3530460298061371, g_loss: 7.242245674133301\n","Epoch 324/4000, Step 56, d_loss: 0.3507443964481354, g_loss: 7.528736591339111\n","Epoch 324/4000, Step 57, d_loss: 0.335810124874115, g_loss: 5.383513450622559\n","Epoch 324/4000, Step 58, d_loss: 0.3358502984046936, g_loss: 5.909429550170898\n","Epoch 324/4000, Step 59, d_loss: 0.3406318128108978, g_loss: 5.600334167480469\n","Epoch 324/4000, Step 60, d_loss: 0.36789774894714355, g_loss: 5.285693645477295\n","Epoch 324/4000, Step 61, d_loss: 0.33284398913383484, g_loss: 5.27895450592041\n","Epoch 324/4000, Step 62, d_loss: 0.3658357262611389, g_loss: 6.267738342285156\n","Epoch 324/4000, Step 63, d_loss: 0.3364677131175995, g_loss: 4.342202186584473\n","Epoch 324/4000, Step 64, d_loss: 0.3673844337463379, g_loss: 5.365945816040039\n","Epoch 324/4000, Step 65, d_loss: 0.340509295463562, g_loss: 5.232745170593262\n","Epoch 324/4000, Step 66, d_loss: 0.34372803568840027, g_loss: 4.972010612487793\n","Epoch 324/4000, Step 67, d_loss: 0.3440071642398834, g_loss: 7.983662128448486\n","Epoch 324/4000, Step 68, d_loss: 0.3403182029724121, g_loss: 5.528128147125244\n","Epoch 324/4000, Step 69, d_loss: 0.35181236267089844, g_loss: 6.601779460906982\n","Epoch 324/4000, Step 70, d_loss: 0.3408840596675873, g_loss: 5.672677993774414\n","Epoch 324/4000, Step 71, d_loss: 0.3595651090145111, g_loss: 4.5136542320251465\n","Epoch 324/4000, Step 72, d_loss: 0.33610787987709045, g_loss: 5.442612171173096\n","Epoch 324/4000, Step 73, d_loss: 0.33351874351501465, g_loss: 5.576228618621826\n","Epoch 324/4000, Step 74, d_loss: 0.35220596194267273, g_loss: 6.074898719787598\n","Epoch 324/4000, Step 75, d_loss: 0.349465936422348, g_loss: 5.524867057800293\n","Epoch 324/4000, Step 76, d_loss: 0.35137417912483215, g_loss: 5.123015880584717\n","Epoch 324/4000, Step 77, d_loss: 0.33126139640808105, g_loss: 5.242445468902588\n","Epoch 324/4000, Step 78, d_loss: 0.33907845616340637, g_loss: 5.174008846282959\n","Epoch 324/4000, Step 79, d_loss: 0.8019415736198425, g_loss: 4.863350868225098\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 325/4000, Step 1, d_loss: 0.3760274350643158, g_loss: 3.3249974250793457\n","Epoch 325/4000, Step 2, d_loss: 0.4196180999279022, g_loss: 3.3552582263946533\n","Epoch 325/4000, Step 3, d_loss: 0.5677911043167114, g_loss: 7.026662826538086\n","Epoch 325/4000, Step 4, d_loss: 0.4755427837371826, g_loss: 4.573612213134766\n","Epoch 325/4000, Step 5, d_loss: 0.47597789764404297, g_loss: 5.179666519165039\n","Epoch 325/4000, Step 6, d_loss: 0.4184110760688782, g_loss: 5.334480285644531\n","Epoch 325/4000, Step 7, d_loss: 0.4238050580024719, g_loss: 5.861352920532227\n","Epoch 325/4000, Step 8, d_loss: 0.4644297957420349, g_loss: 6.250558853149414\n","Epoch 325/4000, Step 9, d_loss: 0.38211822509765625, g_loss: 5.2514328956604\n","Epoch 325/4000, Step 10, d_loss: 0.4646083414554596, g_loss: 3.9123363494873047\n","Epoch 325/4000, Step 11, d_loss: 0.4414357542991638, g_loss: 3.5838491916656494\n","Epoch 325/4000, Step 12, d_loss: 0.46122270822525024, g_loss: 3.8870909214019775\n","Epoch 325/4000, Step 13, d_loss: 0.4078879654407501, g_loss: 3.4587244987487793\n","Epoch 325/4000, Step 14, d_loss: 0.4754648208618164, g_loss: 5.150534152984619\n","Epoch 325/4000, Step 15, d_loss: 0.417599081993103, g_loss: 6.613546848297119\n","Epoch 325/4000, Step 16, d_loss: 0.4931430220603943, g_loss: 5.237792491912842\n","Epoch 325/4000, Step 17, d_loss: 0.36214780807495117, g_loss: 6.3787922859191895\n","Epoch 325/4000, Step 18, d_loss: 0.3463154137134552, g_loss: 5.334743022918701\n","Epoch 325/4000, Step 19, d_loss: 0.3520466685295105, g_loss: 4.95289421081543\n","Epoch 325/4000, Step 20, d_loss: 0.35289764404296875, g_loss: 6.158346652984619\n","Epoch 325/4000, Step 21, d_loss: 0.35928887128829956, g_loss: 5.418763160705566\n","Epoch 325/4000, Step 22, d_loss: 0.36217761039733887, g_loss: 5.902174472808838\n","Epoch 325/4000, Step 23, d_loss: 0.37641477584838867, g_loss: 6.148550510406494\n","Epoch 325/4000, Step 24, d_loss: 0.3758901357650757, g_loss: 6.430872440338135\n","Epoch 325/4000, Step 25, d_loss: 0.34020882844924927, g_loss: 4.389151573181152\n","Epoch 325/4000, Step 26, d_loss: 0.35968804359436035, g_loss: 4.533946514129639\n","Epoch 325/4000, Step 27, d_loss: 0.37906211614608765, g_loss: 5.584895133972168\n","Epoch 325/4000, Step 28, d_loss: 0.37858846783638, g_loss: 4.405720233917236\n","Epoch 325/4000, Step 29, d_loss: 0.3860369026660919, g_loss: 4.5235090255737305\n","Epoch 325/4000, Step 30, d_loss: 0.3565479516983032, g_loss: 4.9358038902282715\n","Epoch 325/4000, Step 31, d_loss: 0.35690951347351074, g_loss: 5.964522838592529\n","Epoch 325/4000, Step 32, d_loss: 0.35929617285728455, g_loss: 4.716334819793701\n","Epoch 325/4000, Step 33, d_loss: 0.341187983751297, g_loss: 7.021117687225342\n","Epoch 325/4000, Step 34, d_loss: 0.34528249502182007, g_loss: 9.490533828735352\n","Epoch 325/4000, Step 35, d_loss: 0.34771037101745605, g_loss: 5.490183353424072\n","Epoch 325/4000, Step 36, d_loss: 0.3392908573150635, g_loss: 5.649135112762451\n","Epoch 325/4000, Step 37, d_loss: 0.3475953936576843, g_loss: 6.54152774810791\n","Epoch 325/4000, Step 38, d_loss: 0.3565472960472107, g_loss: 4.6343994140625\n","Epoch 325/4000, Step 39, d_loss: 0.3523643910884857, g_loss: 5.442842960357666\n","Epoch 325/4000, Step 40, d_loss: 0.35565659403800964, g_loss: 4.530567646026611\n","Epoch 325/4000, Step 41, d_loss: 0.3659626841545105, g_loss: 5.2728190422058105\n","Epoch 325/4000, Step 42, d_loss: 0.33485695719718933, g_loss: 5.129153251647949\n","Epoch 325/4000, Step 43, d_loss: 0.3427926301956177, g_loss: 6.4593915939331055\n","Epoch 325/4000, Step 44, d_loss: 0.33622515201568604, g_loss: 6.327914714813232\n","Epoch 325/4000, Step 45, d_loss: 0.36072224378585815, g_loss: 6.1025567054748535\n","Epoch 325/4000, Step 46, d_loss: 0.34458327293395996, g_loss: 6.211963653564453\n","Epoch 325/4000, Step 47, d_loss: 0.3541158437728882, g_loss: 5.388783931732178\n","Epoch 325/4000, Step 48, d_loss: 0.3413046896457672, g_loss: 5.895623683929443\n","Epoch 325/4000, Step 49, d_loss: 0.3393794298171997, g_loss: 6.333749771118164\n","Epoch 325/4000, Step 50, d_loss: 0.3337569832801819, g_loss: 5.667905330657959\n","Epoch 325/4000, Step 51, d_loss: 0.33753225207328796, g_loss: 4.335395336151123\n","Epoch 325/4000, Step 52, d_loss: 0.35658717155456543, g_loss: 5.570084095001221\n","Epoch 325/4000, Step 53, d_loss: 0.3433038592338562, g_loss: 6.6131157875061035\n","Epoch 325/4000, Step 54, d_loss: 0.33657029271125793, g_loss: 6.111961364746094\n","Epoch 325/4000, Step 55, d_loss: 0.3331528604030609, g_loss: 5.136371612548828\n","Epoch 325/4000, Step 56, d_loss: 0.338655561208725, g_loss: 5.849852085113525\n","Epoch 325/4000, Step 57, d_loss: 0.3763522207736969, g_loss: 7.517847537994385\n","Epoch 325/4000, Step 58, d_loss: 0.338532418012619, g_loss: 7.0677080154418945\n","Epoch 325/4000, Step 59, d_loss: 0.3415871262550354, g_loss: 6.21375846862793\n","Epoch 325/4000, Step 60, d_loss: 0.34691089391708374, g_loss: 6.165351867675781\n","Epoch 325/4000, Step 61, d_loss: 0.34868818521499634, g_loss: 4.481173515319824\n","Epoch 325/4000, Step 62, d_loss: 0.35489147901535034, g_loss: 4.942684650421143\n","Epoch 325/4000, Step 63, d_loss: 0.3493324816226959, g_loss: 6.04315185546875\n","Epoch 325/4000, Step 64, d_loss: 0.3494386672973633, g_loss: 5.981873035430908\n","Epoch 325/4000, Step 65, d_loss: 0.33625465631484985, g_loss: 6.538133144378662\n","Epoch 325/4000, Step 66, d_loss: 0.33422160148620605, g_loss: 6.747858047485352\n","Epoch 325/4000, Step 67, d_loss: 0.36626389622688293, g_loss: 6.2224225997924805\n","Epoch 325/4000, Step 68, d_loss: 0.347444623708725, g_loss: 3.926335334777832\n","Epoch 325/4000, Step 69, d_loss: 0.33300498127937317, g_loss: 5.9878458976745605\n","Epoch 325/4000, Step 70, d_loss: 0.3354433476924896, g_loss: 4.880446434020996\n","Epoch 325/4000, Step 71, d_loss: 0.38084110617637634, g_loss: 5.7517547607421875\n","Epoch 325/4000, Step 72, d_loss: 0.34700870513916016, g_loss: 4.762748718261719\n","Epoch 325/4000, Step 73, d_loss: 0.34133586287498474, g_loss: 5.837326526641846\n","Epoch 325/4000, Step 74, d_loss: 0.3330031931400299, g_loss: 5.993994235992432\n","Epoch 325/4000, Step 75, d_loss: 0.3403127193450928, g_loss: 7.225180149078369\n","Epoch 325/4000, Step 76, d_loss: 0.3528291881084442, g_loss: 5.273911952972412\n","Epoch 325/4000, Step 77, d_loss: 0.33455750346183777, g_loss: 5.5372796058654785\n","Epoch 325/4000, Step 78, d_loss: 0.3335498869419098, g_loss: 5.800479888916016\n","Epoch 325/4000, Step 79, d_loss: 0.6395826935768127, g_loss: 4.319657802581787\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 326/4000, Step 1, d_loss: 0.39339888095855713, g_loss: 3.3548662662506104\n","Epoch 326/4000, Step 2, d_loss: 0.4603843092918396, g_loss: 2.6693506240844727\n","Epoch 326/4000, Step 3, d_loss: 0.6679813265800476, g_loss: 2.7627580165863037\n","Epoch 326/4000, Step 4, d_loss: 0.42608991265296936, g_loss: 4.926113128662109\n","Epoch 326/4000, Step 5, d_loss: 0.4244626462459564, g_loss: 5.167275905609131\n","Epoch 326/4000, Step 6, d_loss: 0.387275367975235, g_loss: 6.341556549072266\n","Epoch 326/4000, Step 7, d_loss: 0.35191622376441956, g_loss: 5.205617904663086\n","Epoch 326/4000, Step 8, d_loss: 0.35937702655792236, g_loss: 7.26166296005249\n","Epoch 326/4000, Step 9, d_loss: 0.380650132894516, g_loss: 8.30615234375\n","Epoch 326/4000, Step 10, d_loss: 0.36913037300109863, g_loss: 6.733124732971191\n","Epoch 326/4000, Step 11, d_loss: 0.38362494111061096, g_loss: 6.971782207489014\n","Epoch 326/4000, Step 12, d_loss: 0.3850990831851959, g_loss: 7.581043720245361\n","Epoch 326/4000, Step 13, d_loss: 0.39030200242996216, g_loss: 4.835279941558838\n","Epoch 326/4000, Step 14, d_loss: 0.37076568603515625, g_loss: 7.566778659820557\n","Epoch 326/4000, Step 15, d_loss: 0.3387645483016968, g_loss: 4.3717851638793945\n","Epoch 326/4000, Step 16, d_loss: 0.34225988388061523, g_loss: 5.437617301940918\n","Epoch 326/4000, Step 17, d_loss: 0.3595469295978546, g_loss: 3.8406033515930176\n","Epoch 326/4000, Step 18, d_loss: 0.3500499427318573, g_loss: 6.61248254776001\n","Epoch 326/4000, Step 19, d_loss: 0.36387956142425537, g_loss: 4.659423351287842\n","Epoch 326/4000, Step 20, d_loss: 0.3449598550796509, g_loss: 4.664688587188721\n","Epoch 326/4000, Step 21, d_loss: 0.35841384530067444, g_loss: 5.2181620597839355\n","Epoch 326/4000, Step 22, d_loss: 0.3445377051830292, g_loss: 7.36196231842041\n","Epoch 326/4000, Step 23, d_loss: 0.3523795008659363, g_loss: 3.978048086166382\n","Epoch 326/4000, Step 24, d_loss: 0.3478389382362366, g_loss: 4.251077651977539\n","Epoch 326/4000, Step 25, d_loss: 0.3454023599624634, g_loss: 6.478238105773926\n","Epoch 326/4000, Step 26, d_loss: 0.33608415722846985, g_loss: 6.444278717041016\n","Epoch 326/4000, Step 27, d_loss: 0.3461311161518097, g_loss: 6.086542129516602\n","Epoch 326/4000, Step 28, d_loss: 0.3716074228286743, g_loss: 5.449640274047852\n","Epoch 326/4000, Step 29, d_loss: 0.348530650138855, g_loss: 5.515221118927002\n","Epoch 326/4000, Step 30, d_loss: 0.37876439094543457, g_loss: 5.385136127471924\n","Epoch 326/4000, Step 31, d_loss: 0.33662042021751404, g_loss: 4.743925094604492\n","Epoch 326/4000, Step 32, d_loss: 0.33456161618232727, g_loss: 4.791563034057617\n","Epoch 326/4000, Step 33, d_loss: 0.3758975863456726, g_loss: 2.4627480506896973\n","Epoch 326/4000, Step 34, d_loss: 0.36610573530197144, g_loss: 6.898812294006348\n","Epoch 326/4000, Step 35, d_loss: 0.34786978363990784, g_loss: 4.9692487716674805\n","Epoch 326/4000, Step 36, d_loss: 0.40728136897087097, g_loss: 3.3797872066497803\n","Epoch 326/4000, Step 37, d_loss: 0.5116155743598938, g_loss: 5.018237113952637\n","Epoch 326/4000, Step 38, d_loss: 0.39688441157341003, g_loss: 5.480889797210693\n","Epoch 326/4000, Step 39, d_loss: 0.4205296039581299, g_loss: 7.711585521697998\n","Epoch 326/4000, Step 40, d_loss: 0.44535306096076965, g_loss: 6.087614059448242\n","Epoch 326/4000, Step 41, d_loss: 0.3437809944152832, g_loss: 7.179447650909424\n","Epoch 326/4000, Step 42, d_loss: 0.3575439453125, g_loss: 8.214533805847168\n","Epoch 326/4000, Step 43, d_loss: 0.3816303610801697, g_loss: 6.438859462738037\n","Epoch 326/4000, Step 44, d_loss: 0.359836220741272, g_loss: 7.261447906494141\n","Epoch 326/4000, Step 45, d_loss: 0.40391889214515686, g_loss: 5.4168195724487305\n","Epoch 326/4000, Step 46, d_loss: 0.4085915684700012, g_loss: 6.7323784828186035\n","Epoch 326/4000, Step 47, d_loss: 0.38424909114837646, g_loss: 7.016225814819336\n","Epoch 326/4000, Step 48, d_loss: 0.34516891837120056, g_loss: 6.559791564941406\n","Epoch 326/4000, Step 49, d_loss: 0.34283730387687683, g_loss: 6.875777244567871\n","Epoch 326/4000, Step 50, d_loss: 0.34252431988716125, g_loss: 6.84651517868042\n","Epoch 326/4000, Step 51, d_loss: 0.3716038763523102, g_loss: 6.725360870361328\n","Epoch 326/4000, Step 52, d_loss: 0.3512345850467682, g_loss: 13.33044147491455\n","Epoch 326/4000, Step 53, d_loss: 0.44330036640167236, g_loss: 10.929141998291016\n","Epoch 326/4000, Step 54, d_loss: 0.34175893664360046, g_loss: 7.791745662689209\n","Epoch 326/4000, Step 55, d_loss: 0.3568088412284851, g_loss: 5.976560115814209\n","Epoch 326/4000, Step 56, d_loss: 0.34937265515327454, g_loss: 9.870964050292969\n","Epoch 326/4000, Step 57, d_loss: 0.35315561294555664, g_loss: 5.892019748687744\n","Epoch 326/4000, Step 58, d_loss: 0.33932435512542725, g_loss: 5.863860607147217\n","Epoch 326/4000, Step 59, d_loss: 0.3439720571041107, g_loss: 5.8750691413879395\n","Epoch 326/4000, Step 60, d_loss: 0.3443930149078369, g_loss: 7.306342601776123\n","Epoch 326/4000, Step 61, d_loss: 0.346229612827301, g_loss: 4.439355373382568\n","Epoch 326/4000, Step 62, d_loss: 0.344392329454422, g_loss: 3.3938262462615967\n","Epoch 326/4000, Step 63, d_loss: 0.33472123742103577, g_loss: 5.802008628845215\n","Epoch 326/4000, Step 64, d_loss: 0.3481922447681427, g_loss: 5.861196041107178\n","Epoch 326/4000, Step 65, d_loss: 0.3773602843284607, g_loss: 5.655427932739258\n","Epoch 326/4000, Step 66, d_loss: 0.5251325368881226, g_loss: 3.3884074687957764\n","Epoch 326/4000, Step 67, d_loss: 0.3301765024662018, g_loss: 7.856395721435547\n","Epoch 326/4000, Step 68, d_loss: 0.3394622206687927, g_loss: 4.472352504730225\n","Epoch 326/4000, Step 69, d_loss: 0.3701747953891754, g_loss: 6.7448577880859375\n","Epoch 326/4000, Step 70, d_loss: 0.3329438269138336, g_loss: 3.976144790649414\n","Epoch 326/4000, Step 71, d_loss: 0.36489081382751465, g_loss: 3.405045747756958\n","Epoch 326/4000, Step 72, d_loss: 0.345903605222702, g_loss: 9.22825813293457\n","Epoch 326/4000, Step 73, d_loss: 0.34635600447654724, g_loss: 7.0277018547058105\n","Epoch 326/4000, Step 74, d_loss: 0.3357616662979126, g_loss: 8.332220077514648\n","Epoch 326/4000, Step 75, d_loss: 0.3377615809440613, g_loss: 5.195304870605469\n","Epoch 326/4000, Step 76, d_loss: 0.33915671706199646, g_loss: 5.790071964263916\n","Epoch 326/4000, Step 77, d_loss: 0.3445374071598053, g_loss: 7.229780197143555\n","Epoch 326/4000, Step 78, d_loss: 0.3402143716812134, g_loss: 6.349790096282959\n","Epoch 326/4000, Step 79, d_loss: 0.8555716872215271, g_loss: 4.588721752166748\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 327/4000, Step 1, d_loss: 0.3538687229156494, g_loss: 5.505161762237549\n","Epoch 327/4000, Step 2, d_loss: 0.41586875915527344, g_loss: 4.366506099700928\n","Epoch 327/4000, Step 3, d_loss: 0.4147557020187378, g_loss: 6.718568801879883\n","Epoch 327/4000, Step 4, d_loss: 0.4654874801635742, g_loss: 3.189840793609619\n","Epoch 327/4000, Step 5, d_loss: 0.5060607194900513, g_loss: 4.515604496002197\n","Epoch 327/4000, Step 6, d_loss: 0.4169461131095886, g_loss: 2.5651516914367676\n","Epoch 327/4000, Step 7, d_loss: 0.4132619798183441, g_loss: 3.0605428218841553\n","Epoch 327/4000, Step 8, d_loss: 0.38031521439552307, g_loss: 3.9748711585998535\n","Epoch 327/4000, Step 9, d_loss: 0.4498938024044037, g_loss: 3.9939944744110107\n","Epoch 327/4000, Step 10, d_loss: 0.4144466817378998, g_loss: 4.880603790283203\n","Epoch 327/4000, Step 11, d_loss: 0.3814989924430847, g_loss: 3.3472676277160645\n","Epoch 327/4000, Step 12, d_loss: 0.4275607168674469, g_loss: 3.7125327587127686\n","Epoch 327/4000, Step 13, d_loss: 0.45578598976135254, g_loss: 3.695507526397705\n","Epoch 327/4000, Step 14, d_loss: 0.4117552638053894, g_loss: 3.3873586654663086\n","Epoch 327/4000, Step 15, d_loss: 0.35678812861442566, g_loss: 4.565515041351318\n","Epoch 327/4000, Step 16, d_loss: 0.3539242446422577, g_loss: 6.312758445739746\n","Epoch 327/4000, Step 17, d_loss: 0.35623443126678467, g_loss: 6.543024063110352\n","Epoch 327/4000, Step 18, d_loss: 0.34286725521087646, g_loss: 7.408712387084961\n","Epoch 327/4000, Step 19, d_loss: 0.3341169059276581, g_loss: 7.019320011138916\n","Epoch 327/4000, Step 20, d_loss: 0.36819133162498474, g_loss: 6.480260372161865\n","Epoch 327/4000, Step 21, d_loss: 0.3411444127559662, g_loss: 7.608392238616943\n","Epoch 327/4000, Step 22, d_loss: 0.34441494941711426, g_loss: 7.299077033996582\n","Epoch 327/4000, Step 23, d_loss: 0.3504217267036438, g_loss: 7.217656135559082\n","Epoch 327/4000, Step 24, d_loss: 0.36241480708122253, g_loss: 6.871341705322266\n","Epoch 327/4000, Step 25, d_loss: 0.3487796485424042, g_loss: 5.569677829742432\n","Epoch 327/4000, Step 26, d_loss: 0.347504198551178, g_loss: 7.328223705291748\n","Epoch 327/4000, Step 27, d_loss: 0.3540877401828766, g_loss: 6.416427135467529\n","Epoch 327/4000, Step 28, d_loss: 0.35665926337242126, g_loss: 4.756143093109131\n","Epoch 327/4000, Step 29, d_loss: 0.38412249088287354, g_loss: 4.495011329650879\n","Epoch 327/4000, Step 30, d_loss: 0.4074554741382599, g_loss: 5.048971176147461\n","Epoch 327/4000, Step 31, d_loss: 0.3900061547756195, g_loss: 4.5638837814331055\n","Epoch 327/4000, Step 32, d_loss: 0.36443716287612915, g_loss: 4.196950912475586\n","Epoch 327/4000, Step 33, d_loss: 0.39459022879600525, g_loss: 5.315332889556885\n","Epoch 327/4000, Step 34, d_loss: 0.3589465916156769, g_loss: 7.405038833618164\n","Epoch 327/4000, Step 35, d_loss: 0.3440020680427551, g_loss: 7.277033805847168\n","Epoch 327/4000, Step 36, d_loss: 0.35351717472076416, g_loss: 5.51320743560791\n","Epoch 327/4000, Step 37, d_loss: 0.3455558121204376, g_loss: 5.331468105316162\n","Epoch 327/4000, Step 38, d_loss: 0.3465786278247833, g_loss: 5.738463878631592\n","Epoch 327/4000, Step 39, d_loss: 0.3557218313217163, g_loss: 6.4273881912231445\n","Epoch 327/4000, Step 40, d_loss: 0.369624525308609, g_loss: 5.104279518127441\n","Epoch 327/4000, Step 41, d_loss: 0.3424687683582306, g_loss: 5.582258224487305\n","Epoch 327/4000, Step 42, d_loss: 0.34674307703971863, g_loss: 4.793485164642334\n","Epoch 327/4000, Step 43, d_loss: 0.3431231379508972, g_loss: 6.360894203186035\n","Epoch 327/4000, Step 44, d_loss: 0.3442268669605255, g_loss: 8.322977066040039\n","Epoch 327/4000, Step 45, d_loss: 0.3419274389743805, g_loss: 5.749388217926025\n","Epoch 327/4000, Step 46, d_loss: 0.37392815947532654, g_loss: 6.272959232330322\n","Epoch 327/4000, Step 47, d_loss: 0.3486841320991516, g_loss: 6.641681671142578\n","Epoch 327/4000, Step 48, d_loss: 0.38400375843048096, g_loss: 5.66572904586792\n","Epoch 327/4000, Step 49, d_loss: 0.34703007340431213, g_loss: 5.257172584533691\n","Epoch 327/4000, Step 50, d_loss: 0.36786144971847534, g_loss: 4.484662055969238\n","Epoch 327/4000, Step 51, d_loss: 0.3688795864582062, g_loss: 4.973973274230957\n","Epoch 327/4000, Step 52, d_loss: 0.3827999234199524, g_loss: 5.4049482345581055\n","Epoch 327/4000, Step 53, d_loss: 0.35999608039855957, g_loss: 5.564293384552002\n","Epoch 327/4000, Step 54, d_loss: 0.3725411593914032, g_loss: 5.832979679107666\n","Epoch 327/4000, Step 55, d_loss: 0.34120458364486694, g_loss: 5.024874687194824\n","Epoch 327/4000, Step 56, d_loss: 0.35122886300086975, g_loss: 6.179337501525879\n","Epoch 327/4000, Step 57, d_loss: 0.34253740310668945, g_loss: 5.888941287994385\n","Epoch 327/4000, Step 58, d_loss: 0.34664973616600037, g_loss: 6.602241039276123\n","Epoch 327/4000, Step 59, d_loss: 0.3461494743824005, g_loss: 6.127781391143799\n","Epoch 327/4000, Step 60, d_loss: 0.35579460859298706, g_loss: 6.015726566314697\n","Epoch 327/4000, Step 61, d_loss: 0.3722664415836334, g_loss: 4.915412425994873\n","Epoch 327/4000, Step 62, d_loss: 0.33902350068092346, g_loss: 5.038696765899658\n","Epoch 327/4000, Step 63, d_loss: 0.3456661105155945, g_loss: 4.222127914428711\n","Epoch 327/4000, Step 64, d_loss: 0.36739373207092285, g_loss: 5.7363810539245605\n","Epoch 327/4000, Step 65, d_loss: 0.36053937673568726, g_loss: 4.37662410736084\n","Epoch 327/4000, Step 66, d_loss: 0.37489867210388184, g_loss: 4.949100017547607\n","Epoch 327/4000, Step 67, d_loss: 0.3495731055736542, g_loss: 5.130974292755127\n","Epoch 327/4000, Step 68, d_loss: 0.3528366684913635, g_loss: 5.063262462615967\n","Epoch 327/4000, Step 69, d_loss: 0.3462750017642975, g_loss: 3.232957124710083\n","Epoch 327/4000, Step 70, d_loss: 0.3560434877872467, g_loss: 3.078552484512329\n","Epoch 327/4000, Step 71, d_loss: 0.355275958776474, g_loss: 6.908043384552002\n","Epoch 327/4000, Step 72, d_loss: 0.37378546595573425, g_loss: 3.9349029064178467\n","Epoch 327/4000, Step 73, d_loss: 0.3449862599372864, g_loss: 4.016075611114502\n","Epoch 327/4000, Step 74, d_loss: 0.3360559642314911, g_loss: 5.79725980758667\n","Epoch 327/4000, Step 75, d_loss: 0.3449987769126892, g_loss: 5.43356466293335\n","Epoch 327/4000, Step 76, d_loss: 0.34359389543533325, g_loss: 5.044981479644775\n","Epoch 327/4000, Step 77, d_loss: 0.35899895429611206, g_loss: 3.9493372440338135\n","Epoch 327/4000, Step 78, d_loss: 0.4164056181907654, g_loss: 4.842411518096924\n","Epoch 327/4000, Step 79, d_loss: 0.35324349999427795, g_loss: 4.401787757873535\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 328/4000, Step 1, d_loss: 0.34583908319473267, g_loss: 2.9198431968688965\n","Epoch 328/4000, Step 2, d_loss: 0.34815582633018494, g_loss: 5.057917594909668\n","Epoch 328/4000, Step 3, d_loss: 0.3610227704048157, g_loss: 5.200971603393555\n","Epoch 328/4000, Step 4, d_loss: 0.35119393467903137, g_loss: 4.7791972160339355\n","Epoch 328/4000, Step 5, d_loss: 0.35226893424987793, g_loss: 4.883424758911133\n","Epoch 328/4000, Step 6, d_loss: 0.3499590754508972, g_loss: 5.406915664672852\n","Epoch 328/4000, Step 7, d_loss: 0.3463901877403259, g_loss: 5.633828639984131\n","Epoch 328/4000, Step 8, d_loss: 0.3635473847389221, g_loss: 5.659652233123779\n","Epoch 328/4000, Step 9, d_loss: 0.35243329405784607, g_loss: 4.819974422454834\n","Epoch 328/4000, Step 10, d_loss: 0.34040844440460205, g_loss: 4.839285373687744\n","Epoch 328/4000, Step 11, d_loss: 0.35773515701293945, g_loss: 5.623820781707764\n","Epoch 328/4000, Step 12, d_loss: 0.36170703172683716, g_loss: 5.80083703994751\n","Epoch 328/4000, Step 13, d_loss: 0.3501092195510864, g_loss: 5.510730743408203\n","Epoch 328/4000, Step 14, d_loss: 0.3435361981391907, g_loss: 3.9103598594665527\n","Epoch 328/4000, Step 15, d_loss: 0.34441420435905457, g_loss: 5.281085968017578\n","Epoch 328/4000, Step 16, d_loss: 0.34497764706611633, g_loss: 4.801703929901123\n","Epoch 328/4000, Step 17, d_loss: 0.35578760504722595, g_loss: 4.929739952087402\n","Epoch 328/4000, Step 18, d_loss: 0.36738690733909607, g_loss: 6.84519100189209\n","Epoch 328/4000, Step 19, d_loss: 0.34877973794937134, g_loss: 3.582347869873047\n","Epoch 328/4000, Step 20, d_loss: 0.3440672755241394, g_loss: 5.341376304626465\n","Epoch 328/4000, Step 21, d_loss: 0.34244105219841003, g_loss: 4.745662212371826\n","Epoch 328/4000, Step 22, d_loss: 0.33281704783439636, g_loss: 3.3817930221557617\n","Epoch 328/4000, Step 23, d_loss: 0.33905890583992004, g_loss: 5.983334541320801\n","Epoch 328/4000, Step 24, d_loss: 0.38291531801223755, g_loss: 5.925813674926758\n","Epoch 328/4000, Step 25, d_loss: 0.410746693611145, g_loss: 6.368236541748047\n","Epoch 328/4000, Step 26, d_loss: 0.3324354290962219, g_loss: 5.805332183837891\n","Epoch 328/4000, Step 27, d_loss: 0.33279600739479065, g_loss: 4.717573642730713\n","Epoch 328/4000, Step 28, d_loss: 0.34383612871170044, g_loss: 6.81593656539917\n","Epoch 328/4000, Step 29, d_loss: 0.3372771739959717, g_loss: 3.750239133834839\n","Epoch 328/4000, Step 30, d_loss: 0.35867583751678467, g_loss: 5.332034111022949\n","Epoch 328/4000, Step 31, d_loss: 0.3832404315471649, g_loss: 5.45557165145874\n","Epoch 328/4000, Step 32, d_loss: 0.35602906346321106, g_loss: 5.083339214324951\n","Epoch 328/4000, Step 33, d_loss: 0.3403193950653076, g_loss: 4.7538042068481445\n","Epoch 328/4000, Step 34, d_loss: 0.36001694202423096, g_loss: 3.9389257431030273\n","Epoch 328/4000, Step 35, d_loss: 0.40701472759246826, g_loss: 3.989772319793701\n","Epoch 328/4000, Step 36, d_loss: 0.36777225136756897, g_loss: 4.923519134521484\n","Epoch 328/4000, Step 37, d_loss: 0.44441673159599304, g_loss: 6.1380696296691895\n","Epoch 328/4000, Step 38, d_loss: 0.3841882050037384, g_loss: 7.268667697906494\n","Epoch 328/4000, Step 39, d_loss: 0.39603233337402344, g_loss: 6.274660110473633\n","Epoch 328/4000, Step 40, d_loss: 0.3786347508430481, g_loss: 5.298473358154297\n","Epoch 328/4000, Step 41, d_loss: 0.3575931191444397, g_loss: 5.147671222686768\n","Epoch 328/4000, Step 42, d_loss: 0.3625045120716095, g_loss: 4.226717472076416\n","Epoch 328/4000, Step 43, d_loss: 0.39506757259368896, g_loss: 4.63116455078125\n","Epoch 328/4000, Step 44, d_loss: 0.4109974205493927, g_loss: 3.3167765140533447\n","Epoch 328/4000, Step 45, d_loss: 0.42563652992248535, g_loss: 4.1243157386779785\n","Epoch 328/4000, Step 46, d_loss: 0.3581004738807678, g_loss: 4.545248508453369\n","Epoch 328/4000, Step 47, d_loss: 0.34646832942962646, g_loss: 6.696794033050537\n","Epoch 328/4000, Step 48, d_loss: 0.37239110469818115, g_loss: 5.620509147644043\n","Epoch 328/4000, Step 49, d_loss: 0.47933685779571533, g_loss: 5.897716045379639\n","Epoch 328/4000, Step 50, d_loss: 0.37560269236564636, g_loss: 5.474349021911621\n","Epoch 328/4000, Step 51, d_loss: 0.33521369099617004, g_loss: 3.0995516777038574\n","Epoch 328/4000, Step 52, d_loss: 0.3811880648136139, g_loss: 4.449252605438232\n","Epoch 328/4000, Step 53, d_loss: 0.434201717376709, g_loss: 5.516155242919922\n","Epoch 328/4000, Step 54, d_loss: 0.4106074273586273, g_loss: 2.901448965072632\n","Epoch 328/4000, Step 55, d_loss: 0.4064512550830841, g_loss: 3.9372661113739014\n","Epoch 328/4000, Step 56, d_loss: 0.3878234922885895, g_loss: 6.0795979499816895\n","Epoch 328/4000, Step 57, d_loss: 0.36808276176452637, g_loss: 6.186181545257568\n","Epoch 328/4000, Step 58, d_loss: 0.38731831312179565, g_loss: 5.995901107788086\n","Epoch 328/4000, Step 59, d_loss: 0.3553183376789093, g_loss: 6.132315158843994\n","Epoch 328/4000, Step 60, d_loss: 0.34366562962532043, g_loss: 4.9194111824035645\n","Epoch 328/4000, Step 61, d_loss: 0.397844523191452, g_loss: 5.923745155334473\n","Epoch 328/4000, Step 62, d_loss: 0.34163081645965576, g_loss: 6.707975387573242\n","Epoch 328/4000, Step 63, d_loss: 0.3359014689922333, g_loss: 6.589168548583984\n","Epoch 328/4000, Step 64, d_loss: 0.3379107713699341, g_loss: 6.909277439117432\n","Epoch 328/4000, Step 65, d_loss: 0.3358401656150818, g_loss: 6.754039764404297\n","Epoch 328/4000, Step 66, d_loss: 0.34101995825767517, g_loss: 6.591100215911865\n","Epoch 328/4000, Step 67, d_loss: 0.37545186281204224, g_loss: 4.704710960388184\n","Epoch 328/4000, Step 68, d_loss: 0.34500637650489807, g_loss: 4.530648231506348\n","Epoch 328/4000, Step 69, d_loss: 0.33866962790489197, g_loss: 4.775847911834717\n","Epoch 328/4000, Step 70, d_loss: 0.3378015160560608, g_loss: 4.969500541687012\n","Epoch 328/4000, Step 71, d_loss: 0.3331279158592224, g_loss: 4.678525924682617\n","Epoch 328/4000, Step 72, d_loss: 0.33923742175102234, g_loss: 6.340137481689453\n","Epoch 328/4000, Step 73, d_loss: 0.3643774688243866, g_loss: 4.757112503051758\n","Epoch 328/4000, Step 74, d_loss: 0.3520345091819763, g_loss: 5.304419040679932\n","Epoch 328/4000, Step 75, d_loss: 0.3531966507434845, g_loss: 5.415770530700684\n","Epoch 328/4000, Step 76, d_loss: 0.34084269404411316, g_loss: 6.235635280609131\n","Epoch 328/4000, Step 77, d_loss: 0.33889636397361755, g_loss: 4.287683010101318\n","Epoch 328/4000, Step 78, d_loss: 0.3405686318874359, g_loss: 5.361739158630371\n","Epoch 328/4000, Step 79, d_loss: 0.34144771099090576, g_loss: 6.982602119445801\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 329/4000, Step 1, d_loss: 0.3499546945095062, g_loss: 7.4622650146484375\n","Epoch 329/4000, Step 2, d_loss: 0.382897287607193, g_loss: 4.972177028656006\n","Epoch 329/4000, Step 3, d_loss: 0.34773364663124084, g_loss: 6.3050079345703125\n","Epoch 329/4000, Step 4, d_loss: 0.36065593361854553, g_loss: 5.391915798187256\n","Epoch 329/4000, Step 5, d_loss: 0.3524650037288666, g_loss: 6.069714069366455\n","Epoch 329/4000, Step 6, d_loss: 0.3627376854419708, g_loss: 5.422337532043457\n","Epoch 329/4000, Step 7, d_loss: 0.35501542687416077, g_loss: 6.457644939422607\n","Epoch 329/4000, Step 8, d_loss: 0.3472920358181, g_loss: 5.549457550048828\n","Epoch 329/4000, Step 9, d_loss: 0.3372453451156616, g_loss: 6.903889179229736\n","Epoch 329/4000, Step 10, d_loss: 0.3353363275527954, g_loss: 6.746631622314453\n","Epoch 329/4000, Step 11, d_loss: 0.3512771725654602, g_loss: 6.499739170074463\n","Epoch 329/4000, Step 12, d_loss: 0.3788580894470215, g_loss: 6.702629566192627\n","Epoch 329/4000, Step 13, d_loss: 0.3378801941871643, g_loss: 5.923738479614258\n","Epoch 329/4000, Step 14, d_loss: 0.3525908291339874, g_loss: 5.041676044464111\n","Epoch 329/4000, Step 15, d_loss: 0.35107776522636414, g_loss: 4.69721794128418\n","Epoch 329/4000, Step 16, d_loss: 0.39043110609054565, g_loss: 7.249969005584717\n","Epoch 329/4000, Step 17, d_loss: 0.3469829261302948, g_loss: 6.436366081237793\n","Epoch 329/4000, Step 18, d_loss: 0.3578471839427948, g_loss: 5.503324508666992\n","Epoch 329/4000, Step 19, d_loss: 0.32827553153038025, g_loss: 7.823171138763428\n","Epoch 329/4000, Step 20, d_loss: 0.3379744589328766, g_loss: 6.525899887084961\n","Epoch 329/4000, Step 21, d_loss: 0.336098849773407, g_loss: 6.5538530349731445\n","Epoch 329/4000, Step 22, d_loss: 0.35485464334487915, g_loss: 6.3451128005981445\n","Epoch 329/4000, Step 23, d_loss: 0.34561774134635925, g_loss: 5.940975189208984\n","Epoch 329/4000, Step 24, d_loss: 0.3374245762825012, g_loss: 6.273492336273193\n","Epoch 329/4000, Step 25, d_loss: 0.3388521671295166, g_loss: 5.2763519287109375\n","Epoch 329/4000, Step 26, d_loss: 0.3578503131866455, g_loss: 6.519967555999756\n","Epoch 329/4000, Step 27, d_loss: 0.34183835983276367, g_loss: 6.7204389572143555\n","Epoch 329/4000, Step 28, d_loss: 0.34194764494895935, g_loss: 8.652503967285156\n","Epoch 329/4000, Step 29, d_loss: 0.33143898844718933, g_loss: 8.937342643737793\n","Epoch 329/4000, Step 30, d_loss: 0.33195242285728455, g_loss: 6.487304210662842\n","Epoch 329/4000, Step 31, d_loss: 0.3544595539569855, g_loss: 6.403807640075684\n","Epoch 329/4000, Step 32, d_loss: 0.33310019969940186, g_loss: 7.642642498016357\n","Epoch 329/4000, Step 33, d_loss: 0.33285707235336304, g_loss: 6.27208948135376\n","Epoch 329/4000, Step 34, d_loss: 0.33086076378822327, g_loss: 7.5526509284973145\n","Epoch 329/4000, Step 35, d_loss: 0.332253098487854, g_loss: 6.922555923461914\n","Epoch 329/4000, Step 36, d_loss: 0.34021830558776855, g_loss: 6.7180070877075195\n","Epoch 329/4000, Step 37, d_loss: 0.33763569593429565, g_loss: 6.282187461853027\n","Epoch 329/4000, Step 38, d_loss: 0.33348506689071655, g_loss: 6.067338943481445\n","Epoch 329/4000, Step 39, d_loss: 0.33311572670936584, g_loss: 6.549357891082764\n","Epoch 329/4000, Step 40, d_loss: 0.34269148111343384, g_loss: 6.001567840576172\n","Epoch 329/4000, Step 41, d_loss: 0.34611833095550537, g_loss: 5.296559810638428\n","Epoch 329/4000, Step 42, d_loss: 0.3327006697654724, g_loss: 5.820399761199951\n","Epoch 329/4000, Step 43, d_loss: 0.3398275375366211, g_loss: 5.820202350616455\n","Epoch 329/4000, Step 44, d_loss: 0.3442845344543457, g_loss: 5.727435111999512\n","Epoch 329/4000, Step 45, d_loss: 0.3359839618206024, g_loss: 6.595968246459961\n","Epoch 329/4000, Step 46, d_loss: 0.3445652425289154, g_loss: 5.669269561767578\n","Epoch 329/4000, Step 47, d_loss: 0.3711932599544525, g_loss: 7.056361675262451\n","Epoch 329/4000, Step 48, d_loss: 0.33347126841545105, g_loss: 6.798985481262207\n","Epoch 329/4000, Step 49, d_loss: 0.3475508689880371, g_loss: 6.251358985900879\n","Epoch 329/4000, Step 50, d_loss: 0.33735746145248413, g_loss: 5.635902404785156\n","Epoch 329/4000, Step 51, d_loss: 0.3383401930332184, g_loss: 5.997822284698486\n","Epoch 329/4000, Step 52, d_loss: 0.3348257541656494, g_loss: 5.475805759429932\n","Epoch 329/4000, Step 53, d_loss: 0.33915627002716064, g_loss: 5.135769844055176\n","Epoch 329/4000, Step 54, d_loss: 0.3363111913204193, g_loss: 8.447494506835938\n","Epoch 329/4000, Step 55, d_loss: 0.3362950384616852, g_loss: 5.77656364440918\n","Epoch 329/4000, Step 56, d_loss: 0.33914273977279663, g_loss: 7.007949352264404\n","Epoch 329/4000, Step 57, d_loss: 0.3389741778373718, g_loss: 6.737330436706543\n","Epoch 329/4000, Step 58, d_loss: 0.3320148289203644, g_loss: 6.1448774337768555\n","Epoch 329/4000, Step 59, d_loss: 0.3307972252368927, g_loss: 7.307805061340332\n","Epoch 329/4000, Step 60, d_loss: 0.3667490780353546, g_loss: 6.4429192543029785\n","Epoch 329/4000, Step 61, d_loss: 0.3381711542606354, g_loss: 5.953535556793213\n","Epoch 329/4000, Step 62, d_loss: 0.35731202363967896, g_loss: 6.198119640350342\n","Epoch 329/4000, Step 63, d_loss: 0.35636451840400696, g_loss: 5.866502285003662\n","Epoch 329/4000, Step 64, d_loss: 0.34115901589393616, g_loss: 6.389928340911865\n","Epoch 329/4000, Step 65, d_loss: 0.34476208686828613, g_loss: 6.615994453430176\n","Epoch 329/4000, Step 66, d_loss: 0.3310461640357971, g_loss: 5.009089469909668\n","Epoch 329/4000, Step 67, d_loss: 0.33540499210357666, g_loss: 6.573115348815918\n","Epoch 329/4000, Step 68, d_loss: 0.33845290541648865, g_loss: 8.084575653076172\n","Epoch 329/4000, Step 69, d_loss: 0.336178719997406, g_loss: 5.247374534606934\n","Epoch 329/4000, Step 70, d_loss: 0.3624488413333893, g_loss: 5.82170295715332\n","Epoch 329/4000, Step 71, d_loss: 0.3422735631465912, g_loss: 5.9434051513671875\n","Epoch 329/4000, Step 72, d_loss: 0.3392183482646942, g_loss: 4.628983974456787\n","Epoch 329/4000, Step 73, d_loss: 0.3565318286418915, g_loss: 4.748819351196289\n","Epoch 329/4000, Step 74, d_loss: 0.36243125796318054, g_loss: 4.933926582336426\n","Epoch 329/4000, Step 75, d_loss: 0.3479039669036865, g_loss: 5.833764553070068\n","Epoch 329/4000, Step 76, d_loss: 0.33965247869491577, g_loss: 7.514040946960449\n","Epoch 329/4000, Step 77, d_loss: 0.3328784704208374, g_loss: 6.780293941497803\n","Epoch 329/4000, Step 78, d_loss: 0.34685254096984863, g_loss: 6.488175868988037\n","Epoch 329/4000, Step 79, d_loss: 0.5917962789535522, g_loss: 4.981419563293457\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 330/4000, Step 1, d_loss: 0.3501976430416107, g_loss: 3.689965009689331\n","Epoch 330/4000, Step 2, d_loss: 0.3957861661911011, g_loss: 3.228503465652466\n","Epoch 330/4000, Step 3, d_loss: 0.4669305384159088, g_loss: 2.5642457008361816\n","Epoch 330/4000, Step 4, d_loss: 0.45368754863739014, g_loss: 4.838350296020508\n","Epoch 330/4000, Step 5, d_loss: 0.48643869161605835, g_loss: 4.374958038330078\n","Epoch 330/4000, Step 6, d_loss: 0.37625569105148315, g_loss: 4.609209060668945\n","Epoch 330/4000, Step 7, d_loss: 0.37672021985054016, g_loss: 8.273801803588867\n","Epoch 330/4000, Step 8, d_loss: 0.3834330439567566, g_loss: 5.757382392883301\n","Epoch 330/4000, Step 9, d_loss: 0.3644862771034241, g_loss: 6.0159220695495605\n","Epoch 330/4000, Step 10, d_loss: 0.33587536215782166, g_loss: 6.2544426918029785\n","Epoch 330/4000, Step 11, d_loss: 0.38433048129081726, g_loss: 4.80784273147583\n","Epoch 330/4000, Step 12, d_loss: 0.3555721044540405, g_loss: 5.0325398445129395\n","Epoch 330/4000, Step 13, d_loss: 0.3790348768234253, g_loss: 4.943272590637207\n","Epoch 330/4000, Step 14, d_loss: 0.3849540948867798, g_loss: 4.857763290405273\n","Epoch 330/4000, Step 15, d_loss: 0.37301337718963623, g_loss: 4.540704727172852\n","Epoch 330/4000, Step 16, d_loss: 0.3776456117630005, g_loss: 6.388523101806641\n","Epoch 330/4000, Step 17, d_loss: 0.3625994622707367, g_loss: 4.799636363983154\n","Epoch 330/4000, Step 18, d_loss: 0.35712745785713196, g_loss: 4.978248119354248\n","Epoch 330/4000, Step 19, d_loss: 0.37224239110946655, g_loss: 5.346577167510986\n","Epoch 330/4000, Step 20, d_loss: 0.3415603041648865, g_loss: 6.665293216705322\n","Epoch 330/4000, Step 21, d_loss: 0.34633177518844604, g_loss: 5.04964017868042\n","Epoch 330/4000, Step 22, d_loss: 0.35555312037467957, g_loss: 8.077656745910645\n","Epoch 330/4000, Step 23, d_loss: 0.3670496344566345, g_loss: 5.323244094848633\n","Epoch 330/4000, Step 24, d_loss: 0.35481926798820496, g_loss: 4.879098892211914\n","Epoch 330/4000, Step 25, d_loss: 0.3453042209148407, g_loss: 5.33381462097168\n","Epoch 330/4000, Step 26, d_loss: 0.3456282615661621, g_loss: 7.148940563201904\n","Epoch 330/4000, Step 27, d_loss: 0.3571659326553345, g_loss: 7.379700183868408\n","Epoch 330/4000, Step 28, d_loss: 0.34511852264404297, g_loss: 5.513031482696533\n","Epoch 330/4000, Step 29, d_loss: 0.34711477160453796, g_loss: 6.601083755493164\n","Epoch 330/4000, Step 30, d_loss: 0.33899062871932983, g_loss: 6.187896251678467\n","Epoch 330/4000, Step 31, d_loss: 0.3468913733959198, g_loss: 6.168176174163818\n","Epoch 330/4000, Step 32, d_loss: 0.3313583433628082, g_loss: 5.0640692710876465\n","Epoch 330/4000, Step 33, d_loss: 0.33946648240089417, g_loss: 5.855981826782227\n","Epoch 330/4000, Step 34, d_loss: 0.33416548371315, g_loss: 6.227184772491455\n","Epoch 330/4000, Step 35, d_loss: 0.3443736433982849, g_loss: 8.0718994140625\n","Epoch 330/4000, Step 36, d_loss: 0.3506213128566742, g_loss: 5.971105575561523\n","Epoch 330/4000, Step 37, d_loss: 0.33922943472862244, g_loss: 5.9198198318481445\n","Epoch 330/4000, Step 38, d_loss: 0.3326142430305481, g_loss: 5.554157257080078\n","Epoch 330/4000, Step 39, d_loss: 0.33831411600112915, g_loss: 5.6154866218566895\n","Epoch 330/4000, Step 40, d_loss: 0.3623320758342743, g_loss: 7.12690544128418\n","Epoch 330/4000, Step 41, d_loss: 0.34511473774909973, g_loss: 5.431318759918213\n","Epoch 330/4000, Step 42, d_loss: 0.33727672696113586, g_loss: 5.474670886993408\n","Epoch 330/4000, Step 43, d_loss: 0.33250585198402405, g_loss: 6.916787624359131\n","Epoch 330/4000, Step 44, d_loss: 0.35221630334854126, g_loss: 6.564154624938965\n","Epoch 330/4000, Step 45, d_loss: 0.3566993772983551, g_loss: 6.044169902801514\n","Epoch 330/4000, Step 46, d_loss: 0.33517423272132874, g_loss: 6.7955522537231445\n","Epoch 330/4000, Step 47, d_loss: 0.34042176604270935, g_loss: 7.7219767570495605\n","Epoch 330/4000, Step 48, d_loss: 0.3555498421192169, g_loss: 6.749938488006592\n","Epoch 330/4000, Step 49, d_loss: 0.3352072536945343, g_loss: 7.1545634269714355\n","Epoch 330/4000, Step 50, d_loss: 0.3374220132827759, g_loss: 7.272104740142822\n","Epoch 330/4000, Step 51, d_loss: 0.34623533487319946, g_loss: 6.1436591148376465\n","Epoch 330/4000, Step 52, d_loss: 0.3405854403972626, g_loss: 5.720657825469971\n","Epoch 330/4000, Step 53, d_loss: 0.3460956811904907, g_loss: 5.161200046539307\n","Epoch 330/4000, Step 54, d_loss: 0.34458139538764954, g_loss: 4.463531017303467\n","Epoch 330/4000, Step 55, d_loss: 0.33248767256736755, g_loss: 6.167426109313965\n","Epoch 330/4000, Step 56, d_loss: 0.35054585337638855, g_loss: 4.627662658691406\n","Epoch 330/4000, Step 57, d_loss: 0.34867218136787415, g_loss: 7.67867374420166\n","Epoch 330/4000, Step 58, d_loss: 0.3526659905910492, g_loss: 6.0964460372924805\n","Epoch 330/4000, Step 59, d_loss: 0.33753901720046997, g_loss: 4.485687732696533\n","Epoch 330/4000, Step 60, d_loss: 0.34169384837150574, g_loss: 7.4411492347717285\n","Epoch 330/4000, Step 61, d_loss: 0.3382619619369507, g_loss: 5.6367998123168945\n","Epoch 330/4000, Step 62, d_loss: 0.344876766204834, g_loss: 6.7372727394104\n","Epoch 330/4000, Step 63, d_loss: 0.33444327116012573, g_loss: 5.4750213623046875\n","Epoch 330/4000, Step 64, d_loss: 0.33578723669052124, g_loss: 6.224749565124512\n","Epoch 330/4000, Step 65, d_loss: 0.33352231979370117, g_loss: 6.9519500732421875\n","Epoch 330/4000, Step 66, d_loss: 0.3355932831764221, g_loss: 6.424046039581299\n","Epoch 330/4000, Step 67, d_loss: 0.3309248983860016, g_loss: 7.340215682983398\n","Epoch 330/4000, Step 68, d_loss: 0.33319738507270813, g_loss: 6.649730682373047\n","Epoch 330/4000, Step 69, d_loss: 0.33404341340065, g_loss: 5.867893695831299\n","Epoch 330/4000, Step 70, d_loss: 0.3286842107772827, g_loss: 8.437529563903809\n","Epoch 330/4000, Step 71, d_loss: 0.33236023783683777, g_loss: 5.881317615509033\n","Epoch 330/4000, Step 72, d_loss: 0.33397579193115234, g_loss: 5.4904465675354\n","Epoch 330/4000, Step 73, d_loss: 0.3511287569999695, g_loss: 5.661235809326172\n","Epoch 330/4000, Step 74, d_loss: 0.3321590721607208, g_loss: 5.4589972496032715\n","Epoch 330/4000, Step 75, d_loss: 0.34449121356010437, g_loss: 6.855745792388916\n","Epoch 330/4000, Step 76, d_loss: 0.33443722128868103, g_loss: 10.804516792297363\n","Epoch 330/4000, Step 77, d_loss: 0.3303784728050232, g_loss: 6.532080173492432\n","Epoch 330/4000, Step 78, d_loss: 0.3333926200866699, g_loss: 6.357005596160889\n","Epoch 330/4000, Step 79, d_loss: 0.35692694783210754, g_loss: 7.360326766967773\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 331/4000, Step 1, d_loss: 0.33384960889816284, g_loss: 6.10936164855957\n","Epoch 331/4000, Step 2, d_loss: 0.33979684114456177, g_loss: 6.945344924926758\n","Epoch 331/4000, Step 3, d_loss: 0.3415100574493408, g_loss: 5.879087924957275\n","Epoch 331/4000, Step 4, d_loss: 0.3487164378166199, g_loss: 5.812801361083984\n","Epoch 331/4000, Step 5, d_loss: 0.3329695761203766, g_loss: 7.471996784210205\n","Epoch 331/4000, Step 6, d_loss: 0.33256614208221436, g_loss: 5.305519104003906\n","Epoch 331/4000, Step 7, d_loss: 0.3338020443916321, g_loss: 7.847630500793457\n","Epoch 331/4000, Step 8, d_loss: 0.33377179503440857, g_loss: 7.742194652557373\n","Epoch 331/4000, Step 9, d_loss: 0.3422441780567169, g_loss: 6.3941192626953125\n","Epoch 331/4000, Step 10, d_loss: 0.3355884253978729, g_loss: 6.112386226654053\n","Epoch 331/4000, Step 11, d_loss: 0.3902381658554077, g_loss: 7.697605609893799\n","Epoch 331/4000, Step 12, d_loss: 0.3390029966831207, g_loss: 4.840657711029053\n","Epoch 331/4000, Step 13, d_loss: 0.3418300449848175, g_loss: 5.17653226852417\n","Epoch 331/4000, Step 14, d_loss: 0.34015169739723206, g_loss: 4.651959419250488\n","Epoch 331/4000, Step 15, d_loss: 0.3565903306007385, g_loss: 5.211239337921143\n","Epoch 331/4000, Step 16, d_loss: 0.34363970160484314, g_loss: 5.78247594833374\n","Epoch 331/4000, Step 17, d_loss: 0.34720563888549805, g_loss: 5.837789058685303\n","Epoch 331/4000, Step 18, d_loss: 0.33780595660209656, g_loss: 7.057974338531494\n","Epoch 331/4000, Step 19, d_loss: 0.3404301404953003, g_loss: 4.87294340133667\n","Epoch 331/4000, Step 20, d_loss: 0.3395541310310364, g_loss: 5.905067443847656\n","Epoch 331/4000, Step 21, d_loss: 0.3331187069416046, g_loss: 5.356832981109619\n","Epoch 331/4000, Step 22, d_loss: 0.33281591534614563, g_loss: 6.6187663078308105\n","Epoch 331/4000, Step 23, d_loss: 0.3483405113220215, g_loss: 5.316824913024902\n","Epoch 331/4000, Step 24, d_loss: 0.3457125425338745, g_loss: 6.363320827484131\n","Epoch 331/4000, Step 25, d_loss: 0.36147087812423706, g_loss: 5.9436726570129395\n","Epoch 331/4000, Step 26, d_loss: 0.33185747265815735, g_loss: 4.816332817077637\n","Epoch 331/4000, Step 27, d_loss: 0.3506947457790375, g_loss: 4.436511039733887\n","Epoch 331/4000, Step 28, d_loss: 0.3363511860370636, g_loss: 5.030267238616943\n","Epoch 331/4000, Step 29, d_loss: 0.3321196138858795, g_loss: 4.754275321960449\n","Epoch 331/4000, Step 30, d_loss: 0.3596160411834717, g_loss: 4.829692363739014\n","Epoch 331/4000, Step 31, d_loss: 0.3446866571903229, g_loss: 6.279767036437988\n","Epoch 331/4000, Step 32, d_loss: 0.34087875485420227, g_loss: 5.775180339813232\n","Epoch 331/4000, Step 33, d_loss: 0.3343638479709625, g_loss: 6.063302516937256\n","Epoch 331/4000, Step 34, d_loss: 0.33649560809135437, g_loss: 5.0893473625183105\n","Epoch 331/4000, Step 35, d_loss: 0.33895012736320496, g_loss: 6.569487571716309\n","Epoch 331/4000, Step 36, d_loss: 0.3542800843715668, g_loss: 5.503602027893066\n","Epoch 331/4000, Step 37, d_loss: 0.3381466269493103, g_loss: 5.083471775054932\n","Epoch 331/4000, Step 38, d_loss: 0.3361382484436035, g_loss: 4.877852439880371\n","Epoch 331/4000, Step 39, d_loss: 0.34183448553085327, g_loss: 4.378903865814209\n","Epoch 331/4000, Step 40, d_loss: 0.35101091861724854, g_loss: 5.501461029052734\n","Epoch 331/4000, Step 41, d_loss: 0.33293724060058594, g_loss: 4.701526165008545\n","Epoch 331/4000, Step 42, d_loss: 0.33538302779197693, g_loss: 8.076298713684082\n","Epoch 331/4000, Step 43, d_loss: 0.35084182024002075, g_loss: 6.07277250289917\n","Epoch 331/4000, Step 44, d_loss: 0.3354518711566925, g_loss: 4.999893665313721\n","Epoch 331/4000, Step 45, d_loss: 0.33273687958717346, g_loss: 6.087275505065918\n","Epoch 331/4000, Step 46, d_loss: 0.34086501598358154, g_loss: 4.55733060836792\n","Epoch 331/4000, Step 47, d_loss: 0.34830543398857117, g_loss: 5.803540229797363\n","Epoch 331/4000, Step 48, d_loss: 0.3490141034126282, g_loss: 4.706153392791748\n","Epoch 331/4000, Step 49, d_loss: 0.33345624804496765, g_loss: 4.914417743682861\n","Epoch 331/4000, Step 50, d_loss: 0.33469071984291077, g_loss: 5.656988143920898\n","Epoch 331/4000, Step 51, d_loss: 0.3330059349536896, g_loss: 8.9290132522583\n","Epoch 331/4000, Step 52, d_loss: 0.3460003435611725, g_loss: 7.073720455169678\n","Epoch 331/4000, Step 53, d_loss: 0.3346668481826782, g_loss: 5.369509220123291\n","Epoch 331/4000, Step 54, d_loss: 0.33488476276397705, g_loss: 5.740909576416016\n","Epoch 331/4000, Step 55, d_loss: 0.3327576518058777, g_loss: 5.47260046005249\n","Epoch 331/4000, Step 56, d_loss: 0.3397807478904724, g_loss: 4.930983066558838\n","Epoch 331/4000, Step 57, d_loss: 0.3364030420780182, g_loss: 6.339915752410889\n","Epoch 331/4000, Step 58, d_loss: 0.3369731903076172, g_loss: 5.154947280883789\n","Epoch 331/4000, Step 59, d_loss: 0.3347991704940796, g_loss: 6.2412919998168945\n","Epoch 331/4000, Step 60, d_loss: 0.36961379647254944, g_loss: 5.760586261749268\n","Epoch 331/4000, Step 61, d_loss: 0.3371150493621826, g_loss: 5.187890529632568\n","Epoch 331/4000, Step 62, d_loss: 0.3384990394115448, g_loss: 5.360857009887695\n","Epoch 331/4000, Step 63, d_loss: 0.34392812848091125, g_loss: 4.4379777908325195\n","Epoch 331/4000, Step 64, d_loss: 0.3381118178367615, g_loss: 6.638887405395508\n","Epoch 331/4000, Step 65, d_loss: 0.3533534109592438, g_loss: 4.606063365936279\n","Epoch 331/4000, Step 66, d_loss: 0.33335328102111816, g_loss: 5.334112167358398\n","Epoch 331/4000, Step 67, d_loss: 0.3307366669178009, g_loss: 5.662361145019531\n","Epoch 331/4000, Step 68, d_loss: 0.34469860792160034, g_loss: 7.861901760101318\n","Epoch 331/4000, Step 69, d_loss: 0.3446802794933319, g_loss: 6.748715877532959\n","Epoch 331/4000, Step 70, d_loss: 0.34167516231536865, g_loss: 6.406086444854736\n","Epoch 331/4000, Step 71, d_loss: 0.3380896747112274, g_loss: 5.0274658203125\n","Epoch 331/4000, Step 72, d_loss: 0.3336917757987976, g_loss: 5.582744598388672\n","Epoch 331/4000, Step 73, d_loss: 0.34027910232543945, g_loss: 5.46743631362915\n","Epoch 331/4000, Step 74, d_loss: 0.34889763593673706, g_loss: 5.389952659606934\n","Epoch 331/4000, Step 75, d_loss: 0.3562830090522766, g_loss: 5.849756717681885\n","Epoch 331/4000, Step 76, d_loss: 0.33627772331237793, g_loss: 6.412621021270752\n","Epoch 331/4000, Step 77, d_loss: 0.34001293778419495, g_loss: 5.6685872077941895\n","Epoch 331/4000, Step 78, d_loss: 0.35263505578041077, g_loss: 4.8522562980651855\n","Epoch 331/4000, Step 79, d_loss: 0.7422598004341125, g_loss: 3.470184326171875\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 332/4000, Step 1, d_loss: 0.42683517932891846, g_loss: 2.120089292526245\n","Epoch 332/4000, Step 2, d_loss: 0.4782232940196991, g_loss: 1.3405632972717285\n","Epoch 332/4000, Step 3, d_loss: 0.7209658026695251, g_loss: 2.4882731437683105\n","Epoch 332/4000, Step 4, d_loss: 0.5990481376647949, g_loss: 3.8776590824127197\n","Epoch 332/4000, Step 5, d_loss: 0.5443156361579895, g_loss: 8.305981636047363\n","Epoch 332/4000, Step 6, d_loss: 0.5327261686325073, g_loss: 4.5977888107299805\n","Epoch 332/4000, Step 7, d_loss: 0.3840530216693878, g_loss: 9.109792709350586\n","Epoch 332/4000, Step 8, d_loss: 0.3652915954589844, g_loss: 7.282516002655029\n","Epoch 332/4000, Step 9, d_loss: 0.40548408031463623, g_loss: 4.448855400085449\n","Epoch 332/4000, Step 10, d_loss: 0.46748533844947815, g_loss: 6.100951194763184\n","Epoch 332/4000, Step 11, d_loss: 0.6557042598724365, g_loss: 6.1917009353637695\n","Epoch 332/4000, Step 12, d_loss: 0.5680328607559204, g_loss: 2.400489568710327\n","Epoch 332/4000, Step 13, d_loss: 0.4259973168373108, g_loss: 1.4391603469848633\n","Epoch 332/4000, Step 14, d_loss: 0.42628782987594604, g_loss: 5.224325180053711\n","Epoch 332/4000, Step 15, d_loss: 0.6891947984695435, g_loss: 2.266484260559082\n","Epoch 332/4000, Step 16, d_loss: 0.6451841592788696, g_loss: 4.908416748046875\n","Epoch 332/4000, Step 17, d_loss: 0.7620346546173096, g_loss: 7.273373603820801\n","Epoch 332/4000, Step 18, d_loss: 0.5772181153297424, g_loss: 6.593194961547852\n","Epoch 332/4000, Step 19, d_loss: 0.3875693678855896, g_loss: 4.505189418792725\n","Epoch 332/4000, Step 20, d_loss: 0.4139999747276306, g_loss: 4.486976623535156\n","Epoch 332/4000, Step 21, d_loss: 0.4686695337295532, g_loss: 4.717204570770264\n","Epoch 332/4000, Step 22, d_loss: 0.45097267627716064, g_loss: 4.732858657836914\n","Epoch 332/4000, Step 23, d_loss: 0.5119000673294067, g_loss: 3.2633137702941895\n","Epoch 332/4000, Step 24, d_loss: 0.4646182060241699, g_loss: 4.058388710021973\n","Epoch 332/4000, Step 25, d_loss: 0.45713451504707336, g_loss: 6.212122440338135\n","Epoch 332/4000, Step 26, d_loss: 0.49650314450263977, g_loss: 4.06324577331543\n","Epoch 332/4000, Step 27, d_loss: 0.37843725085258484, g_loss: 4.521008014678955\n","Epoch 332/4000, Step 28, d_loss: 0.3875974118709564, g_loss: 4.029062271118164\n","Epoch 332/4000, Step 29, d_loss: 0.358476459980011, g_loss: 4.7036027908325195\n","Epoch 332/4000, Step 30, d_loss: 0.3747265934944153, g_loss: 6.533500671386719\n","Epoch 332/4000, Step 31, d_loss: 0.44446754455566406, g_loss: 3.9217915534973145\n","Epoch 332/4000, Step 32, d_loss: 0.35192927718162537, g_loss: 6.717135906219482\n","Epoch 332/4000, Step 33, d_loss: 0.4532817006111145, g_loss: 6.002678394317627\n","Epoch 332/4000, Step 34, d_loss: 0.3828144073486328, g_loss: 3.2978601455688477\n","Epoch 332/4000, Step 35, d_loss: 0.376877099275589, g_loss: 6.884828090667725\n","Epoch 332/4000, Step 36, d_loss: 0.3562924563884735, g_loss: 5.898375034332275\n","Epoch 332/4000, Step 37, d_loss: 0.40388673543930054, g_loss: 4.338916301727295\n","Epoch 332/4000, Step 38, d_loss: 0.3741759955883026, g_loss: 2.6718366146087646\n","Epoch 332/4000, Step 39, d_loss: 0.42976510524749756, g_loss: 4.107648849487305\n","Epoch 332/4000, Step 40, d_loss: 0.3794446885585785, g_loss: 4.957353591918945\n","Epoch 332/4000, Step 41, d_loss: 0.37560248374938965, g_loss: 5.389457702636719\n","Epoch 332/4000, Step 42, d_loss: 0.3807619512081146, g_loss: 6.903398513793945\n","Epoch 332/4000, Step 43, d_loss: 0.35741618275642395, g_loss: 5.785168170928955\n","Epoch 332/4000, Step 44, d_loss: 0.4282403886318207, g_loss: 5.781691074371338\n","Epoch 332/4000, Step 45, d_loss: 0.3535468280315399, g_loss: 7.3667449951171875\n","Epoch 332/4000, Step 46, d_loss: 0.4141644835472107, g_loss: 4.271528720855713\n","Epoch 332/4000, Step 47, d_loss: 0.446239709854126, g_loss: 4.601596832275391\n","Epoch 332/4000, Step 48, d_loss: 0.3948620855808258, g_loss: 5.123983860015869\n","Epoch 332/4000, Step 49, d_loss: 0.3858582079410553, g_loss: 5.917722702026367\n","Epoch 332/4000, Step 50, d_loss: 0.3572255074977875, g_loss: 8.896768569946289\n","Epoch 332/4000, Step 51, d_loss: 0.35838940739631653, g_loss: 7.181426048278809\n","Epoch 332/4000, Step 52, d_loss: 0.3444701135158539, g_loss: 6.9530510902404785\n","Epoch 332/4000, Step 53, d_loss: 0.3736972510814667, g_loss: 7.924145221710205\n","Epoch 332/4000, Step 54, d_loss: 0.3542596101760864, g_loss: 6.256571292877197\n","Epoch 332/4000, Step 55, d_loss: 0.3458072543144226, g_loss: 5.60839319229126\n","Epoch 332/4000, Step 56, d_loss: 0.3454509675502777, g_loss: 5.336032867431641\n","Epoch 332/4000, Step 57, d_loss: 0.4103342592716217, g_loss: 5.79829216003418\n","Epoch 332/4000, Step 58, d_loss: 0.4090891480445862, g_loss: 6.7632575035095215\n","Epoch 332/4000, Step 59, d_loss: 0.36915072798728943, g_loss: 5.976551055908203\n","Epoch 332/4000, Step 60, d_loss: 0.36724016070365906, g_loss: 8.239622116088867\n","Epoch 332/4000, Step 61, d_loss: 0.34220460057258606, g_loss: 6.681593894958496\n","Epoch 332/4000, Step 62, d_loss: 0.35353752970695496, g_loss: 6.754281997680664\n","Epoch 332/4000, Step 63, d_loss: 0.34225428104400635, g_loss: 6.3886871337890625\n","Epoch 332/4000, Step 64, d_loss: 0.36788952350616455, g_loss: 6.327117443084717\n","Epoch 332/4000, Step 65, d_loss: 0.375325471162796, g_loss: 7.346966743469238\n","Epoch 332/4000, Step 66, d_loss: 0.34672489762306213, g_loss: 5.425541877746582\n","Epoch 332/4000, Step 67, d_loss: 0.35439273715019226, g_loss: 4.544743061065674\n","Epoch 332/4000, Step 68, d_loss: 0.3828626573085785, g_loss: 5.1176862716674805\n","Epoch 332/4000, Step 69, d_loss: 0.41108596324920654, g_loss: 2.206364154815674\n","Epoch 332/4000, Step 70, d_loss: 0.3845027983188629, g_loss: 5.486208915710449\n","Epoch 332/4000, Step 71, d_loss: 0.34764742851257324, g_loss: 9.494900703430176\n","Epoch 332/4000, Step 72, d_loss: 0.34466129541397095, g_loss: 7.351207256317139\n","Epoch 332/4000, Step 73, d_loss: 0.35943540930747986, g_loss: 5.355374336242676\n","Epoch 332/4000, Step 74, d_loss: 0.33860525488853455, g_loss: 6.683111667633057\n","Epoch 332/4000, Step 75, d_loss: 0.37312430143356323, g_loss: 6.152791500091553\n","Epoch 332/4000, Step 76, d_loss: 0.3476755917072296, g_loss: 5.166798114776611\n","Epoch 332/4000, Step 77, d_loss: 0.3450821340084076, g_loss: 6.029805660247803\n","Epoch 332/4000, Step 78, d_loss: 0.3407032787799835, g_loss: 5.166679382324219\n","Epoch 332/4000, Step 79, d_loss: 1.760303020477295, g_loss: 3.232146739959717\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 333/4000, Step 1, d_loss: 0.38102900981903076, g_loss: 1.944036841392517\n","Epoch 333/4000, Step 2, d_loss: 0.5727094411849976, g_loss: 2.8100829124450684\n","Epoch 333/4000, Step 3, d_loss: 1.0507131814956665, g_loss: 2.2653696537017822\n","Epoch 333/4000, Step 4, d_loss: 0.7674656510353088, g_loss: 3.819765567779541\n","Epoch 333/4000, Step 5, d_loss: 0.5500150918960571, g_loss: 5.693656921386719\n","Epoch 333/4000, Step 6, d_loss: 0.6902348399162292, g_loss: 4.889408111572266\n","Epoch 333/4000, Step 7, d_loss: 0.5777356624603271, g_loss: 4.540527820587158\n","Epoch 333/4000, Step 8, d_loss: 0.39319565892219543, g_loss: 4.048151969909668\n","Epoch 333/4000, Step 9, d_loss: 0.39415377378463745, g_loss: 4.444781303405762\n","Epoch 333/4000, Step 10, d_loss: 0.44540342688560486, g_loss: 4.618597984313965\n","Epoch 333/4000, Step 11, d_loss: 0.46851682662963867, g_loss: 4.5216755867004395\n","Epoch 333/4000, Step 12, d_loss: 0.5297046303749084, g_loss: 3.94620943069458\n","Epoch 333/4000, Step 13, d_loss: 0.5331876873970032, g_loss: 3.4543614387512207\n","Epoch 333/4000, Step 14, d_loss: 0.4574894309043884, g_loss: 3.6192004680633545\n","Epoch 333/4000, Step 15, d_loss: 0.4800056517124176, g_loss: 6.230778217315674\n","Epoch 333/4000, Step 16, d_loss: 0.43635907769203186, g_loss: 5.680840969085693\n","Epoch 333/4000, Step 17, d_loss: 0.47804078459739685, g_loss: 7.491027355194092\n","Epoch 333/4000, Step 18, d_loss: 0.4064525365829468, g_loss: 4.517985820770264\n","Epoch 333/4000, Step 19, d_loss: 0.3977779150009155, g_loss: 6.407537460327148\n","Epoch 333/4000, Step 20, d_loss: 0.418030709028244, g_loss: 6.022198677062988\n","Epoch 333/4000, Step 21, d_loss: 0.3814557194709778, g_loss: 4.191769599914551\n","Epoch 333/4000, Step 22, d_loss: 0.37418776750564575, g_loss: 5.032785892486572\n","Epoch 333/4000, Step 23, d_loss: 0.3609279692173004, g_loss: 7.003921985626221\n","Epoch 333/4000, Step 24, d_loss: 0.39767515659332275, g_loss: 4.686986446380615\n","Epoch 333/4000, Step 25, d_loss: 0.3725259602069855, g_loss: 4.853873252868652\n","Epoch 333/4000, Step 26, d_loss: 0.37314915657043457, g_loss: 5.113266468048096\n","Epoch 333/4000, Step 27, d_loss: 0.4186457097530365, g_loss: 4.545853614807129\n","Epoch 333/4000, Step 28, d_loss: 0.435398668050766, g_loss: 3.198925018310547\n","Epoch 333/4000, Step 29, d_loss: 0.4091223478317261, g_loss: 5.881779670715332\n","Epoch 333/4000, Step 30, d_loss: 0.3855178654193878, g_loss: 4.699926853179932\n","Epoch 333/4000, Step 31, d_loss: 0.3653098940849304, g_loss: 3.905040740966797\n","Epoch 333/4000, Step 32, d_loss: 0.34836840629577637, g_loss: 4.161247730255127\n","Epoch 333/4000, Step 33, d_loss: 0.3712316155433655, g_loss: 5.268203258514404\n","Epoch 333/4000, Step 34, d_loss: 0.38057225942611694, g_loss: 5.8613080978393555\n","Epoch 333/4000, Step 35, d_loss: 0.36516129970550537, g_loss: 4.007054805755615\n","Epoch 333/4000, Step 36, d_loss: 0.39630603790283203, g_loss: 4.3685994148254395\n","Epoch 333/4000, Step 37, d_loss: 0.3692495822906494, g_loss: 5.6195454597473145\n","Epoch 333/4000, Step 38, d_loss: 0.3834928572177887, g_loss: 3.779081106185913\n","Epoch 333/4000, Step 39, d_loss: 0.35615670680999756, g_loss: 7.133450984954834\n","Epoch 333/4000, Step 40, d_loss: 0.37371841073036194, g_loss: 5.08141565322876\n","Epoch 333/4000, Step 41, d_loss: 0.38113194704055786, g_loss: 4.120954990386963\n","Epoch 333/4000, Step 42, d_loss: 0.3475915491580963, g_loss: 4.671327114105225\n","Epoch 333/4000, Step 43, d_loss: 0.3903035819530487, g_loss: 4.4841413497924805\n","Epoch 333/4000, Step 44, d_loss: 0.3908430337905884, g_loss: 4.724492073059082\n","Epoch 333/4000, Step 45, d_loss: 0.39100638031959534, g_loss: 3.9588589668273926\n","Epoch 333/4000, Step 46, d_loss: 0.4311644732952118, g_loss: 5.15574836730957\n","Epoch 333/4000, Step 47, d_loss: 0.35534635186195374, g_loss: 8.487000465393066\n","Epoch 333/4000, Step 48, d_loss: 0.39273881912231445, g_loss: 3.1839308738708496\n","Epoch 333/4000, Step 49, d_loss: 0.4149368703365326, g_loss: 4.060677528381348\n","Epoch 333/4000, Step 50, d_loss: 0.4143516421318054, g_loss: 4.672756671905518\n","Epoch 333/4000, Step 51, d_loss: 0.3808528482913971, g_loss: 4.491614818572998\n","Epoch 333/4000, Step 52, d_loss: 0.3710263669490814, g_loss: 5.45451021194458\n","Epoch 333/4000, Step 53, d_loss: 0.35624730587005615, g_loss: 4.518799781799316\n","Epoch 333/4000, Step 54, d_loss: 0.42517179250717163, g_loss: 5.694055080413818\n","Epoch 333/4000, Step 55, d_loss: 0.38607266545295715, g_loss: 4.902530670166016\n","Epoch 333/4000, Step 56, d_loss: 0.3611491918563843, g_loss: 4.215615272521973\n","Epoch 333/4000, Step 57, d_loss: 0.3686430752277374, g_loss: 2.848597526550293\n","Epoch 333/4000, Step 58, d_loss: 0.4343677759170532, g_loss: 3.8406951427459717\n","Epoch 333/4000, Step 59, d_loss: 0.39485523104667664, g_loss: 4.170722007751465\n","Epoch 333/4000, Step 60, d_loss: 0.37164658308029175, g_loss: 4.107209205627441\n","Epoch 333/4000, Step 61, d_loss: 0.3589355945587158, g_loss: 6.69431734085083\n","Epoch 333/4000, Step 62, d_loss: 0.36129140853881836, g_loss: 5.575253963470459\n","Epoch 333/4000, Step 63, d_loss: 0.386294960975647, g_loss: 4.613563537597656\n","Epoch 333/4000, Step 64, d_loss: 0.40159958600997925, g_loss: 9.5626220703125\n","Epoch 333/4000, Step 65, d_loss: 0.3503965437412262, g_loss: 5.672684192657471\n","Epoch 333/4000, Step 66, d_loss: 0.3646691143512726, g_loss: 4.0028815269470215\n","Epoch 333/4000, Step 67, d_loss: 0.373357892036438, g_loss: 4.94079065322876\n","Epoch 333/4000, Step 68, d_loss: 0.40776675939559937, g_loss: 3.991795539855957\n","Epoch 333/4000, Step 69, d_loss: 0.3910481631755829, g_loss: 2.092008590698242\n","Epoch 333/4000, Step 70, d_loss: 0.5038611888885498, g_loss: 4.715582370758057\n","Epoch 333/4000, Step 71, d_loss: 0.400627464056015, g_loss: 5.9723711013793945\n","Epoch 333/4000, Step 72, d_loss: 0.4849359393119812, g_loss: 7.860248565673828\n","Epoch 333/4000, Step 73, d_loss: 0.43267330527305603, g_loss: 6.883979320526123\n","Epoch 333/4000, Step 74, d_loss: 0.3517332077026367, g_loss: 8.380202293395996\n","Epoch 333/4000, Step 75, d_loss: 0.37807244062423706, g_loss: 4.711429595947266\n","Epoch 333/4000, Step 76, d_loss: 0.4105370342731476, g_loss: 7.717210292816162\n","Epoch 333/4000, Step 77, d_loss: 0.41489681601524353, g_loss: 6.786214828491211\n","Epoch 333/4000, Step 78, d_loss: 0.42359107732772827, g_loss: 5.524564266204834\n","Epoch 333/4000, Step 79, d_loss: 0.38787370920181274, g_loss: 6.471245288848877\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 334/4000, Step 1, d_loss: 0.3877667486667633, g_loss: 6.35935115814209\n","Epoch 334/4000, Step 2, d_loss: 0.37184396386146545, g_loss: 7.200349807739258\n","Epoch 334/4000, Step 3, d_loss: 0.364228755235672, g_loss: 9.07231616973877\n","Epoch 334/4000, Step 4, d_loss: 0.3585626482963562, g_loss: 5.334873199462891\n","Epoch 334/4000, Step 5, d_loss: 0.36078837513923645, g_loss: 7.6095781326293945\n","Epoch 334/4000, Step 6, d_loss: 0.426400750875473, g_loss: 5.727479457855225\n","Epoch 334/4000, Step 7, d_loss: 0.3446299433708191, g_loss: 5.351599216461182\n","Epoch 334/4000, Step 8, d_loss: 0.34756341576576233, g_loss: 5.122404098510742\n","Epoch 334/4000, Step 9, d_loss: 0.35181891918182373, g_loss: 3.9170644283294678\n","Epoch 334/4000, Step 10, d_loss: 0.37645840644836426, g_loss: 5.13742733001709\n","Epoch 334/4000, Step 11, d_loss: 0.39447659254074097, g_loss: 5.285211086273193\n","Epoch 334/4000, Step 12, d_loss: 0.35256287455558777, g_loss: 4.630805969238281\n","Epoch 334/4000, Step 13, d_loss: 0.34701234102249146, g_loss: 4.214229106903076\n","Epoch 334/4000, Step 14, d_loss: 0.37472277879714966, g_loss: 3.969221830368042\n","Epoch 334/4000, Step 15, d_loss: 0.3473580479621887, g_loss: 4.795652389526367\n","Epoch 334/4000, Step 16, d_loss: 0.35279449820518494, g_loss: 5.526065349578857\n","Epoch 334/4000, Step 17, d_loss: 0.35327914357185364, g_loss: 6.889893054962158\n","Epoch 334/4000, Step 18, d_loss: 0.33176061511039734, g_loss: 7.614975929260254\n","Epoch 334/4000, Step 19, d_loss: 0.35406023263931274, g_loss: 5.50648832321167\n","Epoch 334/4000, Step 20, d_loss: 0.34639930725097656, g_loss: 5.743217468261719\n","Epoch 334/4000, Step 21, d_loss: 0.3514244258403778, g_loss: 5.787959098815918\n","Epoch 334/4000, Step 22, d_loss: 0.3457058370113373, g_loss: 7.763327121734619\n","Epoch 334/4000, Step 23, d_loss: 0.35298311710357666, g_loss: 4.742645740509033\n","Epoch 334/4000, Step 24, d_loss: 0.35884609818458557, g_loss: 6.740428924560547\n","Epoch 334/4000, Step 25, d_loss: 0.34113383293151855, g_loss: 4.949245452880859\n","Epoch 334/4000, Step 26, d_loss: 0.33545008301734924, g_loss: 5.61919641494751\n","Epoch 334/4000, Step 27, d_loss: 0.35383719205856323, g_loss: 5.610795021057129\n","Epoch 334/4000, Step 28, d_loss: 0.3510204553604126, g_loss: 5.300380706787109\n","Epoch 334/4000, Step 29, d_loss: 0.3370041251182556, g_loss: 5.6116156578063965\n","Epoch 334/4000, Step 30, d_loss: 0.3406467139720917, g_loss: 5.09955358505249\n","Epoch 334/4000, Step 31, d_loss: 0.3370526134967804, g_loss: 4.752808570861816\n","Epoch 334/4000, Step 32, d_loss: 0.4159853458404541, g_loss: 10.097929954528809\n","Epoch 334/4000, Step 33, d_loss: 0.3762209415435791, g_loss: 3.9961981773376465\n","Epoch 334/4000, Step 34, d_loss: 0.3638174533843994, g_loss: 6.203742504119873\n","Epoch 334/4000, Step 35, d_loss: 0.37651991844177246, g_loss: 4.347935676574707\n","Epoch 334/4000, Step 36, d_loss: 0.3963406980037689, g_loss: 7.002871990203857\n","Epoch 334/4000, Step 37, d_loss: 0.4058719575405121, g_loss: 5.661563873291016\n","Epoch 334/4000, Step 38, d_loss: 0.34764528274536133, g_loss: 6.796424865722656\n","Epoch 334/4000, Step 39, d_loss: 0.34448379278182983, g_loss: 9.669978141784668\n","Epoch 334/4000, Step 40, d_loss: 0.3677907884120941, g_loss: 6.498529434204102\n","Epoch 334/4000, Step 41, d_loss: 0.390868604183197, g_loss: 5.82067346572876\n","Epoch 334/4000, Step 42, d_loss: 0.35277441143989563, g_loss: 5.11960506439209\n","Epoch 334/4000, Step 43, d_loss: 0.3398253917694092, g_loss: 7.2617716789245605\n","Epoch 334/4000, Step 44, d_loss: 0.3519822061061859, g_loss: 6.04210901260376\n","Epoch 334/4000, Step 45, d_loss: 0.36018702387809753, g_loss: 6.257596015930176\n","Epoch 334/4000, Step 46, d_loss: 0.3734496235847473, g_loss: 6.273046493530273\n","Epoch 334/4000, Step 47, d_loss: 0.3588224947452545, g_loss: 5.8962178230285645\n","Epoch 334/4000, Step 48, d_loss: 0.33839675784111023, g_loss: 5.96513557434082\n","Epoch 334/4000, Step 49, d_loss: 0.3391939699649811, g_loss: 5.317959785461426\n","Epoch 334/4000, Step 50, d_loss: 0.33791986107826233, g_loss: 5.091438293457031\n","Epoch 334/4000, Step 51, d_loss: 0.34543168544769287, g_loss: 5.705459117889404\n","Epoch 334/4000, Step 52, d_loss: 0.34762293100357056, g_loss: 6.128625869750977\n","Epoch 334/4000, Step 53, d_loss: 0.3335583806037903, g_loss: 7.1669793128967285\n","Epoch 334/4000, Step 54, d_loss: 0.33061131834983826, g_loss: 6.283385753631592\n","Epoch 334/4000, Step 55, d_loss: 0.3437384366989136, g_loss: 5.41745662689209\n","Epoch 334/4000, Step 56, d_loss: 0.35144126415252686, g_loss: 7.07881498336792\n","Epoch 334/4000, Step 57, d_loss: 0.34069785475730896, g_loss: 6.079344272613525\n","Epoch 334/4000, Step 58, d_loss: 0.33352309465408325, g_loss: 5.2971696853637695\n","Epoch 334/4000, Step 59, d_loss: 0.3349285125732422, g_loss: 5.805150985717773\n","Epoch 334/4000, Step 60, d_loss: 0.3332309126853943, g_loss: 7.351011276245117\n","Epoch 334/4000, Step 61, d_loss: 0.34289899468421936, g_loss: 6.403218746185303\n","Epoch 334/4000, Step 62, d_loss: 0.3335399329662323, g_loss: 9.021785736083984\n","Epoch 334/4000, Step 63, d_loss: 0.3388552963733673, g_loss: 5.255976676940918\n","Epoch 334/4000, Step 64, d_loss: 0.33506983518600464, g_loss: 6.911247730255127\n","Epoch 334/4000, Step 65, d_loss: 0.33188632130622864, g_loss: 5.170862674713135\n","Epoch 334/4000, Step 66, d_loss: 0.3345751464366913, g_loss: 6.606027603149414\n","Epoch 334/4000, Step 67, d_loss: 0.3364998698234558, g_loss: 9.36528205871582\n","Epoch 334/4000, Step 68, d_loss: 0.33700722455978394, g_loss: 5.489396572113037\n","Epoch 334/4000, Step 69, d_loss: 0.34421801567077637, g_loss: 5.6419997215271\n","Epoch 334/4000, Step 70, d_loss: 0.3285236656665802, g_loss: 4.652066230773926\n","Epoch 334/4000, Step 71, d_loss: 0.3386552929878235, g_loss: 7.907992839813232\n","Epoch 334/4000, Step 72, d_loss: 0.35413235425949097, g_loss: 7.569207191467285\n","Epoch 334/4000, Step 73, d_loss: 0.33136487007141113, g_loss: 4.8790812492370605\n","Epoch 334/4000, Step 74, d_loss: 0.33556994795799255, g_loss: 5.507924556732178\n","Epoch 334/4000, Step 75, d_loss: 0.3582989573478699, g_loss: 5.093994140625\n","Epoch 334/4000, Step 76, d_loss: 0.33270740509033203, g_loss: 5.766486644744873\n","Epoch 334/4000, Step 77, d_loss: 0.34985896944999695, g_loss: 5.41182279586792\n","Epoch 334/4000, Step 78, d_loss: 0.3347523808479309, g_loss: 10.265765190124512\n","Epoch 334/4000, Step 79, d_loss: 2.4563498497009277, g_loss: 4.448617458343506\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 335/4000, Step 1, d_loss: 0.4139060378074646, g_loss: 3.879218816757202\n","Epoch 335/4000, Step 2, d_loss: 0.6629279851913452, g_loss: 2.2587101459503174\n","Epoch 335/4000, Step 3, d_loss: 0.5586657524108887, g_loss: 2.417605400085449\n","Epoch 335/4000, Step 4, d_loss: 0.568899929523468, g_loss: 6.1759443283081055\n","Epoch 335/4000, Step 5, d_loss: 0.5798921585083008, g_loss: 5.434444427490234\n","Epoch 335/4000, Step 6, d_loss: 0.6120601296424866, g_loss: 5.864532947540283\n","Epoch 335/4000, Step 7, d_loss: 0.6230896711349487, g_loss: 6.263927459716797\n","Epoch 335/4000, Step 8, d_loss: 1.2926164865493774, g_loss: 4.257164478302002\n","Epoch 335/4000, Step 9, d_loss: 0.6562580466270447, g_loss: 11.183789253234863\n","Epoch 335/4000, Step 10, d_loss: 0.6405136585235596, g_loss: 10.831835746765137\n","Epoch 335/4000, Step 11, d_loss: 0.7837966084480286, g_loss: 8.702325820922852\n","Epoch 335/4000, Step 12, d_loss: 0.5982419848442078, g_loss: 7.09117317199707\n","Epoch 335/4000, Step 13, d_loss: 0.3872860372066498, g_loss: 2.1251578330993652\n","Epoch 335/4000, Step 14, d_loss: 0.6264469027519226, g_loss: 6.964929580688477\n","Epoch 335/4000, Step 15, d_loss: 0.423484742641449, g_loss: 8.78786563873291\n","Epoch 335/4000, Step 16, d_loss: 0.3961070477962494, g_loss: 5.999815940856934\n","Epoch 335/4000, Step 17, d_loss: 0.42198479175567627, g_loss: 9.068114280700684\n","Epoch 335/4000, Step 18, d_loss: 0.5542123913764954, g_loss: 5.949773788452148\n","Epoch 335/4000, Step 19, d_loss: 0.45086178183555603, g_loss: 8.005023002624512\n","Epoch 335/4000, Step 20, d_loss: 0.36114612221717834, g_loss: 6.320554733276367\n","Epoch 335/4000, Step 21, d_loss: 0.4519674777984619, g_loss: 5.975093364715576\n","Epoch 335/4000, Step 22, d_loss: 0.36489081382751465, g_loss: 6.249903202056885\n","Epoch 335/4000, Step 23, d_loss: 0.38074156641960144, g_loss: 6.6847944259643555\n","Epoch 335/4000, Step 24, d_loss: 0.3815394639968872, g_loss: 5.7761406898498535\n","Epoch 335/4000, Step 25, d_loss: 0.43020257353782654, g_loss: 4.761740207672119\n","Epoch 335/4000, Step 26, d_loss: 0.5798079967498779, g_loss: 4.129498481750488\n","Epoch 335/4000, Step 27, d_loss: 0.40096315741539, g_loss: 4.372408866882324\n","Epoch 335/4000, Step 28, d_loss: 0.4671967327594757, g_loss: 4.6543803215026855\n","Epoch 335/4000, Step 29, d_loss: 0.43238672614097595, g_loss: 2.691258192062378\n","Epoch 335/4000, Step 30, d_loss: 0.4400760531425476, g_loss: 5.34675407409668\n","Epoch 335/4000, Step 31, d_loss: 0.4994572103023529, g_loss: 3.848698139190674\n","Epoch 335/4000, Step 32, d_loss: 0.3857589662075043, g_loss: 3.933903694152832\n","Epoch 335/4000, Step 33, d_loss: 0.5797251462936401, g_loss: 5.010780334472656\n","Epoch 335/4000, Step 34, d_loss: 0.3617215156555176, g_loss: 2.854780435562134\n","Epoch 335/4000, Step 35, d_loss: 0.5198597311973572, g_loss: 2.6736414432525635\n","Epoch 335/4000, Step 36, d_loss: 0.5241348147392273, g_loss: 6.04683256149292\n","Epoch 335/4000, Step 37, d_loss: 0.4394018352031708, g_loss: 3.765122652053833\n","Epoch 335/4000, Step 38, d_loss: 0.38262179493904114, g_loss: 5.713135719299316\n","Epoch 335/4000, Step 39, d_loss: 0.35412368178367615, g_loss: 5.697099208831787\n","Epoch 335/4000, Step 40, d_loss: 0.3748457729816437, g_loss: 5.315749168395996\n","Epoch 335/4000, Step 41, d_loss: 0.3805970847606659, g_loss: 4.836519241333008\n","Epoch 335/4000, Step 42, d_loss: 0.389289915561676, g_loss: 5.095135688781738\n","Epoch 335/4000, Step 43, d_loss: 0.3940887451171875, g_loss: 4.757345676422119\n","Epoch 335/4000, Step 44, d_loss: 0.38327810168266296, g_loss: 4.704921722412109\n","Epoch 335/4000, Step 45, d_loss: 0.357842355966568, g_loss: 5.60881233215332\n","Epoch 335/4000, Step 46, d_loss: 0.3450320363044739, g_loss: 6.106278419494629\n","Epoch 335/4000, Step 47, d_loss: 0.35849687457084656, g_loss: 5.893829822540283\n","Epoch 335/4000, Step 48, d_loss: 0.45043861865997314, g_loss: 4.635190010070801\n","Epoch 335/4000, Step 49, d_loss: 0.36992502212524414, g_loss: 3.674013376235962\n","Epoch 335/4000, Step 50, d_loss: 0.37249764800071716, g_loss: 3.572030544281006\n","Epoch 335/4000, Step 51, d_loss: 0.40229085087776184, g_loss: 4.215700626373291\n","Epoch 335/4000, Step 52, d_loss: 0.3686389625072479, g_loss: 4.055689334869385\n","Epoch 335/4000, Step 53, d_loss: 0.42354896664619446, g_loss: 3.3584580421447754\n","Epoch 335/4000, Step 54, d_loss: 0.4208781123161316, g_loss: 3.725806951522827\n","Epoch 335/4000, Step 55, d_loss: 0.34968245029449463, g_loss: 5.113537311553955\n","Epoch 335/4000, Step 56, d_loss: 0.3737971782684326, g_loss: 5.260563850402832\n","Epoch 335/4000, Step 57, d_loss: 0.36729148030281067, g_loss: 4.441689491271973\n","Epoch 335/4000, Step 58, d_loss: 0.38648489117622375, g_loss: 3.4690101146698\n","Epoch 335/4000, Step 59, d_loss: 0.3865504264831543, g_loss: 4.157919406890869\n","Epoch 335/4000, Step 60, d_loss: 0.3509337902069092, g_loss: 3.7581534385681152\n","Epoch 335/4000, Step 61, d_loss: 0.3554142117500305, g_loss: 4.312190532684326\n","Epoch 335/4000, Step 62, d_loss: 0.38639014959335327, g_loss: 6.551692008972168\n","Epoch 335/4000, Step 63, d_loss: 0.3769690692424774, g_loss: 4.579862594604492\n","Epoch 335/4000, Step 64, d_loss: 0.3574812114238739, g_loss: 5.284757614135742\n","Epoch 335/4000, Step 65, d_loss: 0.3465782403945923, g_loss: 5.82403039932251\n","Epoch 335/4000, Step 66, d_loss: 0.34699562191963196, g_loss: 5.782018184661865\n","Epoch 335/4000, Step 67, d_loss: 0.3648090064525604, g_loss: 5.337841987609863\n","Epoch 335/4000, Step 68, d_loss: 0.34770601987838745, g_loss: 5.792355060577393\n","Epoch 335/4000, Step 69, d_loss: 0.4985751509666443, g_loss: 4.474216938018799\n","Epoch 335/4000, Step 70, d_loss: 0.3646154999732971, g_loss: 4.120429039001465\n","Epoch 335/4000, Step 71, d_loss: 0.3828660845756531, g_loss: 2.6332638263702393\n","Epoch 335/4000, Step 72, d_loss: 0.4153859317302704, g_loss: 5.878418922424316\n","Epoch 335/4000, Step 73, d_loss: 0.3871591091156006, g_loss: 7.37511682510376\n","Epoch 335/4000, Step 74, d_loss: 0.3681294918060303, g_loss: 4.549045085906982\n","Epoch 335/4000, Step 75, d_loss: 0.36885568499565125, g_loss: 4.650676727294922\n","Epoch 335/4000, Step 76, d_loss: 0.38278549909591675, g_loss: 6.650681495666504\n","Epoch 335/4000, Step 77, d_loss: 0.3681849539279938, g_loss: 7.532005310058594\n","Epoch 335/4000, Step 78, d_loss: 0.36796000599861145, g_loss: 5.223644256591797\n","Epoch 335/4000, Step 79, d_loss: 0.3927268385887146, g_loss: 4.888776779174805\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 336/4000, Step 1, d_loss: 0.37995457649230957, g_loss: 4.083460330963135\n","Epoch 336/4000, Step 2, d_loss: 0.397077739238739, g_loss: 5.550408363342285\n","Epoch 336/4000, Step 3, d_loss: 0.41531091928482056, g_loss: 3.940506935119629\n","Epoch 336/4000, Step 4, d_loss: 0.40846166014671326, g_loss: 4.465238571166992\n","Epoch 336/4000, Step 5, d_loss: 0.3609355688095093, g_loss: 9.779692649841309\n","Epoch 336/4000, Step 6, d_loss: 0.3476826846599579, g_loss: 5.239454746246338\n","Epoch 336/4000, Step 7, d_loss: 0.45139938592910767, g_loss: 4.867674350738525\n","Epoch 336/4000, Step 8, d_loss: 0.3674928843975067, g_loss: 5.070638179779053\n","Epoch 336/4000, Step 9, d_loss: 0.3657292425632477, g_loss: 3.9018404483795166\n","Epoch 336/4000, Step 10, d_loss: 0.39012381434440613, g_loss: 3.765364646911621\n","Epoch 336/4000, Step 11, d_loss: 0.3690437078475952, g_loss: 4.800518989562988\n","Epoch 336/4000, Step 12, d_loss: 0.3694690465927124, g_loss: 4.058351039886475\n","Epoch 336/4000, Step 13, d_loss: 0.3887099325656891, g_loss: 6.3786540031433105\n","Epoch 336/4000, Step 14, d_loss: 0.3640463352203369, g_loss: 5.689752101898193\n","Epoch 336/4000, Step 15, d_loss: 0.3454814851284027, g_loss: 8.647528648376465\n","Epoch 336/4000, Step 16, d_loss: 0.558438777923584, g_loss: 5.683111667633057\n","Epoch 336/4000, Step 17, d_loss: 0.3374519646167755, g_loss: 4.061829090118408\n","Epoch 336/4000, Step 18, d_loss: 0.3561755120754242, g_loss: 4.2659382820129395\n","Epoch 336/4000, Step 19, d_loss: 0.3524249494075775, g_loss: 4.286746978759766\n","Epoch 336/4000, Step 20, d_loss: 0.3880643844604492, g_loss: 4.208832263946533\n","Epoch 336/4000, Step 21, d_loss: 0.4004216194152832, g_loss: 4.485473155975342\n","Epoch 336/4000, Step 22, d_loss: 0.39811646938323975, g_loss: 4.349311351776123\n","Epoch 336/4000, Step 23, d_loss: 0.36498019099235535, g_loss: 4.726241588592529\n","Epoch 336/4000, Step 24, d_loss: 0.34677594900131226, g_loss: 4.180246829986572\n","Epoch 336/4000, Step 25, d_loss: 0.34404462575912476, g_loss: 4.70744514465332\n","Epoch 336/4000, Step 26, d_loss: 0.3516663610935211, g_loss: 5.136291027069092\n","Epoch 336/4000, Step 27, d_loss: 0.3549140691757202, g_loss: 4.6847028732299805\n","Epoch 336/4000, Step 28, d_loss: 0.34174758195877075, g_loss: 5.797098636627197\n","Epoch 336/4000, Step 29, d_loss: 0.47750145196914673, g_loss: 5.035329341888428\n","Epoch 336/4000, Step 30, d_loss: 0.3665521740913391, g_loss: 4.990671157836914\n","Epoch 336/4000, Step 31, d_loss: 0.3441324532032013, g_loss: 5.35495662689209\n","Epoch 336/4000, Step 32, d_loss: 0.3536270260810852, g_loss: 5.043789386749268\n","Epoch 336/4000, Step 33, d_loss: 0.3419194221496582, g_loss: 5.425019264221191\n","Epoch 336/4000, Step 34, d_loss: 0.3475682735443115, g_loss: 4.6465559005737305\n","Epoch 336/4000, Step 35, d_loss: 0.369259774684906, g_loss: 5.770114898681641\n","Epoch 336/4000, Step 36, d_loss: 0.3599163293838501, g_loss: 4.809028625488281\n","Epoch 336/4000, Step 37, d_loss: 0.3617810904979706, g_loss: 4.727593421936035\n","Epoch 336/4000, Step 38, d_loss: 0.3384903371334076, g_loss: 5.001858234405518\n","Epoch 336/4000, Step 39, d_loss: 0.355186402797699, g_loss: 4.533289909362793\n","Epoch 336/4000, Step 40, d_loss: 0.3377765119075775, g_loss: 5.7002339363098145\n","Epoch 336/4000, Step 41, d_loss: 0.3577214181423187, g_loss: 6.748189926147461\n","Epoch 336/4000, Step 42, d_loss: 0.36212220788002014, g_loss: 5.223576068878174\n","Epoch 336/4000, Step 43, d_loss: 0.3467731177806854, g_loss: 4.925145149230957\n","Epoch 336/4000, Step 44, d_loss: 0.3856738805770874, g_loss: 5.189548492431641\n","Epoch 336/4000, Step 45, d_loss: 0.3424777388572693, g_loss: 4.7561540603637695\n","Epoch 336/4000, Step 46, d_loss: 0.3467058837413788, g_loss: 3.619091510772705\n","Epoch 336/4000, Step 47, d_loss: 0.3596136271953583, g_loss: 3.5728492736816406\n","Epoch 336/4000, Step 48, d_loss: 0.35134613513946533, g_loss: 4.28822660446167\n","Epoch 336/4000, Step 49, d_loss: 0.36260920763015747, g_loss: 4.110445022583008\n","Epoch 336/4000, Step 50, d_loss: 0.37233877182006836, g_loss: 7.2925190925598145\n","Epoch 336/4000, Step 51, d_loss: 0.3479391634464264, g_loss: 4.803624153137207\n","Epoch 336/4000, Step 52, d_loss: 0.3510637879371643, g_loss: 5.321468830108643\n","Epoch 336/4000, Step 53, d_loss: 0.34697654843330383, g_loss: 5.495250225067139\n","Epoch 336/4000, Step 54, d_loss: 0.35969480872154236, g_loss: 6.047877311706543\n","Epoch 336/4000, Step 55, d_loss: 0.34331053495407104, g_loss: 5.889179706573486\n","Epoch 336/4000, Step 56, d_loss: 0.3489474952220917, g_loss: 4.059640407562256\n","Epoch 336/4000, Step 57, d_loss: 0.3658839166164398, g_loss: 4.311142444610596\n","Epoch 336/4000, Step 58, d_loss: 0.37401333451271057, g_loss: 6.22097110748291\n","Epoch 336/4000, Step 59, d_loss: 0.35601088404655457, g_loss: 3.803351402282715\n","Epoch 336/4000, Step 60, d_loss: 0.3485718369483948, g_loss: 7.4084014892578125\n","Epoch 336/4000, Step 61, d_loss: 0.34185659885406494, g_loss: 5.868868350982666\n","Epoch 336/4000, Step 62, d_loss: 0.38618797063827515, g_loss: 3.6764116287231445\n","Epoch 336/4000, Step 63, d_loss: 0.3731040060520172, g_loss: 6.483407020568848\n","Epoch 336/4000, Step 64, d_loss: 0.34991273283958435, g_loss: 5.139869213104248\n","Epoch 336/4000, Step 65, d_loss: 0.3418453335762024, g_loss: 6.088616371154785\n","Epoch 336/4000, Step 66, d_loss: 0.3371690809726715, g_loss: 5.651125431060791\n","Epoch 336/4000, Step 67, d_loss: 0.3555804193019867, g_loss: 7.331477642059326\n","Epoch 336/4000, Step 68, d_loss: 0.3528670072555542, g_loss: 5.126502990722656\n","Epoch 336/4000, Step 69, d_loss: 0.35690543055534363, g_loss: 4.477885723114014\n","Epoch 336/4000, Step 70, d_loss: 0.35547614097595215, g_loss: 3.8091938495635986\n","Epoch 336/4000, Step 71, d_loss: 0.3427043557167053, g_loss: 5.826868534088135\n","Epoch 336/4000, Step 72, d_loss: 0.34339576959609985, g_loss: 4.8535637855529785\n","Epoch 336/4000, Step 73, d_loss: 0.34827151894569397, g_loss: 5.881079196929932\n","Epoch 336/4000, Step 74, d_loss: 0.34361082315444946, g_loss: 6.847496509552002\n","Epoch 336/4000, Step 75, d_loss: 0.3425479233264923, g_loss: 4.853403091430664\n","Epoch 336/4000, Step 76, d_loss: 0.3467613458633423, g_loss: 5.549877166748047\n","Epoch 336/4000, Step 77, d_loss: 0.34169530868530273, g_loss: 6.529088497161865\n","Epoch 336/4000, Step 78, d_loss: 0.3473571240901947, g_loss: 5.120016098022461\n","Epoch 336/4000, Step 79, d_loss: 0.37170854210853577, g_loss: 4.668200492858887\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 337/4000, Step 1, d_loss: 0.3903232216835022, g_loss: 3.32802152633667\n","Epoch 337/4000, Step 2, d_loss: 0.4094390571117401, g_loss: 4.7511725425720215\n","Epoch 337/4000, Step 3, d_loss: 0.34678852558135986, g_loss: 5.702649116516113\n","Epoch 337/4000, Step 4, d_loss: 0.341560423374176, g_loss: 6.126227378845215\n","Epoch 337/4000, Step 5, d_loss: 0.3411973714828491, g_loss: 5.559141159057617\n","Epoch 337/4000, Step 6, d_loss: 0.34936976432800293, g_loss: 6.666806697845459\n","Epoch 337/4000, Step 7, d_loss: 0.3575594425201416, g_loss: 5.749231815338135\n","Epoch 337/4000, Step 8, d_loss: 0.3754936456680298, g_loss: 7.125522136688232\n","Epoch 337/4000, Step 9, d_loss: 0.3474750518798828, g_loss: 5.07141637802124\n","Epoch 337/4000, Step 10, d_loss: 0.3667345345020294, g_loss: 3.503833055496216\n","Epoch 337/4000, Step 11, d_loss: 0.3862498104572296, g_loss: 7.264894008636475\n","Epoch 337/4000, Step 12, d_loss: 0.3876112699508667, g_loss: 8.130982398986816\n","Epoch 337/4000, Step 13, d_loss: 0.35684439539909363, g_loss: 4.7722392082214355\n","Epoch 337/4000, Step 14, d_loss: 0.3471858501434326, g_loss: 7.34124231338501\n","Epoch 337/4000, Step 15, d_loss: 0.433352530002594, g_loss: 6.020077228546143\n","Epoch 337/4000, Step 16, d_loss: 0.3483479619026184, g_loss: 7.125203609466553\n","Epoch 337/4000, Step 17, d_loss: 0.3297964334487915, g_loss: 4.993032455444336\n","Epoch 337/4000, Step 18, d_loss: 0.34132421016693115, g_loss: 6.749626636505127\n","Epoch 337/4000, Step 19, d_loss: 0.338742196559906, g_loss: 3.5394909381866455\n","Epoch 337/4000, Step 20, d_loss: 0.3510131537914276, g_loss: 3.7370188236236572\n","Epoch 337/4000, Step 21, d_loss: 0.345213919878006, g_loss: 6.754630088806152\n","Epoch 337/4000, Step 22, d_loss: 0.33961644768714905, g_loss: 5.859063625335693\n","Epoch 337/4000, Step 23, d_loss: 0.3355315923690796, g_loss: 6.039042949676514\n","Epoch 337/4000, Step 24, d_loss: 0.37483811378479004, g_loss: 4.451103210449219\n","Epoch 337/4000, Step 25, d_loss: 0.34495624899864197, g_loss: 4.104699611663818\n","Epoch 337/4000, Step 26, d_loss: 0.3605756163597107, g_loss: 9.667284965515137\n","Epoch 337/4000, Step 27, d_loss: 0.3360881507396698, g_loss: 4.341944217681885\n","Epoch 337/4000, Step 28, d_loss: 0.3376808762550354, g_loss: 7.384069442749023\n","Epoch 337/4000, Step 29, d_loss: 0.34268563985824585, g_loss: 6.127123832702637\n","Epoch 337/4000, Step 30, d_loss: 0.3384043276309967, g_loss: 4.235705375671387\n","Epoch 337/4000, Step 31, d_loss: 0.3401978611946106, g_loss: 6.485694885253906\n","Epoch 337/4000, Step 32, d_loss: 0.3581719398498535, g_loss: 4.759911060333252\n","Epoch 337/4000, Step 33, d_loss: 0.3512915372848511, g_loss: 8.116756439208984\n","Epoch 337/4000, Step 34, d_loss: 0.3415115773677826, g_loss: 5.5943498611450195\n","Epoch 337/4000, Step 35, d_loss: 0.3430725038051605, g_loss: 5.558103561401367\n","Epoch 337/4000, Step 36, d_loss: 0.33546820282936096, g_loss: 6.222836017608643\n","Epoch 337/4000, Step 37, d_loss: 0.33692896366119385, g_loss: 5.4051194190979\n","Epoch 337/4000, Step 38, d_loss: 0.33640098571777344, g_loss: 5.913585186004639\n","Epoch 337/4000, Step 39, d_loss: 0.33937951922416687, g_loss: 6.269701957702637\n","Epoch 337/4000, Step 40, d_loss: 0.3425000309944153, g_loss: 7.686657905578613\n","Epoch 337/4000, Step 41, d_loss: 0.35730862617492676, g_loss: 3.9962050914764404\n","Epoch 337/4000, Step 42, d_loss: 0.34963807463645935, g_loss: 4.360509395599365\n","Epoch 337/4000, Step 43, d_loss: 0.36119499802589417, g_loss: 5.075263500213623\n","Epoch 337/4000, Step 44, d_loss: 0.3510933220386505, g_loss: 4.459674835205078\n","Epoch 337/4000, Step 45, d_loss: 0.34404951333999634, g_loss: 4.954056262969971\n","Epoch 337/4000, Step 46, d_loss: 0.35812750458717346, g_loss: 4.834076404571533\n","Epoch 337/4000, Step 47, d_loss: 0.3629656732082367, g_loss: 4.384811878204346\n","Epoch 337/4000, Step 48, d_loss: 0.3443242013454437, g_loss: 3.525704860687256\n","Epoch 337/4000, Step 49, d_loss: 0.3403342664241791, g_loss: 4.26062536239624\n","Epoch 337/4000, Step 50, d_loss: 0.3560766279697418, g_loss: 3.3321902751922607\n","Epoch 337/4000, Step 51, d_loss: 0.34465649724006653, g_loss: 3.5826022624969482\n","Epoch 337/4000, Step 52, d_loss: 0.3589438498020172, g_loss: 4.211156368255615\n","Epoch 337/4000, Step 53, d_loss: 0.3923495411872864, g_loss: 3.935194730758667\n","Epoch 337/4000, Step 54, d_loss: 0.34927892684936523, g_loss: 7.1751604080200195\n","Epoch 337/4000, Step 55, d_loss: 0.3495968282222748, g_loss: 4.43689489364624\n","Epoch 337/4000, Step 56, d_loss: 0.3895947337150574, g_loss: 3.966733455657959\n","Epoch 337/4000, Step 57, d_loss: 0.3454171419143677, g_loss: 5.412259101867676\n","Epoch 337/4000, Step 58, d_loss: 0.35315239429473877, g_loss: 4.7375288009643555\n","Epoch 337/4000, Step 59, d_loss: 0.34395596385002136, g_loss: 4.857259750366211\n","Epoch 337/4000, Step 60, d_loss: 0.3566381633281708, g_loss: 6.0809245109558105\n","Epoch 337/4000, Step 61, d_loss: 0.3447446823120117, g_loss: 7.983395576477051\n","Epoch 337/4000, Step 62, d_loss: 0.3469829261302948, g_loss: 4.808348655700684\n","Epoch 337/4000, Step 63, d_loss: 0.37393683195114136, g_loss: 7.614134788513184\n","Epoch 337/4000, Step 64, d_loss: 0.36824655532836914, g_loss: 4.255513668060303\n","Epoch 337/4000, Step 65, d_loss: 0.3519735038280487, g_loss: 5.045060634613037\n","Epoch 337/4000, Step 66, d_loss: 0.34075912833213806, g_loss: 5.641584873199463\n","Epoch 337/4000, Step 67, d_loss: 0.3658124506473541, g_loss: 12.598665237426758\n","Epoch 337/4000, Step 68, d_loss: 0.3425068259239197, g_loss: 5.131453037261963\n","Epoch 337/4000, Step 69, d_loss: 0.33304768800735474, g_loss: 6.302141189575195\n","Epoch 337/4000, Step 70, d_loss: 0.36124345660209656, g_loss: 8.84407901763916\n","Epoch 337/4000, Step 71, d_loss: 0.3428874909877777, g_loss: 7.780554294586182\n","Epoch 337/4000, Step 72, d_loss: 0.3542429208755493, g_loss: 5.630777359008789\n","Epoch 337/4000, Step 73, d_loss: 0.34675341844558716, g_loss: 5.396356582641602\n","Epoch 337/4000, Step 74, d_loss: 0.34360575675964355, g_loss: 5.626414775848389\n","Epoch 337/4000, Step 75, d_loss: 0.3785451352596283, g_loss: 4.742389678955078\n","Epoch 337/4000, Step 76, d_loss: 0.343178927898407, g_loss: 6.392892837524414\n","Epoch 337/4000, Step 77, d_loss: 0.34256285429000854, g_loss: 6.135182857513428\n","Epoch 337/4000, Step 78, d_loss: 0.3379194736480713, g_loss: 6.393834114074707\n","Epoch 337/4000, Step 79, d_loss: 0.788783073425293, g_loss: 4.291966915130615\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 338/4000, Step 1, d_loss: 0.43044763803482056, g_loss: 2.7492284774780273\n","Epoch 338/4000, Step 2, d_loss: 0.5136812925338745, g_loss: 4.1870951652526855\n","Epoch 338/4000, Step 3, d_loss: 0.5139611959457397, g_loss: 3.854799747467041\n","Epoch 338/4000, Step 4, d_loss: 0.47254884243011475, g_loss: 5.275064945220947\n","Epoch 338/4000, Step 5, d_loss: 0.46265512704849243, g_loss: 6.8902692794799805\n","Epoch 338/4000, Step 6, d_loss: 0.48510852456092834, g_loss: 5.580275535583496\n","Epoch 338/4000, Step 7, d_loss: 0.4837174713611603, g_loss: 9.493866920471191\n","Epoch 338/4000, Step 8, d_loss: 0.39111289381980896, g_loss: 2.673048257827759\n","Epoch 338/4000, Step 9, d_loss: 0.40943899750709534, g_loss: 3.3896734714508057\n","Epoch 338/4000, Step 10, d_loss: 0.36530137062072754, g_loss: 6.419077396392822\n","Epoch 338/4000, Step 11, d_loss: 0.4476318359375, g_loss: 6.352241516113281\n","Epoch 338/4000, Step 12, d_loss: 0.410177081823349, g_loss: 6.916077613830566\n","Epoch 338/4000, Step 13, d_loss: 0.405997097492218, g_loss: 7.3175482749938965\n","Epoch 338/4000, Step 14, d_loss: 0.3837016820907593, g_loss: 6.675506114959717\n","Epoch 338/4000, Step 15, d_loss: 0.3778945207595825, g_loss: 6.848143577575684\n","Epoch 338/4000, Step 16, d_loss: 0.35922321677207947, g_loss: 6.4353928565979\n","Epoch 338/4000, Step 17, d_loss: 0.36669546365737915, g_loss: 8.393939018249512\n","Epoch 338/4000, Step 18, d_loss: 0.36620819568634033, g_loss: 7.095657825469971\n","Epoch 338/4000, Step 19, d_loss: 0.39843928813934326, g_loss: 6.334330081939697\n","Epoch 338/4000, Step 20, d_loss: 0.35877346992492676, g_loss: 4.621117115020752\n","Epoch 338/4000, Step 21, d_loss: 0.3469814360141754, g_loss: 4.69487190246582\n","Epoch 338/4000, Step 22, d_loss: 0.35704660415649414, g_loss: 5.793251991271973\n","Epoch 338/4000, Step 23, d_loss: 0.38486382365226746, g_loss: 6.629082202911377\n","Epoch 338/4000, Step 24, d_loss: 0.3645523190498352, g_loss: 5.66013240814209\n","Epoch 338/4000, Step 25, d_loss: 0.36121609807014465, g_loss: 8.120261192321777\n","Epoch 338/4000, Step 26, d_loss: 0.34234747290611267, g_loss: 6.227194309234619\n","Epoch 338/4000, Step 27, d_loss: 0.3415592312812805, g_loss: 4.968676567077637\n","Epoch 338/4000, Step 28, d_loss: 0.3429282307624817, g_loss: 5.2789082527160645\n","Epoch 338/4000, Step 29, d_loss: 0.3472099006175995, g_loss: 6.975099563598633\n","Epoch 338/4000, Step 30, d_loss: 0.3584630787372589, g_loss: 5.43290901184082\n","Epoch 338/4000, Step 31, d_loss: 0.34641963243484497, g_loss: 6.176124572753906\n","Epoch 338/4000, Step 32, d_loss: 0.35632458329200745, g_loss: 7.198896884918213\n","Epoch 338/4000, Step 33, d_loss: 0.3400591015815735, g_loss: 4.71412467956543\n","Epoch 338/4000, Step 34, d_loss: 0.3412446081638336, g_loss: 4.457165718078613\n","Epoch 338/4000, Step 35, d_loss: 0.3500337600708008, g_loss: 4.2402143478393555\n","Epoch 338/4000, Step 36, d_loss: 0.33724626898765564, g_loss: 8.434985160827637\n","Epoch 338/4000, Step 37, d_loss: 0.342783659696579, g_loss: 4.356484889984131\n","Epoch 338/4000, Step 38, d_loss: 0.3428749144077301, g_loss: 6.783240795135498\n","Epoch 338/4000, Step 39, d_loss: 0.34056520462036133, g_loss: 6.785946846008301\n","Epoch 338/4000, Step 40, d_loss: 0.3580079972743988, g_loss: 4.637834548950195\n","Epoch 338/4000, Step 41, d_loss: 0.3373475968837738, g_loss: 6.161752700805664\n","Epoch 338/4000, Step 42, d_loss: 0.3398401737213135, g_loss: 7.519581317901611\n","Epoch 338/4000, Step 43, d_loss: 0.3368690013885498, g_loss: 4.457745552062988\n","Epoch 338/4000, Step 44, d_loss: 0.3541378378868103, g_loss: 8.173761367797852\n","Epoch 338/4000, Step 45, d_loss: 0.3418327271938324, g_loss: 5.658254146575928\n","Epoch 338/4000, Step 46, d_loss: 0.352774053812027, g_loss: 6.6020612716674805\n","Epoch 338/4000, Step 47, d_loss: 0.33980226516723633, g_loss: 4.552716255187988\n","Epoch 338/4000, Step 48, d_loss: 0.34324559569358826, g_loss: 5.160991191864014\n","Epoch 338/4000, Step 49, d_loss: 0.3372674286365509, g_loss: 5.407984256744385\n","Epoch 338/4000, Step 50, d_loss: 0.3318575620651245, g_loss: 9.39554500579834\n","Epoch 338/4000, Step 51, d_loss: 0.34067797660827637, g_loss: 5.629549980163574\n","Epoch 338/4000, Step 52, d_loss: 0.33999043703079224, g_loss: 7.228967666625977\n","Epoch 338/4000, Step 53, d_loss: 0.3529391586780548, g_loss: 5.57634162902832\n","Epoch 338/4000, Step 54, d_loss: 0.33804360032081604, g_loss: 5.444016933441162\n","Epoch 338/4000, Step 55, d_loss: 0.3487359285354614, g_loss: 5.1816935539245605\n","Epoch 338/4000, Step 56, d_loss: 0.34693706035614014, g_loss: 8.034852981567383\n","Epoch 338/4000, Step 57, d_loss: 0.3465517461299896, g_loss: 4.64828634262085\n","Epoch 338/4000, Step 58, d_loss: 0.3399774432182312, g_loss: 5.978043079376221\n","Epoch 338/4000, Step 59, d_loss: 0.3552413880825043, g_loss: 7.090724945068359\n","Epoch 338/4000, Step 60, d_loss: 0.38784533739089966, g_loss: 5.672199726104736\n","Epoch 338/4000, Step 61, d_loss: 0.3634912967681885, g_loss: 4.507924556732178\n","Epoch 338/4000, Step 62, d_loss: 0.3474231958389282, g_loss: 3.737288475036621\n","Epoch 338/4000, Step 63, d_loss: 0.3508763313293457, g_loss: 3.9903724193573\n","Epoch 338/4000, Step 64, d_loss: 0.3664570152759552, g_loss: 3.5540082454681396\n","Epoch 338/4000, Step 65, d_loss: 0.38107436895370483, g_loss: 5.7836456298828125\n","Epoch 338/4000, Step 66, d_loss: 0.34505847096443176, g_loss: 4.796786785125732\n","Epoch 338/4000, Step 67, d_loss: 0.36659255623817444, g_loss: 4.542039394378662\n","Epoch 338/4000, Step 68, d_loss: 0.33579790592193604, g_loss: 5.00152063369751\n","Epoch 338/4000, Step 69, d_loss: 0.33465155959129333, g_loss: 4.795865058898926\n","Epoch 338/4000, Step 70, d_loss: 0.3351874351501465, g_loss: 8.469402313232422\n","Epoch 338/4000, Step 71, d_loss: 0.34653711318969727, g_loss: 7.166860580444336\n","Epoch 338/4000, Step 72, d_loss: 0.33986204862594604, g_loss: 4.725191116333008\n","Epoch 338/4000, Step 73, d_loss: 0.3374690115451813, g_loss: 4.788034439086914\n","Epoch 338/4000, Step 74, d_loss: 0.3351353704929352, g_loss: 6.702054977416992\n","Epoch 338/4000, Step 75, d_loss: 0.33039289712905884, g_loss: 7.416634559631348\n","Epoch 338/4000, Step 76, d_loss: 0.34285348653793335, g_loss: 6.049599647521973\n","Epoch 338/4000, Step 77, d_loss: 0.3338504731655121, g_loss: 5.536749362945557\n","Epoch 338/4000, Step 78, d_loss: 0.3528207838535309, g_loss: 4.933758735656738\n","Epoch 338/4000, Step 79, d_loss: 0.7484005689620972, g_loss: 3.278322219848633\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 339/4000, Step 1, d_loss: 0.47107386589050293, g_loss: 1.9731837511062622\n","Epoch 339/4000, Step 2, d_loss: 0.6612246632575989, g_loss: 1.4607800245285034\n","Epoch 339/4000, Step 3, d_loss: 0.4484815001487732, g_loss: 1.6504790782928467\n","Epoch 339/4000, Step 4, d_loss: 0.4724791646003723, g_loss: 3.72255802154541\n","Epoch 339/4000, Step 5, d_loss: 0.3922807574272156, g_loss: 2.8882622718811035\n","Epoch 339/4000, Step 6, d_loss: 0.367875337600708, g_loss: 7.084723472595215\n","Epoch 339/4000, Step 7, d_loss: 0.48394885659217834, g_loss: 7.215664386749268\n","Epoch 339/4000, Step 8, d_loss: 0.4006362557411194, g_loss: 7.107235908508301\n","Epoch 339/4000, Step 9, d_loss: 0.35564684867858887, g_loss: 7.199877738952637\n","Epoch 339/4000, Step 10, d_loss: 0.34674903750419617, g_loss: 8.677797317504883\n","Epoch 339/4000, Step 11, d_loss: 0.3622458577156067, g_loss: 5.228543758392334\n","Epoch 339/4000, Step 12, d_loss: 0.36469778418540955, g_loss: 5.289205551147461\n","Epoch 339/4000, Step 13, d_loss: 0.47945353388786316, g_loss: 5.397695064544678\n","Epoch 339/4000, Step 14, d_loss: 0.39878755807876587, g_loss: 5.600942134857178\n","Epoch 339/4000, Step 15, d_loss: 0.43420666456222534, g_loss: 4.363531589508057\n","Epoch 339/4000, Step 16, d_loss: 0.3589726388454437, g_loss: 6.469270706176758\n","Epoch 339/4000, Step 17, d_loss: 0.3497719466686249, g_loss: 8.805057525634766\n","Epoch 339/4000, Step 18, d_loss: 0.3599949777126312, g_loss: 5.937656879425049\n","Epoch 339/4000, Step 19, d_loss: 0.34171706438064575, g_loss: 5.241888523101807\n","Epoch 339/4000, Step 20, d_loss: 0.350807249546051, g_loss: 4.806331634521484\n","Epoch 339/4000, Step 21, d_loss: 0.34815895557403564, g_loss: 7.531187534332275\n","Epoch 339/4000, Step 22, d_loss: 0.3593876361846924, g_loss: 6.28446102142334\n","Epoch 339/4000, Step 23, d_loss: 0.3658284544944763, g_loss: 6.6513190269470215\n","Epoch 339/4000, Step 24, d_loss: 0.3651081919670105, g_loss: 5.280139923095703\n","Epoch 339/4000, Step 25, d_loss: 0.348613440990448, g_loss: 7.049624443054199\n","Epoch 339/4000, Step 26, d_loss: 0.3403695523738861, g_loss: 7.665823459625244\n","Epoch 339/4000, Step 27, d_loss: 0.3684038519859314, g_loss: 5.109591960906982\n","Epoch 339/4000, Step 28, d_loss: 0.3399544954299927, g_loss: 7.194914817810059\n","Epoch 339/4000, Step 29, d_loss: 0.34032660722732544, g_loss: 5.417166709899902\n","Epoch 339/4000, Step 30, d_loss: 0.352390319108963, g_loss: 4.757572650909424\n","Epoch 339/4000, Step 31, d_loss: 0.34536463022232056, g_loss: 4.448712348937988\n","Epoch 339/4000, Step 32, d_loss: 0.3376883566379547, g_loss: 7.14706563949585\n","Epoch 339/4000, Step 33, d_loss: 0.33456650376319885, g_loss: 5.722999095916748\n","Epoch 339/4000, Step 34, d_loss: 0.3344671428203583, g_loss: 6.438083648681641\n","Epoch 339/4000, Step 35, d_loss: 0.33299171924591064, g_loss: 6.075705528259277\n","Epoch 339/4000, Step 36, d_loss: 0.34943291544914246, g_loss: 5.828327178955078\n","Epoch 339/4000, Step 37, d_loss: 0.33038222789764404, g_loss: 5.258907794952393\n","Epoch 339/4000, Step 38, d_loss: 0.346028596162796, g_loss: 4.282986640930176\n","Epoch 339/4000, Step 39, d_loss: 0.33689990639686584, g_loss: 4.35568904876709\n","Epoch 339/4000, Step 40, d_loss: 0.3397798538208008, g_loss: 4.8746113777160645\n","Epoch 339/4000, Step 41, d_loss: 0.3451867997646332, g_loss: 5.075782775878906\n","Epoch 339/4000, Step 42, d_loss: 0.3347025215625763, g_loss: 4.72928524017334\n","Epoch 339/4000, Step 43, d_loss: 0.3337627053260803, g_loss: 5.2663254737854\n","Epoch 339/4000, Step 44, d_loss: 0.3412889838218689, g_loss: 6.3774566650390625\n","Epoch 339/4000, Step 45, d_loss: 0.34008583426475525, g_loss: 4.204277992248535\n","Epoch 339/4000, Step 46, d_loss: 0.32950296998023987, g_loss: 5.642114639282227\n","Epoch 339/4000, Step 47, d_loss: 0.35650959610939026, g_loss: 4.0037384033203125\n","Epoch 339/4000, Step 48, d_loss: 0.367007315158844, g_loss: 6.021481037139893\n","Epoch 339/4000, Step 49, d_loss: 0.35286980867385864, g_loss: 4.8247151374816895\n","Epoch 339/4000, Step 50, d_loss: 0.3493831157684326, g_loss: 6.016497611999512\n","Epoch 339/4000, Step 51, d_loss: 0.33614420890808105, g_loss: 5.164884090423584\n","Epoch 339/4000, Step 52, d_loss: 0.333082377910614, g_loss: 5.813931941986084\n","Epoch 339/4000, Step 53, d_loss: 0.34391576051712036, g_loss: 5.705416202545166\n","Epoch 339/4000, Step 54, d_loss: 0.34270551800727844, g_loss: 8.520374298095703\n","Epoch 339/4000, Step 55, d_loss: 0.34024694561958313, g_loss: 5.446177005767822\n","Epoch 339/4000, Step 56, d_loss: 0.3493499457836151, g_loss: 8.195968627929688\n","Epoch 339/4000, Step 57, d_loss: 0.34543147683143616, g_loss: 4.448433876037598\n","Epoch 339/4000, Step 58, d_loss: 0.3510321378707886, g_loss: 5.923095226287842\n","Epoch 339/4000, Step 59, d_loss: 0.33340221643447876, g_loss: 4.856575012207031\n","Epoch 339/4000, Step 60, d_loss: 0.35361671447753906, g_loss: 8.385738372802734\n","Epoch 339/4000, Step 61, d_loss: 0.331219345331192, g_loss: 8.402859687805176\n","Epoch 339/4000, Step 62, d_loss: 0.3453308343887329, g_loss: 7.198385715484619\n","Epoch 339/4000, Step 63, d_loss: 0.3370370864868164, g_loss: 7.909111976623535\n","Epoch 339/4000, Step 64, d_loss: 0.33646243810653687, g_loss: 5.377057075500488\n","Epoch 339/4000, Step 65, d_loss: 0.3397676646709442, g_loss: 5.736980438232422\n","Epoch 339/4000, Step 66, d_loss: 0.33192533254623413, g_loss: 9.380212783813477\n","Epoch 339/4000, Step 67, d_loss: 0.33743906021118164, g_loss: 5.559932708740234\n","Epoch 339/4000, Step 68, d_loss: 0.3306852877140045, g_loss: 5.5785136222839355\n","Epoch 339/4000, Step 69, d_loss: 0.3388964533805847, g_loss: 5.873023509979248\n","Epoch 339/4000, Step 70, d_loss: 0.3351936638355255, g_loss: 8.577164649963379\n","Epoch 339/4000, Step 71, d_loss: 0.34174779057502747, g_loss: 8.873848915100098\n","Epoch 339/4000, Step 72, d_loss: 0.3370336592197418, g_loss: 8.114025115966797\n","Epoch 339/4000, Step 73, d_loss: 0.33319664001464844, g_loss: 9.124281883239746\n","Epoch 339/4000, Step 74, d_loss: 0.34066399931907654, g_loss: 8.212068557739258\n","Epoch 339/4000, Step 75, d_loss: 0.3423534333705902, g_loss: 6.330724239349365\n","Epoch 339/4000, Step 76, d_loss: 0.3284091055393219, g_loss: 7.126460552215576\n","Epoch 339/4000, Step 77, d_loss: 0.3317587673664093, g_loss: 6.906499862670898\n","Epoch 339/4000, Step 78, d_loss: 0.33379313349723816, g_loss: 5.479114055633545\n","Epoch 339/4000, Step 79, d_loss: 0.46262046694755554, g_loss: 4.882508754730225\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 340/4000, Step 1, d_loss: 0.3493143916130066, g_loss: 3.9033830165863037\n","Epoch 340/4000, Step 2, d_loss: 0.37496232986450195, g_loss: 5.192104339599609\n","Epoch 340/4000, Step 3, d_loss: 0.3847579061985016, g_loss: 4.574404716491699\n","Epoch 340/4000, Step 4, d_loss: 0.40120944380760193, g_loss: 5.130043029785156\n","Epoch 340/4000, Step 5, d_loss: 0.38215187191963196, g_loss: 5.513439655303955\n","Epoch 340/4000, Step 6, d_loss: 0.3595210313796997, g_loss: 7.838438034057617\n","Epoch 340/4000, Step 7, d_loss: 0.33470943570137024, g_loss: 7.198032855987549\n","Epoch 340/4000, Step 8, d_loss: 0.37302637100219727, g_loss: 11.452495574951172\n","Epoch 340/4000, Step 9, d_loss: 0.40433818101882935, g_loss: 6.7626423835754395\n","Epoch 340/4000, Step 10, d_loss: 0.3597748875617981, g_loss: 9.984137535095215\n","Epoch 340/4000, Step 11, d_loss: 0.33747923374176025, g_loss: 5.364805221557617\n","Epoch 340/4000, Step 12, d_loss: 0.34678417444229126, g_loss: 4.296686172485352\n","Epoch 340/4000, Step 13, d_loss: 0.3614833652973175, g_loss: 4.929197788238525\n","Epoch 340/4000, Step 14, d_loss: 0.35895296931266785, g_loss: 4.657577991485596\n","Epoch 340/4000, Step 15, d_loss: 0.3961099088191986, g_loss: 5.280710697174072\n","Epoch 340/4000, Step 16, d_loss: 0.3560510277748108, g_loss: 4.5506510734558105\n","Epoch 340/4000, Step 17, d_loss: 0.34427839517593384, g_loss: 4.791231155395508\n","Epoch 340/4000, Step 18, d_loss: 0.3360155522823334, g_loss: 6.928916931152344\n","Epoch 340/4000, Step 19, d_loss: 0.3375912308692932, g_loss: 11.792961120605469\n","Epoch 340/4000, Step 20, d_loss: 0.34981176257133484, g_loss: 6.8123908042907715\n","Epoch 340/4000, Step 21, d_loss: 0.37800711393356323, g_loss: 6.921419620513916\n","Epoch 340/4000, Step 22, d_loss: 0.3363800644874573, g_loss: 5.835794448852539\n","Epoch 340/4000, Step 23, d_loss: 0.34182241559028625, g_loss: 6.106330394744873\n","Epoch 340/4000, Step 24, d_loss: 0.34313279390335083, g_loss: 5.661065101623535\n","Epoch 340/4000, Step 25, d_loss: 0.3538815975189209, g_loss: 5.488114356994629\n","Epoch 340/4000, Step 26, d_loss: 0.3515455424785614, g_loss: 5.7868123054504395\n","Epoch 340/4000, Step 27, d_loss: 0.343700110912323, g_loss: 7.561037063598633\n","Epoch 340/4000, Step 28, d_loss: 0.3477381765842438, g_loss: 5.686285018920898\n","Epoch 340/4000, Step 29, d_loss: 0.331452339887619, g_loss: 7.710081577301025\n","Epoch 340/4000, Step 30, d_loss: 0.3339596688747406, g_loss: 6.727658748626709\n","Epoch 340/4000, Step 31, d_loss: 0.3385758697986603, g_loss: 9.050027847290039\n","Epoch 340/4000, Step 32, d_loss: 0.3429434895515442, g_loss: 6.698216915130615\n","Epoch 340/4000, Step 33, d_loss: 0.3316982090473175, g_loss: 8.872230529785156\n","Epoch 340/4000, Step 34, d_loss: 0.3510184586048126, g_loss: 8.109987258911133\n","Epoch 340/4000, Step 35, d_loss: 0.3350171148777008, g_loss: 7.0254340171813965\n","Epoch 340/4000, Step 36, d_loss: 0.3390919268131256, g_loss: 4.964160919189453\n","Epoch 340/4000, Step 37, d_loss: 0.34266969561576843, g_loss: 5.205837726593018\n","Epoch 340/4000, Step 38, d_loss: 0.34540247917175293, g_loss: 5.001530647277832\n","Epoch 340/4000, Step 39, d_loss: 0.3468315005302429, g_loss: 8.14224910736084\n","Epoch 340/4000, Step 40, d_loss: 0.331699937582016, g_loss: 5.534978866577148\n","Epoch 340/4000, Step 41, d_loss: 0.33952707052230835, g_loss: 6.87916898727417\n","Epoch 340/4000, Step 42, d_loss: 0.33700594305992126, g_loss: 5.573552131652832\n","Epoch 340/4000, Step 43, d_loss: 0.3397600054740906, g_loss: 4.94378137588501\n","Epoch 340/4000, Step 44, d_loss: 0.3470191955566406, g_loss: 6.986085414886475\n","Epoch 340/4000, Step 45, d_loss: 0.3329143226146698, g_loss: 5.034770965576172\n","Epoch 340/4000, Step 46, d_loss: 0.33687493205070496, g_loss: 4.953123569488525\n","Epoch 340/4000, Step 47, d_loss: 0.33945438265800476, g_loss: 4.803045272827148\n","Epoch 340/4000, Step 48, d_loss: 0.33332493901252747, g_loss: 5.477782249450684\n","Epoch 340/4000, Step 49, d_loss: 0.35329845547676086, g_loss: 7.652970790863037\n","Epoch 340/4000, Step 50, d_loss: 0.3425968289375305, g_loss: 5.013215065002441\n","Epoch 340/4000, Step 51, d_loss: 0.34736451506614685, g_loss: 7.529164791107178\n","Epoch 340/4000, Step 52, d_loss: 0.34033435583114624, g_loss: 6.097433090209961\n","Epoch 340/4000, Step 53, d_loss: 0.33000287413597107, g_loss: 5.698551177978516\n","Epoch 340/4000, Step 54, d_loss: 0.3597620725631714, g_loss: 4.796637535095215\n","Epoch 340/4000, Step 55, d_loss: 0.33077266812324524, g_loss: 5.8806047439575195\n","Epoch 340/4000, Step 56, d_loss: 0.33666592836380005, g_loss: 7.78098726272583\n","Epoch 340/4000, Step 57, d_loss: 0.3550288677215576, g_loss: 6.730759620666504\n","Epoch 340/4000, Step 58, d_loss: 0.339398592710495, g_loss: 9.843236923217773\n","Epoch 340/4000, Step 59, d_loss: 0.3324475586414337, g_loss: 8.37006664276123\n","Epoch 340/4000, Step 60, d_loss: 0.3395530879497528, g_loss: 6.0877532958984375\n","Epoch 340/4000, Step 61, d_loss: 0.33693426847457886, g_loss: 6.3987040519714355\n","Epoch 340/4000, Step 62, d_loss: 0.34112799167633057, g_loss: 6.585799217224121\n","Epoch 340/4000, Step 63, d_loss: 0.33596572279930115, g_loss: 7.085488319396973\n","Epoch 340/4000, Step 64, d_loss: 0.33653193712234497, g_loss: 9.79532241821289\n","Epoch 340/4000, Step 65, d_loss: 0.33113014698028564, g_loss: 8.095122337341309\n","Epoch 340/4000, Step 66, d_loss: 0.3382328152656555, g_loss: 6.9119553565979\n","Epoch 340/4000, Step 67, d_loss: 0.33474409580230713, g_loss: 6.371626377105713\n","Epoch 340/4000, Step 68, d_loss: 0.34362873435020447, g_loss: 8.773592948913574\n","Epoch 340/4000, Step 69, d_loss: 0.33429548144340515, g_loss: 7.210352897644043\n","Epoch 340/4000, Step 70, d_loss: 0.33205485343933105, g_loss: 7.353533744812012\n","Epoch 340/4000, Step 71, d_loss: 0.3435419797897339, g_loss: 9.559833526611328\n","Epoch 340/4000, Step 72, d_loss: 0.33681175112724304, g_loss: 6.534434795379639\n","Epoch 340/4000, Step 73, d_loss: 0.3357357978820801, g_loss: 5.702909469604492\n","Epoch 340/4000, Step 74, d_loss: 0.34642213582992554, g_loss: 4.906595706939697\n","Epoch 340/4000, Step 75, d_loss: 0.3341419994831085, g_loss: 5.713022708892822\n","Epoch 340/4000, Step 76, d_loss: 0.3366606533527374, g_loss: 8.98515510559082\n","Epoch 340/4000, Step 77, d_loss: 0.3359340727329254, g_loss: 5.6901960372924805\n","Epoch 340/4000, Step 78, d_loss: 0.3353275656700134, g_loss: 5.6172590255737305\n","Epoch 340/4000, Step 79, d_loss: 0.5035732984542847, g_loss: 5.6097540855407715\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 341/4000, Step 1, d_loss: 0.35794126987457275, g_loss: 9.64982795715332\n","Epoch 341/4000, Step 2, d_loss: 0.3867899477481842, g_loss: 6.815509796142578\n","Epoch 341/4000, Step 3, d_loss: 0.3899148106575012, g_loss: 4.495343208312988\n","Epoch 341/4000, Step 4, d_loss: 0.38004499673843384, g_loss: 6.460944175720215\n","Epoch 341/4000, Step 5, d_loss: 0.3425883650779724, g_loss: 4.128177642822266\n","Epoch 341/4000, Step 6, d_loss: 0.4076196551322937, g_loss: 4.468540668487549\n","Epoch 341/4000, Step 7, d_loss: 0.355101078748703, g_loss: 4.486913681030273\n","Epoch 341/4000, Step 8, d_loss: 0.3571937382221222, g_loss: 8.100924491882324\n","Epoch 341/4000, Step 9, d_loss: 0.39130035042762756, g_loss: 3.849858045578003\n","Epoch 341/4000, Step 10, d_loss: 0.3718482553958893, g_loss: 5.420494079589844\n","Epoch 341/4000, Step 11, d_loss: 0.366804301738739, g_loss: 6.109921932220459\n","Epoch 341/4000, Step 12, d_loss: 0.364156574010849, g_loss: 3.5274925231933594\n","Epoch 341/4000, Step 13, d_loss: 0.3854638338088989, g_loss: 4.246332168579102\n","Epoch 341/4000, Step 14, d_loss: 0.3920060396194458, g_loss: 6.531714916229248\n","Epoch 341/4000, Step 15, d_loss: 0.37566718459129333, g_loss: 4.333120346069336\n","Epoch 341/4000, Step 16, d_loss: 0.3774287700653076, g_loss: 4.26442289352417\n","Epoch 341/4000, Step 17, d_loss: 0.3709455728530884, g_loss: 7.310122489929199\n","Epoch 341/4000, Step 18, d_loss: 0.35366615653038025, g_loss: 7.478381156921387\n","Epoch 341/4000, Step 19, d_loss: 0.454510897397995, g_loss: 5.478282451629639\n","Epoch 341/4000, Step 20, d_loss: 0.34783992171287537, g_loss: 5.5114054679870605\n","Epoch 341/4000, Step 21, d_loss: 0.358590304851532, g_loss: 5.068345546722412\n","Epoch 341/4000, Step 22, d_loss: 0.33719688653945923, g_loss: 4.753089427947998\n","Epoch 341/4000, Step 23, d_loss: 0.34936216473579407, g_loss: 7.4095659255981445\n","Epoch 341/4000, Step 24, d_loss: 0.33939284086227417, g_loss: 7.716726779937744\n","Epoch 341/4000, Step 25, d_loss: 0.33928564190864563, g_loss: 11.16075611114502\n","Epoch 341/4000, Step 26, d_loss: 0.34906646609306335, g_loss: 6.992502212524414\n","Epoch 341/4000, Step 27, d_loss: 0.33637362718582153, g_loss: 7.2005767822265625\n","Epoch 341/4000, Step 28, d_loss: 0.3442245423793793, g_loss: 7.122901439666748\n","Epoch 341/4000, Step 29, d_loss: 0.3360772728919983, g_loss: 5.793211460113525\n","Epoch 341/4000, Step 30, d_loss: 0.33380937576293945, g_loss: 8.27597713470459\n","Epoch 341/4000, Step 31, d_loss: 0.33789345622062683, g_loss: 7.470778465270996\n","Epoch 341/4000, Step 32, d_loss: 0.34952959418296814, g_loss: 6.7669901847839355\n","Epoch 341/4000, Step 33, d_loss: 0.33576563000679016, g_loss: 10.087685585021973\n","Epoch 341/4000, Step 34, d_loss: 0.33677953481674194, g_loss: 6.724315643310547\n","Epoch 341/4000, Step 35, d_loss: 0.3465142846107483, g_loss: 6.194870471954346\n","Epoch 341/4000, Step 36, d_loss: 0.3424464166164398, g_loss: 5.751518726348877\n","Epoch 341/4000, Step 37, d_loss: 0.3628333806991577, g_loss: 6.197721481323242\n","Epoch 341/4000, Step 38, d_loss: 0.3389227092266083, g_loss: 5.95780611038208\n","Epoch 341/4000, Step 39, d_loss: 0.3338601291179657, g_loss: 5.0806965827941895\n","Epoch 341/4000, Step 40, d_loss: 0.3367455005645752, g_loss: 5.160447597503662\n","Epoch 341/4000, Step 41, d_loss: 0.34564104676246643, g_loss: 4.822761535644531\n","Epoch 341/4000, Step 42, d_loss: 0.34825223684310913, g_loss: 4.296759128570557\n","Epoch 341/4000, Step 43, d_loss: 0.3454069197177887, g_loss: 3.4964306354522705\n","Epoch 341/4000, Step 44, d_loss: 0.3869659900665283, g_loss: 5.021605014801025\n","Epoch 341/4000, Step 45, d_loss: 0.3557092547416687, g_loss: 5.274908542633057\n","Epoch 341/4000, Step 46, d_loss: 0.32958632707595825, g_loss: 7.966114044189453\n","Epoch 341/4000, Step 47, d_loss: 0.3288702070713043, g_loss: 7.087183952331543\n","Epoch 341/4000, Step 48, d_loss: 0.32969892024993896, g_loss: 7.632488250732422\n","Epoch 341/4000, Step 49, d_loss: 0.3306442201137543, g_loss: 7.117700099945068\n","Epoch 341/4000, Step 50, d_loss: 0.339698851108551, g_loss: 5.3007588386535645\n","Epoch 341/4000, Step 51, d_loss: 0.331552654504776, g_loss: 6.71291446685791\n","Epoch 341/4000, Step 52, d_loss: 0.3358843922615051, g_loss: 6.983575820922852\n","Epoch 341/4000, Step 53, d_loss: 0.34097179770469666, g_loss: 4.600183963775635\n","Epoch 341/4000, Step 54, d_loss: 0.33205753564834595, g_loss: 6.166843414306641\n","Epoch 341/4000, Step 55, d_loss: 0.3360300362110138, g_loss: 8.35616397857666\n","Epoch 341/4000, Step 56, d_loss: 0.3347689211368561, g_loss: 6.227133274078369\n","Epoch 341/4000, Step 57, d_loss: 0.3362788259983063, g_loss: 5.976293563842773\n","Epoch 341/4000, Step 58, d_loss: 0.3312443494796753, g_loss: 8.220842361450195\n","Epoch 341/4000, Step 59, d_loss: 0.34149080514907837, g_loss: 9.293540954589844\n","Epoch 341/4000, Step 60, d_loss: 0.3360069692134857, g_loss: 5.979842662811279\n","Epoch 341/4000, Step 61, d_loss: 0.33695945143699646, g_loss: 4.885359287261963\n","Epoch 341/4000, Step 62, d_loss: 0.3332057595252991, g_loss: 7.337015628814697\n","Epoch 341/4000, Step 63, d_loss: 0.3355492651462555, g_loss: 4.541653633117676\n","Epoch 341/4000, Step 64, d_loss: 0.34053656458854675, g_loss: 5.271396160125732\n","Epoch 341/4000, Step 65, d_loss: 0.3336154520511627, g_loss: 6.679924964904785\n","Epoch 341/4000, Step 66, d_loss: 0.33701983094215393, g_loss: 5.28867244720459\n","Epoch 341/4000, Step 67, d_loss: 0.3450864851474762, g_loss: 4.4829421043396\n","Epoch 341/4000, Step 68, d_loss: 0.33861446380615234, g_loss: 7.772037029266357\n","Epoch 341/4000, Step 69, d_loss: 0.351677805185318, g_loss: 6.429274082183838\n","Epoch 341/4000, Step 70, d_loss: 0.3308016061782837, g_loss: 10.006153106689453\n","Epoch 341/4000, Step 71, d_loss: 0.32967308163642883, g_loss: 11.640707015991211\n","Epoch 341/4000, Step 72, d_loss: 0.3451671898365021, g_loss: 7.5974507331848145\n","Epoch 341/4000, Step 73, d_loss: 0.3370305001735687, g_loss: 8.205192565917969\n","Epoch 341/4000, Step 74, d_loss: 0.37756574153900146, g_loss: 3.3013241291046143\n","Epoch 341/4000, Step 75, d_loss: 0.3463934063911438, g_loss: 9.009364128112793\n","Epoch 341/4000, Step 76, d_loss: 0.3309279680252075, g_loss: 3.907280445098877\n","Epoch 341/4000, Step 77, d_loss: 0.3370715379714966, g_loss: 6.1999006271362305\n","Epoch 341/4000, Step 78, d_loss: 0.36800387501716614, g_loss: 5.5745134353637695\n","Epoch 341/4000, Step 79, d_loss: 0.5898180603981018, g_loss: 4.764711380004883\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 342/4000, Step 1, d_loss: 0.36577510833740234, g_loss: 4.959174156188965\n","Epoch 342/4000, Step 2, d_loss: 0.4905908405780792, g_loss: 4.485859394073486\n","Epoch 342/4000, Step 3, d_loss: 0.420322984457016, g_loss: 4.455299377441406\n","Epoch 342/4000, Step 4, d_loss: 0.4003966152667999, g_loss: 4.553385257720947\n","Epoch 342/4000, Step 5, d_loss: 0.4156109094619751, g_loss: 5.559690475463867\n","Epoch 342/4000, Step 6, d_loss: 0.39624059200286865, g_loss: 5.299060344696045\n","Epoch 342/4000, Step 7, d_loss: 0.3549700677394867, g_loss: 5.483837604522705\n","Epoch 342/4000, Step 8, d_loss: 0.38231849670410156, g_loss: 5.803402423858643\n","Epoch 342/4000, Step 9, d_loss: 0.38826972246170044, g_loss: 5.272374153137207\n","Epoch 342/4000, Step 10, d_loss: 0.342956006526947, g_loss: 6.729999542236328\n","Epoch 342/4000, Step 11, d_loss: 0.3592388331890106, g_loss: 5.413190841674805\n","Epoch 342/4000, Step 12, d_loss: 0.35371139645576477, g_loss: 5.3177618980407715\n","Epoch 342/4000, Step 13, d_loss: 0.3648332953453064, g_loss: 9.23291015625\n","Epoch 342/4000, Step 14, d_loss: 0.39510488510131836, g_loss: 5.359838962554932\n","Epoch 342/4000, Step 15, d_loss: 0.356363981962204, g_loss: 4.885908126831055\n","Epoch 342/4000, Step 16, d_loss: 0.35377392172813416, g_loss: 5.230738162994385\n","Epoch 342/4000, Step 17, d_loss: 0.357524037361145, g_loss: 5.081486225128174\n","Epoch 342/4000, Step 18, d_loss: 0.34544244408607483, g_loss: 5.427351474761963\n","Epoch 342/4000, Step 19, d_loss: 0.34912705421447754, g_loss: 5.023802280426025\n","Epoch 342/4000, Step 20, d_loss: 0.35286587476730347, g_loss: 5.408926486968994\n","Epoch 342/4000, Step 21, d_loss: 0.36452168226242065, g_loss: 5.199876308441162\n","Epoch 342/4000, Step 22, d_loss: 0.33725425601005554, g_loss: 5.342124938964844\n","Epoch 342/4000, Step 23, d_loss: 0.33820879459381104, g_loss: 5.37263822555542\n","Epoch 342/4000, Step 24, d_loss: 0.3607615530490875, g_loss: 4.699352264404297\n","Epoch 342/4000, Step 25, d_loss: 0.362545371055603, g_loss: 4.892052173614502\n","Epoch 342/4000, Step 26, d_loss: 0.3564656376838684, g_loss: 4.702798843383789\n","Epoch 342/4000, Step 27, d_loss: 0.355794757604599, g_loss: 5.702329158782959\n","Epoch 342/4000, Step 28, d_loss: 0.3381699323654175, g_loss: 4.725015640258789\n","Epoch 342/4000, Step 29, d_loss: 0.344349205493927, g_loss: 4.89113187789917\n","Epoch 342/4000, Step 30, d_loss: 0.34216225147247314, g_loss: 4.950479030609131\n","Epoch 342/4000, Step 31, d_loss: 0.3525237739086151, g_loss: 4.768792152404785\n","Epoch 342/4000, Step 32, d_loss: 0.3480192720890045, g_loss: 4.963544845581055\n","Epoch 342/4000, Step 33, d_loss: 0.35879722237586975, g_loss: 4.744853973388672\n","Epoch 342/4000, Step 34, d_loss: 0.34399983286857605, g_loss: 4.738798141479492\n","Epoch 342/4000, Step 35, d_loss: 0.3401540517807007, g_loss: 4.747750282287598\n","Epoch 342/4000, Step 36, d_loss: 0.3517386317253113, g_loss: 5.949183940887451\n","Epoch 342/4000, Step 37, d_loss: 0.3484298288822174, g_loss: 5.134666442871094\n","Epoch 342/4000, Step 38, d_loss: 0.34369730949401855, g_loss: 5.192656517028809\n","Epoch 342/4000, Step 39, d_loss: 0.3502131402492523, g_loss: 5.215743064880371\n","Epoch 342/4000, Step 40, d_loss: 0.34006279706954956, g_loss: 5.473232269287109\n","Epoch 342/4000, Step 41, d_loss: 0.34020689129829407, g_loss: 5.903285980224609\n","Epoch 342/4000, Step 42, d_loss: 0.3440445065498352, g_loss: 5.170139312744141\n","Epoch 342/4000, Step 43, d_loss: 0.3389560878276825, g_loss: 5.678950786590576\n","Epoch 342/4000, Step 44, d_loss: 0.34520840644836426, g_loss: 5.290157318115234\n","Epoch 342/4000, Step 45, d_loss: 0.33778825402259827, g_loss: 5.338247299194336\n","Epoch 342/4000, Step 46, d_loss: 0.3379109501838684, g_loss: 5.443724155426025\n","Epoch 342/4000, Step 47, d_loss: 0.3339211344718933, g_loss: 5.022303581237793\n","Epoch 342/4000, Step 48, d_loss: 0.3376981317996979, g_loss: 5.040477275848389\n","Epoch 342/4000, Step 49, d_loss: 0.3348609507083893, g_loss: 5.242861747741699\n","Epoch 342/4000, Step 50, d_loss: 0.3408021330833435, g_loss: 5.393815517425537\n","Epoch 342/4000, Step 51, d_loss: 0.3320082724094391, g_loss: 5.201094627380371\n","Epoch 342/4000, Step 52, d_loss: 0.3353540003299713, g_loss: 6.045446395874023\n","Epoch 342/4000, Step 53, d_loss: 0.3352489769458771, g_loss: 7.564496040344238\n","Epoch 342/4000, Step 54, d_loss: 0.3347315788269043, g_loss: 6.799470901489258\n","Epoch 342/4000, Step 55, d_loss: 0.34933972358703613, g_loss: 6.695372581481934\n","Epoch 342/4000, Step 56, d_loss: 0.32937511801719666, g_loss: 6.279892444610596\n","Epoch 342/4000, Step 57, d_loss: 0.33240175247192383, g_loss: 8.058609962463379\n","Epoch 342/4000, Step 58, d_loss: 0.3344801068305969, g_loss: 8.205517768859863\n","Epoch 342/4000, Step 59, d_loss: 0.3323315978050232, g_loss: 6.561312675476074\n","Epoch 342/4000, Step 60, d_loss: 0.33420079946517944, g_loss: 7.944804668426514\n","Epoch 342/4000, Step 61, d_loss: 0.3377842605113983, g_loss: 7.218555450439453\n","Epoch 342/4000, Step 62, d_loss: 0.33327290415763855, g_loss: 6.989631652832031\n","Epoch 342/4000, Step 63, d_loss: 0.3351611793041229, g_loss: 8.52816390991211\n","Epoch 342/4000, Step 64, d_loss: 0.3334721326828003, g_loss: 8.782439231872559\n","Epoch 342/4000, Step 65, d_loss: 0.3314855992794037, g_loss: 6.581962585449219\n","Epoch 342/4000, Step 66, d_loss: 0.3292645514011383, g_loss: 6.369149208068848\n","Epoch 342/4000, Step 67, d_loss: 0.3285016119480133, g_loss: 6.525134086608887\n","Epoch 342/4000, Step 68, d_loss: 0.3303113579750061, g_loss: 6.49692440032959\n","Epoch 342/4000, Step 69, d_loss: 0.3292015492916107, g_loss: 6.493227958679199\n","Epoch 342/4000, Step 70, d_loss: 0.328673392534256, g_loss: 7.972483158111572\n","Epoch 342/4000, Step 71, d_loss: 0.3298017382621765, g_loss: 6.374325752258301\n","Epoch 342/4000, Step 72, d_loss: 0.337490975856781, g_loss: 6.534426689147949\n","Epoch 342/4000, Step 73, d_loss: 0.3332177698612213, g_loss: 6.129809379577637\n","Epoch 342/4000, Step 74, d_loss: 0.3320262134075165, g_loss: 6.133238792419434\n","Epoch 342/4000, Step 75, d_loss: 0.33661821484565735, g_loss: 6.230443477630615\n","Epoch 342/4000, Step 76, d_loss: 0.3379291296005249, g_loss: 5.987737655639648\n","Epoch 342/4000, Step 77, d_loss: 0.3329280912876129, g_loss: 6.253947734832764\n","Epoch 342/4000, Step 78, d_loss: 0.3309410810470581, g_loss: 6.211162090301514\n","Epoch 342/4000, Step 79, d_loss: 0.8794932961463928, g_loss: 5.5754570960998535\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 343/4000, Step 1, d_loss: 0.3609170615673065, g_loss: 4.975695610046387\n","Epoch 343/4000, Step 2, d_loss: 0.4068838357925415, g_loss: 4.707923412322998\n","Epoch 343/4000, Step 3, d_loss: 0.4706827998161316, g_loss: 4.258011817932129\n","Epoch 343/4000, Step 4, d_loss: 0.46830812096595764, g_loss: 4.435103893280029\n","Epoch 343/4000, Step 5, d_loss: 0.4491194188594818, g_loss: 4.451897144317627\n","Epoch 343/4000, Step 6, d_loss: 0.43727636337280273, g_loss: 4.570486545562744\n","Epoch 343/4000, Step 7, d_loss: 0.3980807960033417, g_loss: 5.267939567565918\n","Epoch 343/4000, Step 8, d_loss: 0.36066755652427673, g_loss: 5.495508193969727\n","Epoch 343/4000, Step 9, d_loss: 0.3685983121395111, g_loss: 6.458737373352051\n","Epoch 343/4000, Step 10, d_loss: 0.4216687083244324, g_loss: 5.819564342498779\n","Epoch 343/4000, Step 11, d_loss: 0.4442322850227356, g_loss: 5.957930564880371\n","Epoch 343/4000, Step 12, d_loss: 0.34490302205085754, g_loss: 5.503207683563232\n","Epoch 343/4000, Step 13, d_loss: 0.3684791922569275, g_loss: 5.493760108947754\n","Epoch 343/4000, Step 14, d_loss: 0.3966979682445526, g_loss: 5.09155797958374\n","Epoch 343/4000, Step 15, d_loss: 0.36014002561569214, g_loss: 4.791695594787598\n","Epoch 343/4000, Step 16, d_loss: 0.3737252354621887, g_loss: 4.983792304992676\n","Epoch 343/4000, Step 17, d_loss: 0.372829407453537, g_loss: 4.890414714813232\n","Epoch 343/4000, Step 18, d_loss: 0.35603493452072144, g_loss: 5.035552978515625\n","Epoch 343/4000, Step 19, d_loss: 0.35905373096466064, g_loss: 5.063735485076904\n","Epoch 343/4000, Step 20, d_loss: 0.3650112748146057, g_loss: 5.380807399749756\n","Epoch 343/4000, Step 21, d_loss: 0.3779372572898865, g_loss: 5.633063793182373\n","Epoch 343/4000, Step 22, d_loss: 0.35717591643333435, g_loss: 5.358517646789551\n","Epoch 343/4000, Step 23, d_loss: 0.34318557381629944, g_loss: 5.659518718719482\n","Epoch 343/4000, Step 24, d_loss: 0.3387560546398163, g_loss: 5.4398193359375\n","Epoch 343/4000, Step 25, d_loss: 0.3449193835258484, g_loss: 5.479877471923828\n","Epoch 343/4000, Step 26, d_loss: 0.34659814834594727, g_loss: 5.475378036499023\n","Epoch 343/4000, Step 27, d_loss: 0.3439536690711975, g_loss: 5.583998203277588\n","Epoch 343/4000, Step 28, d_loss: 0.3422797620296478, g_loss: 5.647764205932617\n","Epoch 343/4000, Step 29, d_loss: 0.33717161417007446, g_loss: 8.019153594970703\n","Epoch 343/4000, Step 30, d_loss: 0.3559827208518982, g_loss: 6.007503986358643\n","Epoch 343/4000, Step 31, d_loss: 0.3321859836578369, g_loss: 5.917022705078125\n","Epoch 343/4000, Step 32, d_loss: 0.3360428512096405, g_loss: 5.8155412673950195\n","Epoch 343/4000, Step 33, d_loss: 0.3333964943885803, g_loss: 5.5290374755859375\n","Epoch 343/4000, Step 34, d_loss: 0.34892335534095764, g_loss: 6.18278694152832\n","Epoch 343/4000, Step 35, d_loss: 0.33403709530830383, g_loss: 7.849393844604492\n","Epoch 343/4000, Step 36, d_loss: 0.3946487605571747, g_loss: 8.969061851501465\n","Epoch 343/4000, Step 37, d_loss: 0.339586466550827, g_loss: 9.589847564697266\n","Epoch 343/4000, Step 38, d_loss: 0.35544174909591675, g_loss: 8.465726852416992\n","Epoch 343/4000, Step 39, d_loss: 0.3512464761734009, g_loss: 8.974260330200195\n","Epoch 343/4000, Step 40, d_loss: 0.34621432423591614, g_loss: 6.90271520614624\n","Epoch 343/4000, Step 41, d_loss: 0.3547399938106537, g_loss: 8.164666175842285\n","Epoch 343/4000, Step 42, d_loss: 0.35318511724472046, g_loss: 8.513142585754395\n","Epoch 343/4000, Step 43, d_loss: 0.35070478916168213, g_loss: 6.854583740234375\n","Epoch 343/4000, Step 44, d_loss: 0.3390588164329529, g_loss: 9.784006118774414\n","Epoch 343/4000, Step 45, d_loss: 0.332164466381073, g_loss: 6.099764347076416\n","Epoch 343/4000, Step 46, d_loss: 0.33295735716819763, g_loss: 6.58112907409668\n","Epoch 343/4000, Step 47, d_loss: 0.3382419943809509, g_loss: 7.709596633911133\n","Epoch 343/4000, Step 48, d_loss: 0.3431493043899536, g_loss: 6.143488883972168\n","Epoch 343/4000, Step 49, d_loss: 0.349642813205719, g_loss: 6.283085346221924\n","Epoch 343/4000, Step 50, d_loss: 0.3427892327308655, g_loss: 6.607558250427246\n","Epoch 343/4000, Step 51, d_loss: 0.34125298261642456, g_loss: 5.755799293518066\n","Epoch 343/4000, Step 52, d_loss: 0.35444390773773193, g_loss: 6.010846138000488\n","Epoch 343/4000, Step 53, d_loss: 0.3455227017402649, g_loss: 5.894472599029541\n","Epoch 343/4000, Step 54, d_loss: 0.3395094573497772, g_loss: 6.029104709625244\n","Epoch 343/4000, Step 55, d_loss: 0.3412526845932007, g_loss: 6.404754638671875\n","Epoch 343/4000, Step 56, d_loss: 0.3362519145011902, g_loss: 6.229395389556885\n","Epoch 343/4000, Step 57, d_loss: 0.3324606120586395, g_loss: 6.454002380371094\n","Epoch 343/4000, Step 58, d_loss: 0.33508872985839844, g_loss: 6.286967754364014\n","Epoch 343/4000, Step 59, d_loss: 0.3335965573787689, g_loss: 6.199225902557373\n","Epoch 343/4000, Step 60, d_loss: 0.3333970308303833, g_loss: 6.116604328155518\n","Epoch 343/4000, Step 61, d_loss: 0.3437099754810333, g_loss: 6.095900535583496\n","Epoch 343/4000, Step 62, d_loss: 0.3371030390262604, g_loss: 5.803581714630127\n","Epoch 343/4000, Step 63, d_loss: 0.34387487173080444, g_loss: 5.925621509552002\n","Epoch 343/4000, Step 64, d_loss: 0.3335512578487396, g_loss: 5.971861362457275\n","Epoch 343/4000, Step 65, d_loss: 0.3284970223903656, g_loss: 5.9800825119018555\n","Epoch 343/4000, Step 66, d_loss: 0.33067548274993896, g_loss: 6.01687479019165\n","Epoch 343/4000, Step 67, d_loss: 0.3349238336086273, g_loss: 5.798411846160889\n","Epoch 343/4000, Step 68, d_loss: 0.33514803647994995, g_loss: 5.963221549987793\n","Epoch 343/4000, Step 69, d_loss: 0.34767141938209534, g_loss: 7.155520439147949\n","Epoch 343/4000, Step 70, d_loss: 0.33236438035964966, g_loss: 5.858220100402832\n","Epoch 343/4000, Step 71, d_loss: 0.3367091715335846, g_loss: 5.7962212562561035\n","Epoch 343/4000, Step 72, d_loss: 0.3310820162296295, g_loss: 5.856049060821533\n","Epoch 343/4000, Step 73, d_loss: 0.33237144351005554, g_loss: 6.005853652954102\n","Epoch 343/4000, Step 74, d_loss: 0.33705851435661316, g_loss: 8.600570678710938\n","Epoch 343/4000, Step 75, d_loss: 0.3332728445529938, g_loss: 7.113039016723633\n","Epoch 343/4000, Step 76, d_loss: 0.3331301212310791, g_loss: 7.152273654937744\n","Epoch 343/4000, Step 77, d_loss: 0.33256301283836365, g_loss: 6.204431056976318\n","Epoch 343/4000, Step 78, d_loss: 0.335776686668396, g_loss: 8.253528594970703\n","Epoch 343/4000, Step 79, d_loss: 0.3859553337097168, g_loss: 6.837442398071289\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 344/4000, Step 1, d_loss: 0.3284773528575897, g_loss: 10.026698112487793\n","Epoch 344/4000, Step 2, d_loss: 0.33227136731147766, g_loss: 7.803271293640137\n","Epoch 344/4000, Step 3, d_loss: 0.333390474319458, g_loss: 6.201241493225098\n","Epoch 344/4000, Step 4, d_loss: 0.33883413672447205, g_loss: 6.553558349609375\n","Epoch 344/4000, Step 5, d_loss: 0.3399196267127991, g_loss: 6.297077178955078\n","Epoch 344/4000, Step 6, d_loss: 0.3390354812145233, g_loss: 6.540036201477051\n","Epoch 344/4000, Step 7, d_loss: 0.34365126490592957, g_loss: 8.306234359741211\n","Epoch 344/4000, Step 8, d_loss: 0.3386669456958771, g_loss: 5.761104106903076\n","Epoch 344/4000, Step 9, d_loss: 0.33326324820518494, g_loss: 7.377773761749268\n","Epoch 344/4000, Step 10, d_loss: 0.3355770707130432, g_loss: 6.525557041168213\n","Epoch 344/4000, Step 11, d_loss: 0.3288628160953522, g_loss: 6.258503437042236\n","Epoch 344/4000, Step 12, d_loss: 0.33328258991241455, g_loss: 6.124570846557617\n","Epoch 344/4000, Step 13, d_loss: 0.3446798622608185, g_loss: 5.698577880859375\n","Epoch 344/4000, Step 14, d_loss: 0.3372749388217926, g_loss: 5.6722869873046875\n","Epoch 344/4000, Step 15, d_loss: 0.3335123658180237, g_loss: 5.432824611663818\n","Epoch 344/4000, Step 16, d_loss: 0.3352147936820984, g_loss: 5.579483509063721\n","Epoch 344/4000, Step 17, d_loss: 0.3310009837150574, g_loss: 5.394305229187012\n","Epoch 344/4000, Step 18, d_loss: 0.32891419529914856, g_loss: 7.312471866607666\n","Epoch 344/4000, Step 19, d_loss: 0.33455172181129456, g_loss: 5.7875285148620605\n","Epoch 344/4000, Step 20, d_loss: 0.3363991379737854, g_loss: 5.5108842849731445\n","Epoch 344/4000, Step 21, d_loss: 0.3360430598258972, g_loss: 5.4192585945129395\n","Epoch 344/4000, Step 22, d_loss: 0.33348843455314636, g_loss: 5.321146011352539\n","Epoch 344/4000, Step 23, d_loss: 0.33944040536880493, g_loss: 5.6139302253723145\n","Epoch 344/4000, Step 24, d_loss: 0.34088292717933655, g_loss: 5.156479835510254\n","Epoch 344/4000, Step 25, d_loss: 0.3317797780036926, g_loss: 5.737597942352295\n","Epoch 344/4000, Step 26, d_loss: 0.3404875099658966, g_loss: 5.560912609100342\n","Epoch 344/4000, Step 27, d_loss: 0.340516597032547, g_loss: 5.520261764526367\n","Epoch 344/4000, Step 28, d_loss: 0.32963719964027405, g_loss: 5.659008979797363\n","Epoch 344/4000, Step 29, d_loss: 0.34719106554985046, g_loss: 5.761598110198975\n","Epoch 344/4000, Step 30, d_loss: 0.333337664604187, g_loss: 5.536253929138184\n","Epoch 344/4000, Step 31, d_loss: 0.33543863892555237, g_loss: 5.753424644470215\n","Epoch 344/4000, Step 32, d_loss: 0.3308149576187134, g_loss: 5.520167350769043\n","Epoch 344/4000, Step 33, d_loss: 0.33315104246139526, g_loss: 5.7278594970703125\n","Epoch 344/4000, Step 34, d_loss: 0.33155184984207153, g_loss: 5.188032150268555\n","Epoch 344/4000, Step 35, d_loss: 0.3307533860206604, g_loss: 5.497071743011475\n","Epoch 344/4000, Step 36, d_loss: 0.3337728977203369, g_loss: 6.528153896331787\n","Epoch 344/4000, Step 37, d_loss: 0.328632116317749, g_loss: 5.568892955780029\n","Epoch 344/4000, Step 38, d_loss: 0.3294859230518341, g_loss: 8.144122123718262\n","Epoch 344/4000, Step 39, d_loss: 0.3294544816017151, g_loss: 9.31489086151123\n","Epoch 344/4000, Step 40, d_loss: 0.32776713371276855, g_loss: 8.3921480178833\n","Epoch 344/4000, Step 41, d_loss: 0.3307965397834778, g_loss: 6.86276912689209\n","Epoch 344/4000, Step 42, d_loss: 0.32801109552383423, g_loss: 7.169372081756592\n","Epoch 344/4000, Step 43, d_loss: 0.32942551374435425, g_loss: 7.067470073699951\n","Epoch 344/4000, Step 44, d_loss: 0.32708320021629333, g_loss: 6.595717430114746\n","Epoch 344/4000, Step 45, d_loss: 0.3284587264060974, g_loss: 7.900907516479492\n","Epoch 344/4000, Step 46, d_loss: 0.33252620697021484, g_loss: 6.863481521606445\n","Epoch 344/4000, Step 47, d_loss: 0.33010339736938477, g_loss: 6.361032009124756\n","Epoch 344/4000, Step 48, d_loss: 0.3367561399936676, g_loss: 6.85014009475708\n","Epoch 344/4000, Step 49, d_loss: 0.32923802733421326, g_loss: 8.710798263549805\n","Epoch 344/4000, Step 50, d_loss: 0.32809919118881226, g_loss: 6.776067733764648\n","Epoch 344/4000, Step 51, d_loss: 0.3269162178039551, g_loss: 7.590738773345947\n","Epoch 344/4000, Step 52, d_loss: 0.32710689306259155, g_loss: 7.134537696838379\n","Epoch 344/4000, Step 53, d_loss: 0.33053454756736755, g_loss: 6.818042278289795\n","Epoch 344/4000, Step 54, d_loss: 0.32933250069618225, g_loss: 6.738288402557373\n","Epoch 344/4000, Step 55, d_loss: 0.33098405599594116, g_loss: 6.963531970977783\n","Epoch 344/4000, Step 56, d_loss: 0.32809779047966003, g_loss: 6.618320941925049\n","Epoch 344/4000, Step 57, d_loss: 0.3280143737792969, g_loss: 6.972427845001221\n","Epoch 344/4000, Step 58, d_loss: 0.33587774634361267, g_loss: 6.602570533752441\n","Epoch 344/4000, Step 59, d_loss: 0.33413851261138916, g_loss: 6.825815677642822\n","Epoch 344/4000, Step 60, d_loss: 0.33374112844467163, g_loss: 6.434671401977539\n","Epoch 344/4000, Step 61, d_loss: 0.3301331698894501, g_loss: 6.169087886810303\n","Epoch 344/4000, Step 62, d_loss: 0.3381738066673279, g_loss: 10.500157356262207\n","Epoch 344/4000, Step 63, d_loss: 0.3332415819168091, g_loss: 6.596859931945801\n","Epoch 344/4000, Step 64, d_loss: 0.329170823097229, g_loss: 8.221796989440918\n","Epoch 344/4000, Step 65, d_loss: 0.3317016363143921, g_loss: 7.097169399261475\n","Epoch 344/4000, Step 66, d_loss: 0.32871246337890625, g_loss: 6.6395087242126465\n","Epoch 344/4000, Step 67, d_loss: 0.32666170597076416, g_loss: 7.799769878387451\n","Epoch 344/4000, Step 68, d_loss: 0.32899582386016846, g_loss: 7.233096599578857\n","Epoch 344/4000, Step 69, d_loss: 0.34015825390815735, g_loss: 9.117770195007324\n","Epoch 344/4000, Step 70, d_loss: 0.33065205812454224, g_loss: 7.118639945983887\n","Epoch 344/4000, Step 71, d_loss: 0.3293193578720093, g_loss: 6.907959461212158\n","Epoch 344/4000, Step 72, d_loss: 0.3283376693725586, g_loss: 7.06650972366333\n","Epoch 344/4000, Step 73, d_loss: 0.3300648331642151, g_loss: 6.672402381896973\n","Epoch 344/4000, Step 74, d_loss: 0.32875344157218933, g_loss: 6.609522819519043\n","Epoch 344/4000, Step 75, d_loss: 0.3264378309249878, g_loss: 7.220066070556641\n","Epoch 344/4000, Step 76, d_loss: 0.3304249942302704, g_loss: 6.633214473724365\n","Epoch 344/4000, Step 77, d_loss: 0.32875943183898926, g_loss: 6.649568557739258\n","Epoch 344/4000, Step 78, d_loss: 0.3277888596057892, g_loss: 6.965434551239014\n","Epoch 344/4000, Step 79, d_loss: 0.3444622755050659, g_loss: 6.656440258026123\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 345/4000, Step 1, d_loss: 0.3292086720466614, g_loss: 6.003035068511963\n","Epoch 345/4000, Step 2, d_loss: 0.3314870595932007, g_loss: 6.3900065422058105\n","Epoch 345/4000, Step 3, d_loss: 0.32991132140159607, g_loss: 6.476954460144043\n","Epoch 345/4000, Step 4, d_loss: 0.3320612907409668, g_loss: 6.10616397857666\n","Epoch 345/4000, Step 5, d_loss: 0.3302980363368988, g_loss: 6.303677082061768\n","Epoch 345/4000, Step 6, d_loss: 0.33088356256484985, g_loss: 6.537138938903809\n","Epoch 345/4000, Step 7, d_loss: 0.32914191484451294, g_loss: 6.563867092132568\n","Epoch 345/4000, Step 8, d_loss: 0.32969456911087036, g_loss: 6.592655658721924\n","Epoch 345/4000, Step 9, d_loss: 0.3310495615005493, g_loss: 6.625885486602783\n","Epoch 345/4000, Step 10, d_loss: 0.33018139004707336, g_loss: 6.576847076416016\n","Epoch 345/4000, Step 11, d_loss: 0.32931193709373474, g_loss: 10.498620986938477\n","Epoch 345/4000, Step 12, d_loss: 0.3501116633415222, g_loss: 6.6521196365356445\n","Epoch 345/4000, Step 13, d_loss: 0.32752466201782227, g_loss: 7.0569891929626465\n","Epoch 345/4000, Step 14, d_loss: 0.33363696932792664, g_loss: 6.494480133056641\n","Epoch 345/4000, Step 15, d_loss: 0.335571825504303, g_loss: 6.109517574310303\n","Epoch 345/4000, Step 16, d_loss: 0.329546719789505, g_loss: 7.369485378265381\n","Epoch 345/4000, Step 17, d_loss: 0.3346829414367676, g_loss: 6.777002334594727\n","Epoch 345/4000, Step 18, d_loss: 0.3283870816230774, g_loss: 7.075953483581543\n","Epoch 345/4000, Step 19, d_loss: 0.334334135055542, g_loss: 7.1752471923828125\n","Epoch 345/4000, Step 20, d_loss: 0.33076387643814087, g_loss: 6.991010665893555\n","Epoch 345/4000, Step 21, d_loss: 0.3295605480670929, g_loss: 7.6040472984313965\n","Epoch 345/4000, Step 22, d_loss: 0.3330378234386444, g_loss: 7.185348987579346\n","Epoch 345/4000, Step 23, d_loss: 0.3287601172924042, g_loss: 6.98700475692749\n","Epoch 345/4000, Step 24, d_loss: 0.3301962912082672, g_loss: 6.574402332305908\n","Epoch 345/4000, Step 25, d_loss: 0.3314625322818756, g_loss: 7.189858436584473\n","Epoch 345/4000, Step 26, d_loss: 0.32796579599380493, g_loss: 6.770804405212402\n","Epoch 345/4000, Step 27, d_loss: 0.32873666286468506, g_loss: 6.984449863433838\n","Epoch 345/4000, Step 28, d_loss: 0.33170977234840393, g_loss: 6.903411865234375\n","Epoch 345/4000, Step 29, d_loss: 0.32776474952697754, g_loss: 7.030122756958008\n","Epoch 345/4000, Step 30, d_loss: 0.3266085684299469, g_loss: 6.6326680183410645\n","Epoch 345/4000, Step 31, d_loss: 0.3295856714248657, g_loss: 9.09980583190918\n","Epoch 345/4000, Step 32, d_loss: 0.3284752070903778, g_loss: 6.553631782531738\n","Epoch 345/4000, Step 33, d_loss: 0.3277892470359802, g_loss: 7.1166486740112305\n","Epoch 345/4000, Step 34, d_loss: 0.32971808314323425, g_loss: 6.741398334503174\n","Epoch 345/4000, Step 35, d_loss: 0.32836243510246277, g_loss: 6.9206461906433105\n","Epoch 345/4000, Step 36, d_loss: 0.326799601316452, g_loss: 6.960161209106445\n","Epoch 345/4000, Step 37, d_loss: 0.3280734717845917, g_loss: 6.874076843261719\n","Epoch 345/4000, Step 38, d_loss: 0.3283076286315918, g_loss: 6.864213943481445\n","Epoch 345/4000, Step 39, d_loss: 0.3276296555995941, g_loss: 7.087666988372803\n","Epoch 345/4000, Step 40, d_loss: 0.3278791010379791, g_loss: 6.7663984298706055\n","Epoch 345/4000, Step 41, d_loss: 0.3283935487270355, g_loss: 6.994095802307129\n","Epoch 345/4000, Step 42, d_loss: 0.3290064036846161, g_loss: 6.732107162475586\n","Epoch 345/4000, Step 43, d_loss: 0.3286711871623993, g_loss: 6.797543048858643\n","Epoch 345/4000, Step 44, d_loss: 0.32750922441482544, g_loss: 7.22719144821167\n","Epoch 345/4000, Step 45, d_loss: 0.3282163739204407, g_loss: 5.659850597381592\n","Epoch 345/4000, Step 46, d_loss: 0.32807883620262146, g_loss: 6.473316669464111\n","Epoch 345/4000, Step 47, d_loss: 0.32699474692344666, g_loss: 6.677437782287598\n","Epoch 345/4000, Step 48, d_loss: 0.32828304171562195, g_loss: 6.71549654006958\n","Epoch 345/4000, Step 49, d_loss: 0.32753807306289673, g_loss: 7.654201507568359\n","Epoch 345/4000, Step 50, d_loss: 0.328622043132782, g_loss: 7.68082857131958\n","Epoch 345/4000, Step 51, d_loss: 0.32696351408958435, g_loss: 7.9772210121154785\n","Epoch 345/4000, Step 52, d_loss: 0.328143835067749, g_loss: 6.650356769561768\n","Epoch 345/4000, Step 53, d_loss: 0.3284727931022644, g_loss: 7.133465766906738\n","Epoch 345/4000, Step 54, d_loss: 0.3278832733631134, g_loss: 6.875826358795166\n","Epoch 345/4000, Step 55, d_loss: 0.32651519775390625, g_loss: 6.851661682128906\n","Epoch 345/4000, Step 56, d_loss: 0.3342740535736084, g_loss: 6.516823768615723\n","Epoch 345/4000, Step 57, d_loss: 0.3277438282966614, g_loss: 7.017544269561768\n","Epoch 345/4000, Step 58, d_loss: 0.32758158445358276, g_loss: 8.720614433288574\n","Epoch 345/4000, Step 59, d_loss: 0.3275887072086334, g_loss: 7.125247478485107\n","Epoch 345/4000, Step 60, d_loss: 0.32726117968559265, g_loss: 7.076641082763672\n","Epoch 345/4000, Step 61, d_loss: 0.32852938771247864, g_loss: 6.852711200714111\n","Epoch 345/4000, Step 62, d_loss: 0.32736796140670776, g_loss: 6.756982803344727\n","Epoch 345/4000, Step 63, d_loss: 0.3292195200920105, g_loss: 6.915803909301758\n","Epoch 345/4000, Step 64, d_loss: 0.3276083469390869, g_loss: 6.798789024353027\n","Epoch 345/4000, Step 65, d_loss: 0.3282264471054077, g_loss: 7.673703193664551\n","Epoch 345/4000, Step 66, d_loss: 0.3272375762462616, g_loss: 6.706192493438721\n","Epoch 345/4000, Step 67, d_loss: 0.34042608737945557, g_loss: 6.63887882232666\n","Epoch 345/4000, Step 68, d_loss: 0.33091285824775696, g_loss: 6.43701171875\n","Epoch 345/4000, Step 69, d_loss: 0.32876092195510864, g_loss: 6.375297546386719\n","Epoch 345/4000, Step 70, d_loss: 0.3314666748046875, g_loss: 6.460541248321533\n","Epoch 345/4000, Step 71, d_loss: 0.33227410912513733, g_loss: 6.456974506378174\n","Epoch 345/4000, Step 72, d_loss: 0.33111605048179626, g_loss: 6.366896629333496\n","Epoch 345/4000, Step 73, d_loss: 0.33623066544532776, g_loss: 7.289153575897217\n","Epoch 345/4000, Step 74, d_loss: 0.3316898047924042, g_loss: 6.5855021476745605\n","Epoch 345/4000, Step 75, d_loss: 0.333507776260376, g_loss: 4.699881553649902\n","Epoch 345/4000, Step 76, d_loss: 0.4295865297317505, g_loss: 2.1879615783691406\n","Epoch 345/4000, Step 77, d_loss: 0.8685581684112549, g_loss: 6.012251853942871\n","Epoch 345/4000, Step 78, d_loss: 0.5443786382675171, g_loss: 6.779426574707031\n","Epoch 345/4000, Step 79, d_loss: 3.4928958415985107, g_loss: 4.7593159675598145\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 346/4000, Step 1, d_loss: 0.38496413826942444, g_loss: 4.784125804901123\n","Epoch 346/4000, Step 2, d_loss: 0.3606511652469635, g_loss: 1.8106483221054077\n","Epoch 346/4000, Step 3, d_loss: 0.48451149463653564, g_loss: 5.442909240722656\n","Epoch 346/4000, Step 4, d_loss: 0.5712010860443115, g_loss: 2.7488577365875244\n","Epoch 346/4000, Step 5, d_loss: 0.5453820824623108, g_loss: 3.799788475036621\n","Epoch 346/4000, Step 6, d_loss: 0.5388761162757874, g_loss: 6.8987345695495605\n","Epoch 346/4000, Step 7, d_loss: 0.48115938901901245, g_loss: 6.690798282623291\n","Epoch 346/4000, Step 8, d_loss: 0.4289363920688629, g_loss: 7.8958001136779785\n","Epoch 346/4000, Step 9, d_loss: 0.5222830772399902, g_loss: 7.326028347015381\n","Epoch 346/4000, Step 10, d_loss: 0.5114265084266663, g_loss: 7.271694660186768\n","Epoch 346/4000, Step 11, d_loss: 0.3690209984779358, g_loss: 4.844343662261963\n","Epoch 346/4000, Step 12, d_loss: 0.4141254127025604, g_loss: 6.976190090179443\n","Epoch 346/4000, Step 13, d_loss: 0.5257169604301453, g_loss: 3.3787953853607178\n","Epoch 346/4000, Step 14, d_loss: 0.4139641225337982, g_loss: 4.834590911865234\n","Epoch 346/4000, Step 15, d_loss: 0.4383600056171417, g_loss: 6.79770040512085\n","Epoch 346/4000, Step 16, d_loss: 0.4552929401397705, g_loss: 6.96985387802124\n","Epoch 346/4000, Step 17, d_loss: 0.5289444327354431, g_loss: 3.3763813972473145\n","Epoch 346/4000, Step 18, d_loss: 0.40486711263656616, g_loss: 1.291913628578186\n","Epoch 346/4000, Step 19, d_loss: 0.6693847179412842, g_loss: 2.8976874351501465\n","Epoch 346/4000, Step 20, d_loss: 0.4665006697177887, g_loss: 4.770864009857178\n","Epoch 346/4000, Step 21, d_loss: 0.38845303654670715, g_loss: 8.937007904052734\n","Epoch 346/4000, Step 22, d_loss: 0.6061076521873474, g_loss: 7.473394870758057\n","Epoch 346/4000, Step 23, d_loss: 0.40618279576301575, g_loss: 6.458473205566406\n","Epoch 346/4000, Step 24, d_loss: 0.3446623384952545, g_loss: 6.444581508636475\n","Epoch 346/4000, Step 25, d_loss: 0.3969224691390991, g_loss: 6.1875319480896\n","Epoch 346/4000, Step 26, d_loss: 0.3780798316001892, g_loss: 5.275688648223877\n","Epoch 346/4000, Step 27, d_loss: 0.4255656599998474, g_loss: 6.320475101470947\n","Epoch 346/4000, Step 28, d_loss: 0.4010317921638489, g_loss: 6.763260364532471\n","Epoch 346/4000, Step 29, d_loss: 0.3848588466644287, g_loss: 6.8203959465026855\n","Epoch 346/4000, Step 30, d_loss: 0.36966466903686523, g_loss: 8.684297561645508\n","Epoch 346/4000, Step 31, d_loss: 0.3465072214603424, g_loss: 9.913820266723633\n","Epoch 346/4000, Step 32, d_loss: 0.3506137728691101, g_loss: 9.421361923217773\n","Epoch 346/4000, Step 33, d_loss: 0.3735995292663574, g_loss: 8.40395450592041\n","Epoch 346/4000, Step 34, d_loss: 0.34781500697135925, g_loss: 7.995809555053711\n","Epoch 346/4000, Step 35, d_loss: 0.3514922857284546, g_loss: 7.6786603927612305\n","Epoch 346/4000, Step 36, d_loss: 0.3589792251586914, g_loss: 7.589371204376221\n","Epoch 346/4000, Step 37, d_loss: 0.34477609395980835, g_loss: 7.628461837768555\n","Epoch 346/4000, Step 38, d_loss: 0.3535043001174927, g_loss: 5.8334126472473145\n","Epoch 346/4000, Step 39, d_loss: 0.3485075533390045, g_loss: 8.398615837097168\n","Epoch 346/4000, Step 40, d_loss: 0.34367239475250244, g_loss: 6.265355587005615\n","Epoch 346/4000, Step 41, d_loss: 0.35285064578056335, g_loss: 6.274200439453125\n","Epoch 346/4000, Step 42, d_loss: 0.3446935713291168, g_loss: 8.066536903381348\n","Epoch 346/4000, Step 43, d_loss: 0.3624628186225891, g_loss: 5.370613098144531\n","Epoch 346/4000, Step 44, d_loss: 0.3445119559764862, g_loss: 6.726187705993652\n","Epoch 346/4000, Step 45, d_loss: 0.3679032325744629, g_loss: 5.071132659912109\n","Epoch 346/4000, Step 46, d_loss: 0.3687853217124939, g_loss: 6.220292568206787\n","Epoch 346/4000, Step 47, d_loss: 0.35184258222579956, g_loss: 6.560014724731445\n","Epoch 346/4000, Step 48, d_loss: 0.3436541259288788, g_loss: 7.626453399658203\n","Epoch 346/4000, Step 49, d_loss: 0.33509236574172974, g_loss: 7.49561071395874\n","Epoch 346/4000, Step 50, d_loss: 0.3467487692832947, g_loss: 6.576768398284912\n","Epoch 346/4000, Step 51, d_loss: 0.3322332501411438, g_loss: 7.2887749671936035\n","Epoch 346/4000, Step 52, d_loss: 0.3409714996814728, g_loss: 5.9295430183410645\n","Epoch 346/4000, Step 53, d_loss: 0.3585876524448395, g_loss: 5.247897148132324\n","Epoch 346/4000, Step 54, d_loss: 0.33480221033096313, g_loss: 4.408111095428467\n","Epoch 346/4000, Step 55, d_loss: 0.35215264558792114, g_loss: 4.818630218505859\n","Epoch 346/4000, Step 56, d_loss: 0.3402530252933502, g_loss: 5.121654033660889\n","Epoch 346/4000, Step 57, d_loss: 0.35186535120010376, g_loss: 4.547158241271973\n","Epoch 346/4000, Step 58, d_loss: 0.34058165550231934, g_loss: 4.267460346221924\n","Epoch 346/4000, Step 59, d_loss: 0.3510313034057617, g_loss: 4.855465888977051\n","Epoch 346/4000, Step 60, d_loss: 0.3449997901916504, g_loss: 5.301955223083496\n","Epoch 346/4000, Step 61, d_loss: 0.3394424021244049, g_loss: 3.909507989883423\n","Epoch 346/4000, Step 62, d_loss: 0.3445478677749634, g_loss: 4.721036911010742\n","Epoch 346/4000, Step 63, d_loss: 0.3476177155971527, g_loss: 7.4949517250061035\n","Epoch 346/4000, Step 64, d_loss: 0.33312422037124634, g_loss: 3.838230848312378\n","Epoch 346/4000, Step 65, d_loss: 0.34240925312042236, g_loss: 3.304006814956665\n","Epoch 346/4000, Step 66, d_loss: 0.37039145827293396, g_loss: 4.133997917175293\n","Epoch 346/4000, Step 67, d_loss: 0.34860867261886597, g_loss: 4.448321342468262\n","Epoch 346/4000, Step 68, d_loss: 0.34106162190437317, g_loss: 4.531863212585449\n","Epoch 346/4000, Step 69, d_loss: 0.34505993127822876, g_loss: 4.541162967681885\n","Epoch 346/4000, Step 70, d_loss: 0.34902045130729675, g_loss: 6.509873867034912\n","Epoch 346/4000, Step 71, d_loss: 0.343005508184433, g_loss: 4.876513957977295\n","Epoch 346/4000, Step 72, d_loss: 0.34246397018432617, g_loss: 7.134043216705322\n","Epoch 346/4000, Step 73, d_loss: 0.33259186148643494, g_loss: 5.142340183258057\n","Epoch 346/4000, Step 74, d_loss: 0.3326408267021179, g_loss: 5.443789958953857\n","Epoch 346/4000, Step 75, d_loss: 0.3465809226036072, g_loss: 6.521717548370361\n","Epoch 346/4000, Step 76, d_loss: 0.329554945230484, g_loss: 4.888625144958496\n","Epoch 346/4000, Step 77, d_loss: 0.33476951718330383, g_loss: 6.78972864151001\n","Epoch 346/4000, Step 78, d_loss: 0.33460596203804016, g_loss: 5.512117862701416\n","Epoch 346/4000, Step 79, d_loss: 0.3287990689277649, g_loss: 6.6299543380737305\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 347/4000, Step 1, d_loss: 0.3470437526702881, g_loss: 7.751699447631836\n","Epoch 347/4000, Step 2, d_loss: 0.3332507312297821, g_loss: 5.297086238861084\n","Epoch 347/4000, Step 3, d_loss: 0.33388417959213257, g_loss: 6.259041786193848\n","Epoch 347/4000, Step 4, d_loss: 0.3488309383392334, g_loss: 4.624520301818848\n","Epoch 347/4000, Step 5, d_loss: 0.34756630659103394, g_loss: 4.782449245452881\n","Epoch 347/4000, Step 6, d_loss: 0.34403225779533386, g_loss: 4.322815895080566\n","Epoch 347/4000, Step 7, d_loss: 0.3567102551460266, g_loss: 4.626128196716309\n","Epoch 347/4000, Step 8, d_loss: 0.3333579897880554, g_loss: 6.721017837524414\n","Epoch 347/4000, Step 9, d_loss: 0.34206631779670715, g_loss: 7.078840255737305\n","Epoch 347/4000, Step 10, d_loss: 0.33625316619873047, g_loss: 6.0350341796875\n","Epoch 347/4000, Step 11, d_loss: 0.3423183858394623, g_loss: 4.657657146453857\n","Epoch 347/4000, Step 12, d_loss: 0.3504030704498291, g_loss: 4.965493202209473\n","Epoch 347/4000, Step 13, d_loss: 0.3470120429992676, g_loss: 4.851921081542969\n","Epoch 347/4000, Step 14, d_loss: 0.33735060691833496, g_loss: 5.224902153015137\n","Epoch 347/4000, Step 15, d_loss: 0.35243478417396545, g_loss: 4.342472076416016\n","Epoch 347/4000, Step 16, d_loss: 0.34920382499694824, g_loss: 4.305320739746094\n","Epoch 347/4000, Step 17, d_loss: 0.3354790508747101, g_loss: 4.28894567489624\n","Epoch 347/4000, Step 18, d_loss: 0.3431378901004791, g_loss: 5.02155065536499\n","Epoch 347/4000, Step 19, d_loss: 0.34671974182128906, g_loss: 7.358911991119385\n","Epoch 347/4000, Step 20, d_loss: 0.3312143385410309, g_loss: 4.912033557891846\n","Epoch 347/4000, Step 21, d_loss: 0.3315906524658203, g_loss: 5.810552597045898\n","Epoch 347/4000, Step 22, d_loss: 0.34077519178390503, g_loss: 6.910710334777832\n","Epoch 347/4000, Step 23, d_loss: 0.3540188670158386, g_loss: 7.019924163818359\n","Epoch 347/4000, Step 24, d_loss: 0.35989928245544434, g_loss: 6.153954982757568\n","Epoch 347/4000, Step 25, d_loss: 0.3326282203197479, g_loss: 6.815516471862793\n","Epoch 347/4000, Step 26, d_loss: 0.34920957684516907, g_loss: 4.322593688964844\n","Epoch 347/4000, Step 27, d_loss: 0.35305678844451904, g_loss: 4.152010440826416\n","Epoch 347/4000, Step 28, d_loss: 0.34368664026260376, g_loss: 4.9284186363220215\n","Epoch 347/4000, Step 29, d_loss: 0.3495205044746399, g_loss: 5.742116451263428\n","Epoch 347/4000, Step 30, d_loss: 0.3529118597507477, g_loss: 4.466151237487793\n","Epoch 347/4000, Step 31, d_loss: 0.3425004482269287, g_loss: 6.599650859832764\n","Epoch 347/4000, Step 32, d_loss: 0.3489915132522583, g_loss: 5.923177719116211\n","Epoch 347/4000, Step 33, d_loss: 0.35709506273269653, g_loss: 5.505986213684082\n","Epoch 347/4000, Step 34, d_loss: 0.42117875814437866, g_loss: 4.992902755737305\n","Epoch 347/4000, Step 35, d_loss: 0.3453775644302368, g_loss: 4.661130428314209\n","Epoch 347/4000, Step 36, d_loss: 0.3748900294303894, g_loss: 3.702404499053955\n","Epoch 347/4000, Step 37, d_loss: 0.3919302821159363, g_loss: 4.053364276885986\n","Epoch 347/4000, Step 38, d_loss: 0.3727096617221832, g_loss: 4.073232650756836\n","Epoch 347/4000, Step 39, d_loss: 0.37703073024749756, g_loss: 4.761408805847168\n","Epoch 347/4000, Step 40, d_loss: 0.3573845326900482, g_loss: 6.570399284362793\n","Epoch 347/4000, Step 41, d_loss: 0.38547950983047485, g_loss: 7.377945423126221\n","Epoch 347/4000, Step 42, d_loss: 0.3502373695373535, g_loss: 5.524869441986084\n","Epoch 347/4000, Step 43, d_loss: 0.341309517621994, g_loss: 4.810984134674072\n","Epoch 347/4000, Step 44, d_loss: 0.36276665329933167, g_loss: 6.618058681488037\n","Epoch 347/4000, Step 45, d_loss: 0.3525552451610565, g_loss: 5.405720233917236\n","Epoch 347/4000, Step 46, d_loss: 0.3565828204154968, g_loss: 6.294973373413086\n","Epoch 347/4000, Step 47, d_loss: 0.3544153571128845, g_loss: 6.374631881713867\n","Epoch 347/4000, Step 48, d_loss: 0.3597903847694397, g_loss: 6.260051250457764\n","Epoch 347/4000, Step 49, d_loss: 0.3747764825820923, g_loss: 5.748788356781006\n","Epoch 347/4000, Step 50, d_loss: 0.3531513512134552, g_loss: 4.062618732452393\n","Epoch 347/4000, Step 51, d_loss: 0.37571924924850464, g_loss: 4.474535942077637\n","Epoch 347/4000, Step 52, d_loss: 0.3546888828277588, g_loss: 6.628876209259033\n","Epoch 347/4000, Step 53, d_loss: 0.358712375164032, g_loss: 4.905168056488037\n","Epoch 347/4000, Step 54, d_loss: 0.3436412811279297, g_loss: 7.7167229652404785\n","Epoch 347/4000, Step 55, d_loss: 0.366704523563385, g_loss: 6.849588871002197\n","Epoch 347/4000, Step 56, d_loss: 0.3443017899990082, g_loss: 6.158623695373535\n","Epoch 347/4000, Step 57, d_loss: 0.34936201572418213, g_loss: 6.514003276824951\n","Epoch 347/4000, Step 58, d_loss: 0.35636386275291443, g_loss: 7.007307052612305\n","Epoch 347/4000, Step 59, d_loss: 0.3388521373271942, g_loss: 7.815664291381836\n","Epoch 347/4000, Step 60, d_loss: 0.33712145686149597, g_loss: 5.27461051940918\n","Epoch 347/4000, Step 61, d_loss: 0.33326590061187744, g_loss: 7.81740140914917\n","Epoch 347/4000, Step 62, d_loss: 0.3402971625328064, g_loss: 5.325562000274658\n","Epoch 347/4000, Step 63, d_loss: 0.34919941425323486, g_loss: 8.291692733764648\n","Epoch 347/4000, Step 64, d_loss: 0.4030308425426483, g_loss: 6.979641437530518\n","Epoch 347/4000, Step 65, d_loss: 0.33702927827835083, g_loss: 6.227366924285889\n","Epoch 347/4000, Step 66, d_loss: 0.35739484429359436, g_loss: 5.823040962219238\n","Epoch 347/4000, Step 67, d_loss: 0.37399405241012573, g_loss: 4.954517841339111\n","Epoch 347/4000, Step 68, d_loss: 0.3542667627334595, g_loss: 6.558794975280762\n","Epoch 347/4000, Step 69, d_loss: 0.3460262417793274, g_loss: 6.804651260375977\n","Epoch 347/4000, Step 70, d_loss: 0.3543072044849396, g_loss: 8.673077583312988\n","Epoch 347/4000, Step 71, d_loss: 0.36069586873054504, g_loss: 6.958737850189209\n","Epoch 347/4000, Step 72, d_loss: 0.3449487090110779, g_loss: 7.3718109130859375\n","Epoch 347/4000, Step 73, d_loss: 0.3575868308544159, g_loss: 6.839487552642822\n","Epoch 347/4000, Step 74, d_loss: 0.3433980345726013, g_loss: 5.641448974609375\n","Epoch 347/4000, Step 75, d_loss: 0.3364310562610626, g_loss: 7.337552070617676\n","Epoch 347/4000, Step 76, d_loss: 0.3364070653915405, g_loss: 6.555994987487793\n","Epoch 347/4000, Step 77, d_loss: 0.3360273540019989, g_loss: 5.247676372528076\n","Epoch 347/4000, Step 78, d_loss: 0.343894898891449, g_loss: 8.553215026855469\n","Epoch 347/4000, Step 79, d_loss: 1.5750150680541992, g_loss: 4.059750080108643\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 348/4000, Step 1, d_loss: 0.4534476101398468, g_loss: 2.210136651992798\n","Epoch 348/4000, Step 2, d_loss: 0.6088815331459045, g_loss: 0.9989670515060425\n","Epoch 348/4000, Step 3, d_loss: 0.7434059381484985, g_loss: 3.1999032497406006\n","Epoch 348/4000, Step 4, d_loss: 0.7980008721351624, g_loss: 1.8432292938232422\n","Epoch 348/4000, Step 5, d_loss: 0.7641282677650452, g_loss: 2.813807487487793\n","Epoch 348/4000, Step 6, d_loss: 0.7121676802635193, g_loss: 5.236752986907959\n","Epoch 348/4000, Step 7, d_loss: 0.6346892714500427, g_loss: 6.346042156219482\n","Epoch 348/4000, Step 8, d_loss: 0.49338340759277344, g_loss: 6.603841304779053\n","Epoch 348/4000, Step 9, d_loss: 0.44445371627807617, g_loss: 4.969114303588867\n","Epoch 348/4000, Step 10, d_loss: 0.5085707902908325, g_loss: 3.793966293334961\n","Epoch 348/4000, Step 11, d_loss: 0.5586175322532654, g_loss: 2.0219156742095947\n","Epoch 348/4000, Step 12, d_loss: 0.4772348701953888, g_loss: 4.3634352684021\n","Epoch 348/4000, Step 13, d_loss: 0.8295941352844238, g_loss: 3.6153078079223633\n","Epoch 348/4000, Step 14, d_loss: 0.44422581791877747, g_loss: 4.772374153137207\n","Epoch 348/4000, Step 15, d_loss: 0.5522392988204956, g_loss: 6.089359283447266\n","Epoch 348/4000, Step 16, d_loss: 0.557020902633667, g_loss: 4.9762678146362305\n","Epoch 348/4000, Step 17, d_loss: 0.43115097284317017, g_loss: 4.7849884033203125\n","Epoch 348/4000, Step 18, d_loss: 0.4518773555755615, g_loss: 2.751875638961792\n","Epoch 348/4000, Step 19, d_loss: 0.4402067959308624, g_loss: 3.808612823486328\n","Epoch 348/4000, Step 20, d_loss: 0.41397956013679504, g_loss: 5.0195841789245605\n","Epoch 348/4000, Step 21, d_loss: 0.3845936059951782, g_loss: 3.5582282543182373\n","Epoch 348/4000, Step 22, d_loss: 0.43019458651542664, g_loss: 4.929439544677734\n","Epoch 348/4000, Step 23, d_loss: 0.40548762679100037, g_loss: 3.5936617851257324\n","Epoch 348/4000, Step 24, d_loss: 0.3946079909801483, g_loss: 4.421352863311768\n","Epoch 348/4000, Step 25, d_loss: 0.3805834949016571, g_loss: 6.5847344398498535\n","Epoch 348/4000, Step 26, d_loss: 0.39753079414367676, g_loss: 6.378332614898682\n","Epoch 348/4000, Step 27, d_loss: 0.4022291302680969, g_loss: 3.0255966186523438\n","Epoch 348/4000, Step 28, d_loss: 0.3915797472000122, g_loss: 5.812160491943359\n","Epoch 348/4000, Step 29, d_loss: 0.44545823335647583, g_loss: 6.408914566040039\n","Epoch 348/4000, Step 30, d_loss: 0.3634684085845947, g_loss: 3.459230661392212\n","Epoch 348/4000, Step 31, d_loss: 0.39925384521484375, g_loss: 5.910378932952881\n","Epoch 348/4000, Step 32, d_loss: 0.38230401277542114, g_loss: 4.456196308135986\n","Epoch 348/4000, Step 33, d_loss: 0.38915279507637024, g_loss: 3.8760244846343994\n","Epoch 348/4000, Step 34, d_loss: 0.34936967492103577, g_loss: 4.293694496154785\n","Epoch 348/4000, Step 35, d_loss: 0.412585973739624, g_loss: 3.910236120223999\n","Epoch 348/4000, Step 36, d_loss: 0.3956257402896881, g_loss: 2.913641929626465\n","Epoch 348/4000, Step 37, d_loss: 0.3839518427848816, g_loss: 4.582777500152588\n","Epoch 348/4000, Step 38, d_loss: 0.38034576177597046, g_loss: 3.8872501850128174\n","Epoch 348/4000, Step 39, d_loss: 0.3688513934612274, g_loss: 2.5810563564300537\n","Epoch 348/4000, Step 40, d_loss: 0.3652041256427765, g_loss: 6.453591823577881\n","Epoch 348/4000, Step 41, d_loss: 0.4440305829048157, g_loss: 4.5124592781066895\n","Epoch 348/4000, Step 42, d_loss: 0.35578683018684387, g_loss: 3.8783152103424072\n","Epoch 348/4000, Step 43, d_loss: 0.37227770686149597, g_loss: 4.370115280151367\n","Epoch 348/4000, Step 44, d_loss: 0.38784170150756836, g_loss: 4.386322021484375\n","Epoch 348/4000, Step 45, d_loss: 0.373389333486557, g_loss: 4.070392608642578\n","Epoch 348/4000, Step 46, d_loss: 0.3798794448375702, g_loss: 4.705699443817139\n","Epoch 348/4000, Step 47, d_loss: 0.3695499300956726, g_loss: 5.190014362335205\n","Epoch 348/4000, Step 48, d_loss: 0.35062846541404724, g_loss: 4.859475135803223\n","Epoch 348/4000, Step 49, d_loss: 0.37578877806663513, g_loss: 4.375628471374512\n","Epoch 348/4000, Step 50, d_loss: 0.34277650713920593, g_loss: 5.342877388000488\n","Epoch 348/4000, Step 51, d_loss: 0.3569355010986328, g_loss: 3.6988894939422607\n","Epoch 348/4000, Step 52, d_loss: 0.35974904894828796, g_loss: 4.61126184463501\n","Epoch 348/4000, Step 53, d_loss: 0.3924653232097626, g_loss: 4.59531307220459\n","Epoch 348/4000, Step 54, d_loss: 0.402294397354126, g_loss: 3.591860771179199\n","Epoch 348/4000, Step 55, d_loss: 0.3678360879421234, g_loss: 5.651618957519531\n","Epoch 348/4000, Step 56, d_loss: 0.3567621409893036, g_loss: 5.466670513153076\n","Epoch 348/4000, Step 57, d_loss: 0.48544377088546753, g_loss: 5.396971225738525\n","Epoch 348/4000, Step 58, d_loss: 0.36522310972213745, g_loss: 3.9666688442230225\n","Epoch 348/4000, Step 59, d_loss: 0.3631661534309387, g_loss: 2.9313056468963623\n","Epoch 348/4000, Step 60, d_loss: 0.4133402109146118, g_loss: 3.1077449321746826\n","Epoch 348/4000, Step 61, d_loss: 0.3676489293575287, g_loss: 5.429994106292725\n","Epoch 348/4000, Step 62, d_loss: 0.37878331542015076, g_loss: 5.390822887420654\n","Epoch 348/4000, Step 63, d_loss: 0.3564547598361969, g_loss: 5.089296340942383\n","Epoch 348/4000, Step 64, d_loss: 0.3944658041000366, g_loss: 5.87915563583374\n","Epoch 348/4000, Step 65, d_loss: 0.3988271951675415, g_loss: 5.096309185028076\n","Epoch 348/4000, Step 66, d_loss: 0.37385866045951843, g_loss: 6.588656902313232\n","Epoch 348/4000, Step 67, d_loss: 0.3505840003490448, g_loss: 4.245865821838379\n","Epoch 348/4000, Step 68, d_loss: 0.350434273481369, g_loss: 3.4350578784942627\n","Epoch 348/4000, Step 69, d_loss: 0.3710874617099762, g_loss: 3.249263286590576\n","Epoch 348/4000, Step 70, d_loss: 0.39938467741012573, g_loss: 3.9344265460968018\n","Epoch 348/4000, Step 71, d_loss: 0.35029175877571106, g_loss: 3.443842649459839\n","Epoch 348/4000, Step 72, d_loss: 0.34405043721199036, g_loss: 4.299192905426025\n","Epoch 348/4000, Step 73, d_loss: 0.3408450782299042, g_loss: 5.638601303100586\n","Epoch 348/4000, Step 74, d_loss: 0.35506686568260193, g_loss: 5.124207019805908\n","Epoch 348/4000, Step 75, d_loss: 0.3528415262699127, g_loss: 5.22854471206665\n","Epoch 348/4000, Step 76, d_loss: 0.3461000919342041, g_loss: 4.706694602966309\n","Epoch 348/4000, Step 77, d_loss: 0.3548864722251892, g_loss: 6.587270259857178\n","Epoch 348/4000, Step 78, d_loss: 0.36226722598075867, g_loss: 5.383510589599609\n","Epoch 348/4000, Step 79, d_loss: 0.5123723745346069, g_loss: 4.531533718109131\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 349/4000, Step 1, d_loss: 0.35429447889328003, g_loss: 4.362662315368652\n","Epoch 349/4000, Step 2, d_loss: 0.3985307216644287, g_loss: 3.436530828475952\n","Epoch 349/4000, Step 3, d_loss: 0.4337175786495209, g_loss: 4.681159019470215\n","Epoch 349/4000, Step 4, d_loss: 0.3833523094654083, g_loss: 4.321168422698975\n","Epoch 349/4000, Step 5, d_loss: 0.37468358874320984, g_loss: 5.092931270599365\n","Epoch 349/4000, Step 6, d_loss: 0.365067720413208, g_loss: 5.3377814292907715\n","Epoch 349/4000, Step 7, d_loss: 0.353979229927063, g_loss: 5.962947845458984\n","Epoch 349/4000, Step 8, d_loss: 0.3567851483821869, g_loss: 6.072962760925293\n","Epoch 349/4000, Step 9, d_loss: 0.38275235891342163, g_loss: 6.525609016418457\n","Epoch 349/4000, Step 10, d_loss: 0.3824237585067749, g_loss: 6.376057147979736\n","Epoch 349/4000, Step 11, d_loss: 0.3587154150009155, g_loss: 4.292427062988281\n","Epoch 349/4000, Step 12, d_loss: 0.3567556142807007, g_loss: 3.767592668533325\n","Epoch 349/4000, Step 13, d_loss: 0.3550589680671692, g_loss: 5.51032829284668\n","Epoch 349/4000, Step 14, d_loss: 0.36823996901512146, g_loss: 4.436424732208252\n","Epoch 349/4000, Step 15, d_loss: 0.3662704527378082, g_loss: 6.570920944213867\n","Epoch 349/4000, Step 16, d_loss: 0.3629514276981354, g_loss: 4.511102676391602\n","Epoch 349/4000, Step 17, d_loss: 0.37178850173950195, g_loss: 8.75682544708252\n","Epoch 349/4000, Step 18, d_loss: 0.40284964442253113, g_loss: 5.485456466674805\n","Epoch 349/4000, Step 19, d_loss: 0.35443875193595886, g_loss: 5.116940975189209\n","Epoch 349/4000, Step 20, d_loss: 0.33823469281196594, g_loss: 5.35994291305542\n","Epoch 349/4000, Step 21, d_loss: 0.349510133266449, g_loss: 7.584084510803223\n","Epoch 349/4000, Step 22, d_loss: 0.36332395672798157, g_loss: 5.05941915512085\n","Epoch 349/4000, Step 23, d_loss: 0.35694408416748047, g_loss: 4.8122172355651855\n","Epoch 349/4000, Step 24, d_loss: 0.36269667744636536, g_loss: 4.728893280029297\n","Epoch 349/4000, Step 25, d_loss: 0.36114874482154846, g_loss: 4.930771827697754\n","Epoch 349/4000, Step 26, d_loss: 0.38110747933387756, g_loss: 4.801089286804199\n","Epoch 349/4000, Step 27, d_loss: 0.35908833146095276, g_loss: 8.635259628295898\n","Epoch 349/4000, Step 28, d_loss: 0.37143415212631226, g_loss: 4.843890190124512\n","Epoch 349/4000, Step 29, d_loss: 0.3728961646556854, g_loss: 5.905898571014404\n","Epoch 349/4000, Step 30, d_loss: 0.4188317358493805, g_loss: 4.012091636657715\n","Epoch 349/4000, Step 31, d_loss: 0.33940213918685913, g_loss: 5.400890350341797\n","Epoch 349/4000, Step 32, d_loss: 0.3507763743400574, g_loss: 3.799989938735962\n","Epoch 349/4000, Step 33, d_loss: 0.40312978625297546, g_loss: 4.508919715881348\n","Epoch 349/4000, Step 34, d_loss: 0.3889443576335907, g_loss: 4.369941234588623\n","Epoch 349/4000, Step 35, d_loss: 0.40953782200813293, g_loss: 5.457090854644775\n","Epoch 349/4000, Step 36, d_loss: 0.35399124026298523, g_loss: 4.93564510345459\n","Epoch 349/4000, Step 37, d_loss: 0.35564950108528137, g_loss: 6.072200775146484\n","Epoch 349/4000, Step 38, d_loss: 0.36682769656181335, g_loss: 6.2686238288879395\n","Epoch 349/4000, Step 39, d_loss: 0.40695276856422424, g_loss: 5.565390110015869\n","Epoch 349/4000, Step 40, d_loss: 0.34636762738227844, g_loss: 5.648128032684326\n","Epoch 349/4000, Step 41, d_loss: 0.3472250998020172, g_loss: 7.309476375579834\n","Epoch 349/4000, Step 42, d_loss: 0.3460843563079834, g_loss: 5.010847091674805\n","Epoch 349/4000, Step 43, d_loss: 0.3713122010231018, g_loss: 3.4124090671539307\n","Epoch 349/4000, Step 44, d_loss: 0.37908512353897095, g_loss: 3.8907313346862793\n","Epoch 349/4000, Step 45, d_loss: 0.40657883882522583, g_loss: 6.048808574676514\n","Epoch 349/4000, Step 46, d_loss: 0.35310786962509155, g_loss: 6.786033630371094\n","Epoch 349/4000, Step 47, d_loss: 0.40423163771629333, g_loss: 7.3489990234375\n","Epoch 349/4000, Step 48, d_loss: 0.3622356951236725, g_loss: 7.969997406005859\n","Epoch 349/4000, Step 49, d_loss: 0.3449596166610718, g_loss: 5.723959445953369\n","Epoch 349/4000, Step 50, d_loss: 0.35017597675323486, g_loss: 4.983653545379639\n","Epoch 349/4000, Step 51, d_loss: 0.3384072482585907, g_loss: 4.5246734619140625\n","Epoch 349/4000, Step 52, d_loss: 0.351548433303833, g_loss: 4.37076997756958\n","Epoch 349/4000, Step 53, d_loss: 0.349399209022522, g_loss: 5.099140644073486\n","Epoch 349/4000, Step 54, d_loss: 0.3789463937282562, g_loss: 5.2675018310546875\n","Epoch 349/4000, Step 55, d_loss: 0.351882666349411, g_loss: 6.224443435668945\n","Epoch 349/4000, Step 56, d_loss: 0.34470269083976746, g_loss: 6.085415840148926\n","Epoch 349/4000, Step 57, d_loss: 0.35108456015586853, g_loss: 5.943665027618408\n","Epoch 349/4000, Step 58, d_loss: 0.3742094337940216, g_loss: 5.150107383728027\n","Epoch 349/4000, Step 59, d_loss: 0.35150134563446045, g_loss: 5.520458221435547\n","Epoch 349/4000, Step 60, d_loss: 0.3414597809314728, g_loss: 4.805984020233154\n","Epoch 349/4000, Step 61, d_loss: 0.35725709795951843, g_loss: 4.499818801879883\n","Epoch 349/4000, Step 62, d_loss: 0.34763211011886597, g_loss: 4.771732330322266\n","Epoch 349/4000, Step 63, d_loss: 0.3558290898799896, g_loss: 4.7800397872924805\n","Epoch 349/4000, Step 64, d_loss: 0.3465270698070526, g_loss: 5.906196594238281\n","Epoch 349/4000, Step 65, d_loss: 0.3394295573234558, g_loss: 6.6873040199279785\n","Epoch 349/4000, Step 66, d_loss: 0.33609768748283386, g_loss: 5.644853115081787\n","Epoch 349/4000, Step 67, d_loss: 0.3507189452648163, g_loss: 6.612776279449463\n","Epoch 349/4000, Step 68, d_loss: 0.34051746129989624, g_loss: 7.687335014343262\n","Epoch 349/4000, Step 69, d_loss: 0.3426237106323242, g_loss: 7.065774917602539\n","Epoch 349/4000, Step 70, d_loss: 0.34134650230407715, g_loss: 5.398852825164795\n","Epoch 349/4000, Step 71, d_loss: 0.3349609971046448, g_loss: 4.612354755401611\n","Epoch 349/4000, Step 72, d_loss: 0.3449183702468872, g_loss: 5.063717365264893\n","Epoch 349/4000, Step 73, d_loss: 0.341461718082428, g_loss: 4.761141300201416\n","Epoch 349/4000, Step 74, d_loss: 0.35113954544067383, g_loss: 4.457788467407227\n","Epoch 349/4000, Step 75, d_loss: 0.3386540114879608, g_loss: 6.851117134094238\n","Epoch 349/4000, Step 76, d_loss: 0.33517298102378845, g_loss: 6.060780048370361\n","Epoch 349/4000, Step 77, d_loss: 0.3322546184062958, g_loss: 6.837708950042725\n","Epoch 349/4000, Step 78, d_loss: 0.33183372020721436, g_loss: 6.309988021850586\n","Epoch 349/4000, Step 79, d_loss: 0.6186763644218445, g_loss: 4.690370559692383\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 350/4000, Step 1, d_loss: 0.3493061363697052, g_loss: 2.9131643772125244\n","Epoch 350/4000, Step 2, d_loss: 0.3879972994327545, g_loss: 3.4803810119628906\n","Epoch 350/4000, Step 3, d_loss: 0.4640532433986664, g_loss: 2.9851512908935547\n","Epoch 350/4000, Step 4, d_loss: 0.45577138662338257, g_loss: 3.57696533203125\n","Epoch 350/4000, Step 5, d_loss: 0.44677096605300903, g_loss: 5.857754707336426\n","Epoch 350/4000, Step 6, d_loss: 0.37558332085609436, g_loss: 5.565250396728516\n","Epoch 350/4000, Step 7, d_loss: 0.3504474461078644, g_loss: 5.847672939300537\n","Epoch 350/4000, Step 8, d_loss: 0.40323761105537415, g_loss: 5.984106540679932\n","Epoch 350/4000, Step 9, d_loss: 0.3824489414691925, g_loss: 7.416985988616943\n","Epoch 350/4000, Step 10, d_loss: 0.3574182093143463, g_loss: 5.972017765045166\n","Epoch 350/4000, Step 11, d_loss: 0.36315566301345825, g_loss: 7.441573619842529\n","Epoch 350/4000, Step 12, d_loss: 0.3566107451915741, g_loss: 4.384513854980469\n","Epoch 350/4000, Step 13, d_loss: 0.3715190887451172, g_loss: 3.7766714096069336\n","Epoch 350/4000, Step 14, d_loss: 0.40775737166404724, g_loss: 4.422489166259766\n","Epoch 350/4000, Step 15, d_loss: 0.3977257013320923, g_loss: 4.589644432067871\n","Epoch 350/4000, Step 16, d_loss: 0.3799746036529541, g_loss: 6.73270845413208\n","Epoch 350/4000, Step 17, d_loss: 0.35125216841697693, g_loss: 7.989922046661377\n","Epoch 350/4000, Step 18, d_loss: 0.3531196415424347, g_loss: 7.72967004776001\n","Epoch 350/4000, Step 19, d_loss: 0.35968726873397827, g_loss: 6.92350435256958\n","Epoch 350/4000, Step 20, d_loss: 0.3587348759174347, g_loss: 7.6134819984436035\n","Epoch 350/4000, Step 21, d_loss: 0.3361055850982666, g_loss: 6.168172359466553\n","Epoch 350/4000, Step 22, d_loss: 0.3409362733364105, g_loss: 6.238184452056885\n","Epoch 350/4000, Step 23, d_loss: 0.3546767234802246, g_loss: 5.367029190063477\n","Epoch 350/4000, Step 24, d_loss: 0.36293479800224304, g_loss: 6.478664875030518\n","Epoch 350/4000, Step 25, d_loss: 0.35444027185440063, g_loss: 6.432819366455078\n","Epoch 350/4000, Step 26, d_loss: 0.35114559531211853, g_loss: 5.289980888366699\n","Epoch 350/4000, Step 27, d_loss: 0.3407121002674103, g_loss: 6.081843376159668\n","Epoch 350/4000, Step 28, d_loss: 0.33783668279647827, g_loss: 5.4027099609375\n","Epoch 350/4000, Step 29, d_loss: 0.338952898979187, g_loss: 5.432147026062012\n","Epoch 350/4000, Step 30, d_loss: 0.3609164357185364, g_loss: 5.589869022369385\n","Epoch 350/4000, Step 31, d_loss: 0.3477475345134735, g_loss: 7.549440383911133\n","Epoch 350/4000, Step 32, d_loss: 0.35575249791145325, g_loss: 5.5848588943481445\n","Epoch 350/4000, Step 33, d_loss: 0.33658096194267273, g_loss: 5.816015243530273\n","Epoch 350/4000, Step 34, d_loss: 0.3357395827770233, g_loss: 6.852509498596191\n","Epoch 350/4000, Step 35, d_loss: 0.3440379500389099, g_loss: 6.121604919433594\n","Epoch 350/4000, Step 36, d_loss: 0.3460073471069336, g_loss: 5.6721110343933105\n","Epoch 350/4000, Step 37, d_loss: 0.3360251486301422, g_loss: 6.300992965698242\n","Epoch 350/4000, Step 38, d_loss: 0.3319924771785736, g_loss: 5.971916198730469\n","Epoch 350/4000, Step 39, d_loss: 0.34366777539253235, g_loss: 5.733295917510986\n","Epoch 350/4000, Step 40, d_loss: 0.3985522985458374, g_loss: 4.336294651031494\n","Epoch 350/4000, Step 41, d_loss: 0.346754252910614, g_loss: 4.8610663414001465\n","Epoch 350/4000, Step 42, d_loss: 0.35050153732299805, g_loss: 4.169417381286621\n","Epoch 350/4000, Step 43, d_loss: 0.37058380246162415, g_loss: 4.795839786529541\n","Epoch 350/4000, Step 44, d_loss: 0.37393811345100403, g_loss: 4.978817462921143\n","Epoch 350/4000, Step 45, d_loss: 0.3584514856338501, g_loss: 5.151618957519531\n","Epoch 350/4000, Step 46, d_loss: 0.33910316228866577, g_loss: 7.806847095489502\n","Epoch 350/4000, Step 47, d_loss: 0.33375611901283264, g_loss: 6.577232837677002\n","Epoch 350/4000, Step 48, d_loss: 0.35777121782302856, g_loss: 7.4172515869140625\n","Epoch 350/4000, Step 49, d_loss: 0.339199960231781, g_loss: 5.682532787322998\n","Epoch 350/4000, Step 50, d_loss: 0.34301453828811646, g_loss: 6.945413589477539\n","Epoch 350/4000, Step 51, d_loss: 0.3476966619491577, g_loss: 6.126791000366211\n","Epoch 350/4000, Step 52, d_loss: 0.33882832527160645, g_loss: 4.73745059967041\n","Epoch 350/4000, Step 53, d_loss: 0.34768348932266235, g_loss: 4.137207984924316\n","Epoch 350/4000, Step 54, d_loss: 0.3364900052547455, g_loss: 5.006826877593994\n","Epoch 350/4000, Step 55, d_loss: 0.3482353389263153, g_loss: 4.605581283569336\n","Epoch 350/4000, Step 56, d_loss: 0.3465029299259186, g_loss: 5.565504550933838\n","Epoch 350/4000, Step 57, d_loss: 0.3377266824245453, g_loss: 5.097121238708496\n","Epoch 350/4000, Step 58, d_loss: 0.3571875989437103, g_loss: 5.508566856384277\n","Epoch 350/4000, Step 59, d_loss: 0.3401513695716858, g_loss: 5.491322994232178\n","Epoch 350/4000, Step 60, d_loss: 0.3425838053226471, g_loss: 6.11554479598999\n","Epoch 350/4000, Step 61, d_loss: 0.3411211669445038, g_loss: 6.1961493492126465\n","Epoch 350/4000, Step 62, d_loss: 0.3434370160102844, g_loss: 6.233545303344727\n","Epoch 350/4000, Step 63, d_loss: 0.3329561948776245, g_loss: 5.841314792633057\n","Epoch 350/4000, Step 64, d_loss: 0.3845575451850891, g_loss: 5.11945915222168\n","Epoch 350/4000, Step 65, d_loss: 0.3385154902935028, g_loss: 4.5491108894348145\n","Epoch 350/4000, Step 66, d_loss: 0.3431572914123535, g_loss: 5.899591445922852\n","Epoch 350/4000, Step 67, d_loss: 0.3449319005012512, g_loss: 6.01581335067749\n","Epoch 350/4000, Step 68, d_loss: 0.34460023045539856, g_loss: 3.812826633453369\n","Epoch 350/4000, Step 69, d_loss: 0.35225966572761536, g_loss: 5.015585422515869\n","Epoch 350/4000, Step 70, d_loss: 0.3418940007686615, g_loss: 7.185693264007568\n","Epoch 350/4000, Step 71, d_loss: 0.34073513746261597, g_loss: 5.75969123840332\n","Epoch 350/4000, Step 72, d_loss: 0.3391578197479248, g_loss: 5.710625648498535\n","Epoch 350/4000, Step 73, d_loss: 0.34306085109710693, g_loss: 5.597801208496094\n","Epoch 350/4000, Step 74, d_loss: 0.35650745034217834, g_loss: 5.768454551696777\n","Epoch 350/4000, Step 75, d_loss: 0.340962290763855, g_loss: 3.762709617614746\n","Epoch 350/4000, Step 76, d_loss: 0.3540451228618622, g_loss: 3.7504191398620605\n","Epoch 350/4000, Step 77, d_loss: 0.36110201478004456, g_loss: 5.274612903594971\n","Epoch 350/4000, Step 78, d_loss: 0.34637391567230225, g_loss: 7.092006206512451\n","Epoch 350/4000, Step 79, d_loss: 0.37035176157951355, g_loss: 5.122798919677734\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 351/4000, Step 1, d_loss: 0.35522887110710144, g_loss: 5.067378997802734\n","Epoch 351/4000, Step 2, d_loss: 0.37680965662002563, g_loss: 4.029630184173584\n","Epoch 351/4000, Step 3, d_loss: 0.3381366729736328, g_loss: 7.730423450469971\n","Epoch 351/4000, Step 4, d_loss: 0.3421717584133148, g_loss: 6.455057621002197\n","Epoch 351/4000, Step 5, d_loss: 0.3448745906352997, g_loss: 6.578675746917725\n","Epoch 351/4000, Step 6, d_loss: 0.36493656039237976, g_loss: 5.244937896728516\n","Epoch 351/4000, Step 7, d_loss: 0.3486173748970032, g_loss: 5.608537197113037\n","Epoch 351/4000, Step 8, d_loss: 0.35385406017303467, g_loss: 5.461208820343018\n","Epoch 351/4000, Step 9, d_loss: 0.3506687879562378, g_loss: 4.456938743591309\n","Epoch 351/4000, Step 10, d_loss: 0.35094374418258667, g_loss: 4.652068138122559\n","Epoch 351/4000, Step 11, d_loss: 0.3438969850540161, g_loss: 5.77442741394043\n","Epoch 351/4000, Step 12, d_loss: 0.3434971570968628, g_loss: 4.781073570251465\n","Epoch 351/4000, Step 13, d_loss: 0.3608100414276123, g_loss: 4.906894683837891\n","Epoch 351/4000, Step 14, d_loss: 0.35757163166999817, g_loss: 5.717631816864014\n","Epoch 351/4000, Step 15, d_loss: 0.34395021200180054, g_loss: 5.462967872619629\n","Epoch 351/4000, Step 16, d_loss: 0.3642827868461609, g_loss: 7.409879684448242\n","Epoch 351/4000, Step 17, d_loss: 0.33588123321533203, g_loss: 8.277300834655762\n","Epoch 351/4000, Step 18, d_loss: 0.3451092839241028, g_loss: 5.571704864501953\n","Epoch 351/4000, Step 19, d_loss: 0.34628450870513916, g_loss: 4.720612525939941\n","Epoch 351/4000, Step 20, d_loss: 0.3456307351589203, g_loss: 6.469169616699219\n","Epoch 351/4000, Step 21, d_loss: 0.36228349804878235, g_loss: 4.2749528884887695\n","Epoch 351/4000, Step 22, d_loss: 0.3540230095386505, g_loss: 5.64976167678833\n","Epoch 351/4000, Step 23, d_loss: 0.3551690876483917, g_loss: 9.62346076965332\n","Epoch 351/4000, Step 24, d_loss: 0.3431479334831238, g_loss: 5.114463806152344\n","Epoch 351/4000, Step 25, d_loss: 0.3526325523853302, g_loss: 5.970808029174805\n","Epoch 351/4000, Step 26, d_loss: 0.3452484607696533, g_loss: 3.857863426208496\n","Epoch 351/4000, Step 27, d_loss: 0.3399050831794739, g_loss: 4.449088096618652\n","Epoch 351/4000, Step 28, d_loss: 0.3417307436466217, g_loss: 4.77134895324707\n","Epoch 351/4000, Step 29, d_loss: 0.3560500144958496, g_loss: 5.3346428871154785\n","Epoch 351/4000, Step 30, d_loss: 0.3536384403705597, g_loss: 5.107413291931152\n","Epoch 351/4000, Step 31, d_loss: 0.3523918688297272, g_loss: 6.20426082611084\n","Epoch 351/4000, Step 32, d_loss: 0.3644733428955078, g_loss: 6.080127716064453\n","Epoch 351/4000, Step 33, d_loss: 0.33316898345947266, g_loss: 5.881931304931641\n","Epoch 351/4000, Step 34, d_loss: 0.33908337354660034, g_loss: 8.45571231842041\n","Epoch 351/4000, Step 35, d_loss: 0.34306275844573975, g_loss: 4.843285083770752\n","Epoch 351/4000, Step 36, d_loss: 0.34303635358810425, g_loss: 5.94309663772583\n","Epoch 351/4000, Step 37, d_loss: 0.3426830470561981, g_loss: 9.373875617980957\n","Epoch 351/4000, Step 38, d_loss: 0.3442062437534332, g_loss: 7.048454761505127\n","Epoch 351/4000, Step 39, d_loss: 0.3342173397541046, g_loss: 10.12837028503418\n","Epoch 351/4000, Step 40, d_loss: 0.3332539498806, g_loss: 7.332080364227295\n","Epoch 351/4000, Step 41, d_loss: 0.33625635504722595, g_loss: 4.780516624450684\n","Epoch 351/4000, Step 42, d_loss: 0.36168935894966125, g_loss: 5.0506415367126465\n","Epoch 351/4000, Step 43, d_loss: 0.35415709018707275, g_loss: 4.62775182723999\n","Epoch 351/4000, Step 44, d_loss: 0.3401305079460144, g_loss: 3.8573520183563232\n","Epoch 351/4000, Step 45, d_loss: 0.3694209158420563, g_loss: 4.851808071136475\n","Epoch 351/4000, Step 46, d_loss: 0.34570321440696716, g_loss: 6.615976333618164\n","Epoch 351/4000, Step 47, d_loss: 0.3406207263469696, g_loss: 8.294221878051758\n","Epoch 351/4000, Step 48, d_loss: 0.33221107721328735, g_loss: 6.315033912658691\n","Epoch 351/4000, Step 49, d_loss: 0.34175431728363037, g_loss: 8.587059020996094\n","Epoch 351/4000, Step 50, d_loss: 0.33209118247032166, g_loss: 6.845558166503906\n","Epoch 351/4000, Step 51, d_loss: 0.33607885241508484, g_loss: 4.961564064025879\n","Epoch 351/4000, Step 52, d_loss: 0.34222349524497986, g_loss: 7.466493606567383\n","Epoch 351/4000, Step 53, d_loss: 0.3448296785354614, g_loss: 5.080430507659912\n","Epoch 351/4000, Step 54, d_loss: 0.3326895534992218, g_loss: 5.591217041015625\n","Epoch 351/4000, Step 55, d_loss: 0.3386125862598419, g_loss: 4.4957404136657715\n","Epoch 351/4000, Step 56, d_loss: 0.3362334966659546, g_loss: 10.514801025390625\n","Epoch 351/4000, Step 57, d_loss: 0.3343828320503235, g_loss: 5.806329250335693\n","Epoch 351/4000, Step 58, d_loss: 0.337573379278183, g_loss: 9.70602035522461\n","Epoch 351/4000, Step 59, d_loss: 0.3349558115005493, g_loss: 8.97340202331543\n","Epoch 351/4000, Step 60, d_loss: 0.33436378836631775, g_loss: 11.047958374023438\n","Epoch 351/4000, Step 61, d_loss: 0.3345613479614258, g_loss: 8.257057189941406\n","Epoch 351/4000, Step 62, d_loss: 0.3408220708370209, g_loss: 9.893631935119629\n","Epoch 351/4000, Step 63, d_loss: 0.3462039530277252, g_loss: 7.824133396148682\n","Epoch 351/4000, Step 64, d_loss: 0.33554795384407043, g_loss: 8.864386558532715\n","Epoch 351/4000, Step 65, d_loss: 0.3516556918621063, g_loss: 10.017110824584961\n","Epoch 351/4000, Step 66, d_loss: 0.3579326570034027, g_loss: 9.328657150268555\n","Epoch 351/4000, Step 67, d_loss: 0.3303471505641937, g_loss: 9.767802238464355\n","Epoch 351/4000, Step 68, d_loss: 0.3470231294631958, g_loss: 7.950836181640625\n","Epoch 351/4000, Step 69, d_loss: 0.3625061810016632, g_loss: 5.768929958343506\n","Epoch 351/4000, Step 70, d_loss: 0.35550811886787415, g_loss: 8.346413612365723\n","Epoch 351/4000, Step 71, d_loss: 0.3365859389305115, g_loss: 5.920160293579102\n","Epoch 351/4000, Step 72, d_loss: 0.35509610176086426, g_loss: 6.647918224334717\n","Epoch 351/4000, Step 73, d_loss: 0.35390704870224, g_loss: 7.152316570281982\n","Epoch 351/4000, Step 74, d_loss: 0.3350902199745178, g_loss: 8.084033012390137\n","Epoch 351/4000, Step 75, d_loss: 0.33769699931144714, g_loss: 7.130209922790527\n","Epoch 351/4000, Step 76, d_loss: 0.3629087209701538, g_loss: 5.127739906311035\n","Epoch 351/4000, Step 77, d_loss: 0.36963626742362976, g_loss: 5.589856147766113\n","Epoch 351/4000, Step 78, d_loss: 0.3417593538761139, g_loss: 6.32852840423584\n","Epoch 351/4000, Step 79, d_loss: 0.37387987971305847, g_loss: 6.212836742401123\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 352/4000, Step 1, d_loss: 0.3376465141773224, g_loss: 5.650488376617432\n","Epoch 352/4000, Step 2, d_loss: 0.349916011095047, g_loss: 4.655552387237549\n","Epoch 352/4000, Step 3, d_loss: 0.3569891154766083, g_loss: 6.473581314086914\n","Epoch 352/4000, Step 4, d_loss: 0.35724833607673645, g_loss: 7.496884346008301\n","Epoch 352/4000, Step 5, d_loss: 0.3616483509540558, g_loss: 5.069953441619873\n","Epoch 352/4000, Step 6, d_loss: 0.35726508498191833, g_loss: 6.0732550621032715\n","Epoch 352/4000, Step 7, d_loss: 0.3465312719345093, g_loss: 4.035485744476318\n","Epoch 352/4000, Step 8, d_loss: 0.376906156539917, g_loss: 5.274867534637451\n","Epoch 352/4000, Step 9, d_loss: 0.33645299077033997, g_loss: 7.239476680755615\n","Epoch 352/4000, Step 10, d_loss: 0.3375970423221588, g_loss: 3.2475905418395996\n","Epoch 352/4000, Step 11, d_loss: 0.35002508759498596, g_loss: 9.065702438354492\n","Epoch 352/4000, Step 12, d_loss: 0.36148789525032043, g_loss: 3.4297056198120117\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print (device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 32\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator was 32\n","ngf = 32\n","\n","# Size of feature maps in discriminator was 16\n","ndf = 160\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='G:/My Drive/Colab Notebooks/alby/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/ag2/',\n","                              transform=transforms.Compose([\n","                              transforms.Resize(image_size),\n","                              transforms.CenterCrop(image_size),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                              ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 16, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 16),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 16, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            nn.Conv2d(3, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.2),\n","            nn.Conv2d(ndf , ndf * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 2, ndf, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ndf),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(ndf, 1, 3, 1, 0, bias=False),\n","\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input).squeeze()\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.02)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.02)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='checkpointAt1.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","            save_image(samples, f'G:/My Drive/Colab Notebooks/dandies/new_dandies/Atra2_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","                current_batch_size = images.size(0)\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((current_batch_size,), .9))\n","\n","\n","\n","\n","\n","\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(current_batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs.view(-1), real_labels.view(-1))\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointAt1.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(4000, dataloader, resume=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1690605787603,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"n7TZvDVQTtN6","outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x500 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1GN2L9UPYR6TCzDsqMBaHwwc6PkLlmsyg","authorship_tag":"ABX9TyMy/qY7oP0brLEezBD/2J65"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}