{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"c9abfd63-4df7-41ca-cf0a-d0b36fa9e41e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 58/4000, Step 164, d_loss: 0.008467257022857666, g_loss: 5.1663970947265625\n","Epoch 58/4000, Step 165, d_loss: 0.01361881010234356, g_loss: 4.747148036956787\n","Epoch 58/4000, Step 166, d_loss: 0.0008498262614011765, g_loss: 3.9682538509368896\n","Epoch 58/4000, Step 167, d_loss: 0.01415366679430008, g_loss: 4.943769454956055\n","Epoch 58/4000, Step 168, d_loss: 0.028692172840237617, g_loss: 4.464747905731201\n","Epoch 58/4000, Step 169, d_loss: 0.0277729295194149, g_loss: 5.471779823303223\n","Epoch 58/4000, Step 170, d_loss: 0.010048474185168743, g_loss: 5.000634670257568\n","Epoch 58/4000, Step 171, d_loss: 0.004411941394209862, g_loss: 5.152468204498291\n","Epoch 58/4000, Step 172, d_loss: 0.0386405773460865, g_loss: 7.118078231811523\n","Epoch 58/4000, Step 173, d_loss: 0.00625674007460475, g_loss: 6.10054349899292\n","Epoch 58/4000, Step 174, d_loss: 0.005135837942361832, g_loss: 7.524270057678223\n","Epoch 58/4000, Step 175, d_loss: 0.00455703167244792, g_loss: 4.563115119934082\n","Epoch 58/4000, Step 176, d_loss: 0.0042036850936710835, g_loss: 6.377946853637695\n","Epoch 58/4000, Step 177, d_loss: 0.008659088984131813, g_loss: 4.700527667999268\n","Epoch 58/4000, Step 178, d_loss: 0.007303998339921236, g_loss: 5.2591118812561035\n","Epoch 58/4000, Step 179, d_loss: 0.0141269750893116, g_loss: 4.592780113220215\n","Epoch 58/4000, Step 180, d_loss: 0.01905158720910549, g_loss: 5.078851699829102\n","Epoch 58/4000, Step 181, d_loss: 0.016300197690725327, g_loss: 6.651942253112793\n","Epoch 58/4000, Step 182, d_loss: 0.0657910704612732, g_loss: 4.806295871734619\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 59/4000, Step 1, d_loss: 0.0031445969361811876, g_loss: 4.9445719718933105\n","Epoch 59/4000, Step 2, d_loss: 0.0156350489705801, g_loss: 2.726555347442627\n","Epoch 59/4000, Step 3, d_loss: 0.024263910949230194, g_loss: 3.49630069732666\n","Epoch 59/4000, Step 4, d_loss: 0.05820307508111, g_loss: 5.264415264129639\n","Epoch 59/4000, Step 5, d_loss: 0.0014543711440637708, g_loss: 3.8288049697875977\n","Epoch 59/4000, Step 6, d_loss: 0.021834302693605423, g_loss: 5.712850093841553\n","Epoch 59/4000, Step 7, d_loss: 0.010490045882761478, g_loss: 3.8520424365997314\n","Epoch 59/4000, Step 8, d_loss: 0.01278819888830185, g_loss: 3.3477702140808105\n","Epoch 59/4000, Step 9, d_loss: 0.016668811440467834, g_loss: 5.777520179748535\n","Epoch 59/4000, Step 10, d_loss: 0.006256551016122103, g_loss: 5.230778694152832\n","Epoch 59/4000, Step 11, d_loss: 0.005702214781194925, g_loss: 4.348292350769043\n","Epoch 59/4000, Step 12, d_loss: 0.00192416668869555, g_loss: 6.4333696365356445\n","Epoch 59/4000, Step 13, d_loss: 0.003011916996911168, g_loss: 5.618011474609375\n","Epoch 59/4000, Step 14, d_loss: 0.008779971860349178, g_loss: 5.692634582519531\n","Epoch 59/4000, Step 15, d_loss: 0.011508136987686157, g_loss: 6.421717166900635\n","Epoch 59/4000, Step 16, d_loss: 0.025564488023519516, g_loss: 5.663578987121582\n","Epoch 59/4000, Step 17, d_loss: 0.014767937362194061, g_loss: 8.532794952392578\n","Epoch 59/4000, Step 18, d_loss: 0.03073669783771038, g_loss: 6.371715545654297\n","Epoch 59/4000, Step 19, d_loss: 0.004774569068104029, g_loss: 8.203332901000977\n","Epoch 59/4000, Step 20, d_loss: 0.007959399372339249, g_loss: 7.193413257598877\n","Epoch 59/4000, Step 21, d_loss: 0.007796085439622402, g_loss: 4.70553731918335\n","Epoch 59/4000, Step 22, d_loss: 0.014021720737218857, g_loss: 7.170475959777832\n","Epoch 59/4000, Step 23, d_loss: 0.01747029460966587, g_loss: 5.825815200805664\n","Epoch 59/4000, Step 24, d_loss: 0.013770898804068565, g_loss: 3.5439395904541016\n","Epoch 59/4000, Step 25, d_loss: 0.009226595982909203, g_loss: 4.794198989868164\n","Epoch 59/4000, Step 26, d_loss: 0.012192896567285061, g_loss: 5.263377666473389\n","Epoch 59/4000, Step 27, d_loss: 0.02281498908996582, g_loss: 6.407503604888916\n","Epoch 59/4000, Step 28, d_loss: 0.011064534075558186, g_loss: 5.384327411651611\n","Epoch 59/4000, Step 29, d_loss: 0.005721466150134802, g_loss: 5.297689437866211\n","Epoch 59/4000, Step 30, d_loss: 0.004207552410662174, g_loss: 5.076838493347168\n","Epoch 59/4000, Step 31, d_loss: 0.01536552608013153, g_loss: 5.963788032531738\n","Epoch 59/4000, Step 32, d_loss: 0.009731117635965347, g_loss: 5.496935844421387\n","Epoch 59/4000, Step 33, d_loss: 0.0029267638456076384, g_loss: 5.460734844207764\n","Epoch 59/4000, Step 34, d_loss: 0.005130999255925417, g_loss: 5.678102970123291\n","Epoch 59/4000, Step 35, d_loss: 0.0037772797513753176, g_loss: 8.24889087677002\n","Epoch 59/4000, Step 36, d_loss: 0.0065169367007911205, g_loss: 6.768077373504639\n","Epoch 59/4000, Step 37, d_loss: 0.008046016097068787, g_loss: 3.6165595054626465\n","Epoch 59/4000, Step 38, d_loss: 0.004254464991390705, g_loss: 6.205526828765869\n","Epoch 59/4000, Step 39, d_loss: 0.0031742872670292854, g_loss: 6.298970699310303\n","Epoch 59/4000, Step 40, d_loss: 0.007188059389591217, g_loss: 5.222255706787109\n","Epoch 59/4000, Step 41, d_loss: 0.002632007235661149, g_loss: 6.257492542266846\n","Epoch 59/4000, Step 42, d_loss: 0.00937764160335064, g_loss: 5.600415229797363\n","Epoch 59/4000, Step 43, d_loss: 0.012581891380250454, g_loss: 7.8760576248168945\n","Epoch 59/4000, Step 44, d_loss: 0.026033394038677216, g_loss: 5.636185646057129\n","Epoch 59/4000, Step 45, d_loss: 0.0011988356709480286, g_loss: 5.239156723022461\n","Epoch 59/4000, Step 46, d_loss: 0.004251978360116482, g_loss: 7.4788103103637695\n","Epoch 59/4000, Step 47, d_loss: 0.007518175523728132, g_loss: 5.424956798553467\n","Epoch 59/4000, Step 48, d_loss: 0.01081469189375639, g_loss: 5.618650913238525\n","Epoch 59/4000, Step 49, d_loss: 0.011785218492150307, g_loss: 6.390420436859131\n","Epoch 59/4000, Step 50, d_loss: 0.012531069107353687, g_loss: 5.358863353729248\n","Epoch 59/4000, Step 51, d_loss: 0.0024154330603778362, g_loss: 5.209893226623535\n","Epoch 59/4000, Step 52, d_loss: 0.00646465877071023, g_loss: 6.58263635635376\n","Epoch 59/4000, Step 53, d_loss: 0.005263288505375385, g_loss: 5.599372386932373\n","Epoch 59/4000, Step 54, d_loss: 0.06532657891511917, g_loss: 6.206697940826416\n","Epoch 59/4000, Step 55, d_loss: 0.002989373402670026, g_loss: 5.880152225494385\n","Epoch 59/4000, Step 56, d_loss: 0.010859359055757523, g_loss: 3.9962143898010254\n","Epoch 59/4000, Step 57, d_loss: 0.002629708033055067, g_loss: 5.92666482925415\n","Epoch 59/4000, Step 58, d_loss: 0.0072005875408649445, g_loss: 4.419894218444824\n","Epoch 59/4000, Step 59, d_loss: 0.0065175388008356094, g_loss: 5.490071773529053\n","Epoch 59/4000, Step 60, d_loss: 0.01614900305867195, g_loss: 6.423605442047119\n","Epoch 59/4000, Step 61, d_loss: 0.0171970222145319, g_loss: 3.935441732406616\n","Epoch 59/4000, Step 62, d_loss: 0.07781381160020828, g_loss: 5.347211837768555\n","Epoch 59/4000, Step 63, d_loss: 0.01716485247015953, g_loss: 6.000705718994141\n","Epoch 59/4000, Step 64, d_loss: 0.016609063372015953, g_loss: 4.536657333374023\n","Epoch 59/4000, Step 65, d_loss: 0.006919011007994413, g_loss: 6.131181240081787\n","Epoch 59/4000, Step 66, d_loss: 0.008005778305232525, g_loss: 5.964798450469971\n","Epoch 59/4000, Step 67, d_loss: 0.01053039450198412, g_loss: 4.188133239746094\n","Epoch 59/4000, Step 68, d_loss: 0.003850666806101799, g_loss: 4.505798816680908\n","Epoch 59/4000, Step 69, d_loss: 0.006224713288247585, g_loss: 4.604962348937988\n","Epoch 59/4000, Step 70, d_loss: 0.0016137110069394112, g_loss: 5.573895454406738\n","Epoch 59/4000, Step 71, d_loss: 0.014298679307103157, g_loss: 5.890286445617676\n","Epoch 59/4000, Step 72, d_loss: 0.005380680318921804, g_loss: 4.730271816253662\n","Epoch 59/4000, Step 73, d_loss: 0.010691432282328606, g_loss: 6.434625625610352\n","Epoch 59/4000, Step 74, d_loss: 0.00545260775834322, g_loss: 9.134478569030762\n","Epoch 59/4000, Step 75, d_loss: 0.07850141078233719, g_loss: 6.348809719085693\n","Epoch 59/4000, Step 76, d_loss: 0.014733899384737015, g_loss: 6.806550025939941\n","Epoch 59/4000, Step 77, d_loss: 0.02318725734949112, g_loss: 5.329378604888916\n","Epoch 59/4000, Step 78, d_loss: 0.006119284313172102, g_loss: 5.587078094482422\n","Epoch 59/4000, Step 79, d_loss: 0.004254336468875408, g_loss: 4.685122966766357\n","Epoch 59/4000, Step 80, d_loss: 0.009492078796029091, g_loss: 6.305411338806152\n","Epoch 59/4000, Step 81, d_loss: 0.007671252824366093, g_loss: 6.213963508605957\n","Epoch 59/4000, Step 82, d_loss: 0.024638963863253593, g_loss: 5.644789218902588\n","Epoch 59/4000, Step 83, d_loss: 0.006310196593403816, g_loss: 4.780253887176514\n","Epoch 59/4000, Step 84, d_loss: 0.001205431530252099, g_loss: 4.572424411773682\n","Epoch 59/4000, Step 85, d_loss: 0.008798926137387753, g_loss: 5.225700378417969\n","Epoch 59/4000, Step 86, d_loss: 0.0026368522085249424, g_loss: 5.179059028625488\n","Epoch 59/4000, Step 87, d_loss: 0.017701683565974236, g_loss: 3.078578472137451\n","Epoch 59/4000, Step 88, d_loss: 0.014816949144005775, g_loss: 4.555161952972412\n","Epoch 59/4000, Step 89, d_loss: 0.037733402103185654, g_loss: 4.001804351806641\n","Epoch 59/4000, Step 90, d_loss: 0.008869263343513012, g_loss: 6.07530403137207\n","Epoch 59/4000, Step 91, d_loss: 0.007206271402537823, g_loss: 7.558070659637451\n","Epoch 59/4000, Step 92, d_loss: 0.010239583440124989, g_loss: 7.154974460601807\n","Epoch 59/4000, Step 93, d_loss: 0.015096292831003666, g_loss: 5.065417766571045\n","Epoch 59/4000, Step 94, d_loss: 0.006870713550597429, g_loss: 5.9165940284729\n","Epoch 59/4000, Step 95, d_loss: 0.012841828167438507, g_loss: 5.480169773101807\n","Epoch 59/4000, Step 96, d_loss: 0.01939845085144043, g_loss: 6.975678443908691\n","Epoch 59/4000, Step 97, d_loss: 0.008751262910664082, g_loss: 5.9142656326293945\n","Epoch 59/4000, Step 98, d_loss: 0.025521185249090195, g_loss: 7.279059410095215\n","Epoch 59/4000, Step 99, d_loss: 0.016023067757487297, g_loss: 5.795115947723389\n","Epoch 59/4000, Step 100, d_loss: 0.02688795141875744, g_loss: 4.757916450500488\n","Epoch 59/4000, Step 101, d_loss: 0.02546571008861065, g_loss: 5.32119607925415\n","Epoch 59/4000, Step 102, d_loss: 0.011172464117407799, g_loss: 4.08791446685791\n","Epoch 59/4000, Step 103, d_loss: 0.0013959938660264015, g_loss: 6.771243572235107\n","Epoch 59/4000, Step 104, d_loss: 0.01798367127776146, g_loss: 5.606196880340576\n","Epoch 59/4000, Step 105, d_loss: 0.01591319404542446, g_loss: 3.763089418411255\n","Epoch 59/4000, Step 106, d_loss: 0.0030490714125335217, g_loss: 5.861789226531982\n","Epoch 59/4000, Step 107, d_loss: 0.009083733893930912, g_loss: 7.549600124359131\n","Epoch 59/4000, Step 108, d_loss: 0.00610731914639473, g_loss: 5.711194038391113\n","Epoch 59/4000, Step 109, d_loss: 0.0006752497283741832, g_loss: 5.503765106201172\n","Epoch 59/4000, Step 110, d_loss: 0.01688368059694767, g_loss: 8.470260620117188\n","Epoch 59/4000, Step 111, d_loss: 0.0032511716708540916, g_loss: 5.505717754364014\n","Epoch 59/4000, Step 112, d_loss: 0.007065820042043924, g_loss: 4.971343517303467\n","Epoch 59/4000, Step 113, d_loss: 0.004282772541046143, g_loss: 6.510093688964844\n","Epoch 59/4000, Step 114, d_loss: 0.004848023876547813, g_loss: 4.824846267700195\n","Epoch 59/4000, Step 115, d_loss: 0.005097710527479649, g_loss: 5.589061737060547\n","Epoch 59/4000, Step 116, d_loss: 0.005751489661633968, g_loss: 5.5584306716918945\n","Epoch 59/4000, Step 117, d_loss: 0.002966099651530385, g_loss: 4.482929706573486\n","Epoch 59/4000, Step 118, d_loss: 0.011729495599865913, g_loss: 5.528681755065918\n","Epoch 59/4000, Step 119, d_loss: 0.023616088554263115, g_loss: 6.546095371246338\n","Epoch 59/4000, Step 120, d_loss: 0.008542454801499844, g_loss: 5.842988014221191\n","Epoch 59/4000, Step 121, d_loss: 0.00418438296765089, g_loss: 4.448210716247559\n","Epoch 59/4000, Step 122, d_loss: 0.0050411345437169075, g_loss: 4.914705753326416\n","Epoch 59/4000, Step 123, d_loss: 0.006970090325921774, g_loss: 3.7155730724334717\n","Epoch 59/4000, Step 124, d_loss: 0.004125156439840794, g_loss: 5.465909481048584\n","Epoch 59/4000, Step 125, d_loss: 0.004571771714836359, g_loss: 7.317408561706543\n","Epoch 59/4000, Step 126, d_loss: 0.0025167036801576614, g_loss: 5.922971725463867\n","Epoch 59/4000, Step 127, d_loss: 0.019929351285099983, g_loss: 7.7111992835998535\n","Epoch 59/4000, Step 128, d_loss: 0.0055785998702049255, g_loss: 5.632564067840576\n","Epoch 59/4000, Step 129, d_loss: 0.003528821747750044, g_loss: 3.8005740642547607\n","Epoch 59/4000, Step 130, d_loss: 0.0012031113728880882, g_loss: 3.914700984954834\n","Epoch 59/4000, Step 131, d_loss: 0.014082755893468857, g_loss: 5.614588737487793\n","Epoch 59/4000, Step 132, d_loss: 0.0015024901367723942, g_loss: 3.619469404220581\n","Epoch 59/4000, Step 133, d_loss: 0.006080660969018936, g_loss: 7.173556804656982\n","Epoch 59/4000, Step 134, d_loss: 0.01174184400588274, g_loss: 5.1570634841918945\n","Epoch 59/4000, Step 135, d_loss: 0.012269999831914902, g_loss: 6.48311710357666\n","Epoch 59/4000, Step 136, d_loss: 0.0053545027039945126, g_loss: 6.614602088928223\n","Epoch 59/4000, Step 137, d_loss: 0.0056563434191048145, g_loss: 7.287092685699463\n","Epoch 59/4000, Step 138, d_loss: 0.007984810508787632, g_loss: 5.93385124206543\n","Epoch 59/4000, Step 139, d_loss: 0.0013742337469011545, g_loss: 5.672488689422607\n","Epoch 59/4000, Step 140, d_loss: 0.0034470180980861187, g_loss: 5.827541351318359\n","Epoch 59/4000, Step 141, d_loss: 0.0032453546300530434, g_loss: 7.208278179168701\n","Epoch 59/4000, Step 142, d_loss: 0.0035134050995111465, g_loss: 7.010579586029053\n","Epoch 59/4000, Step 143, d_loss: 0.002021601889282465, g_loss: 6.033729076385498\n","Epoch 59/4000, Step 144, d_loss: 0.007237804122269154, g_loss: 3.7792954444885254\n","Epoch 59/4000, Step 145, d_loss: 0.03938133642077446, g_loss: 4.396841049194336\n","Epoch 59/4000, Step 146, d_loss: 0.004080560058355331, g_loss: 5.868199348449707\n","Epoch 59/4000, Step 147, d_loss: 0.005219477228820324, g_loss: 4.810225963592529\n","Epoch 59/4000, Step 148, d_loss: 0.004057826474308968, g_loss: 5.990048408508301\n","Epoch 59/4000, Step 149, d_loss: 0.03293756768107414, g_loss: 4.526622295379639\n","Epoch 59/4000, Step 150, d_loss: 0.0057678427547216415, g_loss: 8.97646427154541\n","Epoch 59/4000, Step 151, d_loss: 0.0030154811684042215, g_loss: 6.096295356750488\n","Epoch 59/4000, Step 152, d_loss: 0.007142497226595879, g_loss: 6.242488384246826\n","Epoch 59/4000, Step 153, d_loss: 0.0012657870538532734, g_loss: 4.893166542053223\n","Epoch 59/4000, Step 154, d_loss: 0.018031209707260132, g_loss: 5.719342231750488\n","Epoch 59/4000, Step 155, d_loss: 0.007379962131381035, g_loss: 5.8689188957214355\n","Epoch 59/4000, Step 156, d_loss: 0.002508766483515501, g_loss: 3.7616777420043945\n","Epoch 59/4000, Step 157, d_loss: 0.00340200774371624, g_loss: 5.377336025238037\n","Epoch 59/4000, Step 158, d_loss: 0.024777209386229515, g_loss: 6.497391223907471\n","Epoch 59/4000, Step 159, d_loss: 0.0006184229860082269, g_loss: 4.477479934692383\n","Epoch 59/4000, Step 160, d_loss: 0.013570316135883331, g_loss: 4.484856605529785\n","Epoch 59/4000, Step 161, d_loss: 0.011630658060312271, g_loss: 5.14548397064209\n","Epoch 59/4000, Step 162, d_loss: 0.006476309150457382, g_loss: 6.464590549468994\n","Epoch 59/4000, Step 163, d_loss: 0.006605624221265316, g_loss: 5.9726715087890625\n","Epoch 59/4000, Step 164, d_loss: 0.019528141245245934, g_loss: 4.907902717590332\n","Epoch 59/4000, Step 165, d_loss: 0.007059540133923292, g_loss: 5.360556125640869\n","Epoch 59/4000, Step 166, d_loss: 0.0027900959830731153, g_loss: 4.700666904449463\n","Epoch 59/4000, Step 167, d_loss: 0.002964417915791273, g_loss: 4.468756675720215\n","Epoch 59/4000, Step 168, d_loss: 0.001584248966537416, g_loss: 7.257404804229736\n","Epoch 59/4000, Step 169, d_loss: 0.01682373136281967, g_loss: 3.8901801109313965\n","Epoch 59/4000, Step 170, d_loss: 0.01002818439155817, g_loss: 5.578192234039307\n","Epoch 59/4000, Step 171, d_loss: 0.01732809655368328, g_loss: 7.160549640655518\n","Epoch 59/4000, Step 172, d_loss: 0.008187245577573776, g_loss: 5.177100658416748\n","Epoch 59/4000, Step 173, d_loss: 0.01403859443962574, g_loss: 5.675075531005859\n","Epoch 59/4000, Step 174, d_loss: 0.007891533896327019, g_loss: 5.3597917556762695\n","Epoch 59/4000, Step 175, d_loss: 0.011068031191825867, g_loss: 5.966753005981445\n","Epoch 59/4000, Step 176, d_loss: 0.0515383780002594, g_loss: 8.709370613098145\n","Epoch 59/4000, Step 177, d_loss: 0.023119088262319565, g_loss: 4.156754970550537\n","Epoch 59/4000, Step 178, d_loss: 0.0020714360289275646, g_loss: 4.075616359710693\n","Epoch 59/4000, Step 179, d_loss: 0.004190294072031975, g_loss: 6.283255100250244\n","Epoch 59/4000, Step 180, d_loss: 0.010376956313848495, g_loss: 4.974674224853516\n","Epoch 59/4000, Step 181, d_loss: 0.006187690421938896, g_loss: 4.426680088043213\n","Epoch 59/4000, Step 182, d_loss: 0.015738671645522118, g_loss: 4.638349533081055\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 60/4000, Step 1, d_loss: 0.005652561783790588, g_loss: 4.2219014167785645\n","Epoch 60/4000, Step 2, d_loss: 0.004741320386528969, g_loss: 5.213332176208496\n","Epoch 60/4000, Step 3, d_loss: 0.002521875314414501, g_loss: 5.126517295837402\n","Epoch 60/4000, Step 4, d_loss: 0.03538014739751816, g_loss: 9.250140190124512\n","Epoch 60/4000, Step 5, d_loss: 0.06522001326084137, g_loss: 4.564740180969238\n","Epoch 60/4000, Step 6, d_loss: 0.00598561018705368, g_loss: 5.460178375244141\n","Epoch 60/4000, Step 7, d_loss: 0.010322090238332748, g_loss: 6.044802665710449\n","Epoch 60/4000, Step 8, d_loss: 0.005688846576958895, g_loss: 5.9858198165893555\n","Epoch 60/4000, Step 9, d_loss: 0.0011596416588872671, g_loss: 5.775201797485352\n","Epoch 60/4000, Step 10, d_loss: 0.003246954409405589, g_loss: 7.883096694946289\n","Epoch 60/4000, Step 11, d_loss: 0.0066200075671076775, g_loss: 6.994543552398682\n","Epoch 60/4000, Step 12, d_loss: 0.004927433095872402, g_loss: 5.427436828613281\n","Epoch 60/4000, Step 13, d_loss: 0.008357704617083073, g_loss: 6.060616493225098\n","Epoch 60/4000, Step 14, d_loss: 0.008538492023944855, g_loss: 4.887965679168701\n","Epoch 60/4000, Step 15, d_loss: 0.004505554214119911, g_loss: 8.699430465698242\n","Epoch 60/4000, Step 16, d_loss: 0.01566910557448864, g_loss: 5.845717430114746\n","Epoch 60/4000, Step 17, d_loss: 0.010926766321063042, g_loss: 6.798778057098389\n","Epoch 60/4000, Step 18, d_loss: 0.00417584041133523, g_loss: 6.258351802825928\n","Epoch 60/4000, Step 19, d_loss: 0.0022194264456629753, g_loss: 6.608729362487793\n","Epoch 60/4000, Step 20, d_loss: 0.00775054469704628, g_loss: 4.9691162109375\n","Epoch 60/4000, Step 21, d_loss: 0.013921275734901428, g_loss: 6.160953044891357\n","Epoch 60/4000, Step 22, d_loss: 0.007114002946764231, g_loss: 8.353715896606445\n","Epoch 60/4000, Step 23, d_loss: 0.008197598159313202, g_loss: 7.204380512237549\n","Epoch 60/4000, Step 24, d_loss: 0.0044333478435873985, g_loss: 6.592099666595459\n","Epoch 60/4000, Step 25, d_loss: 0.0033528644125908613, g_loss: 4.904642105102539\n","Epoch 60/4000, Step 26, d_loss: 0.013399169780313969, g_loss: 4.836668014526367\n","Epoch 60/4000, Step 27, d_loss: 0.006997525226324797, g_loss: 4.572770595550537\n","Epoch 60/4000, Step 28, d_loss: 0.0036732617300003767, g_loss: 5.45074987411499\n","Epoch 60/4000, Step 29, d_loss: 0.007452937308698893, g_loss: 5.535735130310059\n","Epoch 60/4000, Step 30, d_loss: 0.005595884285867214, g_loss: 6.0347089767456055\n","Epoch 60/4000, Step 31, d_loss: 0.016278013586997986, g_loss: 3.2382652759552\n","Epoch 60/4000, Step 32, d_loss: 0.00740119069814682, g_loss: 7.800198554992676\n","Epoch 60/4000, Step 33, d_loss: 0.0009718334767967463, g_loss: 7.65681266784668\n","Epoch 60/4000, Step 34, d_loss: 0.022375356405973434, g_loss: 6.0791850090026855\n","Epoch 60/4000, Step 35, d_loss: 0.008968756534159184, g_loss: 5.388392925262451\n","Epoch 60/4000, Step 36, d_loss: 0.007035057060420513, g_loss: 5.64939546585083\n","Epoch 60/4000, Step 37, d_loss: 0.016989540308713913, g_loss: 6.05142068862915\n","Epoch 60/4000, Step 38, d_loss: 0.06110723316669464, g_loss: 5.724908828735352\n","Epoch 60/4000, Step 39, d_loss: 0.007651009596884251, g_loss: 6.235588550567627\n","Epoch 60/4000, Step 40, d_loss: 0.006641827989369631, g_loss: 5.722179889678955\n","Epoch 60/4000, Step 41, d_loss: 0.000604110537096858, g_loss: 5.69227409362793\n","Epoch 60/4000, Step 42, d_loss: 0.04629657417535782, g_loss: 6.3095784187316895\n","Epoch 60/4000, Step 43, d_loss: 0.00583203649148345, g_loss: 8.004505157470703\n","Epoch 60/4000, Step 44, d_loss: 0.008031494915485382, g_loss: 7.888052940368652\n","Epoch 60/4000, Step 45, d_loss: 0.005361504387110472, g_loss: 6.748251438140869\n","Epoch 60/4000, Step 46, d_loss: 0.011400364339351654, g_loss: 5.770894527435303\n","Epoch 60/4000, Step 47, d_loss: 0.011664018034934998, g_loss: 5.598652362823486\n","Epoch 60/4000, Step 48, d_loss: 0.004554685205221176, g_loss: 7.3962016105651855\n","Epoch 60/4000, Step 49, d_loss: 0.0015195601154118776, g_loss: 7.353754043579102\n","Epoch 60/4000, Step 50, d_loss: 0.003864554688334465, g_loss: 6.365703105926514\n","Epoch 60/4000, Step 51, d_loss: 0.005667679477483034, g_loss: 3.81546688079834\n","Epoch 60/4000, Step 52, d_loss: 0.012671830132603645, g_loss: 6.6603569984436035\n","Epoch 60/4000, Step 53, d_loss: 0.014471755363047123, g_loss: 4.995278835296631\n","Epoch 60/4000, Step 54, d_loss: 0.005057967733591795, g_loss: 6.494653224945068\n","Epoch 60/4000, Step 55, d_loss: 0.011554483324289322, g_loss: 7.043704986572266\n","Epoch 60/4000, Step 56, d_loss: 0.005928656551986933, g_loss: 7.604424953460693\n","Epoch 60/4000, Step 57, d_loss: 0.00521803880110383, g_loss: 6.809219837188721\n","Epoch 60/4000, Step 58, d_loss: 0.0006592412828467786, g_loss: 7.54306173324585\n","Epoch 60/4000, Step 59, d_loss: 0.009191463701426983, g_loss: 5.181247711181641\n","Epoch 60/4000, Step 60, d_loss: 0.003995991311967373, g_loss: 6.370434284210205\n","Epoch 60/4000, Step 61, d_loss: 0.020843125879764557, g_loss: 6.500289440155029\n","Epoch 60/4000, Step 62, d_loss: 0.006646803580224514, g_loss: 5.489118576049805\n","Epoch 60/4000, Step 63, d_loss: 0.01604212261736393, g_loss: 7.27940559387207\n","Epoch 60/4000, Step 64, d_loss: 0.00380204850807786, g_loss: 5.812649726867676\n","Epoch 60/4000, Step 65, d_loss: 0.006341814063489437, g_loss: 7.022995471954346\n","Epoch 60/4000, Step 66, d_loss: 0.0031789205968379974, g_loss: 6.651817798614502\n","Epoch 60/4000, Step 67, d_loss: 0.004211912862956524, g_loss: 8.764403343200684\n","Epoch 60/4000, Step 68, d_loss: 0.0016061062924563885, g_loss: 5.076494216918945\n","Epoch 60/4000, Step 69, d_loss: 0.00528664281591773, g_loss: 3.9914145469665527\n","Epoch 60/4000, Step 70, d_loss: 0.006849895231425762, g_loss: 4.574146270751953\n","Epoch 60/4000, Step 71, d_loss: 0.004754773806780577, g_loss: 6.228578567504883\n","Epoch 60/4000, Step 72, d_loss: 0.003973499871790409, g_loss: 7.126338005065918\n","Epoch 60/4000, Step 73, d_loss: 0.005127784330397844, g_loss: 6.031763076782227\n","Epoch 60/4000, Step 74, d_loss: 0.008734403178095818, g_loss: 5.424276351928711\n","Epoch 60/4000, Step 75, d_loss: 0.0035538896918296814, g_loss: 6.96830940246582\n","Epoch 60/4000, Step 76, d_loss: 0.0017455712659284472, g_loss: 6.500425815582275\n","Epoch 60/4000, Step 77, d_loss: 0.0072656720876693726, g_loss: 7.752499103546143\n","Epoch 60/4000, Step 78, d_loss: 0.000907339621335268, g_loss: 6.922991752624512\n","Epoch 60/4000, Step 79, d_loss: 0.019781559705734253, g_loss: 6.565081596374512\n","Epoch 60/4000, Step 80, d_loss: 0.014158111065626144, g_loss: 6.129426956176758\n","Epoch 60/4000, Step 81, d_loss: 0.0022585431579500437, g_loss: 5.673315048217773\n","Epoch 60/4000, Step 82, d_loss: 0.009402974508702755, g_loss: 9.07413101196289\n","Epoch 60/4000, Step 83, d_loss: 0.02258261665701866, g_loss: 7.621660232543945\n","Epoch 60/4000, Step 84, d_loss: 0.07186627388000488, g_loss: 6.892335414886475\n","Epoch 60/4000, Step 85, d_loss: 0.0017748642712831497, g_loss: 6.16404914855957\n","Epoch 60/4000, Step 86, d_loss: 0.007081311196088791, g_loss: 6.466820240020752\n","Epoch 60/4000, Step 87, d_loss: 0.0036045112647116184, g_loss: 4.150495529174805\n","Epoch 60/4000, Step 88, d_loss: 0.003020576201379299, g_loss: 7.255800724029541\n","Epoch 60/4000, Step 89, d_loss: 0.016441769897937775, g_loss: 4.527434349060059\n","Epoch 60/4000, Step 90, d_loss: 0.02097535878419876, g_loss: 4.720606803894043\n","Epoch 60/4000, Step 91, d_loss: 0.001974239945411682, g_loss: 3.7442970275878906\n","Epoch 60/4000, Step 92, d_loss: 0.016750192269682884, g_loss: 2.935056209564209\n","Epoch 60/4000, Step 93, d_loss: 0.011484929360449314, g_loss: 3.4378468990325928\n","Epoch 60/4000, Step 94, d_loss: 0.01450805552303791, g_loss: 3.9040369987487793\n","Epoch 60/4000, Step 95, d_loss: 0.009672240354120731, g_loss: 4.191988468170166\n","Epoch 60/4000, Step 96, d_loss: 0.001367486547678709, g_loss: 3.549497127532959\n","Epoch 60/4000, Step 97, d_loss: 0.021129166707396507, g_loss: 5.115672588348389\n","Epoch 60/4000, Step 98, d_loss: 0.005654288921505213, g_loss: 8.480856895446777\n","Epoch 60/4000, Step 99, d_loss: 0.021039912477135658, g_loss: 2.7825818061828613\n","Epoch 60/4000, Step 100, d_loss: 0.006283336319029331, g_loss: 5.6537017822265625\n","Epoch 60/4000, Step 101, d_loss: 0.0013803270412608981, g_loss: 5.399193286895752\n","Epoch 60/4000, Step 102, d_loss: 0.003927705343812704, g_loss: 6.5443243980407715\n","Epoch 60/4000, Step 103, d_loss: 0.003379589645192027, g_loss: 6.378424167633057\n","Epoch 60/4000, Step 104, d_loss: 0.01930505968630314, g_loss: 4.6405181884765625\n","Epoch 60/4000, Step 105, d_loss: 0.002710487926378846, g_loss: 6.02191686630249\n","Epoch 60/4000, Step 106, d_loss: 0.0014303700299933553, g_loss: 5.0730767250061035\n","Epoch 60/4000, Step 107, d_loss: 0.008415209129452705, g_loss: 6.194782257080078\n","Epoch 60/4000, Step 108, d_loss: 0.002133679809048772, g_loss: 4.983417510986328\n","Epoch 60/4000, Step 109, d_loss: 0.0069529153406620026, g_loss: 5.700349807739258\n","Epoch 60/4000, Step 110, d_loss: 0.0075157033279538155, g_loss: 5.280246734619141\n","Epoch 60/4000, Step 111, d_loss: 0.004951885435730219, g_loss: 4.569226264953613\n","Epoch 60/4000, Step 112, d_loss: 0.004930850584059954, g_loss: 5.232161521911621\n","Epoch 60/4000, Step 113, d_loss: 0.014094172045588493, g_loss: 6.201253890991211\n","Epoch 60/4000, Step 114, d_loss: 0.003097225446254015, g_loss: 4.158538341522217\n","Epoch 60/4000, Step 115, d_loss: 0.019561655819416046, g_loss: 5.541431903839111\n","Epoch 60/4000, Step 116, d_loss: 0.003987080417573452, g_loss: 10.014477729797363\n","Epoch 60/4000, Step 117, d_loss: 0.002739600371569395, g_loss: 4.977726936340332\n","Epoch 60/4000, Step 118, d_loss: 0.006204577628523111, g_loss: 6.334046363830566\n","Epoch 60/4000, Step 119, d_loss: 0.0076428670436143875, g_loss: 10.667502403259277\n","Epoch 60/4000, Step 120, d_loss: 0.006381784565746784, g_loss: 4.767238140106201\n","Epoch 60/4000, Step 121, d_loss: 0.00791539903730154, g_loss: 6.314287185668945\n","Epoch 60/4000, Step 122, d_loss: 0.003546101273968816, g_loss: 5.672573566436768\n","Epoch 60/4000, Step 123, d_loss: 0.006588489282876253, g_loss: 4.720996379852295\n","Epoch 60/4000, Step 124, d_loss: 0.030048653483390808, g_loss: 6.228770732879639\n","Epoch 60/4000, Step 125, d_loss: 0.004855470731854439, g_loss: 7.475552082061768\n","Epoch 60/4000, Step 126, d_loss: 0.010652044788002968, g_loss: 5.578675746917725\n","Epoch 60/4000, Step 127, d_loss: 0.005387693177908659, g_loss: 7.15414571762085\n","Epoch 60/4000, Step 128, d_loss: 0.006864734925329685, g_loss: 5.8462934494018555\n","Epoch 60/4000, Step 129, d_loss: 0.004940883722156286, g_loss: 7.8802103996276855\n","Epoch 60/4000, Step 130, d_loss: 0.0057551926001906395, g_loss: 8.23217487335205\n","Epoch 60/4000, Step 131, d_loss: 0.006983964703977108, g_loss: 6.545465469360352\n","Epoch 60/4000, Step 132, d_loss: 0.004416514188051224, g_loss: 5.8701558113098145\n","Epoch 60/4000, Step 133, d_loss: 0.002028767019510269, g_loss: 7.524348258972168\n","Epoch 60/4000, Step 134, d_loss: 0.0027745801489800215, g_loss: 6.184040546417236\n","Epoch 60/4000, Step 135, d_loss: 0.020775040611624718, g_loss: 7.305447101593018\n","Epoch 60/4000, Step 136, d_loss: 0.006323402747511864, g_loss: 7.137359142303467\n","Epoch 60/4000, Step 137, d_loss: 0.005978079047054052, g_loss: 7.546236515045166\n","Epoch 60/4000, Step 138, d_loss: 0.0068583074025809765, g_loss: 5.395081520080566\n","Epoch 60/4000, Step 139, d_loss: 0.027124520391225815, g_loss: 4.700742244720459\n","Epoch 60/4000, Step 140, d_loss: 0.005175200290977955, g_loss: 6.377384662628174\n","Epoch 60/4000, Step 141, d_loss: 0.01995057240128517, g_loss: 6.607668876647949\n","Epoch 60/4000, Step 142, d_loss: 0.00447051040828228, g_loss: 4.8092732429504395\n","Epoch 60/4000, Step 143, d_loss: 0.00939873605966568, g_loss: 5.728158473968506\n","Epoch 60/4000, Step 144, d_loss: 0.005433901213109493, g_loss: 4.652471542358398\n","Epoch 60/4000, Step 145, d_loss: 0.00800431240350008, g_loss: 6.3903398513793945\n","Epoch 60/4000, Step 146, d_loss: 0.002641148166731, g_loss: 5.312411785125732\n","Epoch 60/4000, Step 147, d_loss: 0.010399107821285725, g_loss: 7.102968215942383\n","Epoch 60/4000, Step 148, d_loss: 0.008716126903891563, g_loss: 4.891242980957031\n","Epoch 60/4000, Step 149, d_loss: 0.0018355159554630518, g_loss: 5.881500244140625\n","Epoch 60/4000, Step 150, d_loss: 0.010966426692903042, g_loss: 8.275790214538574\n","Epoch 60/4000, Step 151, d_loss: 0.0011338447220623493, g_loss: 8.430144309997559\n","Epoch 60/4000, Step 152, d_loss: 0.0016455961158499122, g_loss: 5.495231628417969\n","Epoch 60/4000, Step 153, d_loss: 0.002684488659724593, g_loss: 5.310474395751953\n","Epoch 60/4000, Step 154, d_loss: 0.008259431459009647, g_loss: 6.2840495109558105\n","Epoch 60/4000, Step 155, d_loss: 0.005174470134079456, g_loss: 8.71339225769043\n","Epoch 60/4000, Step 156, d_loss: 0.010635037906467915, g_loss: 7.878251075744629\n","Epoch 60/4000, Step 157, d_loss: 0.032254453748464584, g_loss: 7.081939697265625\n","Epoch 60/4000, Step 158, d_loss: 0.025744721293449402, g_loss: 6.797989845275879\n","Epoch 60/4000, Step 159, d_loss: 0.009688572958111763, g_loss: 8.963258743286133\n","Epoch 60/4000, Step 160, d_loss: 0.0016800343291833997, g_loss: 6.100830554962158\n","Epoch 60/4000, Step 161, d_loss: 0.0027160579338669777, g_loss: 9.365547180175781\n","Epoch 60/4000, Step 162, d_loss: 0.004862023051828146, g_loss: 7.229698657989502\n","Epoch 60/4000, Step 163, d_loss: 0.005401330068707466, g_loss: 5.208972454071045\n","Epoch 60/4000, Step 164, d_loss: 0.004780179355293512, g_loss: 6.044992446899414\n","Epoch 60/4000, Step 165, d_loss: 0.010746898129582405, g_loss: 5.631471633911133\n","Epoch 60/4000, Step 166, d_loss: 0.006975183263421059, g_loss: 5.497079849243164\n","Epoch 60/4000, Step 167, d_loss: 0.014198148623108864, g_loss: 6.5760498046875\n","Epoch 60/4000, Step 168, d_loss: 0.0037726706359535456, g_loss: 7.728003025054932\n","Epoch 60/4000, Step 169, d_loss: 0.005602854769676924, g_loss: 7.156004905700684\n","Epoch 60/4000, Step 170, d_loss: 0.00734538771212101, g_loss: 4.38479471206665\n","Epoch 60/4000, Step 171, d_loss: 0.0020485837012529373, g_loss: 5.848479747772217\n","Epoch 60/4000, Step 172, d_loss: 0.011105749756097794, g_loss: 6.83534574508667\n","Epoch 60/4000, Step 173, d_loss: 0.00782756693661213, g_loss: 7.032459259033203\n","Epoch 60/4000, Step 174, d_loss: 0.013870608061552048, g_loss: 7.072418212890625\n","Epoch 60/4000, Step 175, d_loss: 0.013409361243247986, g_loss: 5.15021276473999\n","Epoch 60/4000, Step 176, d_loss: 0.006724415346980095, g_loss: 6.0949225425720215\n","Epoch 60/4000, Step 177, d_loss: 0.021836228668689728, g_loss: 6.09030294418335\n","Epoch 60/4000, Step 178, d_loss: 0.022004911676049232, g_loss: 5.243425369262695\n","Epoch 60/4000, Step 179, d_loss: 0.008019973523914814, g_loss: 5.194498538970947\n","Epoch 60/4000, Step 180, d_loss: 0.004089947324246168, g_loss: 6.387922286987305\n","Epoch 60/4000, Step 181, d_loss: 0.018510092049837112, g_loss: 6.008017539978027\n","Epoch 60/4000, Step 182, d_loss: 0.16311310231685638, g_loss: 4.1494221687316895\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 61/4000, Step 1, d_loss: 0.002452307380735874, g_loss: 4.872829437255859\n","Epoch 61/4000, Step 2, d_loss: 0.004260464105755091, g_loss: 7.230860233306885\n","Epoch 61/4000, Step 3, d_loss: 0.03274085372686386, g_loss: 4.534389972686768\n","Epoch 61/4000, Step 4, d_loss: 0.012199841439723969, g_loss: 2.9705779552459717\n","Epoch 61/4000, Step 5, d_loss: 0.00974787026643753, g_loss: 3.508348226547241\n","Epoch 61/4000, Step 6, d_loss: 0.017014728859066963, g_loss: 4.422140598297119\n","Epoch 61/4000, Step 7, d_loss: 0.12823821604251862, g_loss: 3.3808836936950684\n","Epoch 61/4000, Step 8, d_loss: 0.02437654696404934, g_loss: 3.2070717811584473\n","Epoch 61/4000, Step 9, d_loss: 0.09925127029418945, g_loss: 3.7747669219970703\n","Epoch 61/4000, Step 10, d_loss: 0.04224593564867973, g_loss: 3.7684059143066406\n","Epoch 61/4000, Step 11, d_loss: 0.05061765015125275, g_loss: 2.7514755725860596\n","Epoch 61/4000, Step 12, d_loss: 0.02315415069460869, g_loss: 4.226611137390137\n","Epoch 61/4000, Step 13, d_loss: 0.009687456302344799, g_loss: 5.199434280395508\n","Epoch 61/4000, Step 14, d_loss: 0.049921780824661255, g_loss: 5.955009460449219\n","Epoch 61/4000, Step 15, d_loss: 0.005092286039143801, g_loss: 3.2817704677581787\n","Epoch 61/4000, Step 16, d_loss: 0.012659095227718353, g_loss: 5.588161945343018\n","Epoch 61/4000, Step 17, d_loss: 0.007876296527683735, g_loss: 7.0027899742126465\n","Epoch 61/4000, Step 18, d_loss: 0.07810226082801819, g_loss: 8.099224090576172\n","Epoch 61/4000, Step 19, d_loss: 0.007094026543200016, g_loss: 4.864870071411133\n","Epoch 61/4000, Step 20, d_loss: 0.01137893833220005, g_loss: 9.556729316711426\n","Epoch 61/4000, Step 21, d_loss: 0.008456514216959476, g_loss: 6.773137092590332\n","Epoch 61/4000, Step 22, d_loss: 0.007408073637634516, g_loss: 5.871395111083984\n","Epoch 61/4000, Step 23, d_loss: 0.26551827788352966, g_loss: 7.022284030914307\n","Epoch 61/4000, Step 24, d_loss: 0.04591604322195053, g_loss: 5.790964126586914\n","Epoch 61/4000, Step 25, d_loss: 0.01569286733865738, g_loss: 4.031752109527588\n","Epoch 61/4000, Step 26, d_loss: 0.009314707480370998, g_loss: 3.797739028930664\n","Epoch 61/4000, Step 27, d_loss: 0.004620167892426252, g_loss: 6.239440441131592\n","Epoch 61/4000, Step 28, d_loss: 0.004061923362314701, g_loss: 6.1569671630859375\n","Epoch 61/4000, Step 29, d_loss: 0.004060451872646809, g_loss: 3.0353033542633057\n","Epoch 61/4000, Step 30, d_loss: 0.1098640039563179, g_loss: 4.453534126281738\n","Epoch 61/4000, Step 31, d_loss: 0.005355656147003174, g_loss: 4.089487552642822\n","Epoch 61/4000, Step 32, d_loss: 0.00939965434372425, g_loss: 4.220025539398193\n","Epoch 61/4000, Step 33, d_loss: 0.06690029054880142, g_loss: 3.942439079284668\n","Epoch 61/4000, Step 34, d_loss: 0.010054401122033596, g_loss: 5.324129104614258\n","Epoch 61/4000, Step 35, d_loss: 0.013445274904370308, g_loss: 3.0545709133148193\n","Epoch 61/4000, Step 36, d_loss: 0.00481855683028698, g_loss: 4.476701736450195\n","Epoch 61/4000, Step 37, d_loss: 0.17203408479690552, g_loss: 4.040675640106201\n","Epoch 61/4000, Step 38, d_loss: 0.010301750153303146, g_loss: 5.270268440246582\n","Epoch 61/4000, Step 39, d_loss: 0.013898647390305996, g_loss: 5.74518346786499\n","Epoch 61/4000, Step 40, d_loss: 0.018563102930784225, g_loss: 7.445365905761719\n","Epoch 61/4000, Step 41, d_loss: 0.010180122219026089, g_loss: 5.212012767791748\n","Epoch 61/4000, Step 42, d_loss: 0.00942213088274002, g_loss: 4.064508438110352\n","Epoch 61/4000, Step 43, d_loss: 0.09939257055521011, g_loss: 4.590710639953613\n","Epoch 61/4000, Step 44, d_loss: 0.011575708165764809, g_loss: 4.217095375061035\n","Epoch 61/4000, Step 45, d_loss: 0.13074474036693573, g_loss: 5.740357398986816\n","Epoch 61/4000, Step 46, d_loss: 0.0021628595422953367, g_loss: 3.89391827583313\n","Epoch 61/4000, Step 47, d_loss: 0.05497924983501434, g_loss: 5.4428629875183105\n","Epoch 61/4000, Step 48, d_loss: 0.06610902398824692, g_loss: 5.654812812805176\n","Epoch 61/4000, Step 49, d_loss: 0.044075675308704376, g_loss: 3.5189223289489746\n","Epoch 61/4000, Step 50, d_loss: 0.036590635776519775, g_loss: 9.146240234375\n","Epoch 61/4000, Step 51, d_loss: 0.04038354754447937, g_loss: 6.2138352394104\n","Epoch 61/4000, Step 52, d_loss: 0.03488697111606598, g_loss: 4.659604072570801\n","Epoch 61/4000, Step 53, d_loss: 0.023865841329097748, g_loss: 5.393150806427002\n","Epoch 61/4000, Step 54, d_loss: 0.014996116980910301, g_loss: 3.255338191986084\n","Epoch 61/4000, Step 55, d_loss: 0.01709531620144844, g_loss: 5.288841247558594\n","Epoch 61/4000, Step 56, d_loss: 0.0611448660492897, g_loss: 6.23416805267334\n","Epoch 61/4000, Step 57, d_loss: 0.02733156830072403, g_loss: 4.77470588684082\n","Epoch 61/4000, Step 58, d_loss: 0.022354433313012123, g_loss: 5.7024054527282715\n","Epoch 61/4000, Step 59, d_loss: 0.09039103239774704, g_loss: 6.0616960525512695\n","Epoch 61/4000, Step 60, d_loss: 0.01606873609125614, g_loss: 5.739677429199219\n","Epoch 61/4000, Step 61, d_loss: 0.012861012481153011, g_loss: 3.448410987854004\n","Epoch 61/4000, Step 62, d_loss: 0.005628988146781921, g_loss: 6.297621726989746\n","Epoch 61/4000, Step 63, d_loss: 0.031341493129730225, g_loss: 5.329588413238525\n","Epoch 61/4000, Step 64, d_loss: 0.0012792181223630905, g_loss: 6.1224045753479\n","Epoch 61/4000, Step 65, d_loss: 0.03133595734834671, g_loss: 5.661652088165283\n","Epoch 61/4000, Step 66, d_loss: 0.008389892056584358, g_loss: 4.668791770935059\n","Epoch 61/4000, Step 67, d_loss: 0.005311350803822279, g_loss: 5.605671405792236\n","Epoch 61/4000, Step 68, d_loss: 0.03429993987083435, g_loss: 3.286653518676758\n","Epoch 61/4000, Step 69, d_loss: 0.09454856812953949, g_loss: 4.815110683441162\n","Epoch 61/4000, Step 70, d_loss: 0.00714477151632309, g_loss: 4.656387805938721\n","Epoch 61/4000, Step 71, d_loss: 0.014033334329724312, g_loss: 3.142915725708008\n","Epoch 61/4000, Step 72, d_loss: 0.010501842945814133, g_loss: 7.378655433654785\n","Epoch 61/4000, Step 73, d_loss: 0.005888167303055525, g_loss: 3.199812412261963\n","Epoch 61/4000, Step 74, d_loss: 0.020382851362228394, g_loss: 4.461790561676025\n","Epoch 61/4000, Step 75, d_loss: 0.016081225126981735, g_loss: 5.155897617340088\n","Epoch 61/4000, Step 76, d_loss: 0.009686579927802086, g_loss: 5.020556449890137\n","Epoch 61/4000, Step 77, d_loss: 0.008652832359075546, g_loss: 6.119523048400879\n","Epoch 61/4000, Step 78, d_loss: 0.020180851221084595, g_loss: 6.550426959991455\n","Epoch 61/4000, Step 79, d_loss: 0.00344982766546309, g_loss: 5.737925052642822\n","Epoch 61/4000, Step 80, d_loss: 0.005859000608325005, g_loss: 4.6322503089904785\n","Epoch 61/4000, Step 81, d_loss: 0.004626511596143246, g_loss: 5.8727874755859375\n","Epoch 61/4000, Step 82, d_loss: 0.02224806882441044, g_loss: 8.454913139343262\n","Epoch 61/4000, Step 83, d_loss: 0.02614651992917061, g_loss: 5.951794624328613\n","Epoch 61/4000, Step 84, d_loss: 0.00938244815915823, g_loss: 3.5540990829467773\n","Epoch 61/4000, Step 85, d_loss: 0.022705726325511932, g_loss: 4.500671863555908\n","Epoch 61/4000, Step 86, d_loss: 0.0035402679350227118, g_loss: 7.360400199890137\n","Epoch 61/4000, Step 87, d_loss: 0.026319030672311783, g_loss: 7.231600284576416\n","Epoch 61/4000, Step 88, d_loss: 0.00900723785161972, g_loss: 9.055203437805176\n","Epoch 61/4000, Step 89, d_loss: 0.0055082193575799465, g_loss: 5.6394453048706055\n","Epoch 61/4000, Step 90, d_loss: 0.007590762339532375, g_loss: 6.587228775024414\n","Epoch 61/4000, Step 91, d_loss: 0.005527462810277939, g_loss: 5.3263630867004395\n","Epoch 61/4000, Step 92, d_loss: 0.0065634832717478275, g_loss: 9.596956253051758\n","Epoch 61/4000, Step 93, d_loss: 0.016829002648591995, g_loss: 5.696412563323975\n","Epoch 61/4000, Step 94, d_loss: 0.00881659984588623, g_loss: 3.8226675987243652\n","Epoch 61/4000, Step 95, d_loss: 0.005355984438210726, g_loss: 6.400217533111572\n","Epoch 61/4000, Step 96, d_loss: 0.003233089577406645, g_loss: 7.164307117462158\n","Epoch 61/4000, Step 97, d_loss: 0.005983714014291763, g_loss: 5.7470383644104\n","Epoch 61/4000, Step 98, d_loss: 0.02909582480788231, g_loss: 4.758759021759033\n","Epoch 61/4000, Step 99, d_loss: 0.008368441835045815, g_loss: 7.603428363800049\n","Epoch 61/4000, Step 100, d_loss: 0.13877244293689728, g_loss: 6.517660140991211\n","Epoch 61/4000, Step 101, d_loss: 0.004994519054889679, g_loss: 4.620963096618652\n","Epoch 61/4000, Step 102, d_loss: 0.0012320185778662562, g_loss: 5.056798934936523\n","Epoch 61/4000, Step 103, d_loss: 0.0042824349366128445, g_loss: 3.2118337154388428\n","Epoch 61/4000, Step 104, d_loss: 0.006111599039286375, g_loss: 4.692905426025391\n","Epoch 61/4000, Step 105, d_loss: 0.008876923471689224, g_loss: 6.153881072998047\n","Epoch 61/4000, Step 106, d_loss: 0.017810136079788208, g_loss: 5.036617279052734\n","Epoch 61/4000, Step 107, d_loss: 0.0316200889647007, g_loss: 4.975380897521973\n","Epoch 61/4000, Step 108, d_loss: 0.01150904968380928, g_loss: 4.8248138427734375\n","Epoch 61/4000, Step 109, d_loss: 0.06217832490801811, g_loss: 4.747828483581543\n","Epoch 61/4000, Step 110, d_loss: 0.034312762320041656, g_loss: 4.805025100708008\n","Epoch 61/4000, Step 111, d_loss: 0.005989690311253071, g_loss: 6.449233055114746\n","Epoch 61/4000, Step 112, d_loss: 0.02708359807729721, g_loss: 7.231460094451904\n","Epoch 61/4000, Step 113, d_loss: 0.014675206504762173, g_loss: 6.782650947570801\n","Epoch 61/4000, Step 114, d_loss: 0.009500417858362198, g_loss: 5.335151672363281\n","Epoch 61/4000, Step 115, d_loss: 0.12261209636926651, g_loss: 5.035252571105957\n","Epoch 61/4000, Step 116, d_loss: 0.01657894253730774, g_loss: 3.778122901916504\n","Epoch 61/4000, Step 117, d_loss: 0.008880319073796272, g_loss: 6.919025421142578\n","Epoch 61/4000, Step 118, d_loss: 0.02742643840610981, g_loss: 5.320464611053467\n","Epoch 61/4000, Step 119, d_loss: 0.01349160447716713, g_loss: 4.900034427642822\n","Epoch 61/4000, Step 120, d_loss: 0.005943932104855776, g_loss: 4.032548904418945\n","Epoch 61/4000, Step 121, d_loss: 0.03576786071062088, g_loss: 3.6958906650543213\n","Epoch 61/4000, Step 122, d_loss: 0.047507062554359436, g_loss: 3.847059488296509\n","Epoch 61/4000, Step 123, d_loss: 0.006002211011946201, g_loss: 4.6680908203125\n","Epoch 61/4000, Step 124, d_loss: 0.020238012075424194, g_loss: 3.250518321990967\n","Epoch 61/4000, Step 125, d_loss: 0.019668392837047577, g_loss: 6.596003532409668\n","Epoch 61/4000, Step 126, d_loss: 0.07739309221506119, g_loss: 3.684154748916626\n","Epoch 61/4000, Step 127, d_loss: 0.013349917717278004, g_loss: 8.321261405944824\n","Epoch 61/4000, Step 128, d_loss: 0.008498888462781906, g_loss: 2.756242513656616\n","Epoch 61/4000, Step 129, d_loss: 0.04079335555434227, g_loss: 6.054012775421143\n","Epoch 61/4000, Step 130, d_loss: 0.007961440831422806, g_loss: 4.198777198791504\n","Epoch 61/4000, Step 131, d_loss: 0.03311798721551895, g_loss: 2.6362428665161133\n","Epoch 61/4000, Step 132, d_loss: 0.07589870691299438, g_loss: 4.329685211181641\n","Epoch 61/4000, Step 133, d_loss: 0.03831936791539192, g_loss: 5.017068386077881\n","Epoch 61/4000, Step 134, d_loss: 0.004338322672992945, g_loss: 4.500401020050049\n","Epoch 61/4000, Step 135, d_loss: 0.006018688902258873, g_loss: 2.7872204780578613\n","Epoch 61/4000, Step 136, d_loss: 0.0077659813687205315, g_loss: 2.831763744354248\n","Epoch 61/4000, Step 137, d_loss: 0.017479626461863518, g_loss: 3.098386287689209\n","Epoch 61/4000, Step 138, d_loss: 0.026613716036081314, g_loss: 5.542342662811279\n","Epoch 61/4000, Step 139, d_loss: 0.043479036539793015, g_loss: 2.762838840484619\n","Epoch 61/4000, Step 140, d_loss: 0.012426912784576416, g_loss: 4.287495136260986\n","Epoch 61/4000, Step 141, d_loss: 0.021659862250089645, g_loss: 5.768565654754639\n","Epoch 61/4000, Step 142, d_loss: 0.01643279381096363, g_loss: 4.456554412841797\n","Epoch 61/4000, Step 143, d_loss: 0.05008137226104736, g_loss: 5.511226654052734\n","Epoch 61/4000, Step 144, d_loss: 0.0041246782056987286, g_loss: 6.194942474365234\n","Epoch 61/4000, Step 145, d_loss: 0.020517826080322266, g_loss: 6.142358303070068\n","Epoch 61/4000, Step 146, d_loss: 0.01382077019661665, g_loss: 4.177292823791504\n","Epoch 61/4000, Step 147, d_loss: 0.047285761684179306, g_loss: 5.108609676361084\n","Epoch 61/4000, Step 148, d_loss: 0.05244981124997139, g_loss: 3.8668441772460938\n","Epoch 61/4000, Step 149, d_loss: 0.002670006128028035, g_loss: 5.069357395172119\n","Epoch 61/4000, Step 150, d_loss: 0.019499050453305244, g_loss: 4.838986873626709\n","Epoch 61/4000, Step 151, d_loss: 0.006943519227206707, g_loss: 3.4274933338165283\n","Epoch 61/4000, Step 152, d_loss: 0.025095876306295395, g_loss: 5.849451065063477\n","Epoch 61/4000, Step 153, d_loss: 0.0059877075254917145, g_loss: 6.137486934661865\n","Epoch 61/4000, Step 154, d_loss: 0.023353490978479385, g_loss: 5.688152313232422\n","Epoch 61/4000, Step 155, d_loss: 0.021837705746293068, g_loss: 4.834307670593262\n","Epoch 61/4000, Step 156, d_loss: 0.03399129584431648, g_loss: 7.151419639587402\n","Epoch 61/4000, Step 157, d_loss: 0.017988059669733047, g_loss: 6.482172966003418\n","Epoch 61/4000, Step 158, d_loss: 0.06276078522205353, g_loss: 6.604294776916504\n","Epoch 61/4000, Step 159, d_loss: 0.011242174543440342, g_loss: 5.800256729125977\n","Epoch 61/4000, Step 160, d_loss: 0.0280196163803339, g_loss: 4.271429538726807\n","Epoch 61/4000, Step 161, d_loss: 0.01541461143642664, g_loss: 5.501954078674316\n","Epoch 61/4000, Step 162, d_loss: 0.00852340366691351, g_loss: 9.178694725036621\n","Epoch 61/4000, Step 163, d_loss: 0.014427618123590946, g_loss: 7.837321758270264\n","Epoch 61/4000, Step 164, d_loss: 0.013320201076567173, g_loss: 5.901601791381836\n","Epoch 61/4000, Step 165, d_loss: 0.00762646459043026, g_loss: 4.02038049697876\n","Epoch 61/4000, Step 166, d_loss: 0.008187146857380867, g_loss: 9.748661994934082\n","Epoch 61/4000, Step 167, d_loss: 0.025911709293723106, g_loss: 6.1507344245910645\n","Epoch 61/4000, Step 168, d_loss: 0.0094996877014637, g_loss: 6.344702243804932\n","Epoch 61/4000, Step 169, d_loss: 0.01658167876303196, g_loss: 4.100832939147949\n","Epoch 61/4000, Step 170, d_loss: 0.01902385801076889, g_loss: 5.375067234039307\n","Epoch 61/4000, Step 171, d_loss: 0.012349965050816536, g_loss: 6.0950093269348145\n","Epoch 61/4000, Step 172, d_loss: 0.006506855599582195, g_loss: 6.243971824645996\n","Epoch 61/4000, Step 173, d_loss: 0.06493143737316132, g_loss: 6.258007526397705\n","Epoch 61/4000, Step 174, d_loss: 0.003863448277115822, g_loss: 4.506170272827148\n","Epoch 61/4000, Step 175, d_loss: 0.003605136414989829, g_loss: 4.977755069732666\n","Epoch 61/4000, Step 176, d_loss: 0.023488152772188187, g_loss: 3.8750298023223877\n","Epoch 61/4000, Step 177, d_loss: 0.007889065891504288, g_loss: 5.677544116973877\n","Epoch 61/4000, Step 178, d_loss: 0.028388896957039833, g_loss: 5.541705131530762\n","Epoch 61/4000, Step 179, d_loss: 0.011128724552690983, g_loss: 5.740558624267578\n","Epoch 61/4000, Step 180, d_loss: 0.03900326043367386, g_loss: 3.8059256076812744\n","Epoch 61/4000, Step 181, d_loss: 0.0015324377454817295, g_loss: 5.398196697235107\n","Epoch 61/4000, Step 182, d_loss: 0.0471293106675148, g_loss: 5.340122222900391\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 62/4000, Step 1, d_loss: 0.011784826405346394, g_loss: 6.989349842071533\n","Epoch 62/4000, Step 2, d_loss: 0.023261288180947304, g_loss: 3.6712214946746826\n","Epoch 62/4000, Step 3, d_loss: 0.02438337728381157, g_loss: 7.145506381988525\n","Epoch 62/4000, Step 4, d_loss: 0.007416314445436001, g_loss: 4.87824010848999\n","Epoch 62/4000, Step 5, d_loss: 0.010634031146764755, g_loss: 5.181869029998779\n","Epoch 62/4000, Step 6, d_loss: 0.004744453821331263, g_loss: 5.927589416503906\n","Epoch 62/4000, Step 7, d_loss: 0.009297903627157211, g_loss: 5.404722690582275\n","Epoch 62/4000, Step 8, d_loss: 0.01190554816275835, g_loss: 8.177395820617676\n","Epoch 62/4000, Step 9, d_loss: 0.005196566693484783, g_loss: 3.681957960128784\n","Epoch 62/4000, Step 10, d_loss: 0.002114763017743826, g_loss: 6.811812877655029\n","Epoch 62/4000, Step 11, d_loss: 0.03978888690471649, g_loss: 4.834803581237793\n","Epoch 62/4000, Step 12, d_loss: 0.00790257565677166, g_loss: 5.411986827850342\n","Epoch 62/4000, Step 13, d_loss: 0.007995018735527992, g_loss: 5.131333827972412\n","Epoch 62/4000, Step 14, d_loss: 0.005314192734658718, g_loss: 4.695512294769287\n","Epoch 62/4000, Step 15, d_loss: 0.0025098042096942663, g_loss: 5.126938819885254\n","Epoch 62/4000, Step 16, d_loss: 0.0038687349297106266, g_loss: 6.169032096862793\n","Epoch 62/4000, Step 17, d_loss: 0.008623597212135792, g_loss: 6.48943567276001\n","Epoch 62/4000, Step 18, d_loss: 0.007766324561089277, g_loss: 4.758711338043213\n","Epoch 62/4000, Step 19, d_loss: 0.012925254181027412, g_loss: 5.504195213317871\n","Epoch 62/4000, Step 20, d_loss: 0.005190998315811157, g_loss: 5.088803768157959\n","Epoch 62/4000, Step 21, d_loss: 0.11338413506746292, g_loss: 6.601856231689453\n","Epoch 62/4000, Step 22, d_loss: 0.0009322045370936394, g_loss: 4.097653388977051\n","Epoch 62/4000, Step 23, d_loss: 0.004710701294243336, g_loss: 7.235594749450684\n","Epoch 62/4000, Step 24, d_loss: 0.006943935062736273, g_loss: 4.329111099243164\n","Epoch 62/4000, Step 25, d_loss: 0.01802639104425907, g_loss: 7.161616325378418\n","Epoch 62/4000, Step 26, d_loss: 0.013768645003437996, g_loss: 7.342361927032471\n","Epoch 62/4000, Step 27, d_loss: 0.037078309804201126, g_loss: 6.196616172790527\n","Epoch 62/4000, Step 28, d_loss: 0.04207739979028702, g_loss: 4.395117282867432\n","Epoch 62/4000, Step 29, d_loss: 0.038265760987997055, g_loss: 5.658726692199707\n","Epoch 62/4000, Step 30, d_loss: 0.007593861781060696, g_loss: 5.2092790603637695\n","Epoch 62/4000, Step 31, d_loss: 0.01584753394126892, g_loss: 6.728194236755371\n","Epoch 62/4000, Step 32, d_loss: 0.1348956972360611, g_loss: 6.520009994506836\n","Epoch 62/4000, Step 33, d_loss: 0.029900729656219482, g_loss: 4.394044876098633\n","Epoch 62/4000, Step 34, d_loss: 0.015484992414712906, g_loss: 5.893406867980957\n","Epoch 62/4000, Step 35, d_loss: 0.003882241901010275, g_loss: 4.803677558898926\n","Epoch 62/4000, Step 36, d_loss: 0.010189701803028584, g_loss: 6.384850978851318\n","Epoch 62/4000, Step 37, d_loss: 0.006354181095957756, g_loss: 8.978582382202148\n","Epoch 62/4000, Step 38, d_loss: 0.004928066395223141, g_loss: 4.692941188812256\n","Epoch 62/4000, Step 39, d_loss: 0.004817273933440447, g_loss: 5.1936540603637695\n","Epoch 62/4000, Step 40, d_loss: 0.08169031143188477, g_loss: 5.070764064788818\n","Epoch 62/4000, Step 41, d_loss: 0.013408325612545013, g_loss: 4.252542972564697\n","Epoch 62/4000, Step 42, d_loss: 0.04884098470211029, g_loss: 4.157646179199219\n","Epoch 62/4000, Step 43, d_loss: 0.008532436564564705, g_loss: 6.360414505004883\n","Epoch 62/4000, Step 44, d_loss: 0.005403380841016769, g_loss: 4.184130668640137\n","Epoch 62/4000, Step 45, d_loss: 0.006317910738289356, g_loss: 7.240708351135254\n","Epoch 62/4000, Step 46, d_loss: 0.00761406309902668, g_loss: 5.161678791046143\n","Epoch 62/4000, Step 47, d_loss: 0.0027421072591096163, g_loss: 6.950684547424316\n","Epoch 62/4000, Step 48, d_loss: 0.018110759556293488, g_loss: 4.461121559143066\n","Epoch 62/4000, Step 49, d_loss: 0.01110961101949215, g_loss: 4.349540710449219\n","Epoch 62/4000, Step 50, d_loss: 0.005056262016296387, g_loss: 3.6300535202026367\n","Epoch 62/4000, Step 51, d_loss: 0.03781785070896149, g_loss: 4.8682379722595215\n","Epoch 62/4000, Step 52, d_loss: 0.02089795283973217, g_loss: 4.178366184234619\n","Epoch 62/4000, Step 53, d_loss: 0.0037848118226975203, g_loss: 3.8451085090637207\n","Epoch 62/4000, Step 54, d_loss: 0.0040598539635539055, g_loss: 3.7273449897766113\n","Epoch 62/4000, Step 55, d_loss: 0.017401300370693207, g_loss: 3.8312458992004395\n","Epoch 62/4000, Step 56, d_loss: 0.025896696373820305, g_loss: 4.070492744445801\n","Epoch 62/4000, Step 57, d_loss: 0.0036626385990530252, g_loss: 9.200850486755371\n","Epoch 62/4000, Step 58, d_loss: 0.024486396461725235, g_loss: 6.396600723266602\n","Epoch 62/4000, Step 59, d_loss: 0.00612604059278965, g_loss: 5.129459381103516\n","Epoch 62/4000, Step 60, d_loss: 0.014942660927772522, g_loss: 7.980467796325684\n","Epoch 62/4000, Step 61, d_loss: 0.024025531485676765, g_loss: 6.400146484375\n","Epoch 62/4000, Step 62, d_loss: 0.01690855249762535, g_loss: 6.060171604156494\n","Epoch 62/4000, Step 63, d_loss: 0.018776165321469307, g_loss: 6.783084869384766\n","Epoch 62/4000, Step 64, d_loss: 0.010253050364553928, g_loss: 3.622744083404541\n","Epoch 62/4000, Step 65, d_loss: 0.004010769072920084, g_loss: 6.709036350250244\n","Epoch 62/4000, Step 66, d_loss: 0.006840547546744347, g_loss: 4.979816436767578\n","Epoch 62/4000, Step 67, d_loss: 0.002201208844780922, g_loss: 7.038183689117432\n","Epoch 62/4000, Step 68, d_loss: 0.007713043596595526, g_loss: 5.833001136779785\n","Epoch 62/4000, Step 69, d_loss: 0.0006026349146850407, g_loss: 7.004960060119629\n","Epoch 62/4000, Step 70, d_loss: 0.009157105349004269, g_loss: 6.314642906188965\n","Epoch 62/4000, Step 71, d_loss: 0.016318827867507935, g_loss: 6.424770832061768\n","Epoch 62/4000, Step 72, d_loss: 0.08038458228111267, g_loss: 4.637968063354492\n","Epoch 62/4000, Step 73, d_loss: 0.0011510817566886544, g_loss: 6.949754238128662\n","Epoch 62/4000, Step 74, d_loss: 0.007676052860915661, g_loss: 3.1090052127838135\n","Epoch 62/4000, Step 75, d_loss: 0.00552502553910017, g_loss: 7.17520809173584\n","Epoch 62/4000, Step 76, d_loss: 0.006366174668073654, g_loss: 6.177669525146484\n","Epoch 62/4000, Step 77, d_loss: 0.002082380000501871, g_loss: 7.3866119384765625\n","Epoch 62/4000, Step 78, d_loss: 0.00889853946864605, g_loss: 4.55131721496582\n","Epoch 62/4000, Step 79, d_loss: 0.004085197113454342, g_loss: 6.501165866851807\n","Epoch 62/4000, Step 80, d_loss: 0.004437266383320093, g_loss: 5.358059883117676\n","Epoch 62/4000, Step 81, d_loss: 0.008190560154616833, g_loss: 6.475465774536133\n","Epoch 62/4000, Step 82, d_loss: 0.0018221212085336447, g_loss: 5.756619453430176\n","Epoch 62/4000, Step 83, d_loss: 0.039962973445653915, g_loss: 5.942352771759033\n","Epoch 62/4000, Step 84, d_loss: 0.0023947088047862053, g_loss: 4.394418239593506\n","Epoch 62/4000, Step 85, d_loss: 0.03283138573169708, g_loss: 4.9295973777771\n","Epoch 62/4000, Step 86, d_loss: 0.004771493840962648, g_loss: 5.3707356452941895\n","Epoch 62/4000, Step 87, d_loss: 0.011767987161874771, g_loss: 3.9157118797302246\n","Epoch 62/4000, Step 88, d_loss: 0.00749839236959815, g_loss: 6.444527626037598\n","Epoch 62/4000, Step 89, d_loss: 0.00675975251942873, g_loss: 5.623924255371094\n","Epoch 62/4000, Step 90, d_loss: 0.004941105842590332, g_loss: 5.1685566902160645\n","Epoch 62/4000, Step 91, d_loss: 0.0019991733133792877, g_loss: 7.462378978729248\n","Epoch 62/4000, Step 92, d_loss: 0.007618388161063194, g_loss: 3.7653937339782715\n","Epoch 62/4000, Step 93, d_loss: 0.0056676557287573814, g_loss: 4.444167613983154\n","Epoch 62/4000, Step 94, d_loss: 0.0030937581323087215, g_loss: 5.788570880889893\n","Epoch 62/4000, Step 95, d_loss: 0.02121579647064209, g_loss: 6.490265369415283\n","Epoch 62/4000, Step 96, d_loss: 0.005164958070963621, g_loss: 4.0316009521484375\n","Epoch 62/4000, Step 97, d_loss: 0.0018807279411703348, g_loss: 5.484467506408691\n","Epoch 62/4000, Step 98, d_loss: 0.003412448801100254, g_loss: 4.12481689453125\n","Epoch 62/4000, Step 99, d_loss: 0.0029391141142696142, g_loss: 3.9055209159851074\n","Epoch 62/4000, Step 100, d_loss: 0.01002463512122631, g_loss: 7.64180326461792\n","Epoch 62/4000, Step 101, d_loss: 0.011581392958760262, g_loss: 4.847602844238281\n","Epoch 62/4000, Step 102, d_loss: 0.005578461103141308, g_loss: 6.02742338180542\n","Epoch 62/4000, Step 103, d_loss: 0.012327084317803383, g_loss: 7.363859176635742\n","Epoch 62/4000, Step 104, d_loss: 0.013573925010859966, g_loss: 5.255217552185059\n","Epoch 62/4000, Step 105, d_loss: 0.004855453502386808, g_loss: 4.5278754234313965\n","Epoch 62/4000, Step 106, d_loss: 0.00335007905960083, g_loss: 6.119807243347168\n","Epoch 62/4000, Step 107, d_loss: 0.004934705328196287, g_loss: 3.9171807765960693\n","Epoch 62/4000, Step 108, d_loss: 0.0022478096652776003, g_loss: 9.269632339477539\n","Epoch 62/4000, Step 109, d_loss: 0.004284054506570101, g_loss: 6.987483978271484\n","Epoch 62/4000, Step 110, d_loss: 0.007774644531309605, g_loss: 5.369529724121094\n","Epoch 62/4000, Step 111, d_loss: 0.03906037285923958, g_loss: 6.749571800231934\n","Epoch 62/4000, Step 112, d_loss: 0.000893008487764746, g_loss: 6.965099811553955\n","Epoch 62/4000, Step 113, d_loss: 0.003227070439606905, g_loss: 8.794046401977539\n","Epoch 62/4000, Step 114, d_loss: 0.008304347284138203, g_loss: 5.163074970245361\n","Epoch 62/4000, Step 115, d_loss: 0.0025035354774445295, g_loss: 5.06140661239624\n","Epoch 62/4000, Step 116, d_loss: 0.0011735328007489443, g_loss: 6.619663715362549\n","Epoch 62/4000, Step 117, d_loss: 0.003555505769327283, g_loss: 5.046821117401123\n","Epoch 62/4000, Step 118, d_loss: 0.002162682358175516, g_loss: 5.500270843505859\n","Epoch 62/4000, Step 119, d_loss: 0.00717784883454442, g_loss: 4.1759934425354\n","Epoch 62/4000, Step 120, d_loss: 0.0035978462547063828, g_loss: 4.718089580535889\n","Epoch 62/4000, Step 121, d_loss: 0.005525575950741768, g_loss: 5.8222551345825195\n","Epoch 62/4000, Step 122, d_loss: 0.007750735618174076, g_loss: 3.113642692565918\n","Epoch 62/4000, Step 123, d_loss: 0.004945846274495125, g_loss: 4.039111614227295\n","Epoch 62/4000, Step 124, d_loss: 0.004118485376238823, g_loss: 6.848280906677246\n","Epoch 62/4000, Step 125, d_loss: 0.002335538389161229, g_loss: 6.991418361663818\n","Epoch 62/4000, Step 126, d_loss: 0.0018171287374570966, g_loss: 3.3156967163085938\n","Epoch 62/4000, Step 127, d_loss: 0.0007385645294561982, g_loss: 6.020438194274902\n","Epoch 62/4000, Step 128, d_loss: 0.04394233599305153, g_loss: 6.381385803222656\n","Epoch 62/4000, Step 129, d_loss: 0.018743980675935745, g_loss: 3.53126859664917\n","Epoch 62/4000, Step 130, d_loss: 0.00144707050640136, g_loss: 5.842039585113525\n","Epoch 62/4000, Step 131, d_loss: 0.042702749371528625, g_loss: 6.2585577964782715\n","Epoch 62/4000, Step 132, d_loss: 0.0012589117977768183, g_loss: 9.772808074951172\n","Epoch 62/4000, Step 133, d_loss: 0.0008180239819921553, g_loss: 5.550479888916016\n","Epoch 62/4000, Step 134, d_loss: 0.005415908060967922, g_loss: 4.489649772644043\n","Epoch 62/4000, Step 135, d_loss: 0.00864633172750473, g_loss: 4.7406463623046875\n","Epoch 62/4000, Step 136, d_loss: 0.005760764703154564, g_loss: 5.699281215667725\n","Epoch 62/4000, Step 137, d_loss: 0.019703514873981476, g_loss: 3.760911226272583\n","Epoch 62/4000, Step 138, d_loss: 0.020893676206469536, g_loss: 5.781184196472168\n","Epoch 62/4000, Step 139, d_loss: 0.02205139584839344, g_loss: 4.674119472503662\n","Epoch 62/4000, Step 140, d_loss: 0.0014323194045573473, g_loss: 7.375492095947266\n","Epoch 62/4000, Step 141, d_loss: 0.010263985954225063, g_loss: 3.8336069583892822\n","Epoch 62/4000, Step 142, d_loss: 0.006899570114910603, g_loss: 6.69588565826416\n","Epoch 62/4000, Step 143, d_loss: 0.002682052319869399, g_loss: 6.892518043518066\n","Epoch 62/4000, Step 144, d_loss: 0.007335158996284008, g_loss: 6.6148152351379395\n","Epoch 62/4000, Step 145, d_loss: 0.0018071725498884916, g_loss: 6.883594989776611\n","Epoch 62/4000, Step 146, d_loss: 0.005190189927816391, g_loss: 6.442232131958008\n","Epoch 62/4000, Step 147, d_loss: 0.006065635476261377, g_loss: 8.636276245117188\n","Epoch 62/4000, Step 148, d_loss: 0.028672965243458748, g_loss: 6.175644874572754\n","Epoch 62/4000, Step 149, d_loss: 0.01379668153822422, g_loss: 7.071442127227783\n","Epoch 62/4000, Step 150, d_loss: 0.005508124828338623, g_loss: 5.910488128662109\n","Epoch 62/4000, Step 151, d_loss: 0.0013191025936976075, g_loss: 5.473930835723877\n","Epoch 62/4000, Step 152, d_loss: 0.0070263720117509365, g_loss: 5.917292594909668\n","Epoch 62/4000, Step 153, d_loss: 0.01847250573337078, g_loss: 5.5759406089782715\n","Epoch 62/4000, Step 154, d_loss: 0.014772005379199982, g_loss: 5.017094135284424\n","Epoch 62/4000, Step 155, d_loss: 0.006730522029101849, g_loss: 5.865303039550781\n","Epoch 62/4000, Step 156, d_loss: 0.0015577876474708319, g_loss: 6.164514064788818\n","Epoch 62/4000, Step 157, d_loss: 0.014291542582213879, g_loss: 6.36192512512207\n","Epoch 62/4000, Step 158, d_loss: 0.03211143985390663, g_loss: 8.2552490234375\n","Epoch 62/4000, Step 159, d_loss: 0.009050069376826286, g_loss: 8.961076736450195\n","Epoch 62/4000, Step 160, d_loss: 0.0065451874397695065, g_loss: 4.931303024291992\n","Epoch 62/4000, Step 161, d_loss: 0.016983889043331146, g_loss: 3.9808473587036133\n","Epoch 62/4000, Step 162, d_loss: 0.0008371695876121521, g_loss: 5.55262565612793\n","Epoch 62/4000, Step 163, d_loss: 0.01812017895281315, g_loss: 8.460928916931152\n","Epoch 62/4000, Step 164, d_loss: 0.002760432194918394, g_loss: 5.004717826843262\n","Epoch 62/4000, Step 165, d_loss: 0.013031483627855778, g_loss: 8.254999160766602\n","Epoch 62/4000, Step 166, d_loss: 0.004006274975836277, g_loss: 7.130098819732666\n","Epoch 62/4000, Step 167, d_loss: 0.0018694634782150388, g_loss: 5.472117900848389\n","Epoch 62/4000, Step 168, d_loss: 0.0026075385976582766, g_loss: 6.388003826141357\n","Epoch 62/4000, Step 169, d_loss: 0.011494103819131851, g_loss: 7.979450225830078\n","Epoch 62/4000, Step 170, d_loss: 0.005501504056155682, g_loss: 5.298553943634033\n","Epoch 62/4000, Step 171, d_loss: 0.007663284428417683, g_loss: 6.112705230712891\n","Epoch 62/4000, Step 172, d_loss: 0.00555892800912261, g_loss: 7.075095176696777\n","Epoch 62/4000, Step 173, d_loss: 0.0014048605225980282, g_loss: 5.633108139038086\n","Epoch 62/4000, Step 174, d_loss: 0.017406199127435684, g_loss: 5.68605375289917\n","Epoch 62/4000, Step 175, d_loss: 0.01604214683175087, g_loss: 6.987776756286621\n","Epoch 62/4000, Step 176, d_loss: 0.05582990497350693, g_loss: 4.685096740722656\n","Epoch 62/4000, Step 177, d_loss: 0.011443443596363068, g_loss: 5.529346466064453\n","Epoch 62/4000, Step 178, d_loss: 0.0025262527633458376, g_loss: 5.473420143127441\n","Epoch 62/4000, Step 179, d_loss: 0.002360351849347353, g_loss: 5.420958995819092\n","Epoch 62/4000, Step 180, d_loss: 0.002361433347687125, g_loss: 5.978727340698242\n","Epoch 62/4000, Step 181, d_loss: 0.0317738838493824, g_loss: 6.398559093475342\n","Epoch 62/4000, Step 182, d_loss: 0.35123416781425476, g_loss: 5.500478267669678\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 63/4000, Step 1, d_loss: 0.026025274768471718, g_loss: 3.3961353302001953\n","Epoch 63/4000, Step 2, d_loss: 0.04455786943435669, g_loss: 4.255763053894043\n","Epoch 63/4000, Step 3, d_loss: 0.06923945993185043, g_loss: 3.325407028198242\n","Epoch 63/4000, Step 4, d_loss: 0.010283800773322582, g_loss: 6.593122482299805\n","Epoch 63/4000, Step 5, d_loss: 0.03982324153184891, g_loss: 5.213315010070801\n","Epoch 63/4000, Step 6, d_loss: 0.03435936197638512, g_loss: 1.9690611362457275\n","Epoch 63/4000, Step 7, d_loss: 0.031238585710525513, g_loss: 3.4320709705352783\n","Epoch 63/4000, Step 8, d_loss: 0.14324137568473816, g_loss: 1.02406644821167\n","Epoch 63/4000, Step 9, d_loss: 0.017339862883090973, g_loss: 4.631234645843506\n","Epoch 63/4000, Step 10, d_loss: 0.05774460360407829, g_loss: 2.163914203643799\n","Epoch 63/4000, Step 11, d_loss: 0.14914140105247498, g_loss: 2.32928729057312\n","Epoch 63/4000, Step 12, d_loss: 0.0024927600752562284, g_loss: 3.06204891204834\n","Epoch 63/4000, Step 13, d_loss: 0.06659442186355591, g_loss: 3.041226387023926\n","Epoch 63/4000, Step 14, d_loss: 0.031401943415403366, g_loss: 2.742910861968994\n","Epoch 63/4000, Step 15, d_loss: 0.01141369342803955, g_loss: 5.564875602722168\n","Epoch 63/4000, Step 16, d_loss: 0.0036436221562325954, g_loss: 4.461009502410889\n","Epoch 63/4000, Step 17, d_loss: 0.015100308693945408, g_loss: 4.344821929931641\n","Epoch 63/4000, Step 18, d_loss: 0.031470343470573425, g_loss: 7.426591873168945\n","Epoch 63/4000, Step 19, d_loss: 0.02116043120622635, g_loss: 7.295243740081787\n","Epoch 63/4000, Step 20, d_loss: 0.055719055235385895, g_loss: 8.191557884216309\n","Epoch 63/4000, Step 21, d_loss: 0.023430658504366875, g_loss: 7.1281657218933105\n","Epoch 63/4000, Step 22, d_loss: 0.10635160654783249, g_loss: 4.78056526184082\n","Epoch 63/4000, Step 23, d_loss: 0.021104278042912483, g_loss: 6.9935173988342285\n","Epoch 63/4000, Step 24, d_loss: 0.0018836564850062132, g_loss: 2.3572492599487305\n","Epoch 63/4000, Step 25, d_loss: 0.008742768317461014, g_loss: 4.62310791015625\n","Epoch 63/4000, Step 26, d_loss: 0.06181648001074791, g_loss: 5.2219319343566895\n","Epoch 63/4000, Step 27, d_loss: 0.02206500619649887, g_loss: 5.088264465332031\n","Epoch 63/4000, Step 28, d_loss: 0.010916379280388355, g_loss: 3.4941906929016113\n","Epoch 63/4000, Step 29, d_loss: 0.006166915409266949, g_loss: 5.580783843994141\n","Epoch 63/4000, Step 30, d_loss: 0.005024057812988758, g_loss: 2.8160667419433594\n","Epoch 63/4000, Step 31, d_loss: 0.012259135022759438, g_loss: 4.43142032623291\n","Epoch 63/4000, Step 32, d_loss: 0.013221649453043938, g_loss: 2.921990156173706\n","Epoch 63/4000, Step 33, d_loss: 0.0023013371974229813, g_loss: 3.0025644302368164\n","Epoch 63/4000, Step 34, d_loss: 0.01435359288007021, g_loss: 10.780394554138184\n","Epoch 63/4000, Step 35, d_loss: 0.012318669818341732, g_loss: 3.8606412410736084\n","Epoch 63/4000, Step 36, d_loss: 0.09211781620979309, g_loss: 2.3432226181030273\n","Epoch 63/4000, Step 37, d_loss: 0.08106923848390579, g_loss: 3.411381721496582\n","Epoch 63/4000, Step 38, d_loss: 0.03368866816163063, g_loss: 6.1126484870910645\n","Epoch 63/4000, Step 39, d_loss: 0.0035400521010160446, g_loss: 6.779510498046875\n","Epoch 63/4000, Step 40, d_loss: 0.07225009799003601, g_loss: 6.8273725509643555\n","Epoch 63/4000, Step 41, d_loss: 0.0013342442689463496, g_loss: 3.7537825107574463\n","Epoch 63/4000, Step 42, d_loss: 0.050565171986818314, g_loss: 7.309126377105713\n","Epoch 63/4000, Step 43, d_loss: 0.0067674545571208, g_loss: 3.7386386394500732\n","Epoch 63/4000, Step 44, d_loss: 0.0043789055198431015, g_loss: 8.478055953979492\n","Epoch 63/4000, Step 45, d_loss: 0.0025288178585469723, g_loss: 3.8848133087158203\n","Epoch 63/4000, Step 46, d_loss: 0.05526188760995865, g_loss: 5.442831993103027\n","Epoch 63/4000, Step 47, d_loss: 0.0018421472050249577, g_loss: 5.9006028175354\n","Epoch 63/4000, Step 48, d_loss: 0.07128974795341492, g_loss: 8.114896774291992\n","Epoch 63/4000, Step 49, d_loss: 0.0018117459258064628, g_loss: 5.247283935546875\n","Epoch 63/4000, Step 50, d_loss: 0.0419178381562233, g_loss: 5.663393497467041\n","Epoch 63/4000, Step 51, d_loss: 0.002192681422457099, g_loss: 5.718533992767334\n","Epoch 63/4000, Step 52, d_loss: 0.02112307958304882, g_loss: 4.591231346130371\n","Epoch 63/4000, Step 53, d_loss: 0.020313186571002007, g_loss: 6.308877944946289\n","Epoch 63/4000, Step 54, d_loss: 0.021839480847120285, g_loss: 5.7789306640625\n","Epoch 63/4000, Step 55, d_loss: 0.015088614076375961, g_loss: 7.657670021057129\n","Epoch 63/4000, Step 56, d_loss: 0.004953247494995594, g_loss: 8.941388130187988\n","Epoch 63/4000, Step 57, d_loss: 0.0047760033048689365, g_loss: 6.864202976226807\n","Epoch 63/4000, Step 58, d_loss: 0.041067562997341156, g_loss: 6.459163188934326\n","Epoch 63/4000, Step 59, d_loss: 0.0016132527962327003, g_loss: 7.935925006866455\n","Epoch 63/4000, Step 60, d_loss: 0.0024555225390940905, g_loss: 4.0137200355529785\n","Epoch 63/4000, Step 61, d_loss: 0.006509487517178059, g_loss: 6.648394584655762\n","Epoch 63/4000, Step 62, d_loss: 0.003098079701885581, g_loss: 5.3551554679870605\n","Epoch 63/4000, Step 63, d_loss: 0.0075350673869252205, g_loss: 4.197692394256592\n","Epoch 63/4000, Step 64, d_loss: 0.015601404942572117, g_loss: 8.650742530822754\n","Epoch 63/4000, Step 65, d_loss: 0.02364674210548401, g_loss: 6.695008277893066\n","Epoch 63/4000, Step 66, d_loss: 0.007273560389876366, g_loss: 7.2728118896484375\n","Epoch 63/4000, Step 67, d_loss: 0.012147011235356331, g_loss: 6.650060653686523\n","Epoch 63/4000, Step 68, d_loss: 0.04589313268661499, g_loss: 4.919347286224365\n","Epoch 63/4000, Step 69, d_loss: 0.00772291561588645, g_loss: 6.5190019607543945\n","Epoch 63/4000, Step 70, d_loss: 0.16674484312534332, g_loss: 6.052206516265869\n","Epoch 63/4000, Step 71, d_loss: 0.006488415412604809, g_loss: 5.564969062805176\n","Epoch 63/4000, Step 72, d_loss: 0.05808158591389656, g_loss: 5.210949897766113\n","Epoch 63/4000, Step 73, d_loss: 0.003305572085082531, g_loss: 2.83206844329834\n","Epoch 63/4000, Step 74, d_loss: 0.07185598462820053, g_loss: 2.581218957901001\n","Epoch 63/4000, Step 75, d_loss: 0.009598170407116413, g_loss: 2.291668653488159\n","Epoch 63/4000, Step 76, d_loss: 0.029421165585517883, g_loss: 1.2004021406173706\n","Epoch 63/4000, Step 77, d_loss: 0.47363394498825073, g_loss: 5.756715297698975\n","Epoch 63/4000, Step 78, d_loss: 0.004209479317069054, g_loss: 4.620087623596191\n","Epoch 63/4000, Step 79, d_loss: 0.35432904958724976, g_loss: 5.69426155090332\n","Epoch 63/4000, Step 80, d_loss: 0.009159546345472336, g_loss: 4.205341815948486\n","Epoch 63/4000, Step 81, d_loss: 0.009292131289839745, g_loss: 7.328590393066406\n","Epoch 63/4000, Step 82, d_loss: 0.009971538558602333, g_loss: 4.728750228881836\n","Epoch 63/4000, Step 83, d_loss: 0.0874088779091835, g_loss: 8.209174156188965\n","Epoch 63/4000, Step 84, d_loss: 0.09422411024570465, g_loss: 8.010810852050781\n","Epoch 63/4000, Step 85, d_loss: 0.08948980271816254, g_loss: 4.188201904296875\n","Epoch 63/4000, Step 86, d_loss: 0.01128979492932558, g_loss: 3.2523200511932373\n","Epoch 63/4000, Step 87, d_loss: 0.15311112999916077, g_loss: 3.5122008323669434\n","Epoch 63/4000, Step 88, d_loss: 0.23915442824363708, g_loss: 7.2109270095825195\n","Epoch 63/4000, Step 89, d_loss: 0.021425943821668625, g_loss: 5.745153427124023\n","Epoch 63/4000, Step 90, d_loss: 0.0007749208016321063, g_loss: 2.286872148513794\n","Epoch 63/4000, Step 91, d_loss: 0.042003974318504333, g_loss: 4.222583770751953\n","Epoch 63/4000, Step 92, d_loss: 0.006224325858056545, g_loss: 3.7345762252807617\n","Epoch 63/4000, Step 93, d_loss: 0.06094079464673996, g_loss: 3.809478759765625\n","Epoch 63/4000, Step 94, d_loss: 0.05107526853680611, g_loss: 3.368380069732666\n","Epoch 63/4000, Step 95, d_loss: 0.018835335969924927, g_loss: 3.5198347568511963\n","Epoch 63/4000, Step 96, d_loss: 0.225590318441391, g_loss: 2.0136592388153076\n","Epoch 63/4000, Step 97, d_loss: 0.16178402304649353, g_loss: 4.021189212799072\n","Epoch 63/4000, Step 98, d_loss: 0.23749834299087524, g_loss: 6.431905746459961\n","Epoch 63/4000, Step 99, d_loss: 0.018314601853489876, g_loss: 4.6038923263549805\n","Epoch 63/4000, Step 100, d_loss: 0.03336646407842636, g_loss: 8.756328582763672\n","Epoch 63/4000, Step 101, d_loss: 0.011896191164851189, g_loss: 8.362935066223145\n","Epoch 63/4000, Step 102, d_loss: 0.03850194811820984, g_loss: 7.517673015594482\n","Epoch 63/4000, Step 103, d_loss: 0.28053203225135803, g_loss: 5.393035411834717\n","Epoch 63/4000, Step 104, d_loss: 0.13085277378559113, g_loss: 3.7609660625457764\n","Epoch 63/4000, Step 105, d_loss: 0.025091219693422318, g_loss: 3.6384522914886475\n","Epoch 63/4000, Step 106, d_loss: 0.01014082320034504, g_loss: 3.364623546600342\n","Epoch 63/4000, Step 107, d_loss: 0.036394067108631134, g_loss: 3.474442481994629\n","Epoch 63/4000, Step 108, d_loss: 0.0765937939286232, g_loss: 3.5503451824188232\n","Epoch 63/4000, Step 109, d_loss: 0.03671431168913841, g_loss: 3.8981690406799316\n","Epoch 63/4000, Step 110, d_loss: 0.18271975219249725, g_loss: 0.41758790612220764\n","Epoch 63/4000, Step 111, d_loss: 0.026848500594496727, g_loss: 0.5181567668914795\n","Epoch 63/4000, Step 112, d_loss: 0.09497220069169998, g_loss: 0.7155470848083496\n","Epoch 63/4000, Step 113, d_loss: 0.06873714178800583, g_loss: 3.659947633743286\n","Epoch 63/4000, Step 114, d_loss: 0.5295314192771912, g_loss: 5.814945220947266\n","Epoch 63/4000, Step 115, d_loss: 0.053098469972610474, g_loss: 3.8769736289978027\n","Epoch 63/4000, Step 116, d_loss: 0.08622704446315765, g_loss: 5.476502895355225\n","Epoch 63/4000, Step 117, d_loss: 0.09641806781291962, g_loss: 8.53299617767334\n","Epoch 63/4000, Step 118, d_loss: 0.05670208856463432, g_loss: 8.511636734008789\n","Epoch 63/4000, Step 119, d_loss: 0.0076247891411185265, g_loss: 5.540009498596191\n","Epoch 63/4000, Step 120, d_loss: 0.03679494559764862, g_loss: 6.731064796447754\n","Epoch 63/4000, Step 121, d_loss: 0.1691446602344513, g_loss: 3.7928364276885986\n","Epoch 63/4000, Step 122, d_loss: 0.010846403427422047, g_loss: 5.281918525695801\n","Epoch 63/4000, Step 123, d_loss: 0.025363564491271973, g_loss: 5.643002510070801\n","Epoch 63/4000, Step 124, d_loss: 0.002013657009229064, g_loss: 6.0903191566467285\n","Epoch 63/4000, Step 125, d_loss: 0.035606738179922104, g_loss: 3.8810245990753174\n","Epoch 63/4000, Step 126, d_loss: 0.03960684314370155, g_loss: 4.457709789276123\n","Epoch 63/4000, Step 127, d_loss: 0.019214347004890442, g_loss: 2.782592296600342\n","Epoch 63/4000, Step 128, d_loss: 0.027469243854284286, g_loss: 3.139056921005249\n","Epoch 63/4000, Step 129, d_loss: 0.14168381690979004, g_loss: 2.2545554637908936\n","Epoch 63/4000, Step 130, d_loss: 0.011130308732390404, g_loss: 2.050367832183838\n","Epoch 63/4000, Step 131, d_loss: 0.020889831706881523, g_loss: 1.0758638381958008\n","Epoch 63/4000, Step 132, d_loss: 0.09253193438053131, g_loss: 4.666289329528809\n","Epoch 63/4000, Step 133, d_loss: 0.006846528500318527, g_loss: 2.631284713745117\n","Epoch 63/4000, Step 134, d_loss: 0.08916443586349487, g_loss: 1.7513365745544434\n","Epoch 63/4000, Step 135, d_loss: 0.007552838884294033, g_loss: 4.834052085876465\n","Epoch 63/4000, Step 136, d_loss: 0.0349612683057785, g_loss: 0.41836032271385193\n","Epoch 63/4000, Step 137, d_loss: 0.08444184809923172, g_loss: 1.3057048320770264\n","Epoch 63/4000, Step 138, d_loss: 0.008926380425691605, g_loss: 4.3497138023376465\n","Epoch 63/4000, Step 139, d_loss: 0.049167193472385406, g_loss: 5.314252853393555\n","Epoch 63/4000, Step 140, d_loss: 0.042665813118219376, g_loss: 4.064252853393555\n","Epoch 63/4000, Step 141, d_loss: 0.08005059510469437, g_loss: 3.5152699947357178\n","Epoch 63/4000, Step 142, d_loss: 0.058996085077524185, g_loss: 2.9317402839660645\n","Epoch 63/4000, Step 143, d_loss: 0.03512194752693176, g_loss: 6.34480619430542\n","Epoch 63/4000, Step 144, d_loss: 0.018913496285676956, g_loss: 3.712522506713867\n","Epoch 63/4000, Step 145, d_loss: 0.0999382734298706, g_loss: 7.808186054229736\n","Epoch 63/4000, Step 146, d_loss: 0.003180172760039568, g_loss: 4.048205852508545\n","Epoch 63/4000, Step 147, d_loss: 0.09121087193489075, g_loss: 8.692073822021484\n","Epoch 63/4000, Step 148, d_loss: 0.10838036239147186, g_loss: 4.595389366149902\n","Epoch 63/4000, Step 149, d_loss: 0.007086038123816252, g_loss: 9.756770133972168\n","Epoch 63/4000, Step 150, d_loss: 0.004453278612345457, g_loss: 4.869726181030273\n","Epoch 63/4000, Step 151, d_loss: 0.024948973208665848, g_loss: 2.6130685806274414\n","Epoch 63/4000, Step 152, d_loss: 0.20407047867774963, g_loss: 1.2446240186691284\n","Epoch 63/4000, Step 153, d_loss: 0.0016596554778516293, g_loss: 4.652588367462158\n","Epoch 63/4000, Step 154, d_loss: 0.06595852971076965, g_loss: 4.6108174324035645\n","Epoch 63/4000, Step 155, d_loss: 0.33374977111816406, g_loss: 7.624292850494385\n","Epoch 63/4000, Step 156, d_loss: 0.02338399924337864, g_loss: 6.987262725830078\n","Epoch 63/4000, Step 157, d_loss: 0.7425346374511719, g_loss: 9.132590293884277\n","Epoch 63/4000, Step 158, d_loss: 0.49003174901008606, g_loss: 5.831322193145752\n","Epoch 63/4000, Step 159, d_loss: 0.029000665992498398, g_loss: 3.673196315765381\n","Epoch 63/4000, Step 160, d_loss: 0.049324195832014084, g_loss: 3.5225727558135986\n","Epoch 63/4000, Step 161, d_loss: 0.2338249683380127, g_loss: 2.9501893520355225\n","Epoch 63/4000, Step 162, d_loss: 0.540691614151001, g_loss: 4.665245532989502\n","Epoch 63/4000, Step 163, d_loss: 0.061324700713157654, g_loss: 4.763645648956299\n","Epoch 63/4000, Step 164, d_loss: 0.32380566000938416, g_loss: 3.692166328430176\n","Epoch 63/4000, Step 165, d_loss: 0.37586089968681335, g_loss: 4.537601947784424\n","Epoch 63/4000, Step 166, d_loss: 0.1013953909277916, g_loss: 6.179355144500732\n","Epoch 63/4000, Step 167, d_loss: 0.10028177499771118, g_loss: 5.1205153465271\n","Epoch 63/4000, Step 168, d_loss: 0.2163020670413971, g_loss: 2.3971667289733887\n","Epoch 63/4000, Step 169, d_loss: 0.09179456532001495, g_loss: 3.9841582775115967\n","Epoch 63/4000, Step 170, d_loss: 0.3000968396663666, g_loss: 3.082634449005127\n","Epoch 63/4000, Step 171, d_loss: 0.3281155228614807, g_loss: 1.4533897638320923\n","Epoch 63/4000, Step 172, d_loss: 0.10277988016605377, g_loss: 1.8762863874435425\n","Epoch 63/4000, Step 173, d_loss: 0.06494884938001633, g_loss: 2.7569215297698975\n","Epoch 63/4000, Step 174, d_loss: 0.11618676036596298, g_loss: 2.223971128463745\n","Epoch 63/4000, Step 175, d_loss: 0.10380705446004868, g_loss: 1.6279630661010742\n","Epoch 63/4000, Step 176, d_loss: 0.15622210502624512, g_loss: 0.7763806581497192\n","Epoch 63/4000, Step 177, d_loss: 0.38356608152389526, g_loss: 4.031356334686279\n","Epoch 63/4000, Step 178, d_loss: 0.024864286184310913, g_loss: 1.3217461109161377\n","Epoch 63/4000, Step 179, d_loss: 0.04117322713136673, g_loss: 2.593003511428833\n","Epoch 63/4000, Step 180, d_loss: 0.15675827860832214, g_loss: 0.9994258284568787\n","Epoch 63/4000, Step 181, d_loss: 0.0950951874256134, g_loss: 2.690506935119629\n","Epoch 63/4000, Step 182, d_loss: 0.03651190549135208, g_loss: 1.9628311395645142\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 64/4000, Step 1, d_loss: 0.47431302070617676, g_loss: 4.496231555938721\n","Epoch 64/4000, Step 2, d_loss: 0.15158285200595856, g_loss: 2.4488494396209717\n","Epoch 64/4000, Step 3, d_loss: 0.01892678812146187, g_loss: 4.971914291381836\n","Epoch 64/4000, Step 4, d_loss: 0.0421065017580986, g_loss: 4.590743064880371\n","Epoch 64/4000, Step 5, d_loss: 0.07960838824510574, g_loss: 2.615070343017578\n","Epoch 64/4000, Step 6, d_loss: 0.09785725176334381, g_loss: 2.719754695892334\n","Epoch 64/4000, Step 7, d_loss: 0.0958552360534668, g_loss: 3.1438639163970947\n","Epoch 64/4000, Step 8, d_loss: 0.1768587976694107, g_loss: 4.084278583526611\n","Epoch 64/4000, Step 9, d_loss: 0.03673078492283821, g_loss: 3.0305118560791016\n","Epoch 64/4000, Step 10, d_loss: 0.045930925756692886, g_loss: 2.0225961208343506\n","Epoch 64/4000, Step 11, d_loss: 0.027606485411524773, g_loss: 4.957229137420654\n","Epoch 64/4000, Step 12, d_loss: 0.0023568705655634403, g_loss: 4.387729167938232\n","Epoch 64/4000, Step 13, d_loss: 0.06700798124074936, g_loss: 3.6096720695495605\n","Epoch 64/4000, Step 14, d_loss: 0.014591064304113388, g_loss: 6.6546244621276855\n","Epoch 64/4000, Step 15, d_loss: 0.019247252494096756, g_loss: 5.031309604644775\n","Epoch 64/4000, Step 16, d_loss: 0.027158506214618683, g_loss: 3.567737579345703\n","Epoch 64/4000, Step 17, d_loss: 0.011912737041711807, g_loss: 4.919869422912598\n","Epoch 64/4000, Step 18, d_loss: 0.017744265496730804, g_loss: 5.833226680755615\n","Epoch 64/4000, Step 19, d_loss: 0.09494335949420929, g_loss: 6.224240779876709\n","Epoch 64/4000, Step 20, d_loss: 0.1496778130531311, g_loss: 3.624051332473755\n","Epoch 64/4000, Step 21, d_loss: 0.013008877635002136, g_loss: 4.092107772827148\n","Epoch 64/4000, Step 22, d_loss: 0.02841559611260891, g_loss: 5.017531871795654\n","Epoch 64/4000, Step 23, d_loss: 0.06322715431451797, g_loss: 3.7190754413604736\n","Epoch 64/4000, Step 24, d_loss: 0.04838871955871582, g_loss: 2.6114463806152344\n","Epoch 64/4000, Step 25, d_loss: 0.0345054529607296, g_loss: 3.228870391845703\n","Epoch 64/4000, Step 26, d_loss: 0.010576019063591957, g_loss: 2.7789392471313477\n","Epoch 64/4000, Step 27, d_loss: 0.006740501616150141, g_loss: 2.4159321784973145\n","Epoch 64/4000, Step 28, d_loss: 0.007272911258041859, g_loss: 4.358147144317627\n","Epoch 64/4000, Step 29, d_loss: 0.17336402833461761, g_loss: 3.2722020149230957\n","Epoch 64/4000, Step 30, d_loss: 0.01183191780000925, g_loss: 5.361165523529053\n","Epoch 64/4000, Step 31, d_loss: 0.06270623952150345, g_loss: 3.203404664993286\n","Epoch 64/4000, Step 32, d_loss: 0.007019532844424248, g_loss: 5.016226768493652\n","Epoch 64/4000, Step 33, d_loss: 0.011913441121578217, g_loss: 3.9254658222198486\n","Epoch 64/4000, Step 34, d_loss: 0.04827846586704254, g_loss: 4.590818405151367\n","Epoch 64/4000, Step 35, d_loss: 0.11576763540506363, g_loss: 4.77956485748291\n","Epoch 64/4000, Step 36, d_loss: 0.006769848521798849, g_loss: 7.166961193084717\n","Epoch 64/4000, Step 37, d_loss: 0.1260230392217636, g_loss: 7.570466041564941\n","Epoch 64/4000, Step 38, d_loss: 0.011858455836772919, g_loss: 3.848158121109009\n","Epoch 64/4000, Step 39, d_loss: 0.01841370388865471, g_loss: 4.484460353851318\n","Epoch 64/4000, Step 40, d_loss: 0.17705358564853668, g_loss: 3.9167277812957764\n","Epoch 64/4000, Step 41, d_loss: 0.026800846680998802, g_loss: 5.10030460357666\n","Epoch 64/4000, Step 42, d_loss: 0.004415053408592939, g_loss: 3.9354584217071533\n","Epoch 64/4000, Step 43, d_loss: 0.026723939925432205, g_loss: 3.7125086784362793\n","Epoch 64/4000, Step 44, d_loss: 0.021735239773988724, g_loss: 3.343043088912964\n","Epoch 64/4000, Step 45, d_loss: 0.1279713362455368, g_loss: 4.809746742248535\n","Epoch 64/4000, Step 46, d_loss: 0.028131945058703423, g_loss: 2.5673975944519043\n","Epoch 64/4000, Step 47, d_loss: 0.08743516355752945, g_loss: 4.416671276092529\n","Epoch 64/4000, Step 48, d_loss: 0.03433578833937645, g_loss: 4.9620771408081055\n","Epoch 64/4000, Step 49, d_loss: 0.019465861842036247, g_loss: 4.797672748565674\n","Epoch 64/4000, Step 50, d_loss: 0.050374507904052734, g_loss: 4.160788536071777\n","Epoch 64/4000, Step 51, d_loss: 0.050118084996938705, g_loss: 6.433200836181641\n","Epoch 64/4000, Step 52, d_loss: 0.011528906412422657, g_loss: 5.235622406005859\n","Epoch 64/4000, Step 53, d_loss: 0.04205750301480293, g_loss: 5.371323585510254\n","Epoch 64/4000, Step 54, d_loss: 0.012145206332206726, g_loss: 4.796868324279785\n","Epoch 64/4000, Step 55, d_loss: 0.016253462061285973, g_loss: 7.961701393127441\n","Epoch 64/4000, Step 56, d_loss: 0.022804496809840202, g_loss: 4.364878177642822\n","Epoch 64/4000, Step 57, d_loss: 0.21396860480308533, g_loss: 5.000035762786865\n","Epoch 64/4000, Step 58, d_loss: 0.06354088336229324, g_loss: 6.188133239746094\n","Epoch 64/4000, Step 59, d_loss: 0.0095283854752779, g_loss: 3.8203072547912598\n","Epoch 64/4000, Step 60, d_loss: 0.021518955007195473, g_loss: 4.812871932983398\n","Epoch 64/4000, Step 61, d_loss: 0.06669122725725174, g_loss: 9.323623657226562\n","Epoch 64/4000, Step 62, d_loss: 0.026609933003783226, g_loss: 3.9401698112487793\n","Epoch 64/4000, Step 63, d_loss: 0.035150881856679916, g_loss: 4.0955119132995605\n","Epoch 64/4000, Step 64, d_loss: 0.06897658854722977, g_loss: 2.5858469009399414\n","Epoch 64/4000, Step 65, d_loss: 0.004856518004089594, g_loss: 4.229550838470459\n","Epoch 64/4000, Step 66, d_loss: 0.02302652597427368, g_loss: 4.74373197555542\n","Epoch 64/4000, Step 67, d_loss: 0.015092725865542889, g_loss: 4.581525802612305\n","Epoch 64/4000, Step 68, d_loss: 0.0018604958895593882, g_loss: 8.371888160705566\n","Epoch 64/4000, Step 69, d_loss: 0.00685389619320631, g_loss: 4.588138580322266\n","Epoch 64/4000, Step 70, d_loss: 0.011516054160892963, g_loss: 8.109105110168457\n","Epoch 64/4000, Step 71, d_loss: 0.018925566226243973, g_loss: 6.348300933837891\n","Epoch 64/4000, Step 72, d_loss: 0.033183470368385315, g_loss: 5.927467346191406\n","Epoch 64/4000, Step 73, d_loss: 0.022344835102558136, g_loss: 7.560077667236328\n","Epoch 64/4000, Step 74, d_loss: 0.008535683155059814, g_loss: 4.622641086578369\n","Epoch 64/4000, Step 75, d_loss: 0.013635270297527313, g_loss: 7.205366611480713\n","Epoch 64/4000, Step 76, d_loss: 0.01994158700108528, g_loss: 4.40349817276001\n","Epoch 64/4000, Step 77, d_loss: 0.0064881350845098495, g_loss: 6.509961128234863\n","Epoch 64/4000, Step 78, d_loss: 0.028018375858664513, g_loss: 5.329827308654785\n","Epoch 64/4000, Step 79, d_loss: 0.01450452022254467, g_loss: 6.983448028564453\n","Epoch 64/4000, Step 80, d_loss: 0.007472628727555275, g_loss: 5.396026611328125\n","Epoch 64/4000, Step 81, d_loss: 0.012462850660085678, g_loss: 6.827356815338135\n","Epoch 64/4000, Step 82, d_loss: 0.0014778372133150697, g_loss: 3.9649651050567627\n","Epoch 64/4000, Step 83, d_loss: 0.01661541499197483, g_loss: 6.2580246925354\n","Epoch 64/4000, Step 84, d_loss: 0.01232360489666462, g_loss: 7.7367353439331055\n","Epoch 64/4000, Step 85, d_loss: 0.006773047149181366, g_loss: 5.530202388763428\n","Epoch 64/4000, Step 86, d_loss: 0.01977732963860035, g_loss: 5.017366409301758\n","Epoch 64/4000, Step 87, d_loss: 0.001139743602834642, g_loss: 5.4808244705200195\n","Epoch 64/4000, Step 88, d_loss: 0.004420326091349125, g_loss: 4.643540382385254\n","Epoch 64/4000, Step 89, d_loss: 0.005126041825860739, g_loss: 5.630420684814453\n","Epoch 64/4000, Step 90, d_loss: 0.0031645046547055244, g_loss: 4.171235084533691\n","Epoch 64/4000, Step 91, d_loss: 0.011449341662228107, g_loss: 6.574019908905029\n","Epoch 64/4000, Step 92, d_loss: 0.002748984843492508, g_loss: 5.094976425170898\n","Epoch 64/4000, Step 93, d_loss: 0.009550485759973526, g_loss: 3.8563315868377686\n","Epoch 64/4000, Step 94, d_loss: 0.01263731624931097, g_loss: 5.398221015930176\n","Epoch 64/4000, Step 95, d_loss: 0.0025460401084274054, g_loss: 3.3849525451660156\n","Epoch 64/4000, Step 96, d_loss: 0.0033041867427527905, g_loss: 4.625765323638916\n","Epoch 64/4000, Step 97, d_loss: 0.05960005894303322, g_loss: 4.281187057495117\n","Epoch 64/4000, Step 98, d_loss: 0.007629983127117157, g_loss: 3.930314779281616\n","Epoch 64/4000, Step 99, d_loss: 0.008995765820145607, g_loss: 7.264868259429932\n","Epoch 64/4000, Step 100, d_loss: 0.014798594638705254, g_loss: 4.695963382720947\n","Epoch 64/4000, Step 101, d_loss: 0.004088880028575659, g_loss: 5.97966194152832\n","Epoch 64/4000, Step 102, d_loss: 0.0029144352301955223, g_loss: 5.585514545440674\n","Epoch 64/4000, Step 103, d_loss: 0.01888453960418701, g_loss: 4.858944416046143\n","Epoch 64/4000, Step 104, d_loss: 0.012258211150765419, g_loss: 4.346048355102539\n","Epoch 64/4000, Step 105, d_loss: 0.048128169029951096, g_loss: 5.182637691497803\n","Epoch 64/4000, Step 106, d_loss: 0.005492318421602249, g_loss: 4.769224166870117\n","Epoch 64/4000, Step 107, d_loss: 0.014306693337857723, g_loss: 5.2446136474609375\n","Epoch 64/4000, Step 108, d_loss: 0.010080552659928799, g_loss: 5.484818458557129\n","Epoch 64/4000, Step 109, d_loss: 0.009552843868732452, g_loss: 5.117175579071045\n","Epoch 64/4000, Step 110, d_loss: 0.00585993891581893, g_loss: 6.6929216384887695\n","Epoch 64/4000, Step 111, d_loss: 0.027347400784492493, g_loss: 6.9067888259887695\n","Epoch 64/4000, Step 112, d_loss: 0.006571468431502581, g_loss: 6.230762004852295\n","Epoch 64/4000, Step 113, d_loss: 0.011124204844236374, g_loss: 3.6641428470611572\n","Epoch 64/4000, Step 114, d_loss: 0.005706914700567722, g_loss: 7.434943675994873\n","Epoch 64/4000, Step 115, d_loss: 0.015086477622389793, g_loss: 4.990891456604004\n","Epoch 64/4000, Step 116, d_loss: 0.0040184082463383675, g_loss: 2.9069249629974365\n","Epoch 64/4000, Step 117, d_loss: 0.04720054566860199, g_loss: 6.911430358886719\n","Epoch 64/4000, Step 118, d_loss: 0.004742417950183153, g_loss: 5.205711364746094\n","Epoch 64/4000, Step 119, d_loss: 0.005273167043924332, g_loss: 4.144773006439209\n","Epoch 64/4000, Step 120, d_loss: 0.009340119548141956, g_loss: 5.394510269165039\n","Epoch 64/4000, Step 121, d_loss: 0.00969000905752182, g_loss: 5.758295059204102\n","Epoch 64/4000, Step 122, d_loss: 0.032407764345407486, g_loss: 6.346705913543701\n","Epoch 64/4000, Step 123, d_loss: 0.021476183086633682, g_loss: 6.189246654510498\n","Epoch 64/4000, Step 124, d_loss: 0.04539799317717552, g_loss: 6.584192276000977\n","Epoch 64/4000, Step 125, d_loss: 0.005313975270837545, g_loss: 4.772215843200684\n","Epoch 64/4000, Step 126, d_loss: 0.007522445637732744, g_loss: 9.00560188293457\n","Epoch 64/4000, Step 127, d_loss: 0.0112912617623806, g_loss: 6.7045722007751465\n","Epoch 64/4000, Step 128, d_loss: 0.009218882769346237, g_loss: 5.165313720703125\n","Epoch 64/4000, Step 129, d_loss: 0.02818666398525238, g_loss: 5.364837169647217\n","Epoch 64/4000, Step 130, d_loss: 0.007135487627238035, g_loss: 6.518087387084961\n","Epoch 64/4000, Step 131, d_loss: 0.018089646473526955, g_loss: 5.3005547523498535\n","Epoch 64/4000, Step 132, d_loss: 0.013379656709730625, g_loss: 6.240814208984375\n","Epoch 64/4000, Step 133, d_loss: 0.008347266353666782, g_loss: 4.612493991851807\n","Epoch 64/4000, Step 134, d_loss: 0.010339388623833656, g_loss: 4.5203375816345215\n","Epoch 64/4000, Step 135, d_loss: 0.0027142404578626156, g_loss: 5.136584758758545\n","Epoch 64/4000, Step 136, d_loss: 0.011511179618537426, g_loss: 5.435461521148682\n","Epoch 64/4000, Step 137, d_loss: 0.002524529816582799, g_loss: 4.659830093383789\n","Epoch 64/4000, Step 138, d_loss: 0.008705206215381622, g_loss: 6.084531784057617\n","Epoch 64/4000, Step 139, d_loss: 0.012355122715234756, g_loss: 6.373268127441406\n","Epoch 64/4000, Step 140, d_loss: 0.008251991122961044, g_loss: 5.349125385284424\n","Epoch 64/4000, Step 141, d_loss: 0.023787882179021835, g_loss: 6.326515197753906\n","Epoch 64/4000, Step 142, d_loss: 0.07931718230247498, g_loss: 5.708649635314941\n","Epoch 64/4000, Step 143, d_loss: 0.004869951866567135, g_loss: 5.124931335449219\n","Epoch 64/4000, Step 144, d_loss: 0.001360677182674408, g_loss: 4.252179145812988\n","Epoch 64/4000, Step 145, d_loss: 0.0013672603527083993, g_loss: 4.945982933044434\n","Epoch 64/4000, Step 146, d_loss: 0.005307123064994812, g_loss: 5.639874458312988\n","Epoch 64/4000, Step 147, d_loss: 0.014234968461096287, g_loss: 4.196079254150391\n","Epoch 64/4000, Step 148, d_loss: 0.0038433060981333256, g_loss: 4.882176876068115\n","Epoch 64/4000, Step 149, d_loss: 0.014285667799413204, g_loss: 4.33560037612915\n","Epoch 64/4000, Step 150, d_loss: 0.022134045138955116, g_loss: 4.676828384399414\n","Epoch 64/4000, Step 151, d_loss: 0.0019870332907885313, g_loss: 2.661027669906616\n","Epoch 64/4000, Step 152, d_loss: 0.002731299726292491, g_loss: 5.427525520324707\n","Epoch 64/4000, Step 153, d_loss: 0.020594486966729164, g_loss: 4.381052017211914\n","Epoch 64/4000, Step 154, d_loss: 0.003118482418358326, g_loss: 5.019569396972656\n","Epoch 64/4000, Step 155, d_loss: 0.0018236036412417889, g_loss: 5.227336406707764\n","Epoch 64/4000, Step 156, d_loss: 0.00434490293264389, g_loss: 3.4351043701171875\n","Epoch 64/4000, Step 157, d_loss: 0.0013302451698109508, g_loss: 4.521509647369385\n","Epoch 64/4000, Step 158, d_loss: 0.01346677727997303, g_loss: 6.549113750457764\n","Epoch 64/4000, Step 159, d_loss: 0.020907169207930565, g_loss: 6.0613603591918945\n","Epoch 64/4000, Step 160, d_loss: 0.010352143086493015, g_loss: 5.281731128692627\n","Epoch 64/4000, Step 161, d_loss: 0.01262630894780159, g_loss: 5.8554487228393555\n","Epoch 64/4000, Step 162, d_loss: 0.007931562140583992, g_loss: 5.904259204864502\n","Epoch 64/4000, Step 163, d_loss: 0.012009893544018269, g_loss: 5.4652862548828125\n","Epoch 64/4000, Step 164, d_loss: 0.0053201038390398026, g_loss: 4.720522880554199\n","Epoch 64/4000, Step 165, d_loss: 0.011689139530062675, g_loss: 5.376272201538086\n","Epoch 64/4000, Step 166, d_loss: 0.009392080828547478, g_loss: 6.686797142028809\n","Epoch 64/4000, Step 167, d_loss: 0.00600612023845315, g_loss: 5.960895538330078\n","Epoch 64/4000, Step 168, d_loss: 0.0036170161329209805, g_loss: 8.974466323852539\n","Epoch 64/4000, Step 169, d_loss: 0.0021660812199115753, g_loss: 7.1376800537109375\n","Epoch 64/4000, Step 170, d_loss: 0.034333907067775726, g_loss: 8.175102233886719\n","Epoch 64/4000, Step 171, d_loss: 0.003737173741683364, g_loss: 5.280251502990723\n","Epoch 64/4000, Step 172, d_loss: 0.020768843591213226, g_loss: 5.9089813232421875\n","Epoch 64/4000, Step 173, d_loss: 0.010718672536313534, g_loss: 5.3693695068359375\n","Epoch 64/4000, Step 174, d_loss: 0.008901580236852169, g_loss: 4.991998672485352\n","Epoch 64/4000, Step 175, d_loss: 0.008125060237944126, g_loss: 7.196681976318359\n","Epoch 64/4000, Step 176, d_loss: 0.002271390985697508, g_loss: 5.985548496246338\n","Epoch 64/4000, Step 177, d_loss: 0.0054200151935219765, g_loss: 6.207758903503418\n","Epoch 64/4000, Step 178, d_loss: 0.005472645163536072, g_loss: 5.529338359832764\n","Epoch 64/4000, Step 179, d_loss: 0.005416742525994778, g_loss: 5.341984748840332\n","Epoch 64/4000, Step 180, d_loss: 0.011187328025698662, g_loss: 5.0416436195373535\n","Epoch 64/4000, Step 181, d_loss: 0.007911734282970428, g_loss: 6.309185028076172\n","Epoch 64/4000, Step 182, d_loss: 0.0437266081571579, g_loss: 4.284585475921631\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 65/4000, Step 1, d_loss: 0.005151643883436918, g_loss: 5.909940719604492\n","Epoch 65/4000, Step 2, d_loss: 0.00535233598202467, g_loss: 4.828708648681641\n","Epoch 65/4000, Step 3, d_loss: 0.0026905816048383713, g_loss: 6.2745537757873535\n","Epoch 65/4000, Step 4, d_loss: 0.010643796995282173, g_loss: 5.672617435455322\n","Epoch 65/4000, Step 5, d_loss: 0.0080382339656353, g_loss: 5.856600284576416\n","Epoch 65/4000, Step 6, d_loss: 0.006878498010337353, g_loss: 7.511504173278809\n","Epoch 65/4000, Step 7, d_loss: 0.01722639612853527, g_loss: 6.222930908203125\n","Epoch 65/4000, Step 8, d_loss: 0.005170687101781368, g_loss: 7.240995407104492\n","Epoch 65/4000, Step 9, d_loss: 0.021206779405474663, g_loss: 4.668022632598877\n","Epoch 65/4000, Step 10, d_loss: 0.00414137402549386, g_loss: 5.298421859741211\n","Epoch 65/4000, Step 11, d_loss: 0.0063238306902348995, g_loss: 3.627673864364624\n","Epoch 65/4000, Step 12, d_loss: 0.010489647276699543, g_loss: 4.007623195648193\n","Epoch 65/4000, Step 13, d_loss: 0.005621287971735001, g_loss: 5.7423295974731445\n","Epoch 65/4000, Step 14, d_loss: 0.0012245934922248125, g_loss: 7.151016712188721\n","Epoch 65/4000, Step 15, d_loss: 0.004219257738441229, g_loss: 6.2935028076171875\n","Epoch 65/4000, Step 16, d_loss: 0.0010925194947049022, g_loss: 5.274331569671631\n","Epoch 65/4000, Step 17, d_loss: 0.035382114350795746, g_loss: 7.475604057312012\n","Epoch 65/4000, Step 18, d_loss: 0.019157394766807556, g_loss: 4.475512504577637\n","Epoch 65/4000, Step 19, d_loss: 0.0054123615846037865, g_loss: 3.563397169113159\n","Epoch 65/4000, Step 20, d_loss: 0.003660561516880989, g_loss: 4.579956531524658\n","Epoch 65/4000, Step 21, d_loss: 0.0067300801165401936, g_loss: 5.428988933563232\n","Epoch 65/4000, Step 22, d_loss: 0.03039761632680893, g_loss: 6.007995128631592\n","Epoch 65/4000, Step 23, d_loss: 0.01867755688726902, g_loss: 7.720844745635986\n","Epoch 65/4000, Step 24, d_loss: 0.009104261174798012, g_loss: 7.756078720092773\n","Epoch 65/4000, Step 25, d_loss: 0.004364748485386372, g_loss: 7.189657688140869\n","Epoch 65/4000, Step 26, d_loss: 0.02135601080954075, g_loss: 5.3682427406311035\n","Epoch 65/4000, Step 27, d_loss: 0.098052017390728, g_loss: 7.859659671783447\n","Epoch 65/4000, Step 28, d_loss: 0.008959868922829628, g_loss: 6.05872106552124\n","Epoch 65/4000, Step 29, d_loss: 0.025695089250802994, g_loss: 5.194690704345703\n","Epoch 65/4000, Step 30, d_loss: 0.0073770261369645596, g_loss: 4.383421897888184\n","Epoch 65/4000, Step 31, d_loss: 0.04674108326435089, g_loss: 4.5354766845703125\n","Epoch 65/4000, Step 32, d_loss: 0.005894042085856199, g_loss: 6.179961681365967\n","Epoch 65/4000, Step 33, d_loss: 0.010909954085946083, g_loss: 8.43744945526123\n","Epoch 65/4000, Step 34, d_loss: 0.010061655193567276, g_loss: 4.172813415527344\n","Epoch 65/4000, Step 35, d_loss: 0.014627549797296524, g_loss: 5.590540885925293\n","Epoch 65/4000, Step 36, d_loss: 0.006941832136362791, g_loss: 6.0353827476501465\n","Epoch 65/4000, Step 37, d_loss: 0.005492311902344227, g_loss: 7.219560146331787\n","Epoch 65/4000, Step 38, d_loss: 0.004279748536646366, g_loss: 8.6358003616333\n","Epoch 65/4000, Step 39, d_loss: 0.0042745075188577175, g_loss: 6.7853827476501465\n","Epoch 65/4000, Step 40, d_loss: 0.0007120427908375859, g_loss: 6.920873641967773\n","Epoch 65/4000, Step 41, d_loss: 0.0014183111488819122, g_loss: 7.01033353805542\n","Epoch 65/4000, Step 42, d_loss: 0.006415679585188627, g_loss: 6.317764759063721\n","Epoch 65/4000, Step 43, d_loss: 0.004581310320645571, g_loss: 7.175841331481934\n","Epoch 65/4000, Step 44, d_loss: 0.005690249614417553, g_loss: 7.590234279632568\n","Epoch 65/4000, Step 45, d_loss: 0.003936876077204943, g_loss: 5.334057331085205\n","Epoch 65/4000, Step 46, d_loss: 0.027494726702570915, g_loss: 6.150757789611816\n","Epoch 65/4000, Step 47, d_loss: 0.00802296120673418, g_loss: 6.510227203369141\n","Epoch 65/4000, Step 48, d_loss: 0.013496479950845242, g_loss: 4.6293864250183105\n","Epoch 65/4000, Step 49, d_loss: 0.0033343122340738773, g_loss: 5.957420349121094\n","Epoch 65/4000, Step 50, d_loss: 0.011182023212313652, g_loss: 6.163941383361816\n","Epoch 65/4000, Step 51, d_loss: 0.008962482213973999, g_loss: 5.639404296875\n","Epoch 65/4000, Step 52, d_loss: 0.014757346361875534, g_loss: 6.8807291984558105\n","Epoch 65/4000, Step 53, d_loss: 0.0032557789236307144, g_loss: 5.308556079864502\n","Epoch 65/4000, Step 54, d_loss: 0.0008895338978618383, g_loss: 7.954538822174072\n","Epoch 65/4000, Step 55, d_loss: 0.0047926148399710655, g_loss: 5.845246315002441\n","Epoch 65/4000, Step 56, d_loss: 0.004866405390202999, g_loss: 6.2607197761535645\n","Epoch 65/4000, Step 57, d_loss: 0.023397915065288544, g_loss: 2.8776779174804688\n","Epoch 65/4000, Step 58, d_loss: 0.006055716425180435, g_loss: 4.484374523162842\n","Epoch 65/4000, Step 59, d_loss: 0.0056486790999770164, g_loss: 4.968851089477539\n","Epoch 65/4000, Step 60, d_loss: 0.0035034222528338432, g_loss: 6.559535503387451\n","Epoch 65/4000, Step 61, d_loss: 0.004278537817299366, g_loss: 7.117339134216309\n","Epoch 65/4000, Step 62, d_loss: 0.004567193798720837, g_loss: 6.450281620025635\n","Epoch 65/4000, Step 63, d_loss: 0.0029913829639554024, g_loss: 5.917619228363037\n","Epoch 65/4000, Step 64, d_loss: 0.002831903984770179, g_loss: 5.125802040100098\n","Epoch 65/4000, Step 65, d_loss: 0.002022504573687911, g_loss: 6.469482421875\n","Epoch 65/4000, Step 66, d_loss: 0.026557378470897675, g_loss: 5.911529541015625\n","Epoch 65/4000, Step 67, d_loss: 0.007929299958050251, g_loss: 9.348563194274902\n","Epoch 65/4000, Step 68, d_loss: 0.0009737231303006411, g_loss: 5.587453365325928\n","Epoch 65/4000, Step 69, d_loss: 0.006834668107330799, g_loss: 5.995704174041748\n","Epoch 65/4000, Step 70, d_loss: 0.008839377202093601, g_loss: 7.2241435050964355\n","Epoch 65/4000, Step 71, d_loss: 0.0052492497488856316, g_loss: 7.140152931213379\n","Epoch 65/4000, Step 72, d_loss: 0.007486232556402683, g_loss: 6.107874393463135\n","Epoch 65/4000, Step 73, d_loss: 0.02082761749625206, g_loss: 6.218625545501709\n","Epoch 65/4000, Step 74, d_loss: 0.016674933955073357, g_loss: 5.1694016456604\n","Epoch 65/4000, Step 75, d_loss: 0.0036909058690071106, g_loss: 6.363557815551758\n","Epoch 65/4000, Step 76, d_loss: 0.008587289601564407, g_loss: 4.635568141937256\n","Epoch 65/4000, Step 77, d_loss: 0.0028086300007998943, g_loss: 6.826737403869629\n","Epoch 65/4000, Step 78, d_loss: 0.007374229840934277, g_loss: 3.854950189590454\n","Epoch 65/4000, Step 79, d_loss: 0.009151785634458065, g_loss: 8.493206977844238\n","Epoch 65/4000, Step 80, d_loss: 0.00256138457916677, g_loss: 5.68121337890625\n","Epoch 65/4000, Step 81, d_loss: 0.0018351785838603973, g_loss: 6.5581488609313965\n","Epoch 65/4000, Step 82, d_loss: 0.002994641661643982, g_loss: 3.8878509998321533\n","Epoch 65/4000, Step 83, d_loss: 0.008372941985726357, g_loss: 8.71674633026123\n","Epoch 65/4000, Step 84, d_loss: 0.002138464478775859, g_loss: 5.831665992736816\n","Epoch 65/4000, Step 85, d_loss: 0.004231192171573639, g_loss: 7.36132287979126\n","Epoch 65/4000, Step 86, d_loss: 0.013547667302191257, g_loss: 7.2144575119018555\n","Epoch 65/4000, Step 87, d_loss: 0.00023010483710095286, g_loss: 7.598722457885742\n","Epoch 65/4000, Step 88, d_loss: 0.0035637205000966787, g_loss: 7.986124515533447\n","Epoch 65/4000, Step 89, d_loss: 0.0009150293772108853, g_loss: 6.888207912445068\n","Epoch 65/4000, Step 90, d_loss: 0.0015951177338138223, g_loss: 9.849199295043945\n","Epoch 65/4000, Step 91, d_loss: 0.0009685651166364551, g_loss: 6.5476975440979\n","Epoch 65/4000, Step 92, d_loss: 0.0005413684993982315, g_loss: 6.254611015319824\n","Epoch 65/4000, Step 93, d_loss: 0.009724526666104794, g_loss: 5.263788223266602\n","Epoch 65/4000, Step 94, d_loss: 0.001818071585148573, g_loss: 5.886125087738037\n","Epoch 65/4000, Step 95, d_loss: 0.009797533974051476, g_loss: 7.587393760681152\n","Epoch 65/4000, Step 96, d_loss: 0.027315778657794, g_loss: 9.230267524719238\n","Epoch 65/4000, Step 97, d_loss: 0.034347280859947205, g_loss: 5.746382236480713\n","Epoch 65/4000, Step 98, d_loss: 0.016942018643021584, g_loss: 6.874707221984863\n","Epoch 65/4000, Step 99, d_loss: 0.003706063609570265, g_loss: 8.689729690551758\n","Epoch 65/4000, Step 100, d_loss: 0.007182351313531399, g_loss: 6.63895845413208\n","Epoch 65/4000, Step 101, d_loss: 0.0005014082416892052, g_loss: 5.155902862548828\n","Epoch 65/4000, Step 102, d_loss: 0.004007148556411266, g_loss: 5.7056193351745605\n","Epoch 65/4000, Step 103, d_loss: 0.0037116860039532185, g_loss: 5.0603437423706055\n","Epoch 65/4000, Step 104, d_loss: 0.01020895130932331, g_loss: 4.556435585021973\n","Epoch 65/4000, Step 105, d_loss: 0.00887400470674038, g_loss: 6.952504634857178\n","Epoch 65/4000, Step 106, d_loss: 0.025793615728616714, g_loss: 6.451364994049072\n","Epoch 65/4000, Step 107, d_loss: 0.012305501848459244, g_loss: 10.661531448364258\n","Epoch 65/4000, Step 108, d_loss: 0.005433621816337109, g_loss: 7.274322986602783\n","Epoch 65/4000, Step 109, d_loss: 0.010920492932200432, g_loss: 4.061974048614502\n","Epoch 65/4000, Step 110, d_loss: 0.014721923507750034, g_loss: 7.30571174621582\n","Epoch 65/4000, Step 111, d_loss: 0.0021237614564597607, g_loss: 5.504566669464111\n","Epoch 65/4000, Step 112, d_loss: 0.01183840911835432, g_loss: 5.938816070556641\n","Epoch 65/4000, Step 113, d_loss: 0.014903459697961807, g_loss: 6.686017036437988\n","Epoch 65/4000, Step 114, d_loss: 0.0049284761771559715, g_loss: 5.474365234375\n","Epoch 65/4000, Step 115, d_loss: 0.012922868132591248, g_loss: 7.813074111938477\n","Epoch 65/4000, Step 116, d_loss: 0.13162574172019958, g_loss: 5.277612686157227\n","Epoch 65/4000, Step 117, d_loss: 0.0022088943514972925, g_loss: 7.899990558624268\n","Epoch 65/4000, Step 118, d_loss: 0.0015534197445958853, g_loss: 5.460531234741211\n","Epoch 65/4000, Step 119, d_loss: 0.007353454828262329, g_loss: 5.848493576049805\n","Epoch 65/4000, Step 120, d_loss: 0.013547347858548164, g_loss: 3.2501015663146973\n","Epoch 65/4000, Step 121, d_loss: 0.007950609549880028, g_loss: 3.2026329040527344\n","Epoch 65/4000, Step 122, d_loss: 0.034415461122989655, g_loss: 3.881822109222412\n","Epoch 65/4000, Step 123, d_loss: 0.019235113635659218, g_loss: 4.048677921295166\n","Epoch 65/4000, Step 124, d_loss: 0.024138472974300385, g_loss: 4.982884407043457\n","Epoch 65/4000, Step 125, d_loss: 0.05145583301782608, g_loss: 3.088789939880371\n","Epoch 65/4000, Step 126, d_loss: 0.15860137343406677, g_loss: 4.821676254272461\n","Epoch 65/4000, Step 127, d_loss: 0.013136235997080803, g_loss: 7.663357734680176\n","Epoch 65/4000, Step 128, d_loss: 0.20337744057178497, g_loss: 6.770015239715576\n","Epoch 65/4000, Step 129, d_loss: 0.0015770221361890435, g_loss: 6.294261455535889\n","Epoch 65/4000, Step 130, d_loss: 0.01831834390759468, g_loss: 7.893492698669434\n","Epoch 65/4000, Step 131, d_loss: 0.02179347723722458, g_loss: 3.385638952255249\n","Epoch 65/4000, Step 132, d_loss: 0.1148596778512001, g_loss: 5.639606952667236\n","Epoch 65/4000, Step 133, d_loss: 0.12634199857711792, g_loss: 6.420270919799805\n","Epoch 65/4000, Step 134, d_loss: 0.026625368744134903, g_loss: 8.54703426361084\n","Epoch 65/4000, Step 135, d_loss: 0.02283916063606739, g_loss: 4.743127346038818\n","Epoch 65/4000, Step 136, d_loss: 0.01735715940594673, g_loss: 4.892168045043945\n","Epoch 65/4000, Step 137, d_loss: 0.03489137813448906, g_loss: 9.251306533813477\n","Epoch 65/4000, Step 138, d_loss: 0.02765430137515068, g_loss: 5.154657363891602\n","Epoch 65/4000, Step 139, d_loss: 0.008634412661194801, g_loss: 3.628654956817627\n","Epoch 65/4000, Step 140, d_loss: 0.019351782277226448, g_loss: 4.543208122253418\n","Epoch 65/4000, Step 141, d_loss: 0.0750080794095993, g_loss: 2.551579475402832\n","Epoch 65/4000, Step 142, d_loss: 0.014279997907578945, g_loss: 6.214919090270996\n","Epoch 65/4000, Step 143, d_loss: 0.17764517664909363, g_loss: 4.871183395385742\n","Epoch 65/4000, Step 144, d_loss: 0.04862762987613678, g_loss: 2.9658126831054688\n","Epoch 65/4000, Step 145, d_loss: 0.043699830770492554, g_loss: 4.608744144439697\n","Epoch 65/4000, Step 146, d_loss: 0.15256227552890778, g_loss: 4.436405658721924\n","Epoch 65/4000, Step 147, d_loss: 0.05127068608999252, g_loss: 6.497107982635498\n","Epoch 65/4000, Step 148, d_loss: 0.13103248178958893, g_loss: 8.169290542602539\n","Epoch 65/4000, Step 149, d_loss: 0.04722573608160019, g_loss: 6.12869119644165\n","Epoch 65/4000, Step 150, d_loss: 0.08702404797077179, g_loss: 6.440795421600342\n","Epoch 65/4000, Step 151, d_loss: 0.010665628127753735, g_loss: 4.8378682136535645\n","Epoch 65/4000, Step 152, d_loss: 0.02576642856001854, g_loss: 3.7014148235321045\n","Epoch 65/4000, Step 153, d_loss: 0.018088439479470253, g_loss: 4.769772529602051\n","Epoch 65/4000, Step 154, d_loss: 0.019726457074284554, g_loss: 4.383838176727295\n","Epoch 65/4000, Step 155, d_loss: 0.03103487193584442, g_loss: 5.384281635284424\n","Epoch 65/4000, Step 156, d_loss: 0.009403940290212631, g_loss: 3.9299890995025635\n","Epoch 65/4000, Step 157, d_loss: 0.011236630380153656, g_loss: 4.015378952026367\n","Epoch 65/4000, Step 158, d_loss: 0.022857286036014557, g_loss: 5.671608924865723\n","Epoch 65/4000, Step 159, d_loss: 0.009480982087552547, g_loss: 6.337855815887451\n","Epoch 65/4000, Step 160, d_loss: 0.014848858118057251, g_loss: 8.213956832885742\n","Epoch 65/4000, Step 161, d_loss: 0.03231774643063545, g_loss: 4.337316989898682\n","Epoch 65/4000, Step 162, d_loss: 0.021699894219636917, g_loss: 4.213009357452393\n","Epoch 65/4000, Step 163, d_loss: 0.011829648166894913, g_loss: 4.429704666137695\n","Epoch 65/4000, Step 164, d_loss: 0.20888134837150574, g_loss: 6.388846397399902\n","Epoch 65/4000, Step 165, d_loss: 0.016587911173701286, g_loss: 6.0779900550842285\n","Epoch 65/4000, Step 166, d_loss: 0.115470752120018, g_loss: 3.361152172088623\n","Epoch 65/4000, Step 167, d_loss: 0.018343621864914894, g_loss: 2.6585519313812256\n","Epoch 65/4000, Step 168, d_loss: 0.10079042613506317, g_loss: 1.668258786201477\n","Epoch 65/4000, Step 169, d_loss: 0.01591404713690281, g_loss: 2.148869276046753\n","Epoch 65/4000, Step 170, d_loss: 0.08169855922460556, g_loss: 0.6541873216629028\n","Epoch 65/4000, Step 171, d_loss: 0.527766764163971, g_loss: 1.9013160467147827\n","Epoch 65/4000, Step 172, d_loss: 0.021225251257419586, g_loss: 2.7617175579071045\n","Epoch 65/4000, Step 173, d_loss: 0.039860717952251434, g_loss: 7.475123882293701\n","Epoch 65/4000, Step 174, d_loss: 0.04142766073346138, g_loss: 5.761370658874512\n","Epoch 65/4000, Step 175, d_loss: 0.04021471366286278, g_loss: 5.203956604003906\n","Epoch 65/4000, Step 176, d_loss: 0.020326880738139153, g_loss: 6.32092809677124\n","Epoch 65/4000, Step 177, d_loss: 0.036253705620765686, g_loss: 6.624532222747803\n","Epoch 65/4000, Step 178, d_loss: 0.02436567097902298, g_loss: 6.641384601593018\n","Epoch 65/4000, Step 179, d_loss: 0.09551818668842316, g_loss: 5.853644371032715\n","Epoch 65/4000, Step 180, d_loss: 0.02249755524098873, g_loss: 7.595695972442627\n","Epoch 65/4000, Step 181, d_loss: 0.057486530393362045, g_loss: 6.856168270111084\n","Epoch 65/4000, Step 182, d_loss: 0.09610407799482346, g_loss: 4.361246109008789\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 66/4000, Step 1, d_loss: 0.06807278096675873, g_loss: 2.0182759761810303\n","Epoch 66/4000, Step 2, d_loss: 0.016655784100294113, g_loss: 4.187645435333252\n","Epoch 66/4000, Step 3, d_loss: 0.03222104161977768, g_loss: 7.792196750640869\n","Epoch 66/4000, Step 4, d_loss: 0.28715190291404724, g_loss: 3.7845587730407715\n","Epoch 66/4000, Step 5, d_loss: 0.11093474179506302, g_loss: 3.7704930305480957\n","Epoch 66/4000, Step 6, d_loss: 0.005788056179881096, g_loss: 6.139327526092529\n","Epoch 66/4000, Step 7, d_loss: 0.005272221751511097, g_loss: 3.0856873989105225\n","Epoch 66/4000, Step 8, d_loss: 0.020599491894245148, g_loss: 4.4010114669799805\n","Epoch 66/4000, Step 9, d_loss: 0.03536173701286316, g_loss: 3.8528592586517334\n","Epoch 66/4000, Step 10, d_loss: 0.11942672729492188, g_loss: 4.696768760681152\n","Epoch 66/4000, Step 11, d_loss: 0.007914029061794281, g_loss: 5.238396167755127\n","Epoch 66/4000, Step 12, d_loss: 0.004024973139166832, g_loss: 1.6345608234405518\n","Epoch 66/4000, Step 13, d_loss: 0.008128831163048744, g_loss: 4.934749126434326\n","Epoch 66/4000, Step 14, d_loss: 0.0013748144265264273, g_loss: 5.395509243011475\n","Epoch 66/4000, Step 15, d_loss: 0.03594924136996269, g_loss: 3.2705109119415283\n","Epoch 66/4000, Step 16, d_loss: 0.006390024442225695, g_loss: 6.477809906005859\n","Epoch 66/4000, Step 17, d_loss: 0.020994387567043304, g_loss: 6.3819260597229\n","Epoch 66/4000, Step 18, d_loss: 0.015926245599985123, g_loss: 7.148954391479492\n","Epoch 66/4000, Step 19, d_loss: 0.04653625935316086, g_loss: 6.957211494445801\n","Epoch 66/4000, Step 20, d_loss: 0.03274188190698624, g_loss: 7.3200249671936035\n","Epoch 66/4000, Step 21, d_loss: 0.006075907498598099, g_loss: 3.6144824028015137\n","Epoch 66/4000, Step 22, d_loss: 0.05477284640073776, g_loss: 2.376516342163086\n","Epoch 66/4000, Step 23, d_loss: 0.041168421506881714, g_loss: 4.360888481140137\n","Epoch 66/4000, Step 24, d_loss: 0.0746176540851593, g_loss: 5.8209943771362305\n","Epoch 66/4000, Step 25, d_loss: 0.0023272172547876835, g_loss: 3.3500845432281494\n","Epoch 66/4000, Step 26, d_loss: 0.0005849163862876594, g_loss: 6.84335994720459\n","Epoch 66/4000, Step 27, d_loss: 0.009391728788614273, g_loss: 5.5400800704956055\n","Epoch 66/4000, Step 28, d_loss: 0.0055429330095648766, g_loss: 4.095718860626221\n","Epoch 66/4000, Step 29, d_loss: 0.015744127333164215, g_loss: 6.77668571472168\n","Epoch 66/4000, Step 30, d_loss: 0.0017351454589515924, g_loss: 6.115590572357178\n","Epoch 66/4000, Step 31, d_loss: 0.006672922521829605, g_loss: 4.473098278045654\n","Epoch 66/4000, Step 32, d_loss: 0.008784009143710136, g_loss: 4.564118385314941\n","Epoch 66/4000, Step 33, d_loss: 0.011571221984922886, g_loss: 5.362494945526123\n","Epoch 66/4000, Step 34, d_loss: 0.09534042328596115, g_loss: 4.962577819824219\n","Epoch 66/4000, Step 35, d_loss: 0.06339244544506073, g_loss: 5.2293572425842285\n","Epoch 66/4000, Step 36, d_loss: 0.020310930907726288, g_loss: 3.7767953872680664\n","Epoch 66/4000, Step 37, d_loss: 0.013631613925099373, g_loss: 5.764608860015869\n","Epoch 66/4000, Step 38, d_loss: 0.0982975959777832, g_loss: 5.299764633178711\n","Epoch 66/4000, Step 39, d_loss: 0.0008612507954239845, g_loss: 3.6916427612304688\n","Epoch 66/4000, Step 40, d_loss: 0.036969225853681564, g_loss: 6.0457987785339355\n","Epoch 66/4000, Step 41, d_loss: 0.03851398080587387, g_loss: 5.468489646911621\n","Epoch 66/4000, Step 42, d_loss: 0.008010790683329105, g_loss: 6.996431350708008\n","Epoch 66/4000, Step 43, d_loss: 0.03321709856390953, g_loss: 9.467931747436523\n","Epoch 66/4000, Step 44, d_loss: 0.0019480567425489426, g_loss: 8.543611526489258\n","Epoch 66/4000, Step 45, d_loss: 0.03544047847390175, g_loss: 7.964022159576416\n","Epoch 66/4000, Step 46, d_loss: 0.006689859554171562, g_loss: 7.333302021026611\n","Epoch 66/4000, Step 47, d_loss: 0.0052932086400687695, g_loss: 8.214802742004395\n","Epoch 66/4000, Step 48, d_loss: 0.07743076980113983, g_loss: 8.41732120513916\n","Epoch 66/4000, Step 49, d_loss: 0.012565105222165585, g_loss: 7.624420166015625\n","Epoch 66/4000, Step 50, d_loss: 0.0060089933685958385, g_loss: 5.6123809814453125\n","Epoch 66/4000, Step 51, d_loss: 0.021940814331173897, g_loss: 8.144990921020508\n","Epoch 66/4000, Step 52, d_loss: 0.003239782527089119, g_loss: 8.297779083251953\n","Epoch 66/4000, Step 53, d_loss: 0.018999353051185608, g_loss: 4.451771259307861\n","Epoch 66/4000, Step 54, d_loss: 0.006415382958948612, g_loss: 6.947615623474121\n","Epoch 66/4000, Step 55, d_loss: 0.007580826990306377, g_loss: 5.831838130950928\n","Epoch 66/4000, Step 56, d_loss: 0.006645689718425274, g_loss: 5.426716327667236\n","Epoch 66/4000, Step 57, d_loss: 0.002880443586036563, g_loss: 7.354814052581787\n","Epoch 66/4000, Step 58, d_loss: 0.012048522010445595, g_loss: 7.444709300994873\n","Epoch 66/4000, Step 59, d_loss: 0.003896318143233657, g_loss: 5.056553363800049\n","Epoch 66/4000, Step 60, d_loss: 0.02877943217754364, g_loss: 7.640663146972656\n","Epoch 66/4000, Step 61, d_loss: 0.01709344983100891, g_loss: 5.003593444824219\n","Epoch 66/4000, Step 62, d_loss: 0.014215108007192612, g_loss: 6.3313727378845215\n","Epoch 66/4000, Step 63, d_loss: 0.013452407903969288, g_loss: 7.645059108734131\n","Epoch 66/4000, Step 64, d_loss: 0.0641249567270279, g_loss: 5.924692630767822\n","Epoch 66/4000, Step 65, d_loss: 0.002384474501013756, g_loss: 6.186856269836426\n","Epoch 66/4000, Step 66, d_loss: 0.005248488858342171, g_loss: 6.288219928741455\n","Epoch 66/4000, Step 67, d_loss: 0.016324615105986595, g_loss: 5.372700214385986\n","Epoch 66/4000, Step 68, d_loss: 0.0027315972838550806, g_loss: 5.023726463317871\n","Epoch 66/4000, Step 69, d_loss: 0.004549242090433836, g_loss: 4.606355667114258\n","Epoch 66/4000, Step 70, d_loss: 0.007453590631484985, g_loss: 4.263381481170654\n","Epoch 66/4000, Step 71, d_loss: 0.0054602245800197124, g_loss: 6.107183933258057\n","Epoch 66/4000, Step 72, d_loss: 0.013784005306661129, g_loss: 7.276773929595947\n","Epoch 66/4000, Step 73, d_loss: 0.02010214328765869, g_loss: 4.66647481918335\n","Epoch 66/4000, Step 74, d_loss: 0.008758554235100746, g_loss: 7.402780532836914\n","Epoch 66/4000, Step 75, d_loss: 0.009818929247558117, g_loss: 9.035956382751465\n","Epoch 66/4000, Step 76, d_loss: 0.03084675222635269, g_loss: 6.403127193450928\n","Epoch 66/4000, Step 77, d_loss: 0.002034697448834777, g_loss: 4.689383029937744\n","Epoch 66/4000, Step 78, d_loss: 0.013292012736201286, g_loss: 4.990714073181152\n","Epoch 66/4000, Step 79, d_loss: 0.06922788918018341, g_loss: 4.753779888153076\n","Epoch 66/4000, Step 80, d_loss: 0.013272620737552643, g_loss: 5.137628078460693\n","Epoch 66/4000, Step 81, d_loss: 0.006641874555498362, g_loss: 6.992777347564697\n","Epoch 66/4000, Step 82, d_loss: 0.017194926738739014, g_loss: 3.9002153873443604\n","Epoch 66/4000, Step 83, d_loss: 0.012415342032909393, g_loss: 4.375779151916504\n","Epoch 66/4000, Step 84, d_loss: 0.004325186833739281, g_loss: 4.257015228271484\n","Epoch 66/4000, Step 85, d_loss: 0.03206304460763931, g_loss: 4.718624114990234\n","Epoch 66/4000, Step 86, d_loss: 0.0021526729688048363, g_loss: 6.277406692504883\n","Epoch 66/4000, Step 87, d_loss: 0.018506184220314026, g_loss: 5.534922122955322\n","Epoch 66/4000, Step 88, d_loss: 0.012627838179469109, g_loss: 5.2313971519470215\n","Epoch 66/4000, Step 89, d_loss: 0.0037633622996509075, g_loss: 3.2381234169006348\n","Epoch 66/4000, Step 90, d_loss: 0.005643672309815884, g_loss: 2.9611520767211914\n","Epoch 66/4000, Step 91, d_loss: 0.08632788062095642, g_loss: 6.375110626220703\n","Epoch 66/4000, Step 92, d_loss: 0.005272070877254009, g_loss: 3.555422067642212\n","Epoch 66/4000, Step 93, d_loss: 0.024044189602136612, g_loss: 6.574342250823975\n","Epoch 66/4000, Step 94, d_loss: 0.029976265504956245, g_loss: 5.041703701019287\n","Epoch 66/4000, Step 95, d_loss: 0.017644109204411507, g_loss: 3.785299301147461\n","Epoch 66/4000, Step 96, d_loss: 0.01752295158803463, g_loss: 4.533392906188965\n","Epoch 66/4000, Step 97, d_loss: 0.0019210928585380316, g_loss: 3.3571059703826904\n","Epoch 66/4000, Step 98, d_loss: 0.1173948273062706, g_loss: 7.3320770263671875\n","Epoch 66/4000, Step 99, d_loss: 0.018247485160827637, g_loss: 6.291404724121094\n","Epoch 66/4000, Step 100, d_loss: 0.009634006768465042, g_loss: 4.108262538909912\n","Epoch 66/4000, Step 101, d_loss: 0.004674272611737251, g_loss: 7.372387409210205\n","Epoch 66/4000, Step 102, d_loss: 0.00092746113659814, g_loss: 5.5788798332214355\n","Epoch 66/4000, Step 103, d_loss: 0.01226015668362379, g_loss: 6.763858795166016\n","Epoch 66/4000, Step 104, d_loss: 0.038131386041641235, g_loss: 6.693932056427002\n","Epoch 66/4000, Step 105, d_loss: 0.007309733424335718, g_loss: 7.272256851196289\n","Epoch 66/4000, Step 106, d_loss: 0.024888191372156143, g_loss: 8.608964920043945\n","Epoch 66/4000, Step 107, d_loss: 0.45226916670799255, g_loss: 3.224431276321411\n","Epoch 66/4000, Step 108, d_loss: 0.011370950378477573, g_loss: 4.756141662597656\n","Epoch 66/4000, Step 109, d_loss: 0.017169838771224022, g_loss: 1.5367062091827393\n","Epoch 66/4000, Step 110, d_loss: 0.05338528752326965, g_loss: 2.9036648273468018\n","Epoch 66/4000, Step 111, d_loss: 0.12300585210323334, g_loss: 2.4981327056884766\n","Epoch 66/4000, Step 112, d_loss: 0.009560364298522472, g_loss: 1.8079979419708252\n","Epoch 66/4000, Step 113, d_loss: 0.0026298414450138807, g_loss: 3.9825804233551025\n","Epoch 66/4000, Step 114, d_loss: 0.019416669383645058, g_loss: 3.018125534057617\n","Epoch 66/4000, Step 115, d_loss: 0.44943633675575256, g_loss: 4.796095371246338\n","Epoch 66/4000, Step 116, d_loss: 0.5222364664077759, g_loss: 5.324861526489258\n","Epoch 66/4000, Step 117, d_loss: 0.006914712488651276, g_loss: 5.661984443664551\n","Epoch 66/4000, Step 118, d_loss: 0.03344481810927391, g_loss: 12.681285858154297\n","Epoch 66/4000, Step 119, d_loss: 0.5632370710372925, g_loss: 11.807812690734863\n","Epoch 66/4000, Step 120, d_loss: 0.4082026183605194, g_loss: 6.858656883239746\n","Epoch 66/4000, Step 121, d_loss: 1.2264050245285034, g_loss: 6.00789213180542\n","Epoch 66/4000, Step 122, d_loss: 0.04055624455213547, g_loss: 6.1802520751953125\n","Epoch 66/4000, Step 123, d_loss: 0.011075441725552082, g_loss: 3.276637315750122\n","Epoch 66/4000, Step 124, d_loss: 0.2232009470462799, g_loss: 8.83012866973877\n","Epoch 66/4000, Step 125, d_loss: 0.156172513961792, g_loss: 4.722057342529297\n","Epoch 66/4000, Step 126, d_loss: 0.06199612468481064, g_loss: 5.274672031402588\n","Epoch 66/4000, Step 127, d_loss: 0.19838476181030273, g_loss: 3.409306049346924\n","Epoch 66/4000, Step 128, d_loss: 0.15827473998069763, g_loss: 3.769179582595825\n","Epoch 66/4000, Step 129, d_loss: 0.07654312252998352, g_loss: 2.2238197326660156\n","Epoch 66/4000, Step 130, d_loss: 0.05958025902509689, g_loss: 1.24746835231781\n","Epoch 66/4000, Step 131, d_loss: 0.08476481586694717, g_loss: 0.41834041476249695\n","Epoch 66/4000, Step 132, d_loss: 0.054460182785987854, g_loss: 2.3228254318237305\n","Epoch 66/4000, Step 133, d_loss: 0.19479577243328094, g_loss: 1.576838493347168\n","Epoch 66/4000, Step 134, d_loss: 0.053656142204999924, g_loss: 3.1327662467956543\n","Epoch 66/4000, Step 135, d_loss: 0.13857659697532654, g_loss: 3.226801633834839\n","Epoch 66/4000, Step 136, d_loss: 0.16575460135936737, g_loss: 3.751891613006592\n","Epoch 66/4000, Step 137, d_loss: 0.04608951881527901, g_loss: 1.956876277923584\n","Epoch 66/4000, Step 138, d_loss: 0.37351447343826294, g_loss: 3.8991549015045166\n","Epoch 66/4000, Step 139, d_loss: 0.1350722759962082, g_loss: 5.168745040893555\n","Epoch 66/4000, Step 140, d_loss: 0.09948498755693436, g_loss: 5.306502819061279\n","Epoch 66/4000, Step 141, d_loss: 0.15075784921646118, g_loss: 6.095649242401123\n","Epoch 66/4000, Step 142, d_loss: 0.07665076106786728, g_loss: 5.41916036605835\n","Epoch 66/4000, Step 143, d_loss: 0.18803775310516357, g_loss: 8.70720100402832\n","Epoch 66/4000, Step 144, d_loss: 0.19324803352355957, g_loss: 5.573558330535889\n","Epoch 66/4000, Step 145, d_loss: 0.015898793935775757, g_loss: 7.34080696105957\n","Epoch 66/4000, Step 146, d_loss: 0.07031351327896118, g_loss: 5.141768932342529\n","Epoch 66/4000, Step 147, d_loss: 0.07645533978939056, g_loss: 4.429892539978027\n","Epoch 66/4000, Step 148, d_loss: 0.042840227484703064, g_loss: 3.6752493381500244\n","Epoch 66/4000, Step 149, d_loss: 0.19632060825824738, g_loss: 4.237854480743408\n","Epoch 66/4000, Step 150, d_loss: 0.021769452840089798, g_loss: 1.0944206714630127\n","Epoch 66/4000, Step 151, d_loss: 0.29638636112213135, g_loss: 3.0189900398254395\n","Epoch 66/4000, Step 152, d_loss: 0.043973494321107864, g_loss: 3.2658629417419434\n","Epoch 66/4000, Step 153, d_loss: 0.11787156760692596, g_loss: 2.9407107830047607\n","Epoch 66/4000, Step 154, d_loss: 0.22160278260707855, g_loss: 3.1520047187805176\n","Epoch 66/4000, Step 155, d_loss: 0.3560866415500641, g_loss: 6.650543689727783\n","Epoch 66/4000, Step 156, d_loss: 0.42537346482276917, g_loss: 4.268360137939453\n","Epoch 66/4000, Step 157, d_loss: 0.11190595477819443, g_loss: 4.041074275970459\n","Epoch 66/4000, Step 158, d_loss: 0.03129307180643082, g_loss: 6.059774875640869\n","Epoch 66/4000, Step 159, d_loss: 0.03199756145477295, g_loss: 5.392529010772705\n","Epoch 66/4000, Step 160, d_loss: 0.0252655241638422, g_loss: 2.6372926235198975\n","Epoch 66/4000, Step 161, d_loss: 0.05556591600179672, g_loss: 1.3486042022705078\n","Epoch 66/4000, Step 162, d_loss: 0.3676339387893677, g_loss: 2.9012134075164795\n","Epoch 66/4000, Step 163, d_loss: 0.16014637053012848, g_loss: 4.131307125091553\n","Epoch 66/4000, Step 164, d_loss: 0.13984987139701843, g_loss: 4.082596778869629\n","Epoch 66/4000, Step 165, d_loss: 0.045857176184654236, g_loss: 1.1577129364013672\n","Epoch 66/4000, Step 166, d_loss: 0.045069850981235504, g_loss: 9.021918296813965\n","Epoch 66/4000, Step 167, d_loss: 0.03648071736097336, g_loss: 1.3788608312606812\n","Epoch 66/4000, Step 168, d_loss: 0.15779469907283783, g_loss: 3.3371076583862305\n","Epoch 66/4000, Step 169, d_loss: 0.12880057096481323, g_loss: 2.2017054557800293\n","Epoch 66/4000, Step 170, d_loss: 0.050019197165966034, g_loss: 0.7160891890525818\n","Epoch 66/4000, Step 171, d_loss: 0.16264918446540833, g_loss: 3.2388720512390137\n","Epoch 66/4000, Step 172, d_loss: 0.08960813283920288, g_loss: 2.855844259262085\n","Epoch 66/4000, Step 173, d_loss: 0.14185978472232819, g_loss: 1.838327169418335\n","Epoch 66/4000, Step 174, d_loss: 0.020814111456274986, g_loss: 2.5142672061920166\n","Epoch 66/4000, Step 175, d_loss: 0.27573564648628235, g_loss: 6.936049938201904\n","Epoch 66/4000, Step 176, d_loss: 0.06159572675824165, g_loss: 6.431516647338867\n","Epoch 66/4000, Step 177, d_loss: 0.42635399103164673, g_loss: 3.5655195713043213\n","Epoch 66/4000, Step 178, d_loss: 0.04227110370993614, g_loss: 6.983388423919678\n","Epoch 66/4000, Step 179, d_loss: 0.0324380099773407, g_loss: 3.6376054286956787\n","Epoch 66/4000, Step 180, d_loss: 0.13028471171855927, g_loss: 7.789249420166016\n","Epoch 66/4000, Step 181, d_loss: 0.0457126684486866, g_loss: 4.283596992492676\n","Epoch 66/4000, Step 182, d_loss: 0.04385213553905487, g_loss: 4.395833969116211\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 67/4000, Step 1, d_loss: 0.024864237755537033, g_loss: 4.56285285949707\n","Epoch 67/4000, Step 2, d_loss: 0.021982019767165184, g_loss: 4.751466751098633\n","Epoch 67/4000, Step 3, d_loss: 0.018493160605430603, g_loss: 4.38901424407959\n","Epoch 67/4000, Step 4, d_loss: 0.16694366931915283, g_loss: 4.594316005706787\n","Epoch 67/4000, Step 5, d_loss: 0.11368878930807114, g_loss: 2.072922706604004\n","Epoch 67/4000, Step 6, d_loss: 0.11573488265275955, g_loss: 3.4589099884033203\n","Epoch 67/4000, Step 7, d_loss: 0.002893721219152212, g_loss: 8.338562965393066\n","Epoch 67/4000, Step 8, d_loss: 0.02627274952828884, g_loss: 4.669208526611328\n","Epoch 67/4000, Step 9, d_loss: 0.012599271722137928, g_loss: 4.520313739776611\n","Epoch 67/4000, Step 10, d_loss: 0.03507490083575249, g_loss: 4.458581924438477\n","Epoch 67/4000, Step 11, d_loss: 0.0025781148578971624, g_loss: 2.4497649669647217\n","Epoch 67/4000, Step 12, d_loss: 0.002013636752963066, g_loss: 4.575029373168945\n","Epoch 67/4000, Step 13, d_loss: 0.08589319884777069, g_loss: 3.4482250213623047\n","Epoch 67/4000, Step 14, d_loss: 0.005948283709585667, g_loss: 3.7320501804351807\n","Epoch 67/4000, Step 15, d_loss: 0.007586109451949596, g_loss: 4.768448352813721\n","Epoch 67/4000, Step 16, d_loss: 0.032422877848148346, g_loss: 4.142087936401367\n","Epoch 67/4000, Step 17, d_loss: 0.09642311930656433, g_loss: 4.327972888946533\n","Epoch 67/4000, Step 18, d_loss: 0.03612378239631653, g_loss: 3.228665590286255\n","Epoch 67/4000, Step 19, d_loss: 0.04046763852238655, g_loss: 2.584219217300415\n","Epoch 67/4000, Step 20, d_loss: 0.050336167216300964, g_loss: 4.970665454864502\n","Epoch 67/4000, Step 21, d_loss: 0.03862663358449936, g_loss: 5.034701824188232\n","Epoch 67/4000, Step 22, d_loss: 0.0051980880089104176, g_loss: 3.131190776824951\n","Epoch 67/4000, Step 23, d_loss: 0.05393941327929497, g_loss: 9.123429298400879\n","Epoch 67/4000, Step 24, d_loss: 0.05291465297341347, g_loss: 8.53377914428711\n","Epoch 67/4000, Step 25, d_loss: 0.004313744604587555, g_loss: 5.867058277130127\n","Epoch 67/4000, Step 26, d_loss: 0.04067309573292732, g_loss: 4.138491153717041\n","Epoch 67/4000, Step 27, d_loss: 0.0030160965397953987, g_loss: 6.579732418060303\n","Epoch 67/4000, Step 28, d_loss: 0.01745353266596794, g_loss: 5.007880687713623\n","Epoch 67/4000, Step 29, d_loss: 0.16054217517375946, g_loss: 3.8230466842651367\n","Epoch 67/4000, Step 30, d_loss: 0.03221239149570465, g_loss: 4.747582912445068\n","Epoch 67/4000, Step 31, d_loss: 0.03739660978317261, g_loss: 4.285518169403076\n","Epoch 67/4000, Step 32, d_loss: 0.07550031691789627, g_loss: 4.707491397857666\n","Epoch 67/4000, Step 33, d_loss: 0.006075471639633179, g_loss: 3.7913689613342285\n","Epoch 67/4000, Step 34, d_loss: 0.012848728336393833, g_loss: 4.062279224395752\n","Epoch 67/4000, Step 35, d_loss: 0.010262480936944485, g_loss: 5.573923587799072\n","Epoch 67/4000, Step 36, d_loss: 0.031670406460762024, g_loss: 5.385863780975342\n","Epoch 67/4000, Step 37, d_loss: 0.014998974278569221, g_loss: 4.850558280944824\n","Epoch 67/4000, Step 38, d_loss: 0.0025168037973344326, g_loss: 3.9499998092651367\n","Epoch 67/4000, Step 39, d_loss: 0.022898901253938675, g_loss: 5.499246597290039\n","Epoch 67/4000, Step 40, d_loss: 0.04529339820146561, g_loss: 4.911872386932373\n","Epoch 67/4000, Step 41, d_loss: 0.003616310888901353, g_loss: 4.742978096008301\n","Epoch 67/4000, Step 42, d_loss: 0.005829291883856058, g_loss: 6.743961811065674\n","Epoch 67/4000, Step 43, d_loss: 0.020947879180312157, g_loss: 8.192715644836426\n","Epoch 67/4000, Step 44, d_loss: 0.02928662858903408, g_loss: 5.442956447601318\n","Epoch 67/4000, Step 45, d_loss: 0.012298252433538437, g_loss: 2.8531601428985596\n","Epoch 67/4000, Step 46, d_loss: 0.007000594865530729, g_loss: 6.989398002624512\n","Epoch 67/4000, Step 47, d_loss: 0.003752694698050618, g_loss: 4.469814300537109\n","Epoch 67/4000, Step 48, d_loss: 0.03950954228639603, g_loss: 4.417599201202393\n","Epoch 67/4000, Step 49, d_loss: 0.011819988489151001, g_loss: 4.031733512878418\n","Epoch 67/4000, Step 50, d_loss: 0.017212342470884323, g_loss: 4.123099327087402\n","Epoch 67/4000, Step 51, d_loss: 0.008732546120882034, g_loss: 3.229619026184082\n","Epoch 67/4000, Step 52, d_loss: 0.022344887256622314, g_loss: 5.227356910705566\n","Epoch 67/4000, Step 53, d_loss: 0.013685416430234909, g_loss: 6.2608418464660645\n","Epoch 67/4000, Step 54, d_loss: 0.010077107697725296, g_loss: 4.836352825164795\n","Epoch 67/4000, Step 55, d_loss: 0.06388850510120392, g_loss: 4.045417785644531\n","Epoch 67/4000, Step 56, d_loss: 0.011976185254752636, g_loss: 4.694765567779541\n","Epoch 67/4000, Step 57, d_loss: 0.02272922173142433, g_loss: 4.269145488739014\n","Epoch 67/4000, Step 58, d_loss: 0.006662650965154171, g_loss: 5.766676425933838\n","Epoch 67/4000, Step 59, d_loss: 0.012801934033632278, g_loss: 6.998456001281738\n","Epoch 67/4000, Step 60, d_loss: 0.0033511032816022635, g_loss: 6.681689739227295\n","Epoch 67/4000, Step 61, d_loss: 0.006685234140604734, g_loss: 5.730161666870117\n","Epoch 67/4000, Step 62, d_loss: 0.006366783753037453, g_loss: 6.207727909088135\n","Epoch 67/4000, Step 63, d_loss: 0.004837594460695982, g_loss: 4.979373455047607\n","Epoch 67/4000, Step 64, d_loss: 0.06326250731945038, g_loss: 4.233826637268066\n","Epoch 67/4000, Step 65, d_loss: 0.0311129167675972, g_loss: 4.127626895904541\n","Epoch 67/4000, Step 66, d_loss: 0.011892146430909634, g_loss: 6.182236671447754\n","Epoch 67/4000, Step 67, d_loss: 0.002222831128165126, g_loss: 8.496648788452148\n","Epoch 67/4000, Step 68, d_loss: 0.010205160826444626, g_loss: 6.121895790100098\n","Epoch 67/4000, Step 69, d_loss: 0.01020267978310585, g_loss: 7.456749439239502\n","Epoch 67/4000, Step 70, d_loss: 0.006630526389926672, g_loss: 4.882342338562012\n","Epoch 67/4000, Step 71, d_loss: 0.01482362486422062, g_loss: 5.987233638763428\n","Epoch 67/4000, Step 72, d_loss: 0.003910185303539038, g_loss: 6.100452423095703\n","Epoch 67/4000, Step 73, d_loss: 0.0044272104278206825, g_loss: 5.820957660675049\n","Epoch 67/4000, Step 74, d_loss: 0.008982537314295769, g_loss: 4.43429708480835\n","Epoch 67/4000, Step 75, d_loss: 0.003116522217169404, g_loss: 5.8364081382751465\n","Epoch 67/4000, Step 76, d_loss: 0.00692228926345706, g_loss: 5.270923614501953\n","Epoch 67/4000, Step 77, d_loss: 0.009897691197693348, g_loss: 4.302492618560791\n","Epoch 67/4000, Step 78, d_loss: 0.02016805112361908, g_loss: 8.14696216583252\n","Epoch 67/4000, Step 79, d_loss: 0.0017794450977817178, g_loss: 5.746265411376953\n","Epoch 67/4000, Step 80, d_loss: 0.01351776160299778, g_loss: 8.453164100646973\n","Epoch 67/4000, Step 81, d_loss: 0.02215358056128025, g_loss: 6.446547031402588\n","Epoch 67/4000, Step 82, d_loss: 0.05514049902558327, g_loss: 5.985603332519531\n","Epoch 67/4000, Step 83, d_loss: 0.0275159552693367, g_loss: 5.943691730499268\n","Epoch 67/4000, Step 84, d_loss: 0.004105042666196823, g_loss: 3.8827710151672363\n","Epoch 67/4000, Step 85, d_loss: 0.00869765318930149, g_loss: 8.242401123046875\n","Epoch 67/4000, Step 86, d_loss: 0.023364027962088585, g_loss: 4.5809526443481445\n","Epoch 67/4000, Step 87, d_loss: 0.016586048528552055, g_loss: 5.480901718139648\n","Epoch 67/4000, Step 88, d_loss: 0.007771209813654423, g_loss: 4.155203819274902\n","Epoch 67/4000, Step 89, d_loss: 0.00918652769178152, g_loss: 5.234729766845703\n","Epoch 67/4000, Step 90, d_loss: 0.008346378803253174, g_loss: 6.430084228515625\n","Epoch 67/4000, Step 91, d_loss: 0.003982536494731903, g_loss: 7.426275730133057\n","Epoch 67/4000, Step 92, d_loss: 0.0222383551299572, g_loss: 5.378856182098389\n","Epoch 67/4000, Step 93, d_loss: 0.004821087699383497, g_loss: 3.594479560852051\n","Epoch 67/4000, Step 94, d_loss: 0.017150985077023506, g_loss: 4.8743109703063965\n","Epoch 67/4000, Step 95, d_loss: 0.007896305993199348, g_loss: 6.823164939880371\n","Epoch 67/4000, Step 96, d_loss: 0.009154224768280983, g_loss: 8.471094131469727\n","Epoch 67/4000, Step 97, d_loss: 0.0033714422024786472, g_loss: 5.913077354431152\n","Epoch 67/4000, Step 98, d_loss: 0.003908772487193346, g_loss: 5.152979373931885\n","Epoch 67/4000, Step 99, d_loss: 0.0035993291530758142, g_loss: 5.1622114181518555\n","Epoch 67/4000, Step 100, d_loss: 0.004723701626062393, g_loss: 6.04921293258667\n","Epoch 67/4000, Step 101, d_loss: 0.005981723312288523, g_loss: 5.9885478019714355\n","Epoch 67/4000, Step 102, d_loss: 0.012281475588679314, g_loss: 7.483580112457275\n","Epoch 67/4000, Step 103, d_loss: 0.009428436867892742, g_loss: 5.3092780113220215\n","Epoch 67/4000, Step 104, d_loss: 0.017790276557207108, g_loss: 6.135082244873047\n","Epoch 67/4000, Step 105, d_loss: 0.018754201009869576, g_loss: 5.379165172576904\n","Epoch 67/4000, Step 106, d_loss: 0.0057497089728713036, g_loss: 7.711535453796387\n","Epoch 67/4000, Step 107, d_loss: 0.04484977573156357, g_loss: 3.9424779415130615\n","Epoch 67/4000, Step 108, d_loss: 0.011894446797668934, g_loss: 4.076070308685303\n","Epoch 67/4000, Step 109, d_loss: 0.0033650551922619343, g_loss: 5.357185363769531\n","Epoch 67/4000, Step 110, d_loss: 0.011183194816112518, g_loss: 5.336280822753906\n","Epoch 67/4000, Step 111, d_loss: 0.03298034891486168, g_loss: 6.234729290008545\n","Epoch 67/4000, Step 112, d_loss: 0.017034148797392845, g_loss: 3.951411724090576\n","Epoch 67/4000, Step 113, d_loss: 0.0030535790137946606, g_loss: 4.8216233253479\n","Epoch 67/4000, Step 114, d_loss: 0.0024152013938874006, g_loss: 4.274329662322998\n","Epoch 67/4000, Step 115, d_loss: 0.008037940599024296, g_loss: 6.960626125335693\n","Epoch 67/4000, Step 116, d_loss: 0.015867214649915695, g_loss: 5.9918718338012695\n","Epoch 67/4000, Step 117, d_loss: 0.014493010938167572, g_loss: 4.69119930267334\n","Epoch 67/4000, Step 118, d_loss: 0.004269013646990061, g_loss: 5.812363147735596\n","Epoch 67/4000, Step 119, d_loss: 0.0013299195561558008, g_loss: 5.677509307861328\n","Epoch 67/4000, Step 120, d_loss: 0.002213336294516921, g_loss: 5.836808681488037\n","Epoch 67/4000, Step 121, d_loss: 0.02656641975045204, g_loss: 5.059804916381836\n","Epoch 67/4000, Step 122, d_loss: 0.002682791091501713, g_loss: 4.702389240264893\n","Epoch 67/4000, Step 123, d_loss: 0.01218776311725378, g_loss: 5.779228210449219\n","Epoch 67/4000, Step 124, d_loss: 0.026201246306300163, g_loss: 7.219269752502441\n","Epoch 67/4000, Step 125, d_loss: 0.00968822930008173, g_loss: 7.409597873687744\n","Epoch 67/4000, Step 126, d_loss: 0.001396305044181645, g_loss: 6.381867408752441\n","Epoch 67/4000, Step 127, d_loss: 0.019744902849197388, g_loss: 5.612407207489014\n","Epoch 67/4000, Step 128, d_loss: 0.0021315214689821005, g_loss: 9.49769401550293\n","Epoch 67/4000, Step 129, d_loss: 0.00725368969142437, g_loss: 6.953106880187988\n","Epoch 67/4000, Step 130, d_loss: 0.0006418618140742183, g_loss: 6.528017044067383\n","Epoch 67/4000, Step 131, d_loss: 0.004289549775421619, g_loss: 5.870682239532471\n","Epoch 67/4000, Step 132, d_loss: 0.02675178460776806, g_loss: 6.937995433807373\n","Epoch 67/4000, Step 133, d_loss: 0.013117450289428234, g_loss: 5.464104175567627\n","Epoch 67/4000, Step 134, d_loss: 0.009762829169631004, g_loss: 8.628498077392578\n","Epoch 67/4000, Step 135, d_loss: 0.007069870829582214, g_loss: 6.627169609069824\n","Epoch 67/4000, Step 136, d_loss: 0.00694540049880743, g_loss: 7.231675624847412\n","Epoch 67/4000, Step 137, d_loss: 0.014195909723639488, g_loss: 6.006927013397217\n","Epoch 67/4000, Step 138, d_loss: 0.00639076204970479, g_loss: 9.575798034667969\n","Epoch 67/4000, Step 139, d_loss: 0.0006702523678541183, g_loss: 4.369966506958008\n","Epoch 67/4000, Step 140, d_loss: 0.0072491285391151905, g_loss: 4.446941375732422\n","Epoch 67/4000, Step 141, d_loss: 0.020273804664611816, g_loss: 5.805390357971191\n","Epoch 67/4000, Step 142, d_loss: 0.011466328985989094, g_loss: 5.98792839050293\n","Epoch 67/4000, Step 143, d_loss: 0.0046828435733914375, g_loss: 5.075347423553467\n","Epoch 67/4000, Step 144, d_loss: 0.01386098563671112, g_loss: 6.487935543060303\n","Epoch 67/4000, Step 145, d_loss: 0.0037786103785037994, g_loss: 5.171627044677734\n","Epoch 67/4000, Step 146, d_loss: 0.0025608171708881855, g_loss: 8.10712718963623\n","Epoch 67/4000, Step 147, d_loss: 0.0030552633106708527, g_loss: 5.8378586769104\n","Epoch 67/4000, Step 148, d_loss: 0.0032794084399938583, g_loss: 6.478866100311279\n","Epoch 67/4000, Step 149, d_loss: 0.001901131123304367, g_loss: 6.451077461242676\n","Epoch 67/4000, Step 150, d_loss: 0.0025548203848302364, g_loss: 7.295454025268555\n","Epoch 67/4000, Step 151, d_loss: 0.01355824526399374, g_loss: 5.1607232093811035\n","Epoch 67/4000, Step 152, d_loss: 0.014070730656385422, g_loss: 8.353286743164062\n","Epoch 67/4000, Step 153, d_loss: 0.011716236360371113, g_loss: 9.744722366333008\n","Epoch 67/4000, Step 154, d_loss: 0.004414482042193413, g_loss: 7.460589408874512\n","Epoch 67/4000, Step 155, d_loss: 0.020821116864681244, g_loss: 5.241060733795166\n","Epoch 67/4000, Step 156, d_loss: 0.0066403960809111595, g_loss: 5.095208644866943\n","Epoch 67/4000, Step 157, d_loss: 0.010525843128561974, g_loss: 6.274752140045166\n","Epoch 67/4000, Step 158, d_loss: 0.017401844263076782, g_loss: 6.260608196258545\n","Epoch 67/4000, Step 159, d_loss: 0.028237611055374146, g_loss: 5.888067245483398\n","Epoch 67/4000, Step 160, d_loss: 0.006889951881021261, g_loss: 5.1763410568237305\n","Epoch 67/4000, Step 161, d_loss: 0.00894028227776289, g_loss: 6.2338128089904785\n","Epoch 67/4000, Step 162, d_loss: 0.0054601989686489105, g_loss: 5.149806499481201\n","Epoch 67/4000, Step 163, d_loss: 0.018535183742642403, g_loss: 5.356039047241211\n","Epoch 67/4000, Step 164, d_loss: 0.018884584307670593, g_loss: 8.175406455993652\n","Epoch 67/4000, Step 165, d_loss: 0.0061981575563549995, g_loss: 5.9264984130859375\n","Epoch 67/4000, Step 166, d_loss: 0.009495045989751816, g_loss: 6.767519474029541\n","Epoch 67/4000, Step 167, d_loss: 0.009065203368663788, g_loss: 8.858048439025879\n","Epoch 67/4000, Step 168, d_loss: 0.006474858149886131, g_loss: 7.501598834991455\n","Epoch 67/4000, Step 169, d_loss: 0.0025022421032190323, g_loss: 7.758225440979004\n","Epoch 67/4000, Step 170, d_loss: 0.0008265041979029775, g_loss: 5.551506042480469\n","Epoch 67/4000, Step 171, d_loss: 0.001994755817577243, g_loss: 5.226534366607666\n","Epoch 67/4000, Step 172, d_loss: 0.00037673377664759755, g_loss: 5.82547664642334\n","Epoch 67/4000, Step 173, d_loss: 0.008779361844062805, g_loss: 8.32097339630127\n","Epoch 67/4000, Step 174, d_loss: 0.0054264916107058525, g_loss: 6.0862250328063965\n","Epoch 67/4000, Step 175, d_loss: 0.004060076083987951, g_loss: 6.26627779006958\n","Epoch 67/4000, Step 176, d_loss: 0.0020268552470952272, g_loss: 5.335882663726807\n","Epoch 67/4000, Step 177, d_loss: 0.011436779983341694, g_loss: 4.5070109367370605\n","Epoch 67/4000, Step 178, d_loss: 0.01217565219849348, g_loss: 6.665843486785889\n","Epoch 67/4000, Step 179, d_loss: 0.0025662053376436234, g_loss: 7.743011951446533\n","Epoch 67/4000, Step 180, d_loss: 0.002778473775833845, g_loss: 7.523270130157471\n","Epoch 67/4000, Step 181, d_loss: 0.004856587387621403, g_loss: 6.504047393798828\n","Epoch 67/4000, Step 182, d_loss: 0.014606697484850883, g_loss: 8.486991882324219\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 68/4000, Step 1, d_loss: 0.002398397773504257, g_loss: 6.524034023284912\n","Epoch 68/4000, Step 2, d_loss: 0.02688262239098549, g_loss: 5.593165874481201\n","Epoch 68/4000, Step 3, d_loss: 0.0008485059952363372, g_loss: 8.301892280578613\n","Epoch 68/4000, Step 4, d_loss: 0.004026198759675026, g_loss: 5.854700088500977\n","Epoch 68/4000, Step 5, d_loss: 0.0008512650965712965, g_loss: 6.184347629547119\n","Epoch 68/4000, Step 6, d_loss: 0.002596405101940036, g_loss: 5.789060115814209\n","Epoch 68/4000, Step 7, d_loss: 0.0018142490880563855, g_loss: 5.6698784828186035\n","Epoch 68/4000, Step 8, d_loss: 0.01644315756857395, g_loss: 6.8759284019470215\n","Epoch 68/4000, Step 9, d_loss: 0.003985236398875713, g_loss: 5.417235851287842\n","Epoch 68/4000, Step 10, d_loss: 0.004773592576384544, g_loss: 6.534688949584961\n","Epoch 68/4000, Step 11, d_loss: 0.003580941818654537, g_loss: 5.152775764465332\n","Epoch 68/4000, Step 12, d_loss: 0.001936083659529686, g_loss: 6.049678802490234\n","Epoch 68/4000, Step 13, d_loss: 0.007156749721616507, g_loss: 6.992379188537598\n","Epoch 68/4000, Step 14, d_loss: 0.007947474718093872, g_loss: 6.242288589477539\n","Epoch 68/4000, Step 15, d_loss: 0.021059246733784676, g_loss: 6.25635290145874\n","Epoch 68/4000, Step 16, d_loss: 0.006432590074837208, g_loss: 7.183071613311768\n","Epoch 68/4000, Step 17, d_loss: 0.006663324311375618, g_loss: 4.794370651245117\n","Epoch 68/4000, Step 18, d_loss: 0.003998652100563049, g_loss: 7.529489040374756\n","Epoch 68/4000, Step 19, d_loss: 0.0022804471664130688, g_loss: 4.228879451751709\n","Epoch 68/4000, Step 20, d_loss: 0.017626337707042694, g_loss: 4.2109479904174805\n","Epoch 68/4000, Step 21, d_loss: 0.01642342284321785, g_loss: 7.061570167541504\n","Epoch 68/4000, Step 22, d_loss: 0.008205028250813484, g_loss: 5.368858337402344\n","Epoch 68/4000, Step 23, d_loss: 0.0025900325272232294, g_loss: 5.001810550689697\n","Epoch 68/4000, Step 24, d_loss: 0.00039689114782959223, g_loss: 5.040590763092041\n","Epoch 68/4000, Step 25, d_loss: 0.0021466745529323816, g_loss: 6.409980773925781\n","Epoch 68/4000, Step 26, d_loss: 0.00287880701944232, g_loss: 5.555421352386475\n","Epoch 68/4000, Step 27, d_loss: 0.0057693710550665855, g_loss: 6.116199970245361\n","Epoch 68/4000, Step 28, d_loss: 0.001684414455667138, g_loss: 4.491578578948975\n","Epoch 68/4000, Step 29, d_loss: 0.004378920421004295, g_loss: 4.722082614898682\n","Epoch 68/4000, Step 30, d_loss: 0.003927112556993961, g_loss: 5.914475917816162\n","Epoch 68/4000, Step 31, d_loss: 0.00508002657443285, g_loss: 5.2190470695495605\n","Epoch 68/4000, Step 32, d_loss: 0.010816735215485096, g_loss: 5.496134281158447\n","Epoch 68/4000, Step 33, d_loss: 0.004230262245982885, g_loss: 5.711609840393066\n","Epoch 68/4000, Step 34, d_loss: 0.0008039741078391671, g_loss: 5.741105556488037\n","Epoch 68/4000, Step 35, d_loss: 0.010493794456124306, g_loss: 5.01734733581543\n","Epoch 68/4000, Step 36, d_loss: 0.011560785584151745, g_loss: 5.214894771575928\n","Epoch 68/4000, Step 37, d_loss: 0.00458226865157485, g_loss: 5.2545294761657715\n","Epoch 68/4000, Step 38, d_loss: 0.0005006204009987414, g_loss: 5.541082859039307\n","Epoch 68/4000, Step 39, d_loss: 0.015004608780145645, g_loss: 6.544306755065918\n","Epoch 68/4000, Step 40, d_loss: 0.002073391806334257, g_loss: 4.235743045806885\n","Epoch 68/4000, Step 41, d_loss: 0.002619595266878605, g_loss: 6.064162254333496\n","Epoch 68/4000, Step 42, d_loss: 0.028413083404302597, g_loss: 7.10318660736084\n","Epoch 68/4000, Step 43, d_loss: 0.01116474624723196, g_loss: 5.928190231323242\n","Epoch 68/4000, Step 44, d_loss: 0.013465506955981255, g_loss: 3.8280272483825684\n","Epoch 68/4000, Step 45, d_loss: 0.010342502035200596, g_loss: 4.951940536499023\n","Epoch 68/4000, Step 46, d_loss: 0.005912999156862497, g_loss: 5.793445587158203\n","Epoch 68/4000, Step 47, d_loss: 0.002961470279842615, g_loss: 7.340127468109131\n","Epoch 68/4000, Step 48, d_loss: 0.010595324449241161, g_loss: 5.328970432281494\n","Epoch 68/4000, Step 49, d_loss: 0.004372366238385439, g_loss: 8.944815635681152\n","Epoch 68/4000, Step 50, d_loss: 0.005214249715209007, g_loss: 6.094675540924072\n","Epoch 68/4000, Step 51, d_loss: 0.001673076767474413, g_loss: 4.869965076446533\n","Epoch 68/4000, Step 52, d_loss: 0.011920434422791004, g_loss: 5.701984882354736\n","Epoch 68/4000, Step 53, d_loss: 0.0037472271360456944, g_loss: 4.957313537597656\n","Epoch 68/4000, Step 54, d_loss: 0.002717093098908663, g_loss: 6.646842002868652\n","Epoch 68/4000, Step 55, d_loss: 0.00080603122478351, g_loss: 5.1139750480651855\n","Epoch 68/4000, Step 56, d_loss: 0.0045696645975112915, g_loss: 6.284519672393799\n","Epoch 68/4000, Step 57, d_loss: 0.006792401894927025, g_loss: 6.863161087036133\n","Epoch 68/4000, Step 58, d_loss: 0.017252912744879723, g_loss: 4.99559211730957\n","Epoch 68/4000, Step 59, d_loss: 0.011586563661694527, g_loss: 7.752297401428223\n","Epoch 68/4000, Step 60, d_loss: 0.010006962344050407, g_loss: 5.03420352935791\n","Epoch 68/4000, Step 61, d_loss: 0.000344784464687109, g_loss: 6.314657688140869\n","Epoch 68/4000, Step 62, d_loss: 0.0008657166035845876, g_loss: 4.222128391265869\n","Epoch 68/4000, Step 63, d_loss: 0.013692829757928848, g_loss: 4.915908336639404\n","Epoch 68/4000, Step 64, d_loss: 0.0067126150242984295, g_loss: 4.851102828979492\n","Epoch 68/4000, Step 65, d_loss: 0.003648448269814253, g_loss: 7.00999116897583\n","Epoch 68/4000, Step 66, d_loss: 0.000946887070313096, g_loss: 6.368776798248291\n","Epoch 68/4000, Step 67, d_loss: 0.00442896131426096, g_loss: 6.61814546585083\n","Epoch 68/4000, Step 68, d_loss: 0.0025716982781887054, g_loss: 6.512604713439941\n","Epoch 68/4000, Step 69, d_loss: 0.0013840841129422188, g_loss: 6.871689796447754\n","Epoch 68/4000, Step 70, d_loss: 0.005627437960356474, g_loss: 4.815563678741455\n","Epoch 68/4000, Step 71, d_loss: 0.008496305905282497, g_loss: 6.558737277984619\n","Epoch 68/4000, Step 72, d_loss: 0.0032831751741468906, g_loss: 7.148499965667725\n","Epoch 68/4000, Step 73, d_loss: 0.012987885624170303, g_loss: 6.609553813934326\n","Epoch 68/4000, Step 74, d_loss: 0.0031340967398136854, g_loss: 5.333245754241943\n","Epoch 68/4000, Step 75, d_loss: 0.0036964844912290573, g_loss: 7.084411144256592\n","Epoch 68/4000, Step 76, d_loss: 0.05111751705408096, g_loss: 6.953882694244385\n","Epoch 68/4000, Step 77, d_loss: 0.025846445932984352, g_loss: 5.920035362243652\n","Epoch 68/4000, Step 78, d_loss: 0.009493056684732437, g_loss: 8.9395751953125\n","Epoch 68/4000, Step 79, d_loss: 0.00888978410512209, g_loss: 5.647843360900879\n","Epoch 68/4000, Step 80, d_loss: 0.03430703654885292, g_loss: 6.6870245933532715\n","Epoch 68/4000, Step 81, d_loss: 0.0058271633461117744, g_loss: 7.734423637390137\n","Epoch 68/4000, Step 82, d_loss: 0.004611744079738855, g_loss: 5.801961421966553\n","Epoch 68/4000, Step 83, d_loss: 0.005869327578693628, g_loss: 4.536221981048584\n","Epoch 68/4000, Step 84, d_loss: 0.005395912099629641, g_loss: 5.630660533905029\n","Epoch 68/4000, Step 85, d_loss: 0.022588832303881645, g_loss: 9.644500732421875\n","Epoch 68/4000, Step 86, d_loss: 0.008838978596031666, g_loss: 8.51839828491211\n","Epoch 68/4000, Step 87, d_loss: 0.004505124408751726, g_loss: 5.491912841796875\n","Epoch 68/4000, Step 88, d_loss: 0.0037803445011377335, g_loss: 7.0978827476501465\n","Epoch 68/4000, Step 89, d_loss: 0.0063913846388459206, g_loss: 3.941803216934204\n","Epoch 68/4000, Step 90, d_loss: 0.0063583082519471645, g_loss: 5.794068336486816\n","Epoch 68/4000, Step 91, d_loss: 0.004018074367195368, g_loss: 6.03964376449585\n","Epoch 68/4000, Step 92, d_loss: 0.006659969687461853, g_loss: 7.383611679077148\n","Epoch 68/4000, Step 93, d_loss: 0.014222908765077591, g_loss: 7.736334800720215\n","Epoch 68/4000, Step 94, d_loss: 0.018529405817389488, g_loss: 7.503513813018799\n","Epoch 68/4000, Step 95, d_loss: 0.0023227098863571882, g_loss: 6.387982368469238\n","Epoch 68/4000, Step 96, d_loss: 0.004095153883099556, g_loss: 4.292140483856201\n","Epoch 68/4000, Step 97, d_loss: 0.0017774882726371288, g_loss: 4.886893272399902\n","Epoch 68/4000, Step 98, d_loss: 0.0042706821113824844, g_loss: 6.333500862121582\n","Epoch 68/4000, Step 99, d_loss: 0.007708282209932804, g_loss: 9.213336944580078\n","Epoch 68/4000, Step 100, d_loss: 0.042159631848335266, g_loss: 11.247719764709473\n","Epoch 68/4000, Step 101, d_loss: 0.002794865984469652, g_loss: 7.378331184387207\n","Epoch 68/4000, Step 102, d_loss: 0.007166973780840635, g_loss: 7.359237194061279\n","Epoch 68/4000, Step 103, d_loss: 0.003582845674827695, g_loss: 8.286393165588379\n","Epoch 68/4000, Step 104, d_loss: 0.0374477282166481, g_loss: 4.5462541580200195\n","Epoch 68/4000, Step 105, d_loss: 0.014082606881856918, g_loss: 6.627538681030273\n","Epoch 68/4000, Step 106, d_loss: 0.010745522566139698, g_loss: 5.905306816101074\n","Epoch 68/4000, Step 107, d_loss: 0.0011513644130900502, g_loss: 7.884856700897217\n","Epoch 68/4000, Step 108, d_loss: 0.0099662309512496, g_loss: 6.227697849273682\n","Epoch 68/4000, Step 109, d_loss: 0.003096038941293955, g_loss: 5.2586350440979\n","Epoch 68/4000, Step 110, d_loss: 0.0014028106816112995, g_loss: 8.322336196899414\n","Epoch 68/4000, Step 111, d_loss: 0.00930776633322239, g_loss: 5.222525596618652\n","Epoch 68/4000, Step 112, d_loss: 0.0017346810782328248, g_loss: 5.6420745849609375\n","Epoch 68/4000, Step 113, d_loss: 0.006294546648859978, g_loss: 5.684707164764404\n","Epoch 68/4000, Step 114, d_loss: 0.04140229523181915, g_loss: 6.014406681060791\n","Epoch 68/4000, Step 115, d_loss: 0.0019520692294463515, g_loss: 5.130088806152344\n","Epoch 68/4000, Step 116, d_loss: 0.004261981230229139, g_loss: 5.7444987297058105\n","Epoch 68/4000, Step 117, d_loss: 0.00959948543459177, g_loss: 6.08892822265625\n","Epoch 68/4000, Step 118, d_loss: 0.008411013521254063, g_loss: 4.983451843261719\n","Epoch 68/4000, Step 119, d_loss: 0.026077846065163612, g_loss: 5.965600490570068\n","Epoch 68/4000, Step 120, d_loss: 0.007005193270742893, g_loss: 5.7723469734191895\n","Epoch 68/4000, Step 121, d_loss: 0.0028891246765851974, g_loss: 6.899171352386475\n","Epoch 68/4000, Step 122, d_loss: 0.0019202149705961347, g_loss: 7.671825408935547\n","Epoch 68/4000, Step 123, d_loss: 0.003681483678519726, g_loss: 6.2435383796691895\n","Epoch 68/4000, Step 124, d_loss: 0.010212546214461327, g_loss: 8.477124214172363\n","Epoch 68/4000, Step 125, d_loss: 0.005141871981322765, g_loss: 4.553467750549316\n","Epoch 68/4000, Step 126, d_loss: 0.0034482632763683796, g_loss: 6.477064609527588\n","Epoch 68/4000, Step 127, d_loss: 0.008143531158566475, g_loss: 6.625632286071777\n","Epoch 68/4000, Step 128, d_loss: 0.0012431544018909335, g_loss: 5.847320556640625\n","Epoch 68/4000, Step 129, d_loss: 0.0038226754404604435, g_loss: 5.0988287925720215\n","Epoch 68/4000, Step 130, d_loss: 0.01956598088145256, g_loss: 3.711291790008545\n","Epoch 68/4000, Step 131, d_loss: 0.012495922856032848, g_loss: 7.807427883148193\n","Epoch 68/4000, Step 132, d_loss: 0.004519009031355381, g_loss: 5.942054748535156\n","Epoch 68/4000, Step 133, d_loss: 0.011067758314311504, g_loss: 5.282102108001709\n","Epoch 68/4000, Step 134, d_loss: 0.02142641879618168, g_loss: 4.4922871589660645\n","Epoch 68/4000, Step 135, d_loss: 0.00843618530780077, g_loss: 4.586193561553955\n","Epoch 68/4000, Step 136, d_loss: 0.0008251487743109465, g_loss: 5.029986381530762\n","Epoch 68/4000, Step 137, d_loss: 0.029332268983125687, g_loss: 4.205543518066406\n","Epoch 68/4000, Step 138, d_loss: 0.02626049891114235, g_loss: 4.247791767120361\n","Epoch 68/4000, Step 139, d_loss: 0.003921082243323326, g_loss: 7.211081027984619\n","Epoch 68/4000, Step 140, d_loss: 0.010615618899464607, g_loss: 5.051867485046387\n","Epoch 68/4000, Step 141, d_loss: 0.0029413800220936537, g_loss: 5.291499614715576\n","Epoch 68/4000, Step 142, d_loss: 0.002288802992552519, g_loss: 6.286203384399414\n","Epoch 68/4000, Step 143, d_loss: 0.0013832338154315948, g_loss: 5.401953220367432\n","Epoch 68/4000, Step 144, d_loss: 0.00850848201662302, g_loss: 6.266914367675781\n","Epoch 68/4000, Step 145, d_loss: 0.002203655894845724, g_loss: 7.826675891876221\n","Epoch 68/4000, Step 146, d_loss: 0.008271620608866215, g_loss: 5.274521827697754\n","Epoch 68/4000, Step 147, d_loss: 0.006130282301455736, g_loss: 7.5987443923950195\n","Epoch 68/4000, Step 148, d_loss: 0.013407165184617043, g_loss: 7.767441272735596\n","Epoch 68/4000, Step 149, d_loss: 0.0016561655793339014, g_loss: 10.23442268371582\n","Epoch 68/4000, Step 150, d_loss: 0.002140794647857547, g_loss: 7.544297695159912\n","Epoch 68/4000, Step 151, d_loss: 0.022472281008958817, g_loss: 10.132475852966309\n","Epoch 68/4000, Step 152, d_loss: 0.004569375887513161, g_loss: 8.716444969177246\n","Epoch 68/4000, Step 153, d_loss: 0.003824986284598708, g_loss: 7.421140193939209\n","Epoch 68/4000, Step 154, d_loss: 0.02932848408818245, g_loss: 5.391798973083496\n","Epoch 68/4000, Step 155, d_loss: 0.004850444849580526, g_loss: 7.954082489013672\n","Epoch 68/4000, Step 156, d_loss: 0.003640565788373351, g_loss: 6.380705833435059\n","Epoch 68/4000, Step 157, d_loss: 0.008925662375986576, g_loss: 7.591733932495117\n","Epoch 68/4000, Step 158, d_loss: 0.005107531324028969, g_loss: 5.5389533042907715\n","Epoch 68/4000, Step 159, d_loss: 0.005307943560183048, g_loss: 5.70076847076416\n","Epoch 68/4000, Step 160, d_loss: 0.0054607996717095375, g_loss: 4.519608020782471\n","Epoch 68/4000, Step 161, d_loss: 0.005531837232410908, g_loss: 6.008063316345215\n","Epoch 68/4000, Step 162, d_loss: 0.0019004150526598096, g_loss: 6.878450393676758\n","Epoch 68/4000, Step 163, d_loss: 0.005531978327780962, g_loss: 5.411288261413574\n","Epoch 68/4000, Step 164, d_loss: 0.003918223083019257, g_loss: 8.124737739562988\n","Epoch 68/4000, Step 165, d_loss: 0.010082900524139404, g_loss: 10.198979377746582\n","Epoch 68/4000, Step 166, d_loss: 0.006345150526612997, g_loss: 4.203305244445801\n","Epoch 68/4000, Step 167, d_loss: 0.012193745002150536, g_loss: 6.868929862976074\n","Epoch 68/4000, Step 168, d_loss: 0.005284478887915611, g_loss: 6.728320598602295\n","Epoch 68/4000, Step 169, d_loss: 0.0014370640274137259, g_loss: 6.356700420379639\n","Epoch 68/4000, Step 170, d_loss: 0.0008586251642554998, g_loss: 7.502004146575928\n","Epoch 68/4000, Step 171, d_loss: 0.0033060533460229635, g_loss: 6.101572036743164\n","Epoch 68/4000, Step 172, d_loss: 0.009747261181473732, g_loss: 5.869961738586426\n","Epoch 68/4000, Step 173, d_loss: 0.003676398191601038, g_loss: 6.147022247314453\n","Epoch 68/4000, Step 174, d_loss: 0.010082333348691463, g_loss: 10.16701602935791\n","Epoch 68/4000, Step 175, d_loss: 0.010155398398637772, g_loss: 6.658287525177002\n","Epoch 68/4000, Step 176, d_loss: 0.001945242052897811, g_loss: 12.741604804992676\n","Epoch 68/4000, Step 177, d_loss: 0.0017543024150654674, g_loss: 5.9764404296875\n","Epoch 68/4000, Step 178, d_loss: 0.011222293600440025, g_loss: 5.801224231719971\n","Epoch 68/4000, Step 179, d_loss: 0.001958458684384823, g_loss: 5.5707526206970215\n","Epoch 68/4000, Step 180, d_loss: 0.010295720770955086, g_loss: 6.753029823303223\n","Epoch 68/4000, Step 181, d_loss: 0.0348498597741127, g_loss: 6.384326457977295\n","Epoch 68/4000, Step 182, d_loss: 0.12902016937732697, g_loss: 7.271170616149902\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 69/4000, Step 1, d_loss: 0.0048070987686514854, g_loss: 5.189075469970703\n","Epoch 69/4000, Step 2, d_loss: 0.0057540163397789, g_loss: 6.263350009918213\n","Epoch 69/4000, Step 3, d_loss: 0.04037605971097946, g_loss: 6.614466190338135\n","Epoch 69/4000, Step 4, d_loss: 0.006676414981484413, g_loss: 3.130856513977051\n","Epoch 69/4000, Step 5, d_loss: 0.07310937345027924, g_loss: 4.192793846130371\n","Epoch 69/4000, Step 6, d_loss: 0.007300657220184803, g_loss: 6.468821048736572\n","Epoch 69/4000, Step 7, d_loss: 0.001442388747818768, g_loss: 4.250764846801758\n","Epoch 69/4000, Step 8, d_loss: 0.07350320369005203, g_loss: 4.055181980133057\n","Epoch 69/4000, Step 9, d_loss: 0.0034442476462572813, g_loss: 3.7116966247558594\n","Epoch 69/4000, Step 10, d_loss: 0.05439028888940811, g_loss: 4.924501419067383\n","Epoch 69/4000, Step 11, d_loss: 0.006363329943269491, g_loss: 9.732181549072266\n","Epoch 69/4000, Step 12, d_loss: 0.0010387975489720702, g_loss: 7.09968376159668\n","Epoch 69/4000, Step 13, d_loss: 0.015887143090367317, g_loss: 6.218214988708496\n","Epoch 69/4000, Step 14, d_loss: 0.029005302116274834, g_loss: 7.522972583770752\n","Epoch 69/4000, Step 15, d_loss: 0.09199104458093643, g_loss: 6.840937614440918\n","Epoch 69/4000, Step 16, d_loss: 0.001237337477505207, g_loss: 6.63238000869751\n","Epoch 69/4000, Step 17, d_loss: 0.012230503372848034, g_loss: 5.664690017700195\n","Epoch 69/4000, Step 18, d_loss: 0.045671246945858, g_loss: 4.981969356536865\n","Epoch 69/4000, Step 19, d_loss: 0.007516093552112579, g_loss: 6.018192291259766\n","Epoch 69/4000, Step 20, d_loss: 0.010355665348470211, g_loss: 5.056907653808594\n","Epoch 69/4000, Step 21, d_loss: 0.07896548509597778, g_loss: 4.676459789276123\n","Epoch 69/4000, Step 22, d_loss: 0.03754863515496254, g_loss: 4.977684497833252\n","Epoch 69/4000, Step 23, d_loss: 0.021525220945477486, g_loss: 8.000234603881836\n","Epoch 69/4000, Step 24, d_loss: 0.005306281615048647, g_loss: 7.714391708374023\n","Epoch 69/4000, Step 25, d_loss: 0.013587519526481628, g_loss: 6.706068992614746\n","Epoch 69/4000, Step 26, d_loss: 0.033891670405864716, g_loss: 4.68613338470459\n","Epoch 69/4000, Step 27, d_loss: 0.007789204828441143, g_loss: 4.535587787628174\n","Epoch 69/4000, Step 28, d_loss: 0.014077352359890938, g_loss: 6.14520263671875\n","Epoch 69/4000, Step 29, d_loss: 0.022621702402830124, g_loss: 6.015373706817627\n","Epoch 69/4000, Step 30, d_loss: 0.019111085683107376, g_loss: 9.869268417358398\n","Epoch 69/4000, Step 31, d_loss: 0.10456457734107971, g_loss: 3.5137338638305664\n","Epoch 69/4000, Step 32, d_loss: 0.013922398909926414, g_loss: 5.33832311630249\n","Epoch 69/4000, Step 33, d_loss: 0.014105672016739845, g_loss: 5.695547103881836\n","Epoch 69/4000, Step 34, d_loss: 0.027424026280641556, g_loss: 5.0456862449646\n","Epoch 69/4000, Step 35, d_loss: 0.00403997115790844, g_loss: 8.589446067810059\n","Epoch 69/4000, Step 36, d_loss: 0.031697239726781845, g_loss: 6.527291774749756\n","Epoch 69/4000, Step 37, d_loss: 0.018555520102381706, g_loss: 5.31076192855835\n","Epoch 69/4000, Step 38, d_loss: 0.0012141187908127904, g_loss: 5.297985076904297\n","Epoch 69/4000, Step 39, d_loss: 0.005613104905933142, g_loss: 4.15187406539917\n","Epoch 69/4000, Step 40, d_loss: 0.0029999096877872944, g_loss: 7.5735697746276855\n","Epoch 69/4000, Step 41, d_loss: 0.025794722139835358, g_loss: 6.721250534057617\n","Epoch 69/4000, Step 42, d_loss: 0.009108640253543854, g_loss: 3.8101511001586914\n","Epoch 69/4000, Step 43, d_loss: 0.015592885203659534, g_loss: 5.417068958282471\n","Epoch 69/4000, Step 44, d_loss: 0.014279257506132126, g_loss: 6.155777931213379\n","Epoch 69/4000, Step 45, d_loss: 0.01045741606503725, g_loss: 7.47166633605957\n","Epoch 69/4000, Step 46, d_loss: 0.009657496586441994, g_loss: 4.723376750946045\n","Epoch 69/4000, Step 47, d_loss: 0.01081112027168274, g_loss: 5.522980213165283\n","Epoch 69/4000, Step 48, d_loss: 0.011813009157776833, g_loss: 6.695098400115967\n","Epoch 69/4000, Step 49, d_loss: 0.001684602815657854, g_loss: 6.614047050476074\n","Epoch 69/4000, Step 50, d_loss: 0.002867741510272026, g_loss: 3.7517173290252686\n","Epoch 69/4000, Step 51, d_loss: 0.0020576389506459236, g_loss: 7.251345634460449\n","Epoch 69/4000, Step 52, d_loss: 0.011731108650565147, g_loss: 8.434117317199707\n","Epoch 69/4000, Step 53, d_loss: 0.021686088293790817, g_loss: 6.6871795654296875\n","Epoch 69/4000, Step 54, d_loss: 0.0017534256912767887, g_loss: 3.9859752655029297\n","Epoch 69/4000, Step 55, d_loss: 0.004435618873685598, g_loss: 5.834208965301514\n","Epoch 69/4000, Step 56, d_loss: 0.0027642229106277227, g_loss: 9.551152229309082\n","Epoch 69/4000, Step 57, d_loss: 0.004806246608495712, g_loss: 6.734978199005127\n","Epoch 69/4000, Step 58, d_loss: 0.015288112685084343, g_loss: 6.050531387329102\n","Epoch 69/4000, Step 59, d_loss: 0.0012228311970829964, g_loss: 5.3340582847595215\n","Epoch 69/4000, Step 60, d_loss: 0.0037027867510914803, g_loss: 5.7630534172058105\n","Epoch 69/4000, Step 61, d_loss: 0.004896702244877815, g_loss: 7.338249683380127\n","Epoch 69/4000, Step 62, d_loss: 0.0015050361398607492, g_loss: 5.634371757507324\n","Epoch 69/4000, Step 63, d_loss: 0.013998781330883503, g_loss: 6.3746466636657715\n","Epoch 69/4000, Step 64, d_loss: 0.003109501674771309, g_loss: 5.965015888214111\n","Epoch 69/4000, Step 65, d_loss: 0.0021150209940969944, g_loss: 6.271644115447998\n","Epoch 69/4000, Step 66, d_loss: 0.003000783734023571, g_loss: 5.894427299499512\n","Epoch 69/4000, Step 67, d_loss: 0.001661387155763805, g_loss: 5.354763031005859\n","Epoch 69/4000, Step 68, d_loss: 0.007505093701183796, g_loss: 5.336688995361328\n","Epoch 69/4000, Step 69, d_loss: 0.0025746305473148823, g_loss: 4.256156921386719\n","Epoch 69/4000, Step 70, d_loss: 0.00921687949448824, g_loss: 5.719213962554932\n","Epoch 69/4000, Step 71, d_loss: 0.0010221176780760288, g_loss: 6.929125785827637\n","Epoch 69/4000, Step 72, d_loss: 0.0019931355491280556, g_loss: 6.4159979820251465\n","Epoch 69/4000, Step 73, d_loss: 0.0028267535381019115, g_loss: 6.687923908233643\n","Epoch 69/4000, Step 74, d_loss: 0.0017422407399863005, g_loss: 5.303685665130615\n","Epoch 69/4000, Step 75, d_loss: 0.009943686425685883, g_loss: 7.770084381103516\n","Epoch 69/4000, Step 76, d_loss: 0.00208953651599586, g_loss: 4.983644962310791\n","Epoch 69/4000, Step 77, d_loss: 0.004841047804802656, g_loss: 6.268589496612549\n","Epoch 69/4000, Step 78, d_loss: 0.0077878800220787525, g_loss: 10.011149406433105\n","Epoch 69/4000, Step 79, d_loss: 0.0201158095151186, g_loss: 7.893112659454346\n","Epoch 69/4000, Step 80, d_loss: 0.015049045905470848, g_loss: 6.683588981628418\n","Epoch 69/4000, Step 81, d_loss: 0.004436646122485399, g_loss: 5.213289737701416\n","Epoch 69/4000, Step 82, d_loss: 0.023013636469841003, g_loss: 6.575922012329102\n","Epoch 69/4000, Step 83, d_loss: 0.0008089119219221175, g_loss: 8.332820892333984\n","Epoch 69/4000, Step 84, d_loss: 0.007417525630444288, g_loss: 5.606262683868408\n","Epoch 69/4000, Step 85, d_loss: 0.0038831070996820927, g_loss: 8.851262092590332\n","Epoch 69/4000, Step 86, d_loss: 0.0021772964391857386, g_loss: 6.989138603210449\n","Epoch 69/4000, Step 87, d_loss: 0.011205650866031647, g_loss: 6.702911376953125\n","Epoch 69/4000, Step 88, d_loss: 0.0038378185126930475, g_loss: 6.0193400382995605\n","Epoch 69/4000, Step 89, d_loss: 0.006058357656002045, g_loss: 7.25484037399292\n","Epoch 69/4000, Step 90, d_loss: 0.003131468314677477, g_loss: 6.306617736816406\n","Epoch 69/4000, Step 91, d_loss: 0.001349758473224938, g_loss: 6.728440761566162\n","Epoch 69/4000, Step 92, d_loss: 0.002098222728818655, g_loss: 8.919944763183594\n","Epoch 69/4000, Step 93, d_loss: 0.0029333923012018204, g_loss: 8.793157577514648\n","Epoch 69/4000, Step 94, d_loss: 0.008869894780218601, g_loss: 6.358497142791748\n","Epoch 69/4000, Step 95, d_loss: 0.005496586672961712, g_loss: 9.652205467224121\n","Epoch 69/4000, Step 96, d_loss: 0.002418374642729759, g_loss: 6.715329170227051\n","Epoch 69/4000, Step 97, d_loss: 0.0043586427345871925, g_loss: 4.707284927368164\n","Epoch 69/4000, Step 98, d_loss: 0.014188192784786224, g_loss: 6.31640625\n","Epoch 69/4000, Step 99, d_loss: 0.00155452243052423, g_loss: 4.814740180969238\n","Epoch 69/4000, Step 100, d_loss: 0.002359905280172825, g_loss: 12.270646095275879\n","Epoch 69/4000, Step 101, d_loss: 0.02723458781838417, g_loss: 7.0937910079956055\n","Epoch 69/4000, Step 102, d_loss: 0.0017523302230983973, g_loss: 6.340324878692627\n","Epoch 69/4000, Step 103, d_loss: 0.0016558929346501827, g_loss: 6.750218868255615\n","Epoch 69/4000, Step 104, d_loss: 0.002972518326714635, g_loss: 6.11470890045166\n","Epoch 69/4000, Step 105, d_loss: 0.0019326976034790277, g_loss: 4.616644382476807\n","Epoch 69/4000, Step 106, d_loss: 0.011732865124940872, g_loss: 5.54812479019165\n","Epoch 69/4000, Step 107, d_loss: 0.0012595781590789557, g_loss: 4.74680233001709\n","Epoch 69/4000, Step 108, d_loss: 0.006538678891956806, g_loss: 7.483479022979736\n","Epoch 69/4000, Step 109, d_loss: 0.005857964977622032, g_loss: 3.8298168182373047\n","Epoch 69/4000, Step 110, d_loss: 0.0031907965894788504, g_loss: 5.39232873916626\n","Epoch 69/4000, Step 111, d_loss: 0.005911753512918949, g_loss: 5.058402061462402\n","Epoch 69/4000, Step 112, d_loss: 0.001325590768828988, g_loss: 6.273099422454834\n","Epoch 69/4000, Step 113, d_loss: 0.0012567159719765186, g_loss: 5.8088297843933105\n","Epoch 69/4000, Step 114, d_loss: 0.0028688220772892237, g_loss: 7.523293495178223\n","Epoch 69/4000, Step 115, d_loss: 0.002462610602378845, g_loss: 4.857058525085449\n","Epoch 69/4000, Step 116, d_loss: 0.008152040652930737, g_loss: 5.222290992736816\n","Epoch 69/4000, Step 117, d_loss: 0.006248511373996735, g_loss: 5.985708236694336\n","Epoch 69/4000, Step 118, d_loss: 0.011750379577279091, g_loss: 4.92584753036499\n","Epoch 69/4000, Step 119, d_loss: 0.0021443222649395466, g_loss: 6.370103359222412\n","Epoch 69/4000, Step 120, d_loss: 0.0008446867577731609, g_loss: 5.409641265869141\n","Epoch 69/4000, Step 121, d_loss: 0.01373667549341917, g_loss: 7.830670356750488\n","Epoch 69/4000, Step 122, d_loss: 0.006190261337906122, g_loss: 6.066500186920166\n","Epoch 69/4000, Step 123, d_loss: 0.00458673108369112, g_loss: 6.277961730957031\n","Epoch 69/4000, Step 124, d_loss: 0.0028448000084608793, g_loss: 4.6949782371521\n","Epoch 69/4000, Step 125, d_loss: 0.0034466024953871965, g_loss: 4.796519756317139\n","Epoch 69/4000, Step 126, d_loss: 0.014322495087981224, g_loss: 5.3598408699035645\n","Epoch 69/4000, Step 127, d_loss: 0.0015518724685534835, g_loss: 5.767312049865723\n","Epoch 69/4000, Step 128, d_loss: 0.0020670131780207157, g_loss: 4.834011554718018\n","Epoch 69/4000, Step 129, d_loss: 0.009590504691004753, g_loss: 4.398033618927002\n","Epoch 69/4000, Step 130, d_loss: 0.02363538183271885, g_loss: 7.596048355102539\n","Epoch 69/4000, Step 131, d_loss: 0.0026655844412744045, g_loss: 5.964887619018555\n","Epoch 69/4000, Step 132, d_loss: 0.0036856113001704216, g_loss: 9.603105545043945\n","Epoch 69/4000, Step 133, d_loss: 0.007607391104102135, g_loss: 8.421515464782715\n","Epoch 69/4000, Step 134, d_loss: 0.004053322598338127, g_loss: 8.499262809753418\n","Epoch 69/4000, Step 135, d_loss: 0.009308145381510258, g_loss: 5.251129150390625\n","Epoch 69/4000, Step 136, d_loss: 0.0252850279211998, g_loss: 7.601226329803467\n","Epoch 69/4000, Step 137, d_loss: 0.001626910176128149, g_loss: 6.154598236083984\n","Epoch 69/4000, Step 138, d_loss: 0.0027637036982923746, g_loss: 4.403407573699951\n","Epoch 69/4000, Step 139, d_loss: 0.008611445315182209, g_loss: 5.463250637054443\n","Epoch 69/4000, Step 140, d_loss: 0.006208507344126701, g_loss: 6.124935626983643\n","Epoch 69/4000, Step 141, d_loss: 0.03280371427536011, g_loss: 7.494797229766846\n","Epoch 69/4000, Step 142, d_loss: 0.008308491669595242, g_loss: 6.408932685852051\n","Epoch 69/4000, Step 143, d_loss: 0.005596238188445568, g_loss: 4.94498348236084\n","Epoch 69/4000, Step 144, d_loss: 0.0009965397184714675, g_loss: 4.139552116394043\n","Epoch 69/4000, Step 145, d_loss: 0.0014581033028662205, g_loss: 6.776280403137207\n","Epoch 69/4000, Step 146, d_loss: 0.0036764047108590603, g_loss: 7.070504665374756\n","Epoch 69/4000, Step 147, d_loss: 0.0017416481859982014, g_loss: 5.59767484664917\n","Epoch 69/4000, Step 148, d_loss: 0.0016914240550249815, g_loss: 5.132901668548584\n","Epoch 69/4000, Step 149, d_loss: 0.005733956582844257, g_loss: 5.277729511260986\n","Epoch 69/4000, Step 150, d_loss: 0.01235794648528099, g_loss: 5.586394309997559\n","Epoch 69/4000, Step 151, d_loss: 0.01583966799080372, g_loss: 4.805962562561035\n","Epoch 69/4000, Step 152, d_loss: 0.002297916915267706, g_loss: 5.606008052825928\n","Epoch 69/4000, Step 153, d_loss: 0.0038822286296635866, g_loss: 7.834223747253418\n","Epoch 69/4000, Step 154, d_loss: 0.0013466899981722236, g_loss: 4.258832931518555\n","Epoch 69/4000, Step 155, d_loss: 0.005550798028707504, g_loss: 4.69577169418335\n","Epoch 69/4000, Step 156, d_loss: 0.02398275025188923, g_loss: 5.654418468475342\n","Epoch 69/4000, Step 157, d_loss: 0.004676991607993841, g_loss: 5.773368835449219\n","Epoch 69/4000, Step 158, d_loss: 0.005930641666054726, g_loss: 4.806650161743164\n","Epoch 69/4000, Step 159, d_loss: 0.0018292202148586512, g_loss: 5.409350395202637\n","Epoch 69/4000, Step 160, d_loss: 0.010189512744545937, g_loss: 7.153919219970703\n","Epoch 69/4000, Step 161, d_loss: 0.0010232897475361824, g_loss: 8.727210998535156\n","Epoch 69/4000, Step 162, d_loss: 0.01292287278920412, g_loss: 5.830593585968018\n","Epoch 69/4000, Step 163, d_loss: 0.008671851828694344, g_loss: 5.276848316192627\n","Epoch 69/4000, Step 164, d_loss: 0.026739004999399185, g_loss: 5.083335876464844\n","Epoch 69/4000, Step 165, d_loss: 0.0020746386144310236, g_loss: 6.180168628692627\n","Epoch 69/4000, Step 166, d_loss: 0.004924083594232798, g_loss: 6.838831424713135\n","Epoch 69/4000, Step 167, d_loss: 0.003017555922269821, g_loss: 6.656495094299316\n","Epoch 69/4000, Step 168, d_loss: 0.0007483411463908851, g_loss: 6.20914888381958\n","Epoch 69/4000, Step 169, d_loss: 0.002847167896106839, g_loss: 6.005518436431885\n","Epoch 69/4000, Step 170, d_loss: 0.04176280274987221, g_loss: 5.7395806312561035\n","Epoch 69/4000, Step 171, d_loss: 0.008239978924393654, g_loss: 5.356048107147217\n","Epoch 69/4000, Step 172, d_loss: 0.008235694840550423, g_loss: 5.846376895904541\n","Epoch 69/4000, Step 173, d_loss: 0.009554412215948105, g_loss: 5.442160606384277\n","Epoch 69/4000, Step 174, d_loss: 0.0008695158176124096, g_loss: 4.3829498291015625\n","Epoch 69/4000, Step 175, d_loss: 0.004023018758744001, g_loss: 6.1447296142578125\n","Epoch 69/4000, Step 176, d_loss: 0.027645858004689217, g_loss: 5.668628692626953\n","Epoch 69/4000, Step 177, d_loss: 0.003501426661387086, g_loss: 7.316874980926514\n","Epoch 69/4000, Step 178, d_loss: 0.002344907494261861, g_loss: 9.238856315612793\n","Epoch 69/4000, Step 179, d_loss: 0.014896598644554615, g_loss: 6.404606342315674\n","Epoch 69/4000, Step 180, d_loss: 0.006434893701225519, g_loss: 4.127553939819336\n","Epoch 69/4000, Step 181, d_loss: 0.019150858744978905, g_loss: 7.882448673248291\n","Epoch 69/4000, Step 182, d_loss: 0.049150098115205765, g_loss: 4.990556716918945\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 70/4000, Step 1, d_loss: 0.02454381063580513, g_loss: 5.493649005889893\n","Epoch 70/4000, Step 2, d_loss: 0.0017297897720709443, g_loss: 5.135664939880371\n","Epoch 70/4000, Step 3, d_loss: 0.006169917993247509, g_loss: 5.572244167327881\n","Epoch 70/4000, Step 4, d_loss: 0.003595380112528801, g_loss: 4.78184700012207\n","Epoch 70/4000, Step 5, d_loss: 0.021150145679712296, g_loss: 4.59613037109375\n","Epoch 70/4000, Step 6, d_loss: 0.0023372131399810314, g_loss: 3.794215679168701\n","Epoch 70/4000, Step 7, d_loss: 0.004230104386806488, g_loss: 4.190530776977539\n","Epoch 70/4000, Step 8, d_loss: 0.007881986908614635, g_loss: 6.858523368835449\n","Epoch 70/4000, Step 9, d_loss: 0.007387123070657253, g_loss: 7.734907627105713\n","Epoch 70/4000, Step 10, d_loss: 0.03751463443040848, g_loss: 6.325300216674805\n","Epoch 70/4000, Step 11, d_loss: 0.020070020109415054, g_loss: 4.2337446212768555\n","Epoch 70/4000, Step 12, d_loss: 0.0026156026870012283, g_loss: 4.620963096618652\n","Epoch 70/4000, Step 13, d_loss: 0.00434323213994503, g_loss: 6.131724834442139\n","Epoch 70/4000, Step 14, d_loss: 0.012042694725096226, g_loss: 4.974336624145508\n","Epoch 70/4000, Step 15, d_loss: 0.011675998568534851, g_loss: 6.62962007522583\n","Epoch 70/4000, Step 16, d_loss: 0.051056113094091415, g_loss: 5.374091148376465\n","Epoch 70/4000, Step 17, d_loss: 0.004846645053476095, g_loss: 4.47643518447876\n","Epoch 70/4000, Step 18, d_loss: 0.09439373016357422, g_loss: 5.609421253204346\n","Epoch 70/4000, Step 19, d_loss: 0.010001256130635738, g_loss: 8.50110149383545\n","Epoch 70/4000, Step 20, d_loss: 0.006276384461671114, g_loss: 5.337342739105225\n","Epoch 70/4000, Step 21, d_loss: 0.013229376636445522, g_loss: 5.714706897735596\n","Epoch 70/4000, Step 22, d_loss: 0.015081290155649185, g_loss: 8.271071434020996\n","Epoch 70/4000, Step 23, d_loss: 0.02114790678024292, g_loss: 8.087067604064941\n","Epoch 70/4000, Step 24, d_loss: 0.004375827964395285, g_loss: 8.764326095581055\n","Epoch 70/4000, Step 25, d_loss: 0.004453485831618309, g_loss: 9.101422309875488\n","Epoch 70/4000, Step 26, d_loss: 0.00455479184165597, g_loss: 7.05504035949707\n","Epoch 70/4000, Step 27, d_loss: 0.02954796701669693, g_loss: 6.492273807525635\n","Epoch 70/4000, Step 28, d_loss: 0.005197511054575443, g_loss: 4.724548816680908\n","Epoch 70/4000, Step 29, d_loss: 0.004137763753533363, g_loss: 7.536001205444336\n","Epoch 70/4000, Step 30, d_loss: 0.0025189423467963934, g_loss: 10.634299278259277\n","Epoch 70/4000, Step 31, d_loss: 0.00782416295260191, g_loss: 8.955519676208496\n","Epoch 70/4000, Step 32, d_loss: 0.016298683360219002, g_loss: 6.219220161437988\n","Epoch 70/4000, Step 33, d_loss: 0.003196282312273979, g_loss: 6.362835884094238\n","Epoch 70/4000, Step 34, d_loss: 0.018858447670936584, g_loss: 8.267051696777344\n","Epoch 70/4000, Step 35, d_loss: 0.0026950205210596323, g_loss: 8.453145027160645\n","Epoch 70/4000, Step 36, d_loss: 0.006451041903346777, g_loss: 9.24178695678711\n","Epoch 70/4000, Step 37, d_loss: 0.006130236200988293, g_loss: 7.353976249694824\n","Epoch 70/4000, Step 38, d_loss: 0.0201035737991333, g_loss: 5.781092166900635\n","Epoch 70/4000, Step 39, d_loss: 0.0022904626093804836, g_loss: 4.170619487762451\n","Epoch 70/4000, Step 40, d_loss: 0.0066809942945837975, g_loss: 5.070891380310059\n","Epoch 70/4000, Step 41, d_loss: 0.0032054041512310505, g_loss: 7.297754764556885\n","Epoch 70/4000, Step 42, d_loss: 0.002012152224779129, g_loss: 4.555639266967773\n","Epoch 70/4000, Step 43, d_loss: 0.003674593521282077, g_loss: 7.3413896560668945\n","Epoch 70/4000, Step 44, d_loss: 0.007120809983462095, g_loss: 6.438169002532959\n","Epoch 70/4000, Step 45, d_loss: 0.00867829192429781, g_loss: 4.542464256286621\n","Epoch 70/4000, Step 46, d_loss: 0.012149276211857796, g_loss: 5.363125801086426\n","Epoch 70/4000, Step 47, d_loss: 0.003555683884769678, g_loss: 5.966610431671143\n","Epoch 70/4000, Step 48, d_loss: 0.002147217281162739, g_loss: 6.661570072174072\n","Epoch 70/4000, Step 49, d_loss: 0.007714490406215191, g_loss: 7.18703556060791\n","Epoch 70/4000, Step 50, d_loss: 0.006149308755993843, g_loss: 6.651485919952393\n","Epoch 70/4000, Step 51, d_loss: 0.00453720660880208, g_loss: 6.511234760284424\n","Epoch 70/4000, Step 52, d_loss: 0.0075209252536296844, g_loss: 6.391627788543701\n","Epoch 70/4000, Step 53, d_loss: 0.004960676189512014, g_loss: 4.631329536437988\n","Epoch 70/4000, Step 54, d_loss: 0.018718337640166283, g_loss: 5.625360488891602\n","Epoch 70/4000, Step 55, d_loss: 0.05529794469475746, g_loss: 6.652808666229248\n","Epoch 70/4000, Step 56, d_loss: 0.003431974910199642, g_loss: 8.960845947265625\n","Epoch 70/4000, Step 57, d_loss: 0.008242230862379074, g_loss: 4.782591819763184\n","Epoch 70/4000, Step 58, d_loss: 0.020360777154564857, g_loss: 4.325366497039795\n","Epoch 70/4000, Step 59, d_loss: 0.0023037355858832598, g_loss: 7.9592108726501465\n","Epoch 70/4000, Step 60, d_loss: 0.021059688180685043, g_loss: 2.9588544368743896\n","Epoch 70/4000, Step 61, d_loss: 0.006076417863368988, g_loss: 6.325351715087891\n","Epoch 70/4000, Step 62, d_loss: 0.0031561371870338917, g_loss: 4.677246570587158\n","Epoch 70/4000, Step 63, d_loss: 0.006177152507007122, g_loss: 4.84508752822876\n","Epoch 70/4000, Step 64, d_loss: 0.026624485850334167, g_loss: 4.7829484939575195\n","Epoch 70/4000, Step 65, d_loss: 0.006946342997252941, g_loss: 6.448247909545898\n","Epoch 70/4000, Step 66, d_loss: 0.014615839347243309, g_loss: 5.538262844085693\n","Epoch 70/4000, Step 67, d_loss: 0.019115228205919266, g_loss: 5.689033508300781\n","Epoch 70/4000, Step 68, d_loss: 0.03214903920888901, g_loss: 5.5729265213012695\n","Epoch 70/4000, Step 69, d_loss: 0.0024969822261482477, g_loss: 6.064826965332031\n","Epoch 70/4000, Step 70, d_loss: 0.001746834022924304, g_loss: 6.005696773529053\n","Epoch 70/4000, Step 71, d_loss: 0.002772921696305275, g_loss: 6.233452320098877\n","Epoch 70/4000, Step 72, d_loss: 0.000834797858260572, g_loss: 7.2463884353637695\n","Epoch 70/4000, Step 73, d_loss: 0.0034664017148315907, g_loss: 7.519230842590332\n","Epoch 70/4000, Step 74, d_loss: 0.008239912800490856, g_loss: 5.3791069984436035\n","Epoch 70/4000, Step 75, d_loss: 0.0004567133146338165, g_loss: 6.999087810516357\n","Epoch 70/4000, Step 76, d_loss: 0.02133098989725113, g_loss: 8.710514068603516\n","Epoch 70/4000, Step 77, d_loss: 0.006822794675827026, g_loss: 7.305187225341797\n","Epoch 70/4000, Step 78, d_loss: 0.01928216591477394, g_loss: 7.898629188537598\n","Epoch 70/4000, Step 79, d_loss: 0.023213017731904984, g_loss: 8.235424995422363\n","Epoch 70/4000, Step 80, d_loss: 0.003927984740585089, g_loss: 6.172348976135254\n","Epoch 70/4000, Step 81, d_loss: 0.0062517402693629265, g_loss: 7.110698223114014\n","Epoch 70/4000, Step 82, d_loss: 0.0020271826069802046, g_loss: 6.469378471374512\n","Epoch 70/4000, Step 83, d_loss: 0.0006330994656309485, g_loss: 6.8406476974487305\n","Epoch 70/4000, Step 84, d_loss: 0.006771828513592482, g_loss: 6.627048492431641\n","Epoch 70/4000, Step 85, d_loss: 0.013499763794243336, g_loss: 5.9151129722595215\n","Epoch 70/4000, Step 86, d_loss: 0.005773327313363552, g_loss: 6.153244972229004\n","Epoch 70/4000, Step 87, d_loss: 0.006389832589775324, g_loss: 5.7429351806640625\n","Epoch 70/4000, Step 88, d_loss: 0.002287044655531645, g_loss: 4.483551979064941\n","Epoch 70/4000, Step 89, d_loss: 0.013897325843572617, g_loss: 5.384720325469971\n","Epoch 70/4000, Step 90, d_loss: 0.031148448586463928, g_loss: 5.426266670227051\n","Epoch 70/4000, Step 91, d_loss: 0.014388520270586014, g_loss: 6.2501959800720215\n","Epoch 70/4000, Step 92, d_loss: 0.016189882531762123, g_loss: 4.448987007141113\n","Epoch 70/4000, Step 93, d_loss: 0.009235000237822533, g_loss: 4.709131240844727\n","Epoch 70/4000, Step 94, d_loss: 0.006156781688332558, g_loss: 8.02637004852295\n","Epoch 70/4000, Step 95, d_loss: 0.002373362658545375, g_loss: 5.854837417602539\n","Epoch 70/4000, Step 96, d_loss: 0.01441534236073494, g_loss: 8.89210319519043\n","Epoch 70/4000, Step 97, d_loss: 0.002662868704646826, g_loss: 5.107443332672119\n","Epoch 70/4000, Step 98, d_loss: 0.003249857574701309, g_loss: 6.050688743591309\n","Epoch 70/4000, Step 99, d_loss: 0.011392692103981972, g_loss: 6.577001094818115\n","Epoch 70/4000, Step 100, d_loss: 0.0011751374695450068, g_loss: 5.712864875793457\n","Epoch 70/4000, Step 101, d_loss: 0.007418307010084391, g_loss: 8.064035415649414\n","Epoch 70/4000, Step 102, d_loss: 0.0056211575865745544, g_loss: 7.157061576843262\n","Epoch 70/4000, Step 103, d_loss: 0.004911437630653381, g_loss: 5.3126540184021\n","Epoch 70/4000, Step 104, d_loss: 0.0010129868751391768, g_loss: 6.97244119644165\n","Epoch 70/4000, Step 105, d_loss: 0.023340284824371338, g_loss: 4.100870609283447\n","Epoch 70/4000, Step 106, d_loss: 0.002814612118527293, g_loss: 4.562536239624023\n","Epoch 70/4000, Step 107, d_loss: 0.0045056394301354885, g_loss: 4.609390735626221\n","Epoch 70/4000, Step 108, d_loss: 0.004425059538334608, g_loss: 9.844924926757812\n","Epoch 70/4000, Step 109, d_loss: 0.0016675357474014163, g_loss: 8.139592170715332\n","Epoch 70/4000, Step 110, d_loss: 0.005578274372965097, g_loss: 7.47552490234375\n","Epoch 70/4000, Step 111, d_loss: 0.0014194587711244822, g_loss: 7.307074546813965\n","Epoch 70/4000, Step 112, d_loss: 0.0017422877717763186, g_loss: 5.34798002243042\n","Epoch 70/4000, Step 113, d_loss: 0.003867523744702339, g_loss: 4.519468307495117\n","Epoch 70/4000, Step 114, d_loss: 0.005104048177599907, g_loss: 4.929317474365234\n","Epoch 70/4000, Step 115, d_loss: 0.0009575602598488331, g_loss: 4.010173797607422\n","Epoch 70/4000, Step 116, d_loss: 0.002882037777453661, g_loss: 6.495007514953613\n","Epoch 70/4000, Step 117, d_loss: 0.0010560380760580301, g_loss: 6.411316871643066\n","Epoch 70/4000, Step 118, d_loss: 0.003307692240923643, g_loss: 6.181807994842529\n","Epoch 70/4000, Step 119, d_loss: 0.0015157368034124374, g_loss: 7.905373573303223\n","Epoch 70/4000, Step 120, d_loss: 0.010094597935676575, g_loss: 5.211279392242432\n","Epoch 70/4000, Step 121, d_loss: 0.0011377469636499882, g_loss: 6.923959255218506\n","Epoch 70/4000, Step 122, d_loss: 0.0018478396814316511, g_loss: 6.668849468231201\n","Epoch 70/4000, Step 123, d_loss: 0.06823653727769852, g_loss: 6.539449691772461\n","Epoch 70/4000, Step 124, d_loss: 0.0033208969980478287, g_loss: 5.085237979888916\n","Epoch 70/4000, Step 125, d_loss: 0.004112569615244865, g_loss: 3.3801069259643555\n","Epoch 70/4000, Step 126, d_loss: 0.0029144103173166513, g_loss: 4.20999002456665\n","Epoch 70/4000, Step 127, d_loss: 0.014590765349566936, g_loss: 5.846643447875977\n","Epoch 70/4000, Step 128, d_loss: 0.012150009162724018, g_loss: 4.589271068572998\n","Epoch 70/4000, Step 129, d_loss: 0.0014250180684030056, g_loss: 6.087804317474365\n","Epoch 70/4000, Step 130, d_loss: 0.008197381161153316, g_loss: 8.36634349822998\n","Epoch 70/4000, Step 131, d_loss: 0.007617061957716942, g_loss: 3.110551118850708\n","Epoch 70/4000, Step 132, d_loss: 0.009156439453363419, g_loss: 5.381289005279541\n","Epoch 70/4000, Step 133, d_loss: 0.004968713037669659, g_loss: 5.77032995223999\n","Epoch 70/4000, Step 134, d_loss: 0.005509808659553528, g_loss: 7.442899227142334\n","Epoch 70/4000, Step 135, d_loss: 0.0028489164542406797, g_loss: 6.134668350219727\n","Epoch 70/4000, Step 136, d_loss: 0.00431001465767622, g_loss: 6.863544464111328\n","Epoch 70/4000, Step 137, d_loss: 0.004918606951832771, g_loss: 7.125312328338623\n","Epoch 70/4000, Step 138, d_loss: 0.015511603094637394, g_loss: 4.153666019439697\n","Epoch 70/4000, Step 139, d_loss: 0.0016806090716272593, g_loss: 7.872509002685547\n","Epoch 70/4000, Step 140, d_loss: 0.0006930343806743622, g_loss: 7.220836162567139\n","Epoch 70/4000, Step 141, d_loss: 0.012188957072794437, g_loss: 7.215141773223877\n","Epoch 70/4000, Step 142, d_loss: 0.003935422282665968, g_loss: 7.111662864685059\n","Epoch 70/4000, Step 143, d_loss: 0.03762657195329666, g_loss: 7.46864652633667\n","Epoch 70/4000, Step 144, d_loss: 0.0028791027143597603, g_loss: 7.325168609619141\n","Epoch 70/4000, Step 145, d_loss: 0.005286126397550106, g_loss: 6.8648881912231445\n","Epoch 70/4000, Step 146, d_loss: 0.035039227455854416, g_loss: 3.5326881408691406\n","Epoch 70/4000, Step 147, d_loss: 0.009212048724293709, g_loss: 7.838797092437744\n","Epoch 70/4000, Step 148, d_loss: 0.002769199665635824, g_loss: 7.265537261962891\n","Epoch 70/4000, Step 149, d_loss: 0.001216692617163062, g_loss: 7.892885208129883\n","Epoch 70/4000, Step 150, d_loss: 0.005571364890784025, g_loss: 7.246006965637207\n","Epoch 70/4000, Step 151, d_loss: 0.006144396960735321, g_loss: 7.046123027801514\n","Epoch 70/4000, Step 152, d_loss: 0.0011192664969712496, g_loss: 4.9516282081604\n","Epoch 70/4000, Step 153, d_loss: 0.0032870580907911062, g_loss: 7.024669647216797\n","Epoch 70/4000, Step 154, d_loss: 0.0014926956500858068, g_loss: 4.2531280517578125\n","Epoch 70/4000, Step 155, d_loss: 0.0025814261753112078, g_loss: 3.9564080238342285\n","Epoch 70/4000, Step 156, d_loss: 0.007013936992734671, g_loss: 4.948923110961914\n","Epoch 70/4000, Step 157, d_loss: 0.03290335461497307, g_loss: 4.743634223937988\n","Epoch 70/4000, Step 158, d_loss: 0.002472112886607647, g_loss: 4.982712745666504\n","Epoch 70/4000, Step 159, d_loss: 0.0024998339358717203, g_loss: 7.460816383361816\n","Epoch 70/4000, Step 160, d_loss: 0.0022259154357016087, g_loss: 6.907243728637695\n","Epoch 70/4000, Step 161, d_loss: 0.003981391433626413, g_loss: 5.562027931213379\n","Epoch 70/4000, Step 162, d_loss: 0.011611208319664001, g_loss: 9.479292869567871\n","Epoch 70/4000, Step 163, d_loss: 0.008297924883663654, g_loss: 7.182162761688232\n","Epoch 70/4000, Step 164, d_loss: 0.0017842462984845042, g_loss: 5.315373420715332\n","Epoch 70/4000, Step 165, d_loss: 0.0009696892229840159, g_loss: 4.96285343170166\n","Epoch 70/4000, Step 166, d_loss: 0.010422938503324986, g_loss: 6.600131988525391\n","Epoch 70/4000, Step 167, d_loss: 0.0026619613636285067, g_loss: 4.81650447845459\n","Epoch 70/4000, Step 168, d_loss: 0.004052521660923958, g_loss: 6.4438018798828125\n","Epoch 70/4000, Step 169, d_loss: 0.0008638966828584671, g_loss: 6.503736972808838\n","Epoch 70/4000, Step 170, d_loss: 0.024606062099337578, g_loss: 6.178677558898926\n","Epoch 70/4000, Step 171, d_loss: 0.0036050109192728996, g_loss: 5.7835845947265625\n","Epoch 70/4000, Step 172, d_loss: 0.016021018847823143, g_loss: 9.500021934509277\n","Epoch 70/4000, Step 173, d_loss: 0.005073812324553728, g_loss: 8.565475463867188\n","Epoch 70/4000, Step 174, d_loss: 0.00250587472692132, g_loss: 6.403914451599121\n","Epoch 70/4000, Step 175, d_loss: 0.001540719997137785, g_loss: 6.540614128112793\n","Epoch 70/4000, Step 176, d_loss: 0.009167935699224472, g_loss: 4.74468469619751\n","Epoch 70/4000, Step 177, d_loss: 0.0003066694480367005, g_loss: 7.118041515350342\n","Epoch 70/4000, Step 178, d_loss: 0.006340469233691692, g_loss: 5.679104328155518\n","Epoch 70/4000, Step 179, d_loss: 0.004265869501978159, g_loss: 8.938509941101074\n","Epoch 70/4000, Step 180, d_loss: 0.002648791763931513, g_loss: 7.60015869140625\n","Epoch 70/4000, Step 181, d_loss: 0.0071798525750637054, g_loss: 8.167752265930176\n","Epoch 70/4000, Step 182, d_loss: 0.01137277390807867, g_loss: 7.658562660217285\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 71/4000, Step 1, d_loss: 0.0002286301169078797, g_loss: 6.998023986816406\n","Epoch 71/4000, Step 2, d_loss: 0.0018394128419458866, g_loss: 8.485466003417969\n","Epoch 71/4000, Step 3, d_loss: 0.014951063320040703, g_loss: 7.771439075469971\n","Epoch 71/4000, Step 4, d_loss: 0.022811612114310265, g_loss: 6.509182929992676\n","Epoch 71/4000, Step 5, d_loss: 0.01098241563886404, g_loss: 7.623693943023682\n","Epoch 71/4000, Step 6, d_loss: 0.003030537161976099, g_loss: 6.181324481964111\n","Epoch 71/4000, Step 7, d_loss: 0.0033176098950207233, g_loss: 7.614010334014893\n","Epoch 71/4000, Step 8, d_loss: 0.0023069814778864384, g_loss: 6.6814284324646\n","Epoch 71/4000, Step 9, d_loss: 0.0032080416567623615, g_loss: 5.497700214385986\n","Epoch 71/4000, Step 10, d_loss: 0.0052988287061452866, g_loss: 5.575775623321533\n","Epoch 71/4000, Step 11, d_loss: 0.0034620838705450296, g_loss: 6.730961799621582\n","Epoch 71/4000, Step 12, d_loss: 0.045184023678302765, g_loss: 6.018978118896484\n","Epoch 71/4000, Step 13, d_loss: 0.005578651558607817, g_loss: 5.540134429931641\n","Epoch 71/4000, Step 14, d_loss: 0.01768641360104084, g_loss: 5.026434421539307\n","Epoch 71/4000, Step 15, d_loss: 0.01117904856801033, g_loss: 5.0646748542785645\n","Epoch 71/4000, Step 16, d_loss: 0.023193830624222755, g_loss: 3.7893848419189453\n","Epoch 71/4000, Step 17, d_loss: 0.10830915719270706, g_loss: 4.442025661468506\n","Epoch 71/4000, Step 18, d_loss: 0.004009437747299671, g_loss: 3.852358102798462\n","Epoch 71/4000, Step 19, d_loss: 0.029668940231204033, g_loss: 5.225616931915283\n","Epoch 71/4000, Step 20, d_loss: 0.0026777745224535465, g_loss: 1.9871147871017456\n","Epoch 71/4000, Step 21, d_loss: 0.005259461235255003, g_loss: 2.0839593410491943\n","Epoch 71/4000, Step 22, d_loss: 0.11609649658203125, g_loss: 3.4506638050079346\n","Epoch 71/4000, Step 23, d_loss: 0.035014133900403976, g_loss: 1.7271819114685059\n","Epoch 71/4000, Step 24, d_loss: 0.10280763357877731, g_loss: 3.1104156970977783\n","Epoch 71/4000, Step 25, d_loss: 0.019476335495710373, g_loss: 2.636965036392212\n","Epoch 71/4000, Step 26, d_loss: 0.031869661062955856, g_loss: 1.8781753778457642\n","Epoch 71/4000, Step 27, d_loss: 0.09071904420852661, g_loss: 5.248100280761719\n","Epoch 71/4000, Step 28, d_loss: 0.010881025344133377, g_loss: 4.174393177032471\n","Epoch 71/4000, Step 29, d_loss: 0.04234540835022926, g_loss: 5.6331562995910645\n","Epoch 71/4000, Step 30, d_loss: 0.003878810442984104, g_loss: 7.938395023345947\n","Epoch 71/4000, Step 31, d_loss: 0.6659722924232483, g_loss: 4.128110885620117\n","Epoch 71/4000, Step 32, d_loss: 0.012429703027009964, g_loss: 4.349940299987793\n","Epoch 71/4000, Step 33, d_loss: 0.006120373960584402, g_loss: 4.3537139892578125\n","Epoch 71/4000, Step 34, d_loss: 0.031275659799575806, g_loss: 2.9523873329162598\n","Epoch 71/4000, Step 35, d_loss: 0.08587757498025894, g_loss: 5.996333599090576\n","Epoch 71/4000, Step 36, d_loss: 0.11934429407119751, g_loss: 3.7061638832092285\n","Epoch 71/4000, Step 37, d_loss: 0.39804452657699585, g_loss: 6.119097709655762\n","Epoch 71/4000, Step 38, d_loss: 0.0996759906411171, g_loss: 2.712256908416748\n","Epoch 71/4000, Step 39, d_loss: 0.004813473671674728, g_loss: 4.05518913269043\n","Epoch 71/4000, Step 40, d_loss: 0.1496252417564392, g_loss: 5.037081241607666\n","Epoch 71/4000, Step 41, d_loss: 0.10180129110813141, g_loss: 5.770328521728516\n","Epoch 71/4000, Step 42, d_loss: 0.0034132914151996374, g_loss: 6.273024082183838\n","Epoch 71/4000, Step 43, d_loss: 0.005670025944709778, g_loss: 2.536189556121826\n","Epoch 71/4000, Step 44, d_loss: 0.6560158133506775, g_loss: 6.299050807952881\n","Epoch 71/4000, Step 45, d_loss: 0.08864200860261917, g_loss: 6.546602249145508\n","Epoch 71/4000, Step 46, d_loss: 0.011622624471783638, g_loss: 6.007618427276611\n","Epoch 71/4000, Step 47, d_loss: 0.047852400690317154, g_loss: 8.333958625793457\n","Epoch 71/4000, Step 48, d_loss: 0.013576599769294262, g_loss: 1.3870521783828735\n","Epoch 71/4000, Step 49, d_loss: 0.0028279859106987715, g_loss: 4.643965721130371\n","Epoch 71/4000, Step 50, d_loss: 0.08271796256303787, g_loss: 2.981818199157715\n","Epoch 71/4000, Step 51, d_loss: 0.2309422791004181, g_loss: 3.7281668186187744\n","Epoch 71/4000, Step 52, d_loss: 0.30221861600875854, g_loss: 3.3295016288757324\n","Epoch 71/4000, Step 53, d_loss: 0.19353759288787842, g_loss: 3.6769375801086426\n","Epoch 71/4000, Step 54, d_loss: 0.1505788117647171, g_loss: 7.370802879333496\n","Epoch 71/4000, Step 55, d_loss: 0.20078279078006744, g_loss: 5.689435958862305\n","Epoch 71/4000, Step 56, d_loss: 0.06858600676059723, g_loss: 7.910674095153809\n","Epoch 71/4000, Step 57, d_loss: 0.02214081957936287, g_loss: 3.661168336868286\n","Epoch 71/4000, Step 58, d_loss: 0.02073424682021141, g_loss: 5.002316474914551\n","Epoch 71/4000, Step 59, d_loss: 0.1635870486497879, g_loss: 5.512053489685059\n","Epoch 71/4000, Step 60, d_loss: 0.05202137306332588, g_loss: 4.853169918060303\n","Epoch 71/4000, Step 61, d_loss: 0.02980753220617771, g_loss: 2.1985650062561035\n","Epoch 71/4000, Step 62, d_loss: 0.014431614428758621, g_loss: 5.5777106285095215\n","Epoch 71/4000, Step 63, d_loss: 0.2656114101409912, g_loss: 7.078368186950684\n","Epoch 71/4000, Step 64, d_loss: 0.024860050529241562, g_loss: 5.603204250335693\n","Epoch 71/4000, Step 65, d_loss: 0.04711219295859337, g_loss: 5.604077339172363\n","Epoch 71/4000, Step 66, d_loss: 0.026269426569342613, g_loss: 4.168388843536377\n","Epoch 71/4000, Step 67, d_loss: 0.0057470235042274, g_loss: 5.865483283996582\n","Epoch 71/4000, Step 68, d_loss: 0.05190424621105194, g_loss: 9.397107124328613\n","Epoch 71/4000, Step 69, d_loss: 0.03898629546165466, g_loss: 4.963251113891602\n","Epoch 71/4000, Step 70, d_loss: 0.038429588079452515, g_loss: 8.394710540771484\n","Epoch 71/4000, Step 71, d_loss: 0.07978816330432892, g_loss: 6.282954692840576\n","Epoch 71/4000, Step 72, d_loss: 0.02473708614706993, g_loss: 6.129006385803223\n","Epoch 71/4000, Step 73, d_loss: 0.007666248828172684, g_loss: 5.741352558135986\n","Epoch 71/4000, Step 74, d_loss: 0.002137568546459079, g_loss: 8.646448135375977\n","Epoch 71/4000, Step 75, d_loss: 0.10229653120040894, g_loss: 5.8453216552734375\n","Epoch 71/4000, Step 76, d_loss: 0.043086059391498566, g_loss: 6.174129962921143\n","Epoch 71/4000, Step 77, d_loss: 0.02284470573067665, g_loss: 6.548346519470215\n","Epoch 71/4000, Step 78, d_loss: 0.014373235404491425, g_loss: 4.850508689880371\n","Epoch 71/4000, Step 79, d_loss: 0.004486779682338238, g_loss: 5.090693473815918\n","Epoch 71/4000, Step 80, d_loss: 0.041784077882766724, g_loss: 4.86419677734375\n","Epoch 71/4000, Step 81, d_loss: 0.010460603050887585, g_loss: 3.868865728378296\n","Epoch 71/4000, Step 82, d_loss: 0.0027564000338315964, g_loss: 5.353716850280762\n","Epoch 71/4000, Step 83, d_loss: 0.018154334276914597, g_loss: 4.05682897567749\n","Epoch 71/4000, Step 84, d_loss: 0.021304955706000328, g_loss: 2.6262283325195312\n","Epoch 71/4000, Step 85, d_loss: 0.02448202669620514, g_loss: 5.709841251373291\n","Epoch 71/4000, Step 86, d_loss: 0.01644480600953102, g_loss: 4.816656112670898\n","Epoch 71/4000, Step 87, d_loss: 0.05011200159788132, g_loss: 4.9708428382873535\n","Epoch 71/4000, Step 88, d_loss: 0.011396999470889568, g_loss: 5.431509017944336\n","Epoch 71/4000, Step 89, d_loss: 0.02878771349787712, g_loss: 3.103043556213379\n","Epoch 71/4000, Step 90, d_loss: 0.04818417876958847, g_loss: 4.870541095733643\n","Epoch 71/4000, Step 91, d_loss: 0.033682048320770264, g_loss: 5.20216703414917\n","Epoch 71/4000, Step 92, d_loss: 0.004247358068823814, g_loss: 7.643954753875732\n","Epoch 71/4000, Step 93, d_loss: 0.003267190884798765, g_loss: 5.7860822677612305\n","Epoch 71/4000, Step 94, d_loss: 0.007205362897366285, g_loss: 7.906967639923096\n","Epoch 71/4000, Step 95, d_loss: 0.013028828427195549, g_loss: 7.297952175140381\n","Epoch 71/4000, Step 96, d_loss: 0.034765519201755524, g_loss: 6.447415828704834\n","Epoch 71/4000, Step 97, d_loss: 0.006309053394943476, g_loss: 6.513560771942139\n","Epoch 71/4000, Step 98, d_loss: 0.0279619712382555, g_loss: 4.957864284515381\n","Epoch 71/4000, Step 99, d_loss: 0.009656502865254879, g_loss: 8.225025177001953\n","Epoch 71/4000, Step 100, d_loss: 0.05824274569749832, g_loss: 6.333184242248535\n","Epoch 71/4000, Step 101, d_loss: 0.021233635023236275, g_loss: 9.100577354431152\n","Epoch 71/4000, Step 102, d_loss: 0.020005859434604645, g_loss: 7.41242790222168\n","Epoch 71/4000, Step 103, d_loss: 0.04304809495806694, g_loss: 6.869943141937256\n","Epoch 71/4000, Step 104, d_loss: 0.036548055708408356, g_loss: 5.854386329650879\n","Epoch 71/4000, Step 105, d_loss: 0.0010114891920238733, g_loss: 4.607435703277588\n","Epoch 71/4000, Step 106, d_loss: 0.0020630538929253817, g_loss: 4.672736167907715\n","Epoch 71/4000, Step 107, d_loss: 0.001867562416009605, g_loss: 9.670767784118652\n","Epoch 71/4000, Step 108, d_loss: 0.0025719376280903816, g_loss: 5.223381996154785\n","Epoch 71/4000, Step 109, d_loss: 0.012357713654637337, g_loss: 5.641008377075195\n","Epoch 71/4000, Step 110, d_loss: 0.021453168243169785, g_loss: 5.407348155975342\n","Epoch 71/4000, Step 111, d_loss: 0.0024913328234106302, g_loss: 4.330353736877441\n","Epoch 71/4000, Step 112, d_loss: 0.027698563411831856, g_loss: 6.394404888153076\n","Epoch 71/4000, Step 113, d_loss: 0.010232880711555481, g_loss: 3.9705982208251953\n","Epoch 71/4000, Step 114, d_loss: 0.023156659677624702, g_loss: 3.5920441150665283\n","Epoch 71/4000, Step 115, d_loss: 0.0015592467971146107, g_loss: 6.320010662078857\n","Epoch 71/4000, Step 116, d_loss: 0.019328894093632698, g_loss: 5.358870029449463\n","Epoch 71/4000, Step 117, d_loss: 0.010934996418654919, g_loss: 6.129051685333252\n","Epoch 71/4000, Step 118, d_loss: 0.0011541550047695637, g_loss: 4.5806145668029785\n","Epoch 71/4000, Step 119, d_loss: 0.01521341037005186, g_loss: 4.9658427238464355\n","Epoch 71/4000, Step 120, d_loss: 0.0058148885145783424, g_loss: 8.407974243164062\n","Epoch 71/4000, Step 121, d_loss: 0.0026476343628019094, g_loss: 6.004040241241455\n","Epoch 71/4000, Step 122, d_loss: 0.011059800162911415, g_loss: 4.523031711578369\n","Epoch 71/4000, Step 123, d_loss: 0.00942757073789835, g_loss: 4.595822811126709\n","Epoch 71/4000, Step 124, d_loss: 0.006450400687754154, g_loss: 4.758188724517822\n","Epoch 71/4000, Step 125, d_loss: 0.017423996701836586, g_loss: 3.3315014839172363\n","Epoch 71/4000, Step 126, d_loss: 0.000470224826131016, g_loss: 4.196131706237793\n","Epoch 71/4000, Step 127, d_loss: 0.00537318317219615, g_loss: 3.498628616333008\n","Epoch 71/4000, Step 128, d_loss: 0.0020698511507362127, g_loss: 3.7329273223876953\n","Epoch 71/4000, Step 129, d_loss: 0.021447762846946716, g_loss: 5.719120502471924\n","Epoch 71/4000, Step 130, d_loss: 0.00756672490388155, g_loss: 6.1141510009765625\n","Epoch 71/4000, Step 131, d_loss: 0.009402010589838028, g_loss: 4.880002975463867\n","Epoch 71/4000, Step 132, d_loss: 0.008301857858896255, g_loss: 6.705806255340576\n","Epoch 71/4000, Step 133, d_loss: 0.006174813956022263, g_loss: 6.115833282470703\n","Epoch 71/4000, Step 134, d_loss: 0.02309577539563179, g_loss: 6.630268096923828\n","Epoch 71/4000, Step 135, d_loss: 0.009252199903130531, g_loss: 6.1159138679504395\n","Epoch 71/4000, Step 136, d_loss: 0.002044747583568096, g_loss: 5.164627552032471\n","Epoch 71/4000, Step 137, d_loss: 0.004066067282110453, g_loss: 6.353305339813232\n","Epoch 71/4000, Step 138, d_loss: 0.007155261468142271, g_loss: 5.95229434967041\n","Epoch 71/4000, Step 139, d_loss: 0.0023494684137403965, g_loss: 4.744126796722412\n","Epoch 71/4000, Step 140, d_loss: 0.003562154481187463, g_loss: 7.796258926391602\n","Epoch 71/4000, Step 141, d_loss: 0.005248852074146271, g_loss: 6.088686466217041\n","Epoch 71/4000, Step 142, d_loss: 0.006017191801220179, g_loss: 7.544615268707275\n","Epoch 71/4000, Step 143, d_loss: 0.0010019894689321518, g_loss: 4.985203742980957\n","Epoch 71/4000, Step 144, d_loss: 0.0014295016881078482, g_loss: 5.619393348693848\n","Epoch 71/4000, Step 145, d_loss: 0.003310849890112877, g_loss: 6.94844388961792\n","Epoch 71/4000, Step 146, d_loss: 0.00641830638051033, g_loss: 6.775545597076416\n","Epoch 71/4000, Step 147, d_loss: 0.06051697954535484, g_loss: 6.343847274780273\n","Epoch 71/4000, Step 148, d_loss: 0.003287286264821887, g_loss: 3.8304245471954346\n","Epoch 71/4000, Step 149, d_loss: 0.0009782284032553434, g_loss: 4.591987133026123\n","Epoch 71/4000, Step 150, d_loss: 0.0065825642086565495, g_loss: 7.244558811187744\n","Epoch 71/4000, Step 151, d_loss: 0.006906238384544849, g_loss: 6.509832859039307\n","Epoch 71/4000, Step 152, d_loss: 0.004195704124867916, g_loss: 5.668212890625\n","Epoch 71/4000, Step 153, d_loss: 0.008986164815723896, g_loss: 3.8494386672973633\n","Epoch 71/4000, Step 154, d_loss: 0.005342758260667324, g_loss: 5.482742786407471\n","Epoch 71/4000, Step 155, d_loss: 0.05975247174501419, g_loss: 4.495136737823486\n","Epoch 71/4000, Step 156, d_loss: 0.014315556734800339, g_loss: 6.194786071777344\n","Epoch 71/4000, Step 157, d_loss: 0.0445614829659462, g_loss: 5.369587421417236\n","Epoch 71/4000, Step 158, d_loss: 0.008704684674739838, g_loss: 7.984142303466797\n","Epoch 71/4000, Step 159, d_loss: 0.02261580154299736, g_loss: 5.64410924911499\n","Epoch 71/4000, Step 160, d_loss: 0.002705642255023122, g_loss: 6.025509357452393\n","Epoch 71/4000, Step 161, d_loss: 0.0006054678815416992, g_loss: 8.354909896850586\n","Epoch 71/4000, Step 162, d_loss: 0.008258594200015068, g_loss: 7.80433464050293\n","Epoch 71/4000, Step 163, d_loss: 0.010103457607328892, g_loss: 6.014367580413818\n","Epoch 71/4000, Step 164, d_loss: 0.001695212908089161, g_loss: 9.486856460571289\n","Epoch 71/4000, Step 165, d_loss: 0.022718682885169983, g_loss: 6.538358688354492\n","Epoch 71/4000, Step 166, d_loss: 0.006484139710664749, g_loss: 3.934253454208374\n","Epoch 71/4000, Step 167, d_loss: 0.00620836578309536, g_loss: 4.999716758728027\n","Epoch 71/4000, Step 168, d_loss: 0.009035439230501652, g_loss: 7.201325416564941\n","Epoch 71/4000, Step 169, d_loss: 0.03374315798282623, g_loss: 5.464191913604736\n","Epoch 71/4000, Step 170, d_loss: 0.011225868947803974, g_loss: 5.378755569458008\n","Epoch 71/4000, Step 171, d_loss: 0.01902734860777855, g_loss: 5.870243072509766\n","Epoch 71/4000, Step 172, d_loss: 0.010627620853483677, g_loss: 5.593862533569336\n","Epoch 71/4000, Step 173, d_loss: 0.008205411955714226, g_loss: 4.333780765533447\n","Epoch 71/4000, Step 174, d_loss: 0.051371634006500244, g_loss: 7.097765922546387\n","Epoch 71/4000, Step 175, d_loss: 0.012392428703606129, g_loss: 5.686988353729248\n","Epoch 71/4000, Step 176, d_loss: 0.002732477616518736, g_loss: 8.359582901000977\n","Epoch 71/4000, Step 177, d_loss: 0.00919708888977766, g_loss: 5.644627571105957\n","Epoch 71/4000, Step 178, d_loss: 0.0013825104106217623, g_loss: 5.659226894378662\n","Epoch 71/4000, Step 179, d_loss: 0.0023125815205276012, g_loss: 5.369987487792969\n","Epoch 71/4000, Step 180, d_loss: 0.0071155051700770855, g_loss: 6.516709804534912\n","Epoch 71/4000, Step 181, d_loss: 0.004767998121678829, g_loss: 5.3037543296813965\n","Epoch 71/4000, Step 182, d_loss: 0.0305984728038311, g_loss: 6.155278205871582\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 72/4000, Step 1, d_loss: 0.005182360298931599, g_loss: 6.604012966156006\n","Epoch 72/4000, Step 2, d_loss: 0.0028808768838644028, g_loss: 6.140853404998779\n","Epoch 72/4000, Step 3, d_loss: 0.00478702038526535, g_loss: 8.333685874938965\n","Epoch 72/4000, Step 4, d_loss: 0.02326209284365177, g_loss: 8.33354377746582\n","Epoch 72/4000, Step 5, d_loss: 0.0060010808520019054, g_loss: 7.738065719604492\n","Epoch 72/4000, Step 6, d_loss: 0.023459043353796005, g_loss: 4.823368072509766\n","Epoch 72/4000, Step 7, d_loss: 0.015711819753050804, g_loss: 7.940674781799316\n","Epoch 72/4000, Step 8, d_loss: 0.000533991027623415, g_loss: 7.1243205070495605\n","Epoch 72/4000, Step 9, d_loss: 0.0018652162980288267, g_loss: 6.098204135894775\n","Epoch 72/4000, Step 10, d_loss: 0.004518395289778709, g_loss: 4.6920576095581055\n","Epoch 72/4000, Step 11, d_loss: 0.0011651992099359632, g_loss: 4.21506404876709\n","Epoch 72/4000, Step 12, d_loss: 0.02944543957710266, g_loss: 7.101064205169678\n","Epoch 72/4000, Step 13, d_loss: 0.007846388965845108, g_loss: 5.641388416290283\n","Epoch 72/4000, Step 14, d_loss: 0.003567771753296256, g_loss: 5.118838787078857\n","Epoch 72/4000, Step 15, d_loss: 0.001651445752941072, g_loss: 6.399515151977539\n","Epoch 72/4000, Step 16, d_loss: 0.013139970600605011, g_loss: 5.444253444671631\n","Epoch 72/4000, Step 17, d_loss: 0.00215722294524312, g_loss: 4.8008599281311035\n","Epoch 72/4000, Step 18, d_loss: 0.007057090289890766, g_loss: 5.157560348510742\n","Epoch 72/4000, Step 19, d_loss: 0.00357117410749197, g_loss: 6.979572296142578\n","Epoch 72/4000, Step 20, d_loss: 0.006745629478245974, g_loss: 5.842302322387695\n","Epoch 72/4000, Step 21, d_loss: 0.0029287186916917562, g_loss: 5.8942108154296875\n","Epoch 72/4000, Step 22, d_loss: 0.011221502907574177, g_loss: 5.620538711547852\n","Epoch 72/4000, Step 23, d_loss: 0.010674811899662018, g_loss: 6.7495012283325195\n","Epoch 72/4000, Step 24, d_loss: 0.019758930429816246, g_loss: 5.525354385375977\n","Epoch 72/4000, Step 25, d_loss: 0.03121125139296055, g_loss: 5.950418472290039\n","Epoch 72/4000, Step 26, d_loss: 0.0056580365635454655, g_loss: 6.28378963470459\n","Epoch 72/4000, Step 27, d_loss: 0.0064916787669062614, g_loss: 7.191710472106934\n","Epoch 72/4000, Step 28, d_loss: 0.019235260784626007, g_loss: 5.605954647064209\n","Epoch 72/4000, Step 29, d_loss: 0.005033474415540695, g_loss: 6.131512641906738\n","Epoch 72/4000, Step 30, d_loss: 0.007419831585139036, g_loss: 6.619937419891357\n","Epoch 72/4000, Step 31, d_loss: 0.010612711310386658, g_loss: 6.8054914474487305\n","Epoch 72/4000, Step 32, d_loss: 0.0049715712666511536, g_loss: 3.9262380599975586\n","Epoch 72/4000, Step 33, d_loss: 0.0017944702412933111, g_loss: 4.609462261199951\n","Epoch 72/4000, Step 34, d_loss: 0.0023514404892921448, g_loss: 5.972983360290527\n","Epoch 72/4000, Step 35, d_loss: 0.005981282331049442, g_loss: 7.267871856689453\n","Epoch 72/4000, Step 36, d_loss: 0.006408748682588339, g_loss: 5.40704345703125\n","Epoch 72/4000, Step 37, d_loss: 0.02067399024963379, g_loss: 6.207544326782227\n","Epoch 72/4000, Step 38, d_loss: 0.00506756454706192, g_loss: 5.576677322387695\n","Epoch 72/4000, Step 39, d_loss: 0.0014436874771490693, g_loss: 3.7774901390075684\n","Epoch 72/4000, Step 40, d_loss: 0.00899585708975792, g_loss: 7.914568901062012\n","Epoch 72/4000, Step 41, d_loss: 0.005548390559852123, g_loss: 6.008546829223633\n","Epoch 72/4000, Step 42, d_loss: 0.00409983703866601, g_loss: 4.944291114807129\n","Epoch 72/4000, Step 43, d_loss: 0.005121446214616299, g_loss: 7.766204357147217\n","Epoch 72/4000, Step 44, d_loss: 0.00543108070269227, g_loss: 6.437532424926758\n","Epoch 72/4000, Step 45, d_loss: 0.0031355314422398806, g_loss: 6.598917007446289\n","Epoch 72/4000, Step 46, d_loss: 0.010061316192150116, g_loss: 4.923553466796875\n","Epoch 72/4000, Step 47, d_loss: 0.008238911628723145, g_loss: 9.330484390258789\n","Epoch 72/4000, Step 48, d_loss: 0.009518515318632126, g_loss: 6.046387672424316\n","Epoch 72/4000, Step 49, d_loss: 0.009409396909177303, g_loss: 7.062985420227051\n","Epoch 72/4000, Step 50, d_loss: 0.0034988028928637505, g_loss: 7.065442085266113\n","Epoch 72/4000, Step 51, d_loss: 0.002161063253879547, g_loss: 6.222587585449219\n","Epoch 72/4000, Step 52, d_loss: 0.002673414535820484, g_loss: 5.746068000793457\n","Epoch 72/4000, Step 53, d_loss: 0.0036359094083309174, g_loss: 5.005527496337891\n","Epoch 72/4000, Step 54, d_loss: 0.001960469875484705, g_loss: 5.6353631019592285\n","Epoch 72/4000, Step 55, d_loss: 0.01675325445830822, g_loss: 7.892901420593262\n","Epoch 72/4000, Step 56, d_loss: 0.004007562529295683, g_loss: 6.795302867889404\n","Epoch 72/4000, Step 57, d_loss: 0.0025417571887373924, g_loss: 5.95949125289917\n","Epoch 72/4000, Step 58, d_loss: 0.001931257313117385, g_loss: 7.194043159484863\n","Epoch 72/4000, Step 59, d_loss: 0.0004017821920569986, g_loss: 7.391130447387695\n","Epoch 72/4000, Step 60, d_loss: 0.0007283000741153955, g_loss: 5.4746928215026855\n","Epoch 72/4000, Step 61, d_loss: 0.007198923267424107, g_loss: 8.939835548400879\n","Epoch 72/4000, Step 62, d_loss: 0.00652651721611619, g_loss: 8.532150268554688\n","Epoch 72/4000, Step 63, d_loss: 0.004738468676805496, g_loss: 7.092048168182373\n","Epoch 72/4000, Step 64, d_loss: 0.003951624501496553, g_loss: 7.641445636749268\n","Epoch 72/4000, Step 65, d_loss: 0.004661059472709894, g_loss: 7.10776948928833\n","Epoch 72/4000, Step 66, d_loss: 0.007046398241072893, g_loss: 8.45962905883789\n","Epoch 72/4000, Step 67, d_loss: 0.003404112532734871, g_loss: 7.52249813079834\n","Epoch 72/4000, Step 68, d_loss: 0.19302056729793549, g_loss: 8.029382705688477\n","Epoch 72/4000, Step 69, d_loss: 0.019348369911313057, g_loss: 3.3850455284118652\n","Epoch 72/4000, Step 70, d_loss: 0.05903657153248787, g_loss: 3.90722918510437\n","Epoch 72/4000, Step 71, d_loss: 0.006244569085538387, g_loss: 3.1174302101135254\n","Epoch 72/4000, Step 72, d_loss: 0.01708172634243965, g_loss: 2.8932831287384033\n","Epoch 72/4000, Step 73, d_loss: 0.08331149816513062, g_loss: 3.7556850910186768\n","Epoch 72/4000, Step 74, d_loss: 0.008396252058446407, g_loss: 3.8239505290985107\n","Epoch 72/4000, Step 75, d_loss: 0.04120428487658501, g_loss: 3.810746431350708\n","Epoch 72/4000, Step 76, d_loss: 0.21994724869728088, g_loss: 5.234941005706787\n","Epoch 72/4000, Step 77, d_loss: 0.030415672808885574, g_loss: 5.490623950958252\n","Epoch 72/4000, Step 78, d_loss: 0.07944653928279877, g_loss: 5.811221122741699\n","Epoch 72/4000, Step 79, d_loss: 0.04205511510372162, g_loss: 4.971381664276123\n","Epoch 72/4000, Step 80, d_loss: 0.0052137235179543495, g_loss: 5.803793907165527\n","Epoch 72/4000, Step 81, d_loss: 0.020334869623184204, g_loss: 8.306120872497559\n","Epoch 72/4000, Step 82, d_loss: 0.014259472489356995, g_loss: 7.118237495422363\n","Epoch 72/4000, Step 83, d_loss: 0.08041918277740479, g_loss: 3.9020252227783203\n","Epoch 72/4000, Step 84, d_loss: 0.01919453963637352, g_loss: 9.090855598449707\n","Epoch 72/4000, Step 85, d_loss: 0.006648064590990543, g_loss: 4.771662712097168\n","Epoch 72/4000, Step 86, d_loss: 0.002839801600202918, g_loss: 7.316188812255859\n","Epoch 72/4000, Step 87, d_loss: 0.002804724732413888, g_loss: 6.274179458618164\n","Epoch 72/4000, Step 88, d_loss: 0.005098842084407806, g_loss: 4.922029495239258\n","Epoch 72/4000, Step 89, d_loss: 0.013525630347430706, g_loss: 9.264365196228027\n","Epoch 72/4000, Step 90, d_loss: 0.08197644352912903, g_loss: 8.92329216003418\n","Epoch 72/4000, Step 91, d_loss: 0.006011003628373146, g_loss: 4.371315002441406\n","Epoch 72/4000, Step 92, d_loss: 0.017294131219387054, g_loss: 3.8155710697174072\n","Epoch 72/4000, Step 93, d_loss: 0.4257151782512665, g_loss: 4.096194744110107\n","Epoch 72/4000, Step 94, d_loss: 0.15769709646701813, g_loss: 6.121714115142822\n","Epoch 72/4000, Step 95, d_loss: 0.0040932996198534966, g_loss: 2.5630064010620117\n","Epoch 72/4000, Step 96, d_loss: 0.18341238796710968, g_loss: 4.415790557861328\n","Epoch 72/4000, Step 97, d_loss: 0.011350736953318119, g_loss: 6.1298065185546875\n","Epoch 72/4000, Step 98, d_loss: 0.01326540857553482, g_loss: 4.987847805023193\n","Epoch 72/4000, Step 99, d_loss: 0.08862511068582535, g_loss: 5.6181182861328125\n","Epoch 72/4000, Step 100, d_loss: 0.06633738428354263, g_loss: 6.429707050323486\n","Epoch 72/4000, Step 101, d_loss: 0.009403997100889683, g_loss: 5.087972164154053\n","Epoch 72/4000, Step 102, d_loss: 0.04513564705848694, g_loss: 5.971747875213623\n","Epoch 72/4000, Step 103, d_loss: 0.010978508740663528, g_loss: 5.1459879875183105\n","Epoch 72/4000, Step 104, d_loss: 0.017555449157953262, g_loss: 3.2705554962158203\n","Epoch 72/4000, Step 105, d_loss: 0.04528215527534485, g_loss: 6.565891265869141\n","Epoch 72/4000, Step 106, d_loss: 0.03062259964644909, g_loss: 3.3917102813720703\n","Epoch 72/4000, Step 107, d_loss: 0.03519336134195328, g_loss: 4.614175319671631\n","Epoch 72/4000, Step 108, d_loss: 0.004169608000665903, g_loss: 6.40685510635376\n","Epoch 72/4000, Step 109, d_loss: 0.026251966133713722, g_loss: 3.996957540512085\n","Epoch 72/4000, Step 110, d_loss: 0.02146606147289276, g_loss: 5.559617519378662\n","Epoch 72/4000, Step 111, d_loss: 0.014274980872869492, g_loss: 5.145958423614502\n","Epoch 72/4000, Step 112, d_loss: 0.00874713622033596, g_loss: 3.1291892528533936\n","Epoch 72/4000, Step 113, d_loss: 0.04739627242088318, g_loss: 6.382319450378418\n","Epoch 72/4000, Step 114, d_loss: 0.017435666173696518, g_loss: 5.0973029136657715\n","Epoch 72/4000, Step 115, d_loss: 0.041421569883823395, g_loss: 3.697568416595459\n","Epoch 72/4000, Step 116, d_loss: 0.011102108284831047, g_loss: 5.158113956451416\n","Epoch 72/4000, Step 117, d_loss: 0.015365353785455227, g_loss: 4.061242580413818\n","Epoch 72/4000, Step 118, d_loss: 0.024634959176182747, g_loss: 7.5265350341796875\n","Epoch 72/4000, Step 119, d_loss: 0.02133864350616932, g_loss: 8.136971473693848\n","Epoch 72/4000, Step 120, d_loss: 0.016351331025362015, g_loss: 6.58889627456665\n","Epoch 72/4000, Step 121, d_loss: 0.025126051157712936, g_loss: 4.622239589691162\n","Epoch 72/4000, Step 122, d_loss: 0.006553271319717169, g_loss: 4.56106424331665\n","Epoch 72/4000, Step 123, d_loss: 0.009925562888383865, g_loss: 5.3451032638549805\n","Epoch 72/4000, Step 124, d_loss: 0.005300407763570547, g_loss: 8.238492012023926\n","Epoch 72/4000, Step 125, d_loss: 0.002789775375276804, g_loss: 7.834110736846924\n","Epoch 72/4000, Step 126, d_loss: 0.009348036721348763, g_loss: 6.907008647918701\n","Epoch 72/4000, Step 127, d_loss: 0.0041464813984930515, g_loss: 7.2601799964904785\n","Epoch 72/4000, Step 128, d_loss: 0.05072077363729477, g_loss: 4.775115489959717\n","Epoch 72/4000, Step 129, d_loss: 0.003364311531186104, g_loss: 7.134871959686279\n","Epoch 72/4000, Step 130, d_loss: 0.019094545394182205, g_loss: 6.005366802215576\n","Epoch 72/4000, Step 131, d_loss: 0.009605873376131058, g_loss: 5.803374290466309\n","Epoch 72/4000, Step 132, d_loss: 0.004321937449276447, g_loss: 6.884694576263428\n","Epoch 72/4000, Step 133, d_loss: 0.021083315834403038, g_loss: 4.65021276473999\n","Epoch 72/4000, Step 134, d_loss: 0.012341228313744068, g_loss: 7.195313930511475\n","Epoch 72/4000, Step 135, d_loss: 0.008554277941584587, g_loss: 6.61627721786499\n","Epoch 72/4000, Step 136, d_loss: 0.009593149647116661, g_loss: 6.331275939941406\n","Epoch 72/4000, Step 137, d_loss: 0.017721906304359436, g_loss: 7.696837425231934\n","Epoch 72/4000, Step 138, d_loss: 0.0055415271781384945, g_loss: 4.75236701965332\n","Epoch 72/4000, Step 139, d_loss: 0.001366425072774291, g_loss: 6.381621837615967\n","Epoch 72/4000, Step 140, d_loss: 0.0004246695025358349, g_loss: 7.148073673248291\n","Epoch 72/4000, Step 141, d_loss: 0.004597461316734552, g_loss: 6.113286018371582\n","Epoch 72/4000, Step 142, d_loss: 0.0036414756905287504, g_loss: 7.702914714813232\n","Epoch 72/4000, Step 143, d_loss: 0.0050222910940647125, g_loss: 6.188869476318359\n","Epoch 72/4000, Step 144, d_loss: 0.00283194612711668, g_loss: 6.1962890625\n","Epoch 72/4000, Step 145, d_loss: 0.013815243728458881, g_loss: 5.548914432525635\n","Epoch 72/4000, Step 146, d_loss: 0.026353906840085983, g_loss: 5.235317230224609\n","Epoch 72/4000, Step 147, d_loss: 0.016724301502108574, g_loss: 3.8717164993286133\n","Epoch 72/4000, Step 148, d_loss: 0.005776485428214073, g_loss: 6.751490592956543\n","Epoch 72/4000, Step 149, d_loss: 0.004125747364014387, g_loss: 5.204413890838623\n","Epoch 72/4000, Step 150, d_loss: 0.18909680843353271, g_loss: 8.381377220153809\n","Epoch 72/4000, Step 151, d_loss: 0.0025360477156937122, g_loss: 5.9876227378845215\n","Epoch 72/4000, Step 152, d_loss: 0.0115595031529665, g_loss: 4.463630676269531\n","Epoch 72/4000, Step 153, d_loss: 0.01565769873559475, g_loss: 5.688904285430908\n","Epoch 72/4000, Step 154, d_loss: 0.009452056139707565, g_loss: 6.8202223777771\n","Epoch 72/4000, Step 155, d_loss: 0.01842341013252735, g_loss: 5.130897521972656\n","Epoch 72/4000, Step 156, d_loss: 0.035443589091300964, g_loss: 3.482736587524414\n","Epoch 72/4000, Step 157, d_loss: 0.03106888011097908, g_loss: 1.8017895221710205\n","Epoch 72/4000, Step 158, d_loss: 0.22090044617652893, g_loss: 4.680179119110107\n","Epoch 72/4000, Step 159, d_loss: 0.05812118947505951, g_loss: 6.1884236335754395\n","Epoch 72/4000, Step 160, d_loss: 0.021084008738398552, g_loss: 4.2986159324646\n","Epoch 72/4000, Step 161, d_loss: 0.017676781862974167, g_loss: 6.174724578857422\n","Epoch 72/4000, Step 162, d_loss: 0.003051187377423048, g_loss: 7.6664886474609375\n","Epoch 72/4000, Step 163, d_loss: 0.0044075557962059975, g_loss: 8.028610229492188\n","Epoch 72/4000, Step 164, d_loss: 0.055597733706235886, g_loss: 7.102046012878418\n","Epoch 72/4000, Step 165, d_loss: 0.00971429143100977, g_loss: 8.821560859680176\n","Epoch 72/4000, Step 166, d_loss: 0.009685717523097992, g_loss: 9.461577415466309\n","Epoch 72/4000, Step 167, d_loss: 0.3067815601825714, g_loss: 9.764817237854004\n","Epoch 72/4000, Step 168, d_loss: 0.1270890235900879, g_loss: 4.913593292236328\n","Epoch 72/4000, Step 169, d_loss: 0.013548099435865879, g_loss: 6.69343900680542\n","Epoch 72/4000, Step 170, d_loss: 0.0594092458486557, g_loss: 3.392519474029541\n","Epoch 72/4000, Step 171, d_loss: 0.036785297095775604, g_loss: 4.945345878601074\n","Epoch 72/4000, Step 172, d_loss: 0.1951373815536499, g_loss: 5.215912342071533\n","Epoch 72/4000, Step 173, d_loss: 0.038309916853904724, g_loss: 6.849379062652588\n","Epoch 72/4000, Step 174, d_loss: 0.1078283041715622, g_loss: 4.519657135009766\n","Epoch 72/4000, Step 175, d_loss: 0.018975917249917984, g_loss: 1.7688584327697754\n","Epoch 72/4000, Step 176, d_loss: 0.05190146341919899, g_loss: 3.373560667037964\n","Epoch 72/4000, Step 177, d_loss: 0.009631507098674774, g_loss: 4.593344211578369\n","Epoch 72/4000, Step 178, d_loss: 0.08550912141799927, g_loss: 3.963951587677002\n","Epoch 72/4000, Step 179, d_loss: 0.02501078136265278, g_loss: 4.485362529754639\n","Epoch 72/4000, Step 180, d_loss: 0.04435674846172333, g_loss: 5.565180778503418\n","Epoch 72/4000, Step 181, d_loss: 0.1517743617296219, g_loss: 8.75057601928711\n","Epoch 72/4000, Step 182, d_loss: 0.3376464247703552, g_loss: 3.6777663230895996\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 73/4000, Step 1, d_loss: 0.0024821998085826635, g_loss: 4.133631706237793\n","Epoch 73/4000, Step 2, d_loss: 0.04205012321472168, g_loss: 3.4899332523345947\n","Epoch 73/4000, Step 3, d_loss: 0.2164413183927536, g_loss: 3.440565824508667\n","Epoch 73/4000, Step 4, d_loss: 0.04452751576900482, g_loss: 4.546933650970459\n","Epoch 73/4000, Step 5, d_loss: 0.09382612258195877, g_loss: 3.6305058002471924\n","Epoch 73/4000, Step 6, d_loss: 0.09967580437660217, g_loss: 6.042965412139893\n","Epoch 73/4000, Step 7, d_loss: 0.031199857592582703, g_loss: 3.145037889480591\n","Epoch 73/4000, Step 8, d_loss: 0.040806472301483154, g_loss: 5.49315071105957\n","Epoch 73/4000, Step 9, d_loss: 0.15828128159046173, g_loss: 3.5474164485931396\n","Epoch 73/4000, Step 10, d_loss: 0.06316913664340973, g_loss: 4.757254600524902\n","Epoch 73/4000, Step 11, d_loss: 0.01645747199654579, g_loss: 3.246485948562622\n","Epoch 73/4000, Step 12, d_loss: 0.015629993751645088, g_loss: 4.068118095397949\n","Epoch 73/4000, Step 13, d_loss: 0.09199458360671997, g_loss: 3.537914991378784\n","Epoch 73/4000, Step 14, d_loss: 0.09043921530246735, g_loss: 3.1953530311584473\n","Epoch 73/4000, Step 15, d_loss: 0.010873145423829556, g_loss: 5.481152534484863\n","Epoch 73/4000, Step 16, d_loss: 0.1507788598537445, g_loss: 3.372509002685547\n","Epoch 73/4000, Step 17, d_loss: 0.014646564610302448, g_loss: 5.002862930297852\n","Epoch 73/4000, Step 18, d_loss: 0.008138350211083889, g_loss: 6.8569512367248535\n","Epoch 73/4000, Step 19, d_loss: 0.022331275045871735, g_loss: 4.935949325561523\n","Epoch 73/4000, Step 20, d_loss: 0.005465823691338301, g_loss: 3.458902597427368\n","Epoch 73/4000, Step 21, d_loss: 0.029368218034505844, g_loss: 6.049188613891602\n","Epoch 73/4000, Step 22, d_loss: 0.02753259986639023, g_loss: 3.8705947399139404\n","Epoch 73/4000, Step 23, d_loss: 0.005113649182021618, g_loss: 5.452205181121826\n","Epoch 73/4000, Step 24, d_loss: 0.012864829041063786, g_loss: 7.404747486114502\n","Epoch 73/4000, Step 25, d_loss: 0.010418027639389038, g_loss: 6.028677463531494\n","Epoch 73/4000, Step 26, d_loss: 0.00534861208871007, g_loss: 6.658632278442383\n","Epoch 73/4000, Step 27, d_loss: 0.019091123715043068, g_loss: 6.3044233322143555\n","Epoch 73/4000, Step 28, d_loss: 0.00892447680234909, g_loss: 6.337264537811279\n","Epoch 73/4000, Step 29, d_loss: 0.017285533249378204, g_loss: 7.574003219604492\n","Epoch 73/4000, Step 30, d_loss: 0.02445036545395851, g_loss: 2.6422736644744873\n","Epoch 73/4000, Step 31, d_loss: 0.024099737405776978, g_loss: 6.516294956207275\n","Epoch 73/4000, Step 32, d_loss: 0.011987419798970222, g_loss: 6.491670608520508\n","Epoch 73/4000, Step 33, d_loss: 0.02996184304356575, g_loss: 4.8667144775390625\n","Epoch 73/4000, Step 34, d_loss: 0.01453376468271017, g_loss: 5.132664203643799\n","Epoch 73/4000, Step 35, d_loss: 0.024118894711136818, g_loss: 9.149084091186523\n","Epoch 73/4000, Step 36, d_loss: 0.0026607648469507694, g_loss: 5.114119052886963\n","Epoch 73/4000, Step 37, d_loss: 0.03513588383793831, g_loss: 6.299613952636719\n","Epoch 73/4000, Step 38, d_loss: 0.010417965240776539, g_loss: 5.358541011810303\n","Epoch 73/4000, Step 39, d_loss: 0.005889387335628271, g_loss: 5.216928482055664\n","Epoch 73/4000, Step 40, d_loss: 0.008169280365109444, g_loss: 7.010551929473877\n","Epoch 73/4000, Step 41, d_loss: 0.04943447560071945, g_loss: 5.872725963592529\n","Epoch 73/4000, Step 42, d_loss: 0.005341116804629564, g_loss: 4.331143856048584\n","Epoch 73/4000, Step 43, d_loss: 0.005184619687497616, g_loss: 5.199473857879639\n","Epoch 73/4000, Step 44, d_loss: 0.004532730206847191, g_loss: 4.69686222076416\n","Epoch 73/4000, Step 45, d_loss: 0.009599806740880013, g_loss: 4.481290817260742\n","Epoch 73/4000, Step 46, d_loss: 0.053152766078710556, g_loss: 6.042547702789307\n","Epoch 73/4000, Step 47, d_loss: 0.014355503022670746, g_loss: 4.000460147857666\n","Epoch 73/4000, Step 48, d_loss: 0.01873733475804329, g_loss: 8.919820785522461\n","Epoch 73/4000, Step 49, d_loss: 0.013174587860703468, g_loss: 4.650186061859131\n","Epoch 73/4000, Step 50, d_loss: 0.014174089767038822, g_loss: 3.580665111541748\n","Epoch 73/4000, Step 51, d_loss: 0.04634730517864227, g_loss: 6.28203010559082\n","Epoch 73/4000, Step 52, d_loss: 0.016199585050344467, g_loss: 2.6031570434570312\n","Epoch 73/4000, Step 53, d_loss: 0.025096246972680092, g_loss: 5.8516645431518555\n","Epoch 73/4000, Step 54, d_loss: 0.031107008457183838, g_loss: 4.697293281555176\n","Epoch 73/4000, Step 55, d_loss: 0.005988471210002899, g_loss: 4.056632041931152\n","Epoch 73/4000, Step 56, d_loss: 0.004904685541987419, g_loss: 8.569568634033203\n","Epoch 73/4000, Step 57, d_loss: 0.018180765211582184, g_loss: 3.8600053787231445\n","Epoch 73/4000, Step 58, d_loss: 0.0024676520843058825, g_loss: 3.2222704887390137\n","Epoch 73/4000, Step 59, d_loss: 0.004071416333317757, g_loss: 5.184962749481201\n","Epoch 73/4000, Step 60, d_loss: 0.027321068570017815, g_loss: 4.595165252685547\n","Epoch 73/4000, Step 61, d_loss: 0.006331504322588444, g_loss: 5.073215961456299\n","Epoch 73/4000, Step 62, d_loss: 0.024619996547698975, g_loss: 7.7900800704956055\n","Epoch 73/4000, Step 63, d_loss: 0.003915175795555115, g_loss: 8.984626770019531\n","Epoch 73/4000, Step 64, d_loss: 0.02009834535419941, g_loss: 7.3212809562683105\n","Epoch 73/4000, Step 65, d_loss: 0.03719077631831169, g_loss: 4.553638935089111\n","Epoch 73/4000, Step 66, d_loss: 0.0059922304935753345, g_loss: 6.37069034576416\n","Epoch 73/4000, Step 67, d_loss: 0.012446407228708267, g_loss: 5.476970195770264\n","Epoch 73/4000, Step 68, d_loss: 0.0063231103122234344, g_loss: 6.335028648376465\n","Epoch 73/4000, Step 69, d_loss: 0.04877074435353279, g_loss: 6.245977401733398\n","Epoch 73/4000, Step 70, d_loss: 0.002950985450297594, g_loss: 4.361667156219482\n","Epoch 73/4000, Step 71, d_loss: 0.002645663684234023, g_loss: 4.759767055511475\n","Epoch 73/4000, Step 72, d_loss: 0.03823232650756836, g_loss: 6.117922306060791\n","Epoch 73/4000, Step 73, d_loss: 0.009407922625541687, g_loss: 5.110779285430908\n","Epoch 73/4000, Step 74, d_loss: 0.0038087572902441025, g_loss: 7.651811122894287\n","Epoch 73/4000, Step 75, d_loss: 0.004500969313085079, g_loss: 5.8537278175354\n","Epoch 73/4000, Step 76, d_loss: 0.012069551274180412, g_loss: 10.62071704864502\n","Epoch 73/4000, Step 77, d_loss: 0.0014059814857318997, g_loss: 8.418462753295898\n","Epoch 73/4000, Step 78, d_loss: 0.003148552030324936, g_loss: 5.5802836418151855\n","Epoch 73/4000, Step 79, d_loss: 0.010572774335741997, g_loss: 4.695497035980225\n","Epoch 73/4000, Step 80, d_loss: 0.0015078326687216759, g_loss: 6.878489971160889\n","Epoch 73/4000, Step 81, d_loss: 0.003309184219688177, g_loss: 6.063641548156738\n","Epoch 73/4000, Step 82, d_loss: 0.004298917483538389, g_loss: 6.4528303146362305\n","Epoch 73/4000, Step 83, d_loss: 0.003929583355784416, g_loss: 7.902810573577881\n","Epoch 73/4000, Step 84, d_loss: 0.004128636792302132, g_loss: 7.334932804107666\n","Epoch 73/4000, Step 85, d_loss: 0.0050422013737261295, g_loss: 6.6013407707214355\n","Epoch 73/4000, Step 86, d_loss: 0.0035936757922172546, g_loss: 6.313641548156738\n","Epoch 73/4000, Step 87, d_loss: 0.0034111831337213516, g_loss: 6.8407368659973145\n","Epoch 73/4000, Step 88, d_loss: 0.0011962421704083681, g_loss: 5.080944061279297\n","Epoch 73/4000, Step 89, d_loss: 0.004078352823853493, g_loss: 4.255978107452393\n","Epoch 73/4000, Step 90, d_loss: 0.0035862054210156202, g_loss: 5.144547462463379\n","Epoch 73/4000, Step 91, d_loss: 0.004066525027155876, g_loss: 6.243966579437256\n","Epoch 73/4000, Step 92, d_loss: 0.004521535709500313, g_loss: 5.815303802490234\n","Epoch 73/4000, Step 93, d_loss: 0.003141791792586446, g_loss: 6.8526811599731445\n","Epoch 73/4000, Step 94, d_loss: 0.0035614394582808018, g_loss: 6.868833065032959\n","Epoch 73/4000, Step 95, d_loss: 0.03128683567047119, g_loss: 5.903050899505615\n","Epoch 73/4000, Step 96, d_loss: 0.008161583915352821, g_loss: 4.692541122436523\n","Epoch 73/4000, Step 97, d_loss: 0.0035499511286616325, g_loss: 8.115774154663086\n","Epoch 73/4000, Step 98, d_loss: 0.018675735220313072, g_loss: 5.83458137512207\n","Epoch 73/4000, Step 99, d_loss: 0.0051505970768630505, g_loss: 5.472761154174805\n","Epoch 73/4000, Step 100, d_loss: 0.002075220923870802, g_loss: 5.345545768737793\n","Epoch 73/4000, Step 101, d_loss: 0.0027706203982234, g_loss: 5.213042736053467\n","Epoch 73/4000, Step 102, d_loss: 0.0036293792072683573, g_loss: 7.860629558563232\n","Epoch 73/4000, Step 103, d_loss: 0.00038696968113072217, g_loss: 6.539313793182373\n","Epoch 73/4000, Step 104, d_loss: 0.0024351372849196196, g_loss: 5.086064338684082\n","Epoch 73/4000, Step 105, d_loss: 0.003835779381915927, g_loss: 6.534012317657471\n","Epoch 73/4000, Step 106, d_loss: 0.011596603319048882, g_loss: 6.283149242401123\n","Epoch 73/4000, Step 107, d_loss: 0.009119018912315369, g_loss: 5.710805892944336\n","Epoch 73/4000, Step 108, d_loss: 0.002927579917013645, g_loss: 6.221146583557129\n","Epoch 73/4000, Step 109, d_loss: 0.004388876259326935, g_loss: 7.880362510681152\n","Epoch 73/4000, Step 110, d_loss: 0.03143055737018585, g_loss: 8.452826499938965\n","Epoch 73/4000, Step 111, d_loss: 0.010107322596013546, g_loss: 6.5396623611450195\n","Epoch 73/4000, Step 112, d_loss: 0.003210341325029731, g_loss: 6.410758972167969\n","Epoch 73/4000, Step 113, d_loss: 0.0011384508106857538, g_loss: 6.934551239013672\n","Epoch 73/4000, Step 114, d_loss: 0.0917530357837677, g_loss: 6.4011616706848145\n","Epoch 73/4000, Step 115, d_loss: 0.012857098132371902, g_loss: 6.7245659828186035\n","Epoch 73/4000, Step 116, d_loss: 0.0213111974298954, g_loss: 5.139127731323242\n","Epoch 73/4000, Step 117, d_loss: 0.0014307004166767001, g_loss: 6.139513969421387\n","Epoch 73/4000, Step 118, d_loss: 0.015557834878563881, g_loss: 4.385561943054199\n","Epoch 73/4000, Step 119, d_loss: 0.01061325054615736, g_loss: 8.329609870910645\n","Epoch 73/4000, Step 120, d_loss: 0.003283004043623805, g_loss: 3.3084657192230225\n","Epoch 73/4000, Step 121, d_loss: 0.016831444576382637, g_loss: 4.566868305206299\n","Epoch 73/4000, Step 122, d_loss: 0.0063386959955096245, g_loss: 5.142904281616211\n","Epoch 73/4000, Step 123, d_loss: 0.0015339842066168785, g_loss: 4.143686771392822\n","Epoch 73/4000, Step 124, d_loss: 0.031002750620245934, g_loss: 3.5333502292633057\n","Epoch 73/4000, Step 125, d_loss: 0.05153678357601166, g_loss: 4.836574554443359\n","Epoch 73/4000, Step 126, d_loss: 0.005733834113925695, g_loss: 5.047660827636719\n","Epoch 73/4000, Step 127, d_loss: 0.0027511189691722393, g_loss: 5.217641830444336\n","Epoch 73/4000, Step 128, d_loss: 0.0014099986292421818, g_loss: 5.594747066497803\n","Epoch 73/4000, Step 129, d_loss: 0.01319193933159113, g_loss: 5.205944538116455\n","Epoch 73/4000, Step 130, d_loss: 0.009611079469323158, g_loss: 7.8341755867004395\n","Epoch 73/4000, Step 131, d_loss: 0.001709967153146863, g_loss: 3.538323163986206\n","Epoch 73/4000, Step 132, d_loss: 0.0024903370067477226, g_loss: 7.1831159591674805\n","Epoch 73/4000, Step 133, d_loss: 0.028462372720241547, g_loss: 5.491086483001709\n","Epoch 73/4000, Step 134, d_loss: 0.0026797549799084663, g_loss: 6.875325679779053\n","Epoch 73/4000, Step 135, d_loss: 0.0047825174406170845, g_loss: 3.619938373565674\n","Epoch 73/4000, Step 136, d_loss: 0.0016932261642068624, g_loss: 5.205348968505859\n","Epoch 73/4000, Step 137, d_loss: 0.014517048373818398, g_loss: 3.9951233863830566\n","Epoch 73/4000, Step 138, d_loss: 0.0033435479272156954, g_loss: 4.463018894195557\n","Epoch 73/4000, Step 139, d_loss: 0.021556388586759567, g_loss: 7.228122234344482\n","Epoch 73/4000, Step 140, d_loss: 0.008916681632399559, g_loss: 8.455727577209473\n","Epoch 73/4000, Step 141, d_loss: 0.011224309913814068, g_loss: 5.369507312774658\n","Epoch 73/4000, Step 142, d_loss: 0.00853455625474453, g_loss: 7.1927289962768555\n","Epoch 73/4000, Step 143, d_loss: 0.014773321337997913, g_loss: 8.260383605957031\n","Epoch 73/4000, Step 144, d_loss: 0.00021263715461827815, g_loss: 5.553447723388672\n","Epoch 73/4000, Step 145, d_loss: 0.0046235970221459866, g_loss: 6.291538238525391\n","Epoch 73/4000, Step 146, d_loss: 0.014952355064451694, g_loss: 5.737129211425781\n","Epoch 73/4000, Step 147, d_loss: 0.004707405809313059, g_loss: 7.004444599151611\n","Epoch 73/4000, Step 148, d_loss: 0.005293905735015869, g_loss: 4.338230609893799\n","Epoch 73/4000, Step 149, d_loss: 0.009274333715438843, g_loss: 4.518196105957031\n","Epoch 73/4000, Step 150, d_loss: 0.002437837887555361, g_loss: 5.6471638679504395\n","Epoch 73/4000, Step 151, d_loss: 0.0066558984108269215, g_loss: 6.9847941398620605\n","Epoch 73/4000, Step 152, d_loss: 0.0021588103845715523, g_loss: 7.753894805908203\n","Epoch 73/4000, Step 153, d_loss: 0.002041660714894533, g_loss: 4.788808345794678\n","Epoch 73/4000, Step 154, d_loss: 0.002899816958233714, g_loss: 6.651854038238525\n","Epoch 73/4000, Step 155, d_loss: 0.0064123584888875484, g_loss: 7.351298809051514\n","Epoch 73/4000, Step 156, d_loss: 0.0049264212138950825, g_loss: 6.024450778961182\n","Epoch 73/4000, Step 157, d_loss: 0.008936293423175812, g_loss: 7.185445785522461\n","Epoch 73/4000, Step 158, d_loss: 0.003356429748237133, g_loss: 7.971126556396484\n","Epoch 73/4000, Step 159, d_loss: 0.020781880244612694, g_loss: 4.924655437469482\n","Epoch 73/4000, Step 160, d_loss: 0.015099165961146355, g_loss: 6.327184200286865\n","Epoch 73/4000, Step 161, d_loss: 0.006335987709462643, g_loss: 6.539065837860107\n","Epoch 73/4000, Step 162, d_loss: 0.009192007593810558, g_loss: 6.0997819900512695\n","Epoch 73/4000, Step 163, d_loss: 0.0016387645155191422, g_loss: 6.926350116729736\n","Epoch 73/4000, Step 164, d_loss: 0.002123878337442875, g_loss: 7.2225341796875\n","Epoch 73/4000, Step 165, d_loss: 0.0031155752949416637, g_loss: 6.537339687347412\n","Epoch 73/4000, Step 166, d_loss: 0.004630040843039751, g_loss: 5.531856060028076\n","Epoch 73/4000, Step 167, d_loss: 0.0019962035585194826, g_loss: 7.9970383644104\n","Epoch 73/4000, Step 168, d_loss: 0.00319076469168067, g_loss: 5.5114569664001465\n","Epoch 73/4000, Step 169, d_loss: 0.004352523945271969, g_loss: 7.749768257141113\n","Epoch 73/4000, Step 170, d_loss: 0.0026000153739005327, g_loss: 6.618081569671631\n","Epoch 73/4000, Step 171, d_loss: 0.003142898203805089, g_loss: 6.815535068511963\n","Epoch 73/4000, Step 172, d_loss: 0.0044195810332894325, g_loss: 6.962016582489014\n","Epoch 73/4000, Step 173, d_loss: 0.0006732468027621508, g_loss: 7.477384567260742\n","Epoch 73/4000, Step 174, d_loss: 0.003087676363065839, g_loss: 6.590395927429199\n","Epoch 73/4000, Step 175, d_loss: 0.0018198202596977353, g_loss: 7.22750997543335\n","Epoch 73/4000, Step 176, d_loss: 0.0034254048950970173, g_loss: 6.100330352783203\n","Epoch 73/4000, Step 177, d_loss: 0.002696710405871272, g_loss: 6.547845840454102\n","Epoch 73/4000, Step 178, d_loss: 0.002860274165868759, g_loss: 7.124029636383057\n","Epoch 73/4000, Step 179, d_loss: 0.012401700019836426, g_loss: 9.87150764465332\n","Epoch 73/4000, Step 180, d_loss: 0.011660497635602951, g_loss: 5.398374080657959\n","Epoch 73/4000, Step 181, d_loss: 0.0033755386248230934, g_loss: 6.1600141525268555\n","Epoch 73/4000, Step 182, d_loss: 0.05154362693428993, g_loss: 4.92206335067749\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 74/4000, Step 1, d_loss: 0.0034982331562787294, g_loss: 6.626535892486572\n","Epoch 74/4000, Step 2, d_loss: 0.004668452776968479, g_loss: 5.561335563659668\n","Epoch 74/4000, Step 3, d_loss: 0.0008883213740773499, g_loss: 4.505215644836426\n","Epoch 74/4000, Step 4, d_loss: 0.003867261577397585, g_loss: 13.808692932128906\n","Epoch 74/4000, Step 5, d_loss: 0.009456194005906582, g_loss: 5.9866156578063965\n","Epoch 74/4000, Step 6, d_loss: 0.0032182778231799603, g_loss: 6.782401084899902\n","Epoch 74/4000, Step 7, d_loss: 0.005986183416098356, g_loss: 4.883493423461914\n","Epoch 74/4000, Step 8, d_loss: 0.00793427787721157, g_loss: 4.795482158660889\n","Epoch 74/4000, Step 9, d_loss: 0.001692851074039936, g_loss: 3.0623769760131836\n","Epoch 74/4000, Step 10, d_loss: 0.003307949984446168, g_loss: 5.403963565826416\n","Epoch 74/4000, Step 11, d_loss: 0.005986253265291452, g_loss: 6.120178699493408\n","Epoch 74/4000, Step 12, d_loss: 0.0004201193223707378, g_loss: 6.625939846038818\n","Epoch 74/4000, Step 13, d_loss: 0.004231625236570835, g_loss: 5.239027500152588\n","Epoch 74/4000, Step 14, d_loss: 0.0070489076897501945, g_loss: 4.433963298797607\n","Epoch 74/4000, Step 15, d_loss: 0.004875400569289923, g_loss: 6.692012310028076\n","Epoch 74/4000, Step 16, d_loss: 0.004920939449220896, g_loss: 5.240864276885986\n","Epoch 74/4000, Step 17, d_loss: 0.0033306104596704245, g_loss: 5.100545406341553\n","Epoch 74/4000, Step 18, d_loss: 0.021021254360675812, g_loss: 7.0958027839660645\n","Epoch 74/4000, Step 19, d_loss: 0.0016285667661577463, g_loss: 5.153700828552246\n","Epoch 74/4000, Step 20, d_loss: 0.003756316378712654, g_loss: 6.186575412750244\n","Epoch 74/4000, Step 21, d_loss: 0.0030906612519174814, g_loss: 5.318264961242676\n","Epoch 74/4000, Step 22, d_loss: 0.002363134641200304, g_loss: 13.882820129394531\n","Epoch 74/4000, Step 23, d_loss: 0.0019809782970696688, g_loss: 10.044150352478027\n","Epoch 74/4000, Step 24, d_loss: 0.013988667167723179, g_loss: 7.333520412445068\n","Epoch 74/4000, Step 25, d_loss: 0.0039453050121665, g_loss: 5.801756381988525\n","Epoch 74/4000, Step 26, d_loss: 0.0006098446901887655, g_loss: 6.133978366851807\n","Epoch 74/4000, Step 27, d_loss: 0.003234538249671459, g_loss: 5.0380144119262695\n","Epoch 74/4000, Step 28, d_loss: 0.0006479170406237245, g_loss: 7.972526550292969\n","Epoch 74/4000, Step 29, d_loss: 0.010315962135791779, g_loss: 7.910858154296875\n","Epoch 74/4000, Step 30, d_loss: 0.003861853154376149, g_loss: 5.35085916519165\n","Epoch 74/4000, Step 31, d_loss: 0.001538431504741311, g_loss: 8.21195125579834\n","Epoch 74/4000, Step 32, d_loss: 0.0073930565267801285, g_loss: 4.9631195068359375\n","Epoch 74/4000, Step 33, d_loss: 0.004633148666471243, g_loss: 4.804973602294922\n","Epoch 74/4000, Step 34, d_loss: 0.009479731321334839, g_loss: 6.370372295379639\n","Epoch 74/4000, Step 35, d_loss: 0.002374934498220682, g_loss: 8.387699127197266\n","Epoch 74/4000, Step 36, d_loss: 0.002349243964999914, g_loss: 7.366857051849365\n","Epoch 74/4000, Step 37, d_loss: 0.006553282495588064, g_loss: 6.201237678527832\n","Epoch 74/4000, Step 38, d_loss: 0.0003770003095269203, g_loss: 4.162815570831299\n","Epoch 74/4000, Step 39, d_loss: 0.0007545221596956253, g_loss: 5.627938747406006\n","Epoch 74/4000, Step 40, d_loss: 0.00404351344332099, g_loss: 6.263484001159668\n","Epoch 74/4000, Step 41, d_loss: 0.0019894868601113558, g_loss: 7.138620376586914\n","Epoch 74/4000, Step 42, d_loss: 0.0021991925314068794, g_loss: 6.685366153717041\n","Epoch 74/4000, Step 43, d_loss: 0.0027605905197560787, g_loss: 6.069782257080078\n","Epoch 74/4000, Step 44, d_loss: 0.004122704267501831, g_loss: 5.254904270172119\n","Epoch 74/4000, Step 45, d_loss: 0.007338401395827532, g_loss: 6.682305335998535\n","Epoch 74/4000, Step 46, d_loss: 0.003090478479862213, g_loss: 6.740472316741943\n","Epoch 74/4000, Step 47, d_loss: 0.010710205882787704, g_loss: 6.081575870513916\n","Epoch 74/4000, Step 48, d_loss: 0.005830021575093269, g_loss: 6.329775333404541\n","Epoch 74/4000, Step 49, d_loss: 0.006662797182798386, g_loss: 6.516931056976318\n","Epoch 74/4000, Step 50, d_loss: 0.009729274548590183, g_loss: 7.177791595458984\n","Epoch 74/4000, Step 51, d_loss: 0.0027503613382577896, g_loss: 7.979211807250977\n","Epoch 74/4000, Step 52, d_loss: 0.00039668427780270576, g_loss: 7.227047443389893\n","Epoch 74/4000, Step 53, d_loss: 0.005235694348812103, g_loss: 6.6717352867126465\n","Epoch 74/4000, Step 54, d_loss: 0.0016964096575975418, g_loss: 9.590933799743652\n","Epoch 74/4000, Step 55, d_loss: 0.003940508235245943, g_loss: 6.8783860206604\n","Epoch 74/4000, Step 56, d_loss: 0.004923241212964058, g_loss: 6.397241592407227\n","Epoch 74/4000, Step 57, d_loss: 0.001709655043669045, g_loss: 6.506607532501221\n","Epoch 74/4000, Step 58, d_loss: 0.005296329967677593, g_loss: 8.975621223449707\n","Epoch 74/4000, Step 59, d_loss: 0.004002793692052364, g_loss: 6.883042335510254\n","Epoch 74/4000, Step 60, d_loss: 0.005008240696042776, g_loss: 7.802281379699707\n","Epoch 74/4000, Step 61, d_loss: 0.006981642451137304, g_loss: 8.002285957336426\n","Epoch 74/4000, Step 62, d_loss: 0.003294668160378933, g_loss: 7.018002510070801\n","Epoch 74/4000, Step 63, d_loss: 0.0054192193783819675, g_loss: 6.415417671203613\n","Epoch 74/4000, Step 64, d_loss: 0.002663356252014637, g_loss: 6.427060127258301\n","Epoch 74/4000, Step 65, d_loss: 0.002187812700867653, g_loss: 3.8752641677856445\n","Epoch 74/4000, Step 66, d_loss: 0.002062179846689105, g_loss: 5.112333297729492\n","Epoch 74/4000, Step 67, d_loss: 0.0034283341374248266, g_loss: 4.615825176239014\n","Epoch 74/4000, Step 68, d_loss: 0.0018840183038264513, g_loss: 5.267943382263184\n","Epoch 74/4000, Step 69, d_loss: 0.007687733508646488, g_loss: 5.413619041442871\n","Epoch 74/4000, Step 70, d_loss: 0.0034825652837753296, g_loss: 7.732834339141846\n","Epoch 74/4000, Step 71, d_loss: 0.0028373482637107372, g_loss: 5.415292263031006\n","Epoch 74/4000, Step 72, d_loss: 0.0011922031408175826, g_loss: 6.928825855255127\n","Epoch 74/4000, Step 73, d_loss: 0.004942504223436117, g_loss: 5.868385314941406\n","Epoch 74/4000, Step 74, d_loss: 0.005791477393358946, g_loss: 8.359647750854492\n","Epoch 74/4000, Step 75, d_loss: 0.0014656441053375602, g_loss: 6.197723865509033\n","Epoch 74/4000, Step 76, d_loss: 0.003746975678950548, g_loss: 9.271313667297363\n","Epoch 74/4000, Step 77, d_loss: 0.0012135428842157125, g_loss: 7.9499077796936035\n","Epoch 74/4000, Step 78, d_loss: 0.0008599427528679371, g_loss: 10.746391296386719\n","Epoch 74/4000, Step 79, d_loss: 0.003900987096130848, g_loss: 8.668176651000977\n","Epoch 74/4000, Step 80, d_loss: 0.002693175571039319, g_loss: 6.399772644042969\n","Epoch 74/4000, Step 81, d_loss: 0.0011857429053634405, g_loss: 8.506230354309082\n","Epoch 74/4000, Step 82, d_loss: 0.0003280155942775309, g_loss: 7.434332370758057\n","Epoch 74/4000, Step 83, d_loss: 0.0010821361793205142, g_loss: 4.9634504318237305\n","Epoch 74/4000, Step 84, d_loss: 0.0014321678318083286, g_loss: 5.643717288970947\n","Epoch 74/4000, Step 85, d_loss: 0.0029123197309672832, g_loss: 5.014130592346191\n","Epoch 74/4000, Step 86, d_loss: 0.000813181686680764, g_loss: 6.824206829071045\n","Epoch 74/4000, Step 87, d_loss: 0.0042128367349505424, g_loss: 7.44936466217041\n","Epoch 74/4000, Step 88, d_loss: 0.024930007755756378, g_loss: 7.401996612548828\n","Epoch 74/4000, Step 89, d_loss: 0.005642326548695564, g_loss: 4.945427417755127\n","Epoch 74/4000, Step 90, d_loss: 0.002259086584672332, g_loss: 8.462081909179688\n","Epoch 74/4000, Step 91, d_loss: 0.011268172413110733, g_loss: 6.4563374519348145\n","Epoch 74/4000, Step 92, d_loss: 0.0031312289647758007, g_loss: 7.986644744873047\n","Epoch 74/4000, Step 93, d_loss: 0.024536531418561935, g_loss: 5.610985279083252\n","Epoch 74/4000, Step 94, d_loss: 0.0031897597946226597, g_loss: 6.459943771362305\n","Epoch 74/4000, Step 95, d_loss: 0.0004873785946983844, g_loss: 7.708657741546631\n","Epoch 74/4000, Step 96, d_loss: 0.0029650931246578693, g_loss: 8.811202049255371\n","Epoch 74/4000, Step 97, d_loss: 0.0036014444194734097, g_loss: 6.933663845062256\n","Epoch 74/4000, Step 98, d_loss: 0.0037210448645055294, g_loss: 7.64362907409668\n","Epoch 74/4000, Step 99, d_loss: 0.0014048442244529724, g_loss: 7.352173805236816\n","Epoch 74/4000, Step 100, d_loss: 0.0020782463252544403, g_loss: 4.909806251525879\n","Epoch 74/4000, Step 101, d_loss: 0.006134779192507267, g_loss: 7.598631858825684\n","Epoch 74/4000, Step 102, d_loss: 0.0023841150104999542, g_loss: 7.648514747619629\n","Epoch 74/4000, Step 103, d_loss: 0.007396828383207321, g_loss: 7.781673431396484\n","Epoch 74/4000, Step 104, d_loss: 0.0067522646859288216, g_loss: 5.76582670211792\n","Epoch 74/4000, Step 105, d_loss: 0.003135478589683771, g_loss: 6.983633041381836\n","Epoch 74/4000, Step 106, d_loss: 0.010275843553245068, g_loss: 5.2881340980529785\n","Epoch 74/4000, Step 107, d_loss: 0.0026964207645505667, g_loss: 6.001945495605469\n","Epoch 74/4000, Step 108, d_loss: 0.004722560755908489, g_loss: 4.603237628936768\n","Epoch 74/4000, Step 109, d_loss: 0.001564564649015665, g_loss: 5.141862869262695\n","Epoch 74/4000, Step 110, d_loss: 0.010417267680168152, g_loss: 4.389101028442383\n","Epoch 74/4000, Step 111, d_loss: 0.0011863920371979475, g_loss: 8.578670501708984\n","Epoch 74/4000, Step 112, d_loss: 0.01398814469575882, g_loss: 6.000828742980957\n","Epoch 74/4000, Step 113, d_loss: 0.007206660695374012, g_loss: 5.228427410125732\n","Epoch 74/4000, Step 114, d_loss: 0.037528201937675476, g_loss: 7.156762599945068\n","Epoch 74/4000, Step 115, d_loss: 0.0017666600178927183, g_loss: 7.730642795562744\n","Epoch 74/4000, Step 116, d_loss: 0.005667233839631081, g_loss: 4.758935451507568\n","Epoch 74/4000, Step 117, d_loss: 0.006739735137671232, g_loss: 7.303228855133057\n","Epoch 74/4000, Step 118, d_loss: 0.0031340299174189568, g_loss: 5.735579490661621\n","Epoch 74/4000, Step 119, d_loss: 0.0021539560984820127, g_loss: 5.126956462860107\n","Epoch 74/4000, Step 120, d_loss: 0.0032453611493110657, g_loss: 4.066106796264648\n","Epoch 74/4000, Step 121, d_loss: 0.014040548354387283, g_loss: 4.480685234069824\n","Epoch 74/4000, Step 122, d_loss: 0.0028102535288780928, g_loss: 4.090722560882568\n","Epoch 74/4000, Step 123, d_loss: 0.019422242417931557, g_loss: 6.562880992889404\n","Epoch 74/4000, Step 124, d_loss: 0.006940994877368212, g_loss: 5.564492702484131\n","Epoch 74/4000, Step 125, d_loss: 0.007243350148200989, g_loss: 3.636044502258301\n","Epoch 74/4000, Step 126, d_loss: 0.0027359938248991966, g_loss: 6.861032009124756\n","Epoch 74/4000, Step 127, d_loss: 0.011929817497730255, g_loss: 6.140395164489746\n","Epoch 74/4000, Step 128, d_loss: 0.004907403141260147, g_loss: 4.844687461853027\n","Epoch 74/4000, Step 129, d_loss: 0.009503062814474106, g_loss: 12.405326843261719\n","Epoch 74/4000, Step 130, d_loss: 0.003018720541149378, g_loss: 6.008941173553467\n","Epoch 74/4000, Step 131, d_loss: 0.0007482916116714478, g_loss: 6.922314167022705\n","Epoch 74/4000, Step 132, d_loss: 0.004032294265925884, g_loss: 7.886933326721191\n","Epoch 74/4000, Step 133, d_loss: 0.011349612846970558, g_loss: 5.076474666595459\n","Epoch 74/4000, Step 134, d_loss: 0.0012103813933208585, g_loss: 8.119710922241211\n","Epoch 74/4000, Step 135, d_loss: 0.0010156744392588735, g_loss: 6.519033908843994\n","Epoch 74/4000, Step 136, d_loss: 0.002022093627601862, g_loss: 6.6745147705078125\n","Epoch 74/4000, Step 137, d_loss: 0.007959782145917416, g_loss: 7.223382472991943\n","Epoch 74/4000, Step 138, d_loss: 0.0043557570315897465, g_loss: 7.2556562423706055\n","Epoch 74/4000, Step 139, d_loss: 0.0029069860465824604, g_loss: 6.277426242828369\n","Epoch 74/4000, Step 140, d_loss: 0.00391517486423254, g_loss: 5.239245891571045\n","Epoch 74/4000, Step 141, d_loss: 0.0046682776883244514, g_loss: 4.979413986206055\n","Epoch 74/4000, Step 142, d_loss: 0.0010579004883766174, g_loss: 8.514992713928223\n","Epoch 74/4000, Step 143, d_loss: 0.0011119134724140167, g_loss: 6.439317226409912\n","Epoch 74/4000, Step 144, d_loss: 0.009679132141172886, g_loss: 6.383469581604004\n","Epoch 74/4000, Step 145, d_loss: 0.006978809833526611, g_loss: 8.29571533203125\n","Epoch 74/4000, Step 146, d_loss: 0.008899149484932423, g_loss: 5.8555908203125\n","Epoch 74/4000, Step 147, d_loss: 0.0030428869649767876, g_loss: 11.364789009094238\n","Epoch 74/4000, Step 148, d_loss: 0.0016408117953687906, g_loss: 6.892393112182617\n","Epoch 74/4000, Step 149, d_loss: 0.0055229091085493565, g_loss: 5.078158378601074\n","Epoch 74/4000, Step 150, d_loss: 0.003269344102591276, g_loss: 7.77840518951416\n","Epoch 74/4000, Step 151, d_loss: 0.011617297306656837, g_loss: 6.033239364624023\n","Epoch 74/4000, Step 152, d_loss: 0.0013571868184953928, g_loss: 4.841547012329102\n","Epoch 74/4000, Step 153, d_loss: 0.006155763752758503, g_loss: 8.499580383300781\n","Epoch 74/4000, Step 154, d_loss: 0.03882192075252533, g_loss: 8.007719039916992\n","Epoch 74/4000, Step 155, d_loss: 0.0029825791716575623, g_loss: 7.038810729980469\n","Epoch 74/4000, Step 156, d_loss: 0.005429554730653763, g_loss: 6.2115864753723145\n","Epoch 74/4000, Step 157, d_loss: 0.010939903557300568, g_loss: 6.597993850708008\n","Epoch 74/4000, Step 158, d_loss: 0.0020986823365092278, g_loss: 6.739915370941162\n","Epoch 74/4000, Step 159, d_loss: 0.00036106506013311446, g_loss: 9.486554145812988\n","Epoch 74/4000, Step 160, d_loss: 0.011173849925398827, g_loss: 7.598264217376709\n","Epoch 74/4000, Step 161, d_loss: 0.011564018204808235, g_loss: 5.508341312408447\n","Epoch 74/4000, Step 162, d_loss: 0.007912030443549156, g_loss: 5.433695316314697\n","Epoch 74/4000, Step 163, d_loss: 0.02867966704070568, g_loss: 5.317456245422363\n","Epoch 74/4000, Step 164, d_loss: 0.002717580646276474, g_loss: 6.892751693725586\n","Epoch 74/4000, Step 165, d_loss: 0.001186529640108347, g_loss: 5.765300750732422\n","Epoch 74/4000, Step 166, d_loss: 0.006076008081436157, g_loss: 7.135831356048584\n","Epoch 74/4000, Step 167, d_loss: 0.0026430562138557434, g_loss: 5.557595252990723\n","Epoch 74/4000, Step 168, d_loss: 0.00041237592813558877, g_loss: 6.52329683303833\n","Epoch 74/4000, Step 169, d_loss: 0.00399767467752099, g_loss: 6.907535552978516\n","Epoch 74/4000, Step 170, d_loss: 0.001066079013980925, g_loss: 8.459137916564941\n","Epoch 74/4000, Step 171, d_loss: 0.0010908220428973436, g_loss: 7.048699378967285\n","Epoch 74/4000, Step 172, d_loss: 0.008735735900700092, g_loss: 6.158962726593018\n","Epoch 74/4000, Step 173, d_loss: 0.0025203831028193235, g_loss: 7.75577449798584\n","Epoch 74/4000, Step 174, d_loss: 0.003436008235439658, g_loss: 7.296720504760742\n","Epoch 74/4000, Step 175, d_loss: 0.005926267709583044, g_loss: 8.949577331542969\n","Epoch 74/4000, Step 176, d_loss: 0.0003065789642278105, g_loss: 7.325948238372803\n","Epoch 74/4000, Step 177, d_loss: 0.005943575408309698, g_loss: 6.62396240234375\n","Epoch 74/4000, Step 178, d_loss: 0.018047919496893883, g_loss: 6.331482410430908\n","Epoch 74/4000, Step 179, d_loss: 0.0008551093051210046, g_loss: 7.212617874145508\n","Epoch 74/4000, Step 180, d_loss: 0.002424337202683091, g_loss: 5.406856060028076\n","Epoch 74/4000, Step 181, d_loss: 0.004599804058670998, g_loss: 4.891819000244141\n","Epoch 74/4000, Step 182, d_loss: 2.174586772918701, g_loss: 6.429190158843994\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 75/4000, Step 1, d_loss: 0.03539016842842102, g_loss: 1.7900925874710083\n","Epoch 75/4000, Step 2, d_loss: 0.16009017825126648, g_loss: 1.962219476699829\n","Epoch 75/4000, Step 3, d_loss: 0.10444371402263641, g_loss: 3.462355375289917\n","Epoch 75/4000, Step 4, d_loss: 0.015251629054546356, g_loss: 0.5919952392578125\n","Epoch 75/4000, Step 5, d_loss: 0.5305240750312805, g_loss: 1.3510607481002808\n","Epoch 75/4000, Step 6, d_loss: 0.01422138698399067, g_loss: 3.452089309692383\n","Epoch 75/4000, Step 7, d_loss: 0.13072164356708527, g_loss: 4.7437567710876465\n","Epoch 75/4000, Step 8, d_loss: 0.2015424519777298, g_loss: 4.517940044403076\n","Epoch 75/4000, Step 9, d_loss: 0.08034999668598175, g_loss: 2.033092498779297\n","Epoch 75/4000, Step 10, d_loss: 0.06706062704324722, g_loss: 3.508558988571167\n","Epoch 75/4000, Step 11, d_loss: 0.022138599306344986, g_loss: 5.75567102432251\n","Epoch 75/4000, Step 12, d_loss: 0.34662407636642456, g_loss: 4.330883979797363\n","Epoch 75/4000, Step 13, d_loss: 0.025233030319213867, g_loss: 3.194347620010376\n","Epoch 75/4000, Step 14, d_loss: 0.0069779749028384686, g_loss: 2.650723457336426\n","Epoch 75/4000, Step 15, d_loss: 0.8696120381355286, g_loss: 4.08120059967041\n","Epoch 75/4000, Step 16, d_loss: 0.15494802594184875, g_loss: 7.4466776847839355\n","Epoch 75/4000, Step 17, d_loss: 0.3648197650909424, g_loss: 8.527923583984375\n","Epoch 75/4000, Step 18, d_loss: 0.47991126775741577, g_loss: 4.695797443389893\n","Epoch 75/4000, Step 19, d_loss: 0.42237305641174316, g_loss: 3.1502439975738525\n","Epoch 75/4000, Step 20, d_loss: 0.348773717880249, g_loss: 5.588836193084717\n","Epoch 75/4000, Step 21, d_loss: 0.06420091539621353, g_loss: 2.4816691875457764\n","Epoch 75/4000, Step 22, d_loss: 0.5949347019195557, g_loss: 5.280643939971924\n","Epoch 75/4000, Step 23, d_loss: 0.19410525262355804, g_loss: 3.5728085041046143\n","Epoch 75/4000, Step 24, d_loss: 0.08769361674785614, g_loss: 4.638102054595947\n","Epoch 75/4000, Step 25, d_loss: 0.3402773141860962, g_loss: 5.625025272369385\n","Epoch 75/4000, Step 26, d_loss: 0.43580543994903564, g_loss: 6.186885356903076\n","Epoch 75/4000, Step 27, d_loss: 0.03119526617228985, g_loss: 3.226163864135742\n","Epoch 75/4000, Step 28, d_loss: 0.1504262387752533, g_loss: 5.618329048156738\n","Epoch 75/4000, Step 29, d_loss: 0.40865257382392883, g_loss: 2.5726211071014404\n","Epoch 75/4000, Step 30, d_loss: 0.04523419961333275, g_loss: 1.2249621152877808\n","Epoch 75/4000, Step 31, d_loss: 0.12280850112438202, g_loss: 2.289634943008423\n","Epoch 75/4000, Step 32, d_loss: 0.02864183858036995, g_loss: 1.9672865867614746\n","Epoch 75/4000, Step 33, d_loss: 0.017166486009955406, g_loss: 1.6885756254196167\n","Epoch 75/4000, Step 34, d_loss: 1.0668087005615234, g_loss: 1.2563676834106445\n","Epoch 75/4000, Step 35, d_loss: 0.5277485847473145, g_loss: 5.343319416046143\n","Epoch 75/4000, Step 36, d_loss: 0.15414053201675415, g_loss: 3.1556901931762695\n","Epoch 75/4000, Step 37, d_loss: 0.10754667222499847, g_loss: 4.624669075012207\n","Epoch 75/4000, Step 38, d_loss: 0.07446016371250153, g_loss: 3.2531840801239014\n","Epoch 75/4000, Step 39, d_loss: 0.059844017028808594, g_loss: 6.526466369628906\n","Epoch 75/4000, Step 40, d_loss: 0.04569757729768753, g_loss: 4.42450475692749\n","Epoch 75/4000, Step 41, d_loss: 0.0874362587928772, g_loss: 4.263211727142334\n","Epoch 75/4000, Step 42, d_loss: 0.07051604986190796, g_loss: 3.179629325866699\n","Epoch 75/4000, Step 43, d_loss: 0.1448153257369995, g_loss: 3.6745564937591553\n","Epoch 75/4000, Step 44, d_loss: 0.028118418529629707, g_loss: 5.56312894821167\n","Epoch 75/4000, Step 45, d_loss: 0.09217933565378189, g_loss: 4.036640644073486\n","Epoch 75/4000, Step 46, d_loss: 0.01865324005484581, g_loss: 8.152510643005371\n","Epoch 75/4000, Step 47, d_loss: 0.051958873867988586, g_loss: 6.871953010559082\n","Epoch 75/4000, Step 48, d_loss: 0.03474821150302887, g_loss: 6.362703800201416\n","Epoch 75/4000, Step 49, d_loss: 0.014017841778695583, g_loss: 1.2005611658096313\n","Epoch 75/4000, Step 50, d_loss: 0.1802377849817276, g_loss: 5.025476455688477\n","Epoch 75/4000, Step 51, d_loss: 0.013150304555892944, g_loss: 6.517389297485352\n","Epoch 75/4000, Step 52, d_loss: 0.019307300448417664, g_loss: 4.10066556930542\n","Epoch 75/4000, Step 53, d_loss: 0.08156118541955948, g_loss: 4.697455883026123\n","Epoch 75/4000, Step 54, d_loss: 0.0672941654920578, g_loss: 2.3812973499298096\n","Epoch 75/4000, Step 55, d_loss: 0.0431787371635437, g_loss: 2.187457799911499\n","Epoch 75/4000, Step 56, d_loss: 0.041313380002975464, g_loss: 4.442772388458252\n","Epoch 75/4000, Step 57, d_loss: 0.05913551151752472, g_loss: 4.047772407531738\n","Epoch 75/4000, Step 58, d_loss: 0.006339759565889835, g_loss: 0.3377601206302643\n","Epoch 75/4000, Step 59, d_loss: 0.5642621517181396, g_loss: 3.1039888858795166\n","Epoch 75/4000, Step 60, d_loss: 0.009196218103170395, g_loss: 6.400673866271973\n","Epoch 75/4000, Step 61, d_loss: 0.03495744988322258, g_loss: 6.331273555755615\n","Epoch 75/4000, Step 62, d_loss: 0.07986506819725037, g_loss: 6.607564926147461\n","Epoch 75/4000, Step 63, d_loss: 0.08623607456684113, g_loss: 6.406641960144043\n","Epoch 75/4000, Step 64, d_loss: 0.2741723954677582, g_loss: 8.682852745056152\n","Epoch 75/4000, Step 65, d_loss: 0.012151259928941727, g_loss: 4.786753177642822\n","Epoch 75/4000, Step 66, d_loss: 0.2051582634449005, g_loss: 5.080019950866699\n","Epoch 75/4000, Step 67, d_loss: 0.07810626924037933, g_loss: 4.388822078704834\n","Epoch 75/4000, Step 68, d_loss: 0.5732585787773132, g_loss: 5.940688610076904\n","Epoch 75/4000, Step 69, d_loss: 0.08911329507827759, g_loss: 6.607511520385742\n","Epoch 75/4000, Step 70, d_loss: 0.27448442578315735, g_loss: 7.852151393890381\n","Epoch 75/4000, Step 71, d_loss: 0.09455610066652298, g_loss: 5.483911991119385\n","Epoch 75/4000, Step 72, d_loss: 0.4176504611968994, g_loss: 6.438276290893555\n","Epoch 75/4000, Step 73, d_loss: 0.004472998436540365, g_loss: 8.675291061401367\n","Epoch 75/4000, Step 74, d_loss: 0.003013653215020895, g_loss: 5.036111354827881\n","Epoch 75/4000, Step 75, d_loss: 0.014396188780665398, g_loss: 2.984550714492798\n","Epoch 75/4000, Step 76, d_loss: 0.07033460587263107, g_loss: 2.9114983081817627\n","Epoch 75/4000, Step 77, d_loss: 0.1315203607082367, g_loss: 1.6244442462921143\n","Epoch 75/4000, Step 78, d_loss: 0.1313910335302353, g_loss: 2.8806166648864746\n","Epoch 75/4000, Step 79, d_loss: 0.023888615891337395, g_loss: 6.722846984863281\n","Epoch 75/4000, Step 80, d_loss: 0.020584790036082268, g_loss: 4.08986759185791\n","Epoch 75/4000, Step 81, d_loss: 0.02373257465660572, g_loss: 0.45765382051467896\n","Epoch 75/4000, Step 82, d_loss: 0.23499973118305206, g_loss: 0.729964017868042\n","Epoch 75/4000, Step 83, d_loss: 0.1491164118051529, g_loss: 3.4954819679260254\n","Epoch 75/4000, Step 84, d_loss: 0.30657830834388733, g_loss: 0.7520676255226135\n","Epoch 75/4000, Step 85, d_loss: 0.1448242962360382, g_loss: 3.5095248222351074\n","Epoch 75/4000, Step 86, d_loss: 0.1110370010137558, g_loss: 4.058620929718018\n","Epoch 75/4000, Step 87, d_loss: 0.12084907293319702, g_loss: 5.006313323974609\n","Epoch 75/4000, Step 88, d_loss: 0.1667705476284027, g_loss: 10.288578033447266\n","Epoch 75/4000, Step 89, d_loss: 0.06376798450946808, g_loss: 6.336843967437744\n","Epoch 75/4000, Step 90, d_loss: 0.006390410475432873, g_loss: 5.915456771850586\n","Epoch 75/4000, Step 91, d_loss: 0.11181993782520294, g_loss: 8.83105754852295\n","Epoch 75/4000, Step 92, d_loss: 0.27389657497406006, g_loss: 6.346963882446289\n","Epoch 75/4000, Step 93, d_loss: 0.015325755812227726, g_loss: 8.979881286621094\n","Epoch 75/4000, Step 94, d_loss: 0.022740712389349937, g_loss: 6.041538715362549\n","Epoch 75/4000, Step 95, d_loss: 0.0019851119723170996, g_loss: 6.485924243927002\n","Epoch 75/4000, Step 96, d_loss: 0.016373924911022186, g_loss: 5.056576728820801\n","Epoch 75/4000, Step 97, d_loss: 0.017760735005140305, g_loss: 3.8499600887298584\n","Epoch 75/4000, Step 98, d_loss: 0.05722914636135101, g_loss: 2.9018638134002686\n","Epoch 75/4000, Step 99, d_loss: 0.028626147657632828, g_loss: 5.852003574371338\n","Epoch 75/4000, Step 100, d_loss: 0.09470954537391663, g_loss: 6.077641487121582\n","Epoch 75/4000, Step 101, d_loss: 0.012546371668577194, g_loss: 5.6699323654174805\n","Epoch 75/4000, Step 102, d_loss: 0.07213538885116577, g_loss: 5.522430419921875\n","Epoch 75/4000, Step 103, d_loss: 0.03017917461693287, g_loss: 6.511919975280762\n","Epoch 75/4000, Step 104, d_loss: 0.0432882234454155, g_loss: 4.030150890350342\n","Epoch 75/4000, Step 105, d_loss: 0.12754464149475098, g_loss: 2.3809664249420166\n","Epoch 75/4000, Step 106, d_loss: 0.03400952368974686, g_loss: 2.4896090030670166\n","Epoch 75/4000, Step 107, d_loss: 0.045738380402326584, g_loss: 5.420857906341553\n","Epoch 75/4000, Step 108, d_loss: 0.029978686943650246, g_loss: 3.9798595905303955\n","Epoch 75/4000, Step 109, d_loss: 0.08791206777095795, g_loss: 2.319218635559082\n","Epoch 75/4000, Step 110, d_loss: 0.0010431562550365925, g_loss: 4.487010478973389\n","Epoch 75/4000, Step 111, d_loss: 0.027091914787888527, g_loss: 4.512251377105713\n","Epoch 75/4000, Step 112, d_loss: 0.005698687396943569, g_loss: 3.4010465145111084\n","Epoch 75/4000, Step 113, d_loss: 0.0020054583437740803, g_loss: 6.5762176513671875\n","Epoch 75/4000, Step 114, d_loss: 0.7870306372642517, g_loss: 4.493190765380859\n","Epoch 75/4000, Step 115, d_loss: 0.02902188152074814, g_loss: 7.961240768432617\n","Epoch 75/4000, Step 116, d_loss: 0.2414151132106781, g_loss: 6.928037166595459\n","Epoch 75/4000, Step 117, d_loss: 0.16575975716114044, g_loss: 9.817910194396973\n","Epoch 75/4000, Step 118, d_loss: 0.04603763669729233, g_loss: 8.869210243225098\n","Epoch 75/4000, Step 119, d_loss: 0.2122591733932495, g_loss: 9.799189567565918\n","Epoch 75/4000, Step 120, d_loss: 0.03937443345785141, g_loss: 8.016151428222656\n","Epoch 75/4000, Step 121, d_loss: 0.021044326946139336, g_loss: 8.761469841003418\n","Epoch 75/4000, Step 122, d_loss: 0.018289409577846527, g_loss: 4.743660926818848\n","Epoch 75/4000, Step 123, d_loss: 0.0631108507514, g_loss: 5.766526699066162\n","Epoch 75/4000, Step 124, d_loss: 0.04683377593755722, g_loss: 5.486757755279541\n","Epoch 75/4000, Step 125, d_loss: 0.030597377568483353, g_loss: 4.816087245941162\n","Epoch 75/4000, Step 126, d_loss: 0.24000534415245056, g_loss: 4.528908729553223\n","Epoch 75/4000, Step 127, d_loss: 0.02863362804055214, g_loss: 6.815011978149414\n","Epoch 75/4000, Step 128, d_loss: 0.052123960107564926, g_loss: 6.500150680541992\n","Epoch 75/4000, Step 129, d_loss: 0.07683294266462326, g_loss: 4.495441913604736\n","Epoch 75/4000, Step 130, d_loss: 0.019807839766144753, g_loss: 2.1237378120422363\n","Epoch 75/4000, Step 131, d_loss: 0.008372105658054352, g_loss: 4.805457592010498\n","Epoch 75/4000, Step 132, d_loss: 0.023006536066532135, g_loss: 4.255633354187012\n","Epoch 75/4000, Step 133, d_loss: 0.02878975309431553, g_loss: 2.522299289703369\n","Epoch 75/4000, Step 134, d_loss: 0.042661141604185104, g_loss: 4.137102127075195\n","Epoch 75/4000, Step 135, d_loss: 0.01535971462726593, g_loss: 4.653086185455322\n","Epoch 75/4000, Step 136, d_loss: 0.24997888505458832, g_loss: 4.947948455810547\n","Epoch 75/4000, Step 137, d_loss: 0.10388694703578949, g_loss: 3.5922536849975586\n","Epoch 75/4000, Step 138, d_loss: 0.054315246641635895, g_loss: 4.582081317901611\n","Epoch 75/4000, Step 139, d_loss: 0.21774804592132568, g_loss: 5.008547306060791\n","Epoch 75/4000, Step 140, d_loss: 0.1969594806432724, g_loss: 4.976800918579102\n","Epoch 75/4000, Step 141, d_loss: 0.29187649488449097, g_loss: 6.9740681648254395\n","Epoch 75/4000, Step 142, d_loss: 0.006186182610690594, g_loss: 6.288024425506592\n","Epoch 75/4000, Step 143, d_loss: 0.0106525719165802, g_loss: 3.3843133449554443\n","Epoch 75/4000, Step 144, d_loss: 0.04032149165868759, g_loss: 5.46703577041626\n","Epoch 75/4000, Step 145, d_loss: 0.008958287537097931, g_loss: 4.0476298332214355\n","Epoch 75/4000, Step 146, d_loss: 0.09460055828094482, g_loss: 3.6687369346618652\n","Epoch 75/4000, Step 147, d_loss: 0.00714675709605217, g_loss: 4.475037097930908\n","Epoch 75/4000, Step 148, d_loss: 0.08470603823661804, g_loss: 2.821357488632202\n","Epoch 75/4000, Step 149, d_loss: 0.04655246436595917, g_loss: 4.386123180389404\n","Epoch 75/4000, Step 150, d_loss: 0.05420326814055443, g_loss: 3.3906338214874268\n","Epoch 75/4000, Step 151, d_loss: 0.03183301165699959, g_loss: 2.3538241386413574\n","Epoch 75/4000, Step 152, d_loss: 0.041937198489904404, g_loss: 5.783805847167969\n","Epoch 75/4000, Step 153, d_loss: 0.06074871867895126, g_loss: 5.889427185058594\n","Epoch 75/4000, Step 154, d_loss: 0.007836494594812393, g_loss: 4.594611644744873\n","Epoch 75/4000, Step 155, d_loss: 0.051527850329875946, g_loss: 6.047606468200684\n","Epoch 75/4000, Step 156, d_loss: 0.013275821693241596, g_loss: 8.16128921508789\n","Epoch 75/4000, Step 157, d_loss: 0.034044381231069565, g_loss: 5.666928768157959\n","Epoch 75/4000, Step 158, d_loss: 0.02301877737045288, g_loss: 6.963300704956055\n","Epoch 75/4000, Step 159, d_loss: 0.011699117720127106, g_loss: 5.89516019821167\n","Epoch 75/4000, Step 160, d_loss: 0.03365160897374153, g_loss: 5.510554313659668\n","Epoch 75/4000, Step 161, d_loss: 0.07696189731359482, g_loss: 3.834609270095825\n","Epoch 75/4000, Step 162, d_loss: 0.003350798971951008, g_loss: 6.392266750335693\n","Epoch 75/4000, Step 163, d_loss: 0.0014830981381237507, g_loss: 7.359285354614258\n","Epoch 75/4000, Step 164, d_loss: 0.010974516160786152, g_loss: 3.362217903137207\n","Epoch 75/4000, Step 165, d_loss: 0.016535038128495216, g_loss: 3.734297037124634\n","Epoch 75/4000, Step 166, d_loss: 0.0019623888656497, g_loss: 4.425481796264648\n","Epoch 75/4000, Step 167, d_loss: 0.040449000895023346, g_loss: 3.153778314590454\n","Epoch 75/4000, Step 168, d_loss: 0.023982413113117218, g_loss: 4.511822700500488\n","Epoch 75/4000, Step 169, d_loss: 0.0029093052726238966, g_loss: 5.584380149841309\n","Epoch 75/4000, Step 170, d_loss: 0.0015249337302520871, g_loss: 4.9087090492248535\n","Epoch 75/4000, Step 171, d_loss: 0.0052420999854803085, g_loss: 3.4638378620147705\n","Epoch 75/4000, Step 172, d_loss: 0.01362322736531496, g_loss: 4.061232089996338\n","Epoch 75/4000, Step 173, d_loss: 0.0036785798147320747, g_loss: 4.664527893066406\n","Epoch 75/4000, Step 174, d_loss: 0.0008048046147450805, g_loss: 3.147139310836792\n","Epoch 75/4000, Step 175, d_loss: 0.0184728242456913, g_loss: 3.707702159881592\n","Epoch 75/4000, Step 176, d_loss: 0.03384093940258026, g_loss: 3.1651275157928467\n","Epoch 75/4000, Step 177, d_loss: 0.01625572144985199, g_loss: 4.720327377319336\n","Epoch 75/4000, Step 178, d_loss: 0.028273239731788635, g_loss: 4.385511875152588\n","Epoch 75/4000, Step 179, d_loss: 0.009399217553436756, g_loss: 4.600916862487793\n","Epoch 75/4000, Step 180, d_loss: 0.012116460129618645, g_loss: 6.345689296722412\n","Epoch 75/4000, Step 181, d_loss: 0.018588220700621605, g_loss: 4.955829620361328\n","Epoch 75/4000, Step 182, d_loss: 0.05586957931518555, g_loss: 4.10042667388916\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 76/4000, Step 1, d_loss: 0.03930383175611496, g_loss: 2.55721116065979\n","Epoch 76/4000, Step 2, d_loss: 0.027646919712424278, g_loss: 3.861815929412842\n","Epoch 76/4000, Step 3, d_loss: 0.008029506541788578, g_loss: 4.708556652069092\n","Epoch 76/4000, Step 4, d_loss: 0.015707235783338547, g_loss: 7.53176212310791\n","Epoch 76/4000, Step 5, d_loss: 0.032050520181655884, g_loss: 3.4572267532348633\n","Epoch 76/4000, Step 6, d_loss: 0.016698535531759262, g_loss: 4.544952392578125\n","Epoch 76/4000, Step 7, d_loss: 0.004600293934345245, g_loss: 6.2095746994018555\n","Epoch 76/4000, Step 8, d_loss: 0.0212424136698246, g_loss: 5.236362934112549\n","Epoch 76/4000, Step 9, d_loss: 0.017782608047127724, g_loss: 4.178654670715332\n","Epoch 76/4000, Step 10, d_loss: 0.011554520577192307, g_loss: 7.109443664550781\n","Epoch 76/4000, Step 11, d_loss: 0.015171410515904427, g_loss: 5.123899459838867\n","Epoch 76/4000, Step 12, d_loss: 0.0027463342994451523, g_loss: 4.219145774841309\n","Epoch 76/4000, Step 13, d_loss: 0.027971312403678894, g_loss: 5.081719875335693\n","Epoch 76/4000, Step 14, d_loss: 0.009306071326136589, g_loss: 5.77022647857666\n","Epoch 76/4000, Step 15, d_loss: 0.0059137772768735886, g_loss: 6.743024826049805\n","Epoch 76/4000, Step 16, d_loss: 0.01945815049111843, g_loss: 5.1946258544921875\n","Epoch 76/4000, Step 17, d_loss: 0.010301950387656689, g_loss: 4.6961283683776855\n","Epoch 76/4000, Step 18, d_loss: 0.0015520243905484676, g_loss: 6.071177959442139\n","Epoch 76/4000, Step 19, d_loss: 0.025563068687915802, g_loss: 4.654863357543945\n","Epoch 76/4000, Step 20, d_loss: 0.005772534292191267, g_loss: 5.643290042877197\n","Epoch 76/4000, Step 21, d_loss: 0.010364300571382046, g_loss: 5.521175384521484\n","Epoch 76/4000, Step 22, d_loss: 0.013397576287388802, g_loss: 6.759767532348633\n","Epoch 76/4000, Step 23, d_loss: 0.06108928099274635, g_loss: 11.801916122436523\n","Epoch 76/4000, Step 24, d_loss: 0.0031459778547286987, g_loss: 7.258285045623779\n","Epoch 76/4000, Step 25, d_loss: 0.03547884523868561, g_loss: 4.804275989532471\n","Epoch 76/4000, Step 26, d_loss: 0.012160293757915497, g_loss: 5.042158126831055\n","Epoch 76/4000, Step 27, d_loss: 0.0032519528176635504, g_loss: 3.7625601291656494\n","Epoch 76/4000, Step 28, d_loss: 0.009306592866778374, g_loss: 3.830601215362549\n","Epoch 76/4000, Step 29, d_loss: 0.041228245943784714, g_loss: 6.431258678436279\n","Epoch 76/4000, Step 30, d_loss: 0.01604348048567772, g_loss: 4.948738098144531\n","Epoch 76/4000, Step 31, d_loss: 0.03249610960483551, g_loss: 7.112863063812256\n","Epoch 76/4000, Step 32, d_loss: 0.01740967482328415, g_loss: 6.952218532562256\n","Epoch 76/4000, Step 33, d_loss: 0.017435811460018158, g_loss: 6.8635969161987305\n","Epoch 76/4000, Step 34, d_loss: 0.0020679952576756477, g_loss: 6.509136199951172\n","Epoch 76/4000, Step 35, d_loss: 0.015037915669381618, g_loss: 7.09030818939209\n","Epoch 76/4000, Step 36, d_loss: 0.005292599089443684, g_loss: 5.126932144165039\n","Epoch 76/4000, Step 37, d_loss: 0.0074318209663033485, g_loss: 5.204068660736084\n","Epoch 76/4000, Step 38, d_loss: 0.0015380725963041186, g_loss: 9.255284309387207\n","Epoch 76/4000, Step 39, d_loss: 0.020639635622501373, g_loss: 5.956550598144531\n","Epoch 76/4000, Step 40, d_loss: 0.00539840804412961, g_loss: 4.922093868255615\n","Epoch 76/4000, Step 41, d_loss: 0.004930955357849598, g_loss: 5.15593957901001\n","Epoch 76/4000, Step 42, d_loss: 0.010579272173345089, g_loss: 5.1611738204956055\n","Epoch 76/4000, Step 43, d_loss: 0.001597096212208271, g_loss: 4.088763236999512\n","Epoch 76/4000, Step 44, d_loss: 0.01315656490623951, g_loss: 5.912474155426025\n","Epoch 76/4000, Step 45, d_loss: 0.004712128546088934, g_loss: 6.580295085906982\n","Epoch 76/4000, Step 46, d_loss: 0.03285395726561546, g_loss: 5.190736293792725\n","Epoch 76/4000, Step 47, d_loss: 0.018039679154753685, g_loss: 10.607391357421875\n","Epoch 76/4000, Step 48, d_loss: 0.004373827483505011, g_loss: 7.180070400238037\n","Epoch 76/4000, Step 49, d_loss: 0.010498755611479282, g_loss: 6.299272537231445\n","Epoch 76/4000, Step 50, d_loss: 0.008626936934888363, g_loss: 5.171073913574219\n","Epoch 76/4000, Step 51, d_loss: 0.004046963527798653, g_loss: 4.596190452575684\n","Epoch 76/4000, Step 52, d_loss: 0.006787315011024475, g_loss: 4.084104061126709\n","Epoch 76/4000, Step 53, d_loss: 0.025131825357675552, g_loss: 3.668814182281494\n","Epoch 76/4000, Step 54, d_loss: 0.013268298469483852, g_loss: 5.123013496398926\n","Epoch 76/4000, Step 55, d_loss: 0.006953560281544924, g_loss: 6.111522197723389\n","Epoch 76/4000, Step 56, d_loss: 0.01810019463300705, g_loss: 6.173050403594971\n","Epoch 76/4000, Step 57, d_loss: 0.033347513526678085, g_loss: 3.932213544845581\n","Epoch 76/4000, Step 58, d_loss: 0.03695990517735481, g_loss: 6.827540874481201\n","Epoch 76/4000, Step 59, d_loss: 0.006255755200982094, g_loss: 6.827270030975342\n","Epoch 76/4000, Step 60, d_loss: 0.0286583099514246, g_loss: 8.638250350952148\n","Epoch 76/4000, Step 61, d_loss: 0.008454667404294014, g_loss: 3.9798994064331055\n","Epoch 76/4000, Step 62, d_loss: 0.008743133395910263, g_loss: 3.988182544708252\n","Epoch 76/4000, Step 63, d_loss: 0.0066430517472326756, g_loss: 4.389680862426758\n","Epoch 76/4000, Step 64, d_loss: 0.008342800661921501, g_loss: 6.628486633300781\n","Epoch 76/4000, Step 65, d_loss: 0.014707442373037338, g_loss: 6.639251708984375\n","Epoch 76/4000, Step 66, d_loss: 0.007860356941819191, g_loss: 4.282731533050537\n","Epoch 76/4000, Step 67, d_loss: 0.009473134763538837, g_loss: 5.844759941101074\n","Epoch 76/4000, Step 68, d_loss: 0.006372659467160702, g_loss: 4.831481456756592\n","Epoch 76/4000, Step 69, d_loss: 0.02793765440583229, g_loss: 5.824347972869873\n","Epoch 76/4000, Step 70, d_loss: 0.006879420951008797, g_loss: 8.266507148742676\n","Epoch 76/4000, Step 71, d_loss: 0.015325208194553852, g_loss: 5.860589981079102\n","Epoch 76/4000, Step 72, d_loss: 0.005855081137269735, g_loss: 5.918250560760498\n","Epoch 76/4000, Step 73, d_loss: 0.0012531138490885496, g_loss: 5.488216400146484\n","Epoch 76/4000, Step 74, d_loss: 0.0034521957859396935, g_loss: 4.478427886962891\n","Epoch 76/4000, Step 75, d_loss: 0.0010287261102348566, g_loss: 4.265350341796875\n","Epoch 76/4000, Step 76, d_loss: 0.0007744275499135256, g_loss: 6.027796268463135\n","Epoch 76/4000, Step 77, d_loss: 0.013810854405164719, g_loss: 5.92313289642334\n","Epoch 76/4000, Step 78, d_loss: 0.0031671952456235886, g_loss: 4.512964248657227\n","Epoch 76/4000, Step 79, d_loss: 0.003329602535814047, g_loss: 9.56009578704834\n","Epoch 76/4000, Step 80, d_loss: 0.005858353804796934, g_loss: 5.560793399810791\n","Epoch 76/4000, Step 81, d_loss: 0.007617445662617683, g_loss: 8.233375549316406\n","Epoch 76/4000, Step 82, d_loss: 0.0058004166930913925, g_loss: 11.532342910766602\n","Epoch 76/4000, Step 83, d_loss: 0.0020311304833739996, g_loss: 7.16835355758667\n","Epoch 76/4000, Step 84, d_loss: 0.009423455223441124, g_loss: 5.779374122619629\n","Epoch 76/4000, Step 85, d_loss: 0.008261788636446, g_loss: 5.963059425354004\n","Epoch 76/4000, Step 86, d_loss: 0.004997060168534517, g_loss: 11.147845268249512\n","Epoch 76/4000, Step 87, d_loss: 0.00240875780582428, g_loss: 6.518582344055176\n","Epoch 76/4000, Step 88, d_loss: 0.0113496333360672, g_loss: 7.621117115020752\n","Epoch 76/4000, Step 89, d_loss: 0.0030250842683017254, g_loss: 7.5687031745910645\n","Epoch 76/4000, Step 90, d_loss: 0.006777596194297075, g_loss: 8.8013277053833\n","Epoch 76/4000, Step 91, d_loss: 0.004032186698168516, g_loss: 8.278063774108887\n","Epoch 76/4000, Step 92, d_loss: 0.006495916750282049, g_loss: 7.934273719787598\n","Epoch 76/4000, Step 93, d_loss: 0.008170201443135738, g_loss: 6.4507317543029785\n","Epoch 76/4000, Step 94, d_loss: 0.006164257414638996, g_loss: 4.564512252807617\n","Epoch 76/4000, Step 95, d_loss: 0.004074546974152327, g_loss: 7.59857177734375\n","Epoch 76/4000, Step 96, d_loss: 0.015814807265996933, g_loss: 4.920007705688477\n","Epoch 76/4000, Step 97, d_loss: 0.007678226567804813, g_loss: 3.602492570877075\n","Epoch 76/4000, Step 98, d_loss: 0.004501049872487783, g_loss: 6.494793891906738\n","Epoch 76/4000, Step 99, d_loss: 0.0022748068440705538, g_loss: 7.4774956703186035\n","Epoch 76/4000, Step 100, d_loss: 0.004312274511903524, g_loss: 5.560848712921143\n","Epoch 76/4000, Step 101, d_loss: 0.001975881401449442, g_loss: 6.232199192047119\n","Epoch 76/4000, Step 102, d_loss: 0.004805310629308224, g_loss: 8.434432029724121\n","Epoch 76/4000, Step 103, d_loss: 0.05572495609521866, g_loss: 6.215937614440918\n","Epoch 76/4000, Step 104, d_loss: 0.002765114651992917, g_loss: 7.460336208343506\n","Epoch 76/4000, Step 105, d_loss: 0.011212226003408432, g_loss: 5.444066524505615\n","Epoch 76/4000, Step 106, d_loss: 0.006821474991738796, g_loss: 6.497250080108643\n","Epoch 76/4000, Step 107, d_loss: 0.005185951944440603, g_loss: 6.374699115753174\n","Epoch 76/4000, Step 108, d_loss: 0.0052757528610527515, g_loss: 4.840321063995361\n","Epoch 76/4000, Step 109, d_loss: 0.006039773114025593, g_loss: 4.897295951843262\n","Epoch 76/4000, Step 110, d_loss: 0.009947054088115692, g_loss: 7.893917083740234\n","Epoch 76/4000, Step 111, d_loss: 0.002629323396831751, g_loss: 5.764822006225586\n","Epoch 76/4000, Step 112, d_loss: 0.003338651265949011, g_loss: 5.42503023147583\n","Epoch 76/4000, Step 113, d_loss: 0.00221589719876647, g_loss: 6.5702691078186035\n","Epoch 76/4000, Step 114, d_loss: 0.0075184861198067665, g_loss: 7.3652729988098145\n","Epoch 76/4000, Step 115, d_loss: 0.00149215804412961, g_loss: 5.014333248138428\n","Epoch 76/4000, Step 116, d_loss: 0.008570141158998013, g_loss: 4.222818374633789\n","Epoch 76/4000, Step 117, d_loss: 0.007483751978725195, g_loss: 6.478567123413086\n","Epoch 76/4000, Step 118, d_loss: 0.00670903455466032, g_loss: 6.337193965911865\n","Epoch 76/4000, Step 119, d_loss: 0.01278702262789011, g_loss: 4.823947906494141\n","Epoch 76/4000, Step 120, d_loss: 0.0016803534235805273, g_loss: 5.069361209869385\n","Epoch 76/4000, Step 121, d_loss: 0.0017896039644256234, g_loss: 6.147237300872803\n","Epoch 76/4000, Step 122, d_loss: 0.004780062474310398, g_loss: 3.0667226314544678\n","Epoch 76/4000, Step 123, d_loss: 0.0030071628279983997, g_loss: 5.575689792633057\n","Epoch 76/4000, Step 124, d_loss: 0.004507979843765497, g_loss: 6.04498291015625\n","Epoch 76/4000, Step 125, d_loss: 0.00433948403224349, g_loss: 6.841495513916016\n","Epoch 76/4000, Step 126, d_loss: 0.006391945295035839, g_loss: 5.772482872009277\n","Epoch 76/4000, Step 127, d_loss: 0.008918534964323044, g_loss: 8.534112930297852\n","Epoch 76/4000, Step 128, d_loss: 0.01303212158381939, g_loss: 10.720373153686523\n","Epoch 76/4000, Step 129, d_loss: 0.002284843008965254, g_loss: 4.877313137054443\n","Epoch 76/4000, Step 130, d_loss: 0.004339156672358513, g_loss: 10.85576057434082\n","Epoch 76/4000, Step 131, d_loss: 0.005268641747534275, g_loss: 4.694216728210449\n","Epoch 76/4000, Step 132, d_loss: 0.0066604092717170715, g_loss: 5.84514045715332\n","Epoch 76/4000, Step 133, d_loss: 0.002277699764817953, g_loss: 4.76561164855957\n","Epoch 76/4000, Step 134, d_loss: 0.00273231603205204, g_loss: 6.642761707305908\n","Epoch 76/4000, Step 135, d_loss: 0.001145603833720088, g_loss: 5.52701473236084\n","Epoch 76/4000, Step 136, d_loss: 0.0015389587497338653, g_loss: 7.354806900024414\n","Epoch 76/4000, Step 137, d_loss: 0.004661652259528637, g_loss: 6.163951873779297\n","Epoch 76/4000, Step 138, d_loss: 0.013636428862810135, g_loss: 6.5918474197387695\n","Epoch 76/4000, Step 139, d_loss: 0.004635076969861984, g_loss: 5.404052257537842\n","Epoch 76/4000, Step 140, d_loss: 0.0090639041736722, g_loss: 5.353183269500732\n","Epoch 76/4000, Step 141, d_loss: 0.0029834082815796137, g_loss: 4.952047824859619\n","Epoch 76/4000, Step 142, d_loss: 0.011998124420642853, g_loss: 7.373976707458496\n","Epoch 76/4000, Step 143, d_loss: 0.046191178262233734, g_loss: 4.999157428741455\n","Epoch 76/4000, Step 144, d_loss: 0.011126182973384857, g_loss: 6.054401397705078\n","Epoch 76/4000, Step 145, d_loss: 0.0018048021011054516, g_loss: 6.578780651092529\n","Epoch 76/4000, Step 146, d_loss: 0.0053173149935901165, g_loss: 6.959659099578857\n","Epoch 76/4000, Step 147, d_loss: 0.0018469174392521381, g_loss: 4.320643424987793\n","Epoch 76/4000, Step 148, d_loss: 0.01553436927497387, g_loss: 7.555466175079346\n","Epoch 76/4000, Step 149, d_loss: 0.01767771504819393, g_loss: 7.3067145347595215\n","Epoch 76/4000, Step 150, d_loss: 0.0010211145272478461, g_loss: 5.2286505699157715\n","Epoch 76/4000, Step 151, d_loss: 0.0024684262461960316, g_loss: 5.995388031005859\n","Epoch 76/4000, Step 152, d_loss: 0.004218814894556999, g_loss: 4.551993370056152\n","Epoch 76/4000, Step 153, d_loss: 0.04156285524368286, g_loss: 9.3652925491333\n","Epoch 76/4000, Step 154, d_loss: 0.004018859937787056, g_loss: 5.669880390167236\n","Epoch 76/4000, Step 155, d_loss: 0.003119537839666009, g_loss: 6.848450660705566\n","Epoch 76/4000, Step 156, d_loss: 0.0008534174412488937, g_loss: 10.092799186706543\n","Epoch 76/4000, Step 157, d_loss: 0.014065757393836975, g_loss: 6.136788845062256\n","Epoch 76/4000, Step 158, d_loss: 0.0007160672103054821, g_loss: 6.473017692565918\n","Epoch 76/4000, Step 159, d_loss: 0.00784973707050085, g_loss: 8.692464828491211\n","Epoch 76/4000, Step 160, d_loss: 0.013848021626472473, g_loss: 7.444094181060791\n","Epoch 76/4000, Step 161, d_loss: 0.005234468728303909, g_loss: 5.78764009475708\n","Epoch 76/4000, Step 162, d_loss: 0.002444896614179015, g_loss: 6.490004062652588\n","Epoch 76/4000, Step 163, d_loss: 0.012726038694381714, g_loss: 7.897622585296631\n","Epoch 76/4000, Step 164, d_loss: 0.0052343434654176235, g_loss: 7.090649127960205\n","Epoch 76/4000, Step 165, d_loss: 0.006283865775913, g_loss: 5.698896884918213\n","Epoch 76/4000, Step 166, d_loss: 0.005237285513430834, g_loss: 7.2831902503967285\n","Epoch 76/4000, Step 167, d_loss: 0.003674847539514303, g_loss: 5.334980010986328\n","Epoch 76/4000, Step 168, d_loss: 0.011299828067421913, g_loss: 5.879955291748047\n","Epoch 76/4000, Step 169, d_loss: 0.00297756539657712, g_loss: 5.9715576171875\n","Epoch 76/4000, Step 170, d_loss: 0.010794390924274921, g_loss: 4.004817485809326\n","Epoch 76/4000, Step 171, d_loss: 0.004146613646298647, g_loss: 7.09342622756958\n","Epoch 76/4000, Step 172, d_loss: 0.000716249574907124, g_loss: 6.979565143585205\n","Epoch 76/4000, Step 173, d_loss: 0.00266686431132257, g_loss: 7.5462470054626465\n","Epoch 76/4000, Step 174, d_loss: 0.005606034304946661, g_loss: 6.866559028625488\n","Epoch 76/4000, Step 175, d_loss: 0.0028354343958199024, g_loss: 5.507235050201416\n","Epoch 76/4000, Step 176, d_loss: 0.0014434984186664224, g_loss: 5.558430194854736\n","Epoch 76/4000, Step 177, d_loss: 0.004846449941396713, g_loss: 8.944779396057129\n","Epoch 76/4000, Step 178, d_loss: 0.0013921536738052964, g_loss: 4.439006328582764\n","Epoch 76/4000, Step 179, d_loss: 0.009142623282968998, g_loss: 6.600878715515137\n","Epoch 76/4000, Step 180, d_loss: 0.008928649127483368, g_loss: 5.684738636016846\n","Epoch 76/4000, Step 181, d_loss: 0.0070273783057928085, g_loss: 4.556942462921143\n","Epoch 76/4000, Step 182, d_loss: 0.014080222696065903, g_loss: 6.572370529174805\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 77/4000, Step 1, d_loss: 0.007932008244097233, g_loss: 5.864986419677734\n","Epoch 77/4000, Step 2, d_loss: 0.010377226397395134, g_loss: 6.984276294708252\n","Epoch 77/4000, Step 3, d_loss: 0.004632093477994204, g_loss: 5.209306240081787\n","Epoch 77/4000, Step 4, d_loss: 0.0016146874986588955, g_loss: 7.878242015838623\n","Epoch 77/4000, Step 5, d_loss: 0.0028105750679969788, g_loss: 5.3259596824646\n","Epoch 77/4000, Step 6, d_loss: 0.008051488548517227, g_loss: 4.876979827880859\n","Epoch 77/4000, Step 7, d_loss: 0.006909716408699751, g_loss: 8.342458724975586\n","Epoch 77/4000, Step 8, d_loss: 0.020186778157949448, g_loss: 7.775447845458984\n","Epoch 77/4000, Step 9, d_loss: 0.008264908567070961, g_loss: 6.217019081115723\n","Epoch 77/4000, Step 10, d_loss: 0.0022968335542827845, g_loss: 6.134075164794922\n","Epoch 77/4000, Step 11, d_loss: 0.0037085111252963543, g_loss: 7.227785110473633\n","Epoch 77/4000, Step 12, d_loss: 0.003727649338543415, g_loss: 8.120108604431152\n","Epoch 77/4000, Step 13, d_loss: 0.005471639335155487, g_loss: 9.770914077758789\n","Epoch 77/4000, Step 14, d_loss: 0.009582671336829662, g_loss: 9.644662857055664\n","Epoch 77/4000, Step 15, d_loss: 0.0027463410515338182, g_loss: 6.780755996704102\n","Epoch 77/4000, Step 16, d_loss: 0.001348360674455762, g_loss: 6.0172953605651855\n","Epoch 77/4000, Step 17, d_loss: 0.0013953382149338722, g_loss: 7.307271957397461\n","Epoch 77/4000, Step 18, d_loss: 0.0021658786572515965, g_loss: 6.790598392486572\n","Epoch 77/4000, Step 19, d_loss: 0.003297034651041031, g_loss: 6.192063331604004\n","Epoch 77/4000, Step 20, d_loss: 0.003783483523875475, g_loss: 6.759495735168457\n","Epoch 77/4000, Step 21, d_loss: 0.00869603082537651, g_loss: 5.883308410644531\n","Epoch 77/4000, Step 22, d_loss: 0.008597184903919697, g_loss: 6.693389415740967\n","Epoch 77/4000, Step 23, d_loss: 0.01381385512650013, g_loss: 3.21181321144104\n","Epoch 77/4000, Step 24, d_loss: 0.0020938345696777105, g_loss: 8.531571388244629\n","Epoch 77/4000, Step 25, d_loss: 0.012874248437583447, g_loss: 5.9064483642578125\n","Epoch 77/4000, Step 26, d_loss: 0.003937943372875452, g_loss: 6.019549369812012\n","Epoch 77/4000, Step 27, d_loss: 0.009471235796809196, g_loss: 6.276545524597168\n","Epoch 77/4000, Step 28, d_loss: 0.004100926220417023, g_loss: 5.578954696655273\n","Epoch 77/4000, Step 29, d_loss: 0.013789532706141472, g_loss: 5.062369346618652\n","Epoch 77/4000, Step 30, d_loss: 0.002727612853050232, g_loss: 9.485749244689941\n","Epoch 77/4000, Step 31, d_loss: 0.0006970241665840149, g_loss: 5.91157341003418\n","Epoch 77/4000, Step 32, d_loss: 0.01191786304116249, g_loss: 6.464013576507568\n","Epoch 77/4000, Step 33, d_loss: 0.0034026051871478558, g_loss: 5.4673919677734375\n","Epoch 77/4000, Step 34, d_loss: 0.01033447403460741, g_loss: 6.716671466827393\n","Epoch 77/4000, Step 35, d_loss: 0.010563408955931664, g_loss: 6.812014102935791\n","Epoch 77/4000, Step 36, d_loss: 0.004837030079215765, g_loss: 4.9088029861450195\n","Epoch 77/4000, Step 37, d_loss: 0.014218103140592575, g_loss: 5.503257751464844\n","Epoch 77/4000, Step 38, d_loss: 0.0015185038791969419, g_loss: 5.649494171142578\n","Epoch 77/4000, Step 39, d_loss: 0.001586309401318431, g_loss: 5.955266952514648\n","Epoch 77/4000, Step 40, d_loss: 0.007492670789361, g_loss: 4.789205074310303\n","Epoch 77/4000, Step 41, d_loss: 0.003011964960023761, g_loss: 8.220612525939941\n","Epoch 77/4000, Step 42, d_loss: 0.0014034515479579568, g_loss: 6.6905341148376465\n","Epoch 77/4000, Step 43, d_loss: 0.002881827997043729, g_loss: 5.476588726043701\n","Epoch 77/4000, Step 44, d_loss: 0.010514809750020504, g_loss: 6.784045219421387\n","Epoch 77/4000, Step 45, d_loss: 0.002325114095583558, g_loss: 6.550743103027344\n","Epoch 77/4000, Step 46, d_loss: 0.04061932489275932, g_loss: 6.912332534790039\n","Epoch 77/4000, Step 47, d_loss: 0.0024065594188869, g_loss: 7.070570945739746\n","Epoch 77/4000, Step 48, d_loss: 0.003353712148964405, g_loss: 6.132527828216553\n","Epoch 77/4000, Step 49, d_loss: 0.0015956792049109936, g_loss: 4.364070892333984\n","Epoch 77/4000, Step 50, d_loss: 0.003327737795189023, g_loss: 7.854502201080322\n","Epoch 77/4000, Step 51, d_loss: 0.003329185303300619, g_loss: 7.488281726837158\n","Epoch 77/4000, Step 52, d_loss: 0.0026499826926738024, g_loss: 7.352268218994141\n","Epoch 77/4000, Step 53, d_loss: 0.0034989998675882816, g_loss: 5.059562683105469\n","Epoch 77/4000, Step 54, d_loss: 0.01431669294834137, g_loss: 7.099948883056641\n","Epoch 77/4000, Step 55, d_loss: 0.011413585394620895, g_loss: 6.251288890838623\n","Epoch 77/4000, Step 56, d_loss: 0.0025668239686638117, g_loss: 8.283665657043457\n","Epoch 77/4000, Step 57, d_loss: 0.006705313455313444, g_loss: 7.271570205688477\n","Epoch 77/4000, Step 58, d_loss: 0.00121672626119107, g_loss: 4.904685974121094\n","Epoch 77/4000, Step 59, d_loss: 0.016395190730690956, g_loss: 4.445530414581299\n","Epoch 77/4000, Step 60, d_loss: 0.012254761531949043, g_loss: 7.620138645172119\n","Epoch 77/4000, Step 61, d_loss: 0.016071468591690063, g_loss: 7.272871494293213\n","Epoch 77/4000, Step 62, d_loss: 0.0041884444653987885, g_loss: 6.527812957763672\n","Epoch 77/4000, Step 63, d_loss: 0.02995849773287773, g_loss: 5.7828874588012695\n","Epoch 77/4000, Step 64, d_loss: 0.00525879068300128, g_loss: 5.709097385406494\n","Epoch 77/4000, Step 65, d_loss: 0.00392205361276865, g_loss: 4.7139811515808105\n","Epoch 77/4000, Step 66, d_loss: 0.0029164250008761883, g_loss: 5.5227131843566895\n","Epoch 77/4000, Step 67, d_loss: 0.0050483220256865025, g_loss: 6.166720390319824\n","Epoch 77/4000, Step 68, d_loss: 0.0015225402312353253, g_loss: 6.540830135345459\n","Epoch 77/4000, Step 69, d_loss: 0.013535448350012302, g_loss: 9.847400665283203\n","Epoch 77/4000, Step 70, d_loss: 0.010563015937805176, g_loss: 4.7527570724487305\n","Epoch 77/4000, Step 71, d_loss: 0.005428030155599117, g_loss: 5.285417556762695\n","Epoch 77/4000, Step 72, d_loss: 0.037449732422828674, g_loss: 3.572190761566162\n","Epoch 77/4000, Step 73, d_loss: 0.0163797065615654, g_loss: 6.519728183746338\n","Epoch 77/4000, Step 74, d_loss: 0.01566430926322937, g_loss: 4.089306831359863\n","Epoch 77/4000, Step 75, d_loss: 0.0025164075195789337, g_loss: 5.63431978225708\n","Epoch 77/4000, Step 76, d_loss: 0.03281760588288307, g_loss: 7.014853477478027\n","Epoch 77/4000, Step 77, d_loss: 0.010902819223701954, g_loss: 8.47002124786377\n","Epoch 77/4000, Step 78, d_loss: 0.002223195042461157, g_loss: 6.978297710418701\n","Epoch 77/4000, Step 79, d_loss: 0.014523101039230824, g_loss: 6.6051788330078125\n","Epoch 77/4000, Step 80, d_loss: 0.0014433838659897447, g_loss: 8.519416809082031\n","Epoch 77/4000, Step 81, d_loss: 0.006481936667114496, g_loss: 7.4553327560424805\n","Epoch 77/4000, Step 82, d_loss: 0.00105890235863626, g_loss: 6.63218879699707\n","Epoch 77/4000, Step 83, d_loss: 0.007684437092393637, g_loss: 7.948740005493164\n","Epoch 77/4000, Step 84, d_loss: 0.010655084624886513, g_loss: 4.821638107299805\n","Epoch 77/4000, Step 85, d_loss: 0.00036351921153254807, g_loss: 6.357701778411865\n","Epoch 77/4000, Step 86, d_loss: 0.0035678234416991472, g_loss: 8.977129936218262\n","Epoch 77/4000, Step 87, d_loss: 0.0014331762213259935, g_loss: 9.705482482910156\n","Epoch 77/4000, Step 88, d_loss: 0.0022539389319717884, g_loss: 6.8253278732299805\n","Epoch 77/4000, Step 89, d_loss: 0.003811953356489539, g_loss: 6.5997443199157715\n","Epoch 77/4000, Step 90, d_loss: 0.0029685695189982653, g_loss: 6.655066967010498\n","Epoch 77/4000, Step 91, d_loss: 0.001569619169458747, g_loss: 5.285396575927734\n","Epoch 77/4000, Step 92, d_loss: 0.04533856734633446, g_loss: 7.263308525085449\n","Epoch 77/4000, Step 93, d_loss: 0.006614340469241142, g_loss: 6.137059211730957\n","Epoch 77/4000, Step 94, d_loss: 0.0024563740007579327, g_loss: 6.114444732666016\n","Epoch 77/4000, Step 95, d_loss: 0.040483929216861725, g_loss: 6.176830291748047\n","Epoch 77/4000, Step 96, d_loss: 0.002574892481788993, g_loss: 7.482701778411865\n","Epoch 77/4000, Step 97, d_loss: 0.0071103982627391815, g_loss: 8.155193328857422\n","Epoch 77/4000, Step 98, d_loss: 0.0029407464899122715, g_loss: 5.710183620452881\n","Epoch 77/4000, Step 99, d_loss: 0.012094501405954361, g_loss: 5.343529224395752\n","Epoch 77/4000, Step 100, d_loss: 0.0035242456942796707, g_loss: 6.677754878997803\n","Epoch 77/4000, Step 101, d_loss: 0.011610053479671478, g_loss: 4.110089302062988\n","Epoch 77/4000, Step 102, d_loss: 0.0017857117345556617, g_loss: 4.164469242095947\n","Epoch 77/4000, Step 103, d_loss: 0.002353596966713667, g_loss: 5.958155155181885\n","Epoch 77/4000, Step 104, d_loss: 0.004696447402238846, g_loss: 5.59290075302124\n","Epoch 77/4000, Step 105, d_loss: 0.011089353822171688, g_loss: 5.848388671875\n","Epoch 77/4000, Step 106, d_loss: 0.012702662497758865, g_loss: 6.663972854614258\n","Epoch 77/4000, Step 107, d_loss: 0.004774120636284351, g_loss: 6.561089992523193\n","Epoch 77/4000, Step 108, d_loss: 0.013500472530722618, g_loss: 6.359287738800049\n","Epoch 77/4000, Step 109, d_loss: 0.004810454323887825, g_loss: 3.8685996532440186\n","Epoch 77/4000, Step 110, d_loss: 0.008885659277439117, g_loss: 6.124409198760986\n","Epoch 77/4000, Step 111, d_loss: 0.005969097372144461, g_loss: 5.940441131591797\n","Epoch 77/4000, Step 112, d_loss: 0.006031945813447237, g_loss: 7.561611652374268\n","Epoch 77/4000, Step 113, d_loss: 0.01785251311957836, g_loss: 3.6862878799438477\n","Epoch 77/4000, Step 114, d_loss: 0.005208323709666729, g_loss: 7.396663665771484\n","Epoch 77/4000, Step 115, d_loss: 0.007306287996470928, g_loss: 3.5438899993896484\n","Epoch 77/4000, Step 116, d_loss: 0.015047590248286724, g_loss: 4.928561687469482\n","Epoch 77/4000, Step 117, d_loss: 0.016556307673454285, g_loss: 8.927555084228516\n","Epoch 77/4000, Step 118, d_loss: 0.0316985622048378, g_loss: 6.125340461730957\n","Epoch 77/4000, Step 119, d_loss: 0.0028643335681408644, g_loss: 6.559816837310791\n","Epoch 77/4000, Step 120, d_loss: 0.001237890450283885, g_loss: 6.054722785949707\n","Epoch 77/4000, Step 121, d_loss: 0.004700975026935339, g_loss: 5.267391204833984\n","Epoch 77/4000, Step 122, d_loss: 0.03264167159795761, g_loss: 9.012341499328613\n","Epoch 77/4000, Step 123, d_loss: 0.03470355272293091, g_loss: 3.621584177017212\n","Epoch 77/4000, Step 124, d_loss: 0.004939127713441849, g_loss: 5.589035511016846\n","Epoch 77/4000, Step 125, d_loss: 0.02949891984462738, g_loss: 4.935785293579102\n","Epoch 77/4000, Step 126, d_loss: 0.002409307286143303, g_loss: 6.755392074584961\n","Epoch 77/4000, Step 127, d_loss: 0.014260156080126762, g_loss: 6.203660488128662\n","Epoch 77/4000, Step 128, d_loss: 0.014162380248308182, g_loss: 7.194600582122803\n","Epoch 77/4000, Step 129, d_loss: 0.015577252022922039, g_loss: 4.86190938949585\n","Epoch 77/4000, Step 130, d_loss: 0.002562138484790921, g_loss: 6.475411891937256\n","Epoch 77/4000, Step 131, d_loss: 0.009836351498961449, g_loss: 5.248894691467285\n","Epoch 77/4000, Step 132, d_loss: 0.009312418289482594, g_loss: 5.512552738189697\n","Epoch 77/4000, Step 133, d_loss: 0.018569491803646088, g_loss: 5.268788814544678\n","Epoch 77/4000, Step 134, d_loss: 0.031985245645046234, g_loss: 7.279512405395508\n","Epoch 77/4000, Step 135, d_loss: 0.002647749148309231, g_loss: 6.736968040466309\n","Epoch 77/4000, Step 136, d_loss: 0.0060086483135819435, g_loss: 4.416889667510986\n","Epoch 77/4000, Step 137, d_loss: 0.0032281745225191116, g_loss: 4.8540449142456055\n","Epoch 77/4000, Step 138, d_loss: 0.01937573216855526, g_loss: 4.3650007247924805\n","Epoch 77/4000, Step 139, d_loss: 0.02552635967731476, g_loss: 6.996195316314697\n","Epoch 77/4000, Step 140, d_loss: 0.004058930091559887, g_loss: 6.721901893615723\n","Epoch 77/4000, Step 141, d_loss: 0.0055376021191477776, g_loss: 6.130552291870117\n","Epoch 77/4000, Step 142, d_loss: 0.008173172362148762, g_loss: 6.822080612182617\n","Epoch 77/4000, Step 143, d_loss: 0.008889295160770416, g_loss: 6.457643985748291\n","Epoch 77/4000, Step 144, d_loss: 0.017701735720038414, g_loss: 4.730266094207764\n","Epoch 77/4000, Step 145, d_loss: 0.0052716536447405815, g_loss: 7.789141654968262\n","Epoch 77/4000, Step 146, d_loss: 0.018811484798789024, g_loss: 7.369077682495117\n","Epoch 77/4000, Step 147, d_loss: 0.001546379760839045, g_loss: 7.086371898651123\n","Epoch 77/4000, Step 148, d_loss: 0.0021882043220102787, g_loss: 5.9611077308654785\n","Epoch 77/4000, Step 149, d_loss: 0.00560486176982522, g_loss: 7.040650844573975\n","Epoch 77/4000, Step 150, d_loss: 0.01115894690155983, g_loss: 5.89015531539917\n","Epoch 77/4000, Step 151, d_loss: 0.00712810643017292, g_loss: 5.9481658935546875\n","Epoch 77/4000, Step 152, d_loss: 0.008655856363475323, g_loss: 5.1723761558532715\n","Epoch 77/4000, Step 153, d_loss: 0.002291046781465411, g_loss: 5.715661525726318\n","Epoch 77/4000, Step 154, d_loss: 0.005714436061680317, g_loss: 6.3116350173950195\n","Epoch 77/4000, Step 155, d_loss: 0.004135599825531244, g_loss: 6.710083961486816\n","Epoch 77/4000, Step 156, d_loss: 0.0016624897252768278, g_loss: 7.322697639465332\n","Epoch 77/4000, Step 157, d_loss: 0.0013839196180924773, g_loss: 6.50837516784668\n","Epoch 77/4000, Step 158, d_loss: 0.0035826656967401505, g_loss: 8.217759132385254\n","Epoch 77/4000, Step 159, d_loss: 0.0015265247784554958, g_loss: 6.6585187911987305\n","Epoch 77/4000, Step 160, d_loss: 0.0034096133895218372, g_loss: 6.5366997718811035\n","Epoch 77/4000, Step 161, d_loss: 0.0019040838815271854, g_loss: 5.659948825836182\n","Epoch 77/4000, Step 162, d_loss: 0.0026210758369416, g_loss: 6.589280605316162\n","Epoch 77/4000, Step 163, d_loss: 0.004630097188055515, g_loss: 5.444696426391602\n","Epoch 77/4000, Step 164, d_loss: 0.0070815375074744225, g_loss: 10.362274169921875\n","Epoch 77/4000, Step 165, d_loss: 0.004928165581077337, g_loss: 6.98298454284668\n","Epoch 77/4000, Step 166, d_loss: 0.022972386330366135, g_loss: 6.660140514373779\n","Epoch 77/4000, Step 167, d_loss: 0.0014122225111350417, g_loss: 5.741182804107666\n","Epoch 77/4000, Step 168, d_loss: 0.0038670431822538376, g_loss: 5.36556339263916\n","Epoch 77/4000, Step 169, d_loss: 0.007695742417126894, g_loss: 6.554501056671143\n","Epoch 77/4000, Step 170, d_loss: 0.002818632870912552, g_loss: 5.829885482788086\n","Epoch 77/4000, Step 171, d_loss: 0.006495814770460129, g_loss: 6.584529876708984\n","Epoch 77/4000, Step 172, d_loss: 0.004091319628059864, g_loss: 7.701447486877441\n","Epoch 77/4000, Step 173, d_loss: 0.009626742452383041, g_loss: 6.663933753967285\n","Epoch 77/4000, Step 174, d_loss: 0.00422434788197279, g_loss: 6.865450859069824\n","Epoch 77/4000, Step 175, d_loss: 0.013237138278782368, g_loss: 5.624798774719238\n","Epoch 77/4000, Step 176, d_loss: 0.0032708547078073025, g_loss: 6.003917694091797\n","Epoch 77/4000, Step 177, d_loss: 0.004671810194849968, g_loss: 6.154494285583496\n","Epoch 77/4000, Step 178, d_loss: 0.004283769987523556, g_loss: 6.445186614990234\n","Epoch 77/4000, Step 179, d_loss: 0.003692857688292861, g_loss: 8.631091117858887\n","Epoch 77/4000, Step 180, d_loss: 0.008172599598765373, g_loss: 6.250509262084961\n","Epoch 77/4000, Step 181, d_loss: 0.004389742389321327, g_loss: 5.820639133453369\n","Epoch 77/4000, Step 182, d_loss: 0.46512511372566223, g_loss: 5.184200763702393\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 78/4000, Step 1, d_loss: 0.016734391450881958, g_loss: 4.223627090454102\n","Epoch 78/4000, Step 2, d_loss: 0.18922467529773712, g_loss: 3.85986065864563\n","Epoch 78/4000, Step 3, d_loss: 0.042478516697883606, g_loss: 3.32738995552063\n","Epoch 78/4000, Step 4, d_loss: 0.04612823575735092, g_loss: 3.229478120803833\n","Epoch 78/4000, Step 5, d_loss: 0.18755027651786804, g_loss: 1.0384725332260132\n","Epoch 78/4000, Step 6, d_loss: 0.17503806948661804, g_loss: 1.6259881258010864\n","Epoch 78/4000, Step 7, d_loss: 0.24824689328670502, g_loss: 3.014486074447632\n","Epoch 78/4000, Step 8, d_loss: 0.02345103770494461, g_loss: 5.5068769454956055\n","Epoch 78/4000, Step 9, d_loss: 0.04314903914928436, g_loss: 2.208252429962158\n","Epoch 78/4000, Step 10, d_loss: 0.008541172370314598, g_loss: 1.7002636194229126\n","Epoch 78/4000, Step 11, d_loss: 0.014177331700921059, g_loss: 3.9539332389831543\n","Epoch 78/4000, Step 12, d_loss: 0.005178402177989483, g_loss: 4.045025825500488\n","Epoch 78/4000, Step 13, d_loss: 0.035381849855184555, g_loss: 2.0016655921936035\n","Epoch 78/4000, Step 14, d_loss: 0.020731844007968903, g_loss: 6.451857566833496\n","Epoch 78/4000, Step 15, d_loss: 0.3226313292980194, g_loss: 5.752384185791016\n","Epoch 78/4000, Step 16, d_loss: 0.06855598837137222, g_loss: 5.534911155700684\n","Epoch 78/4000, Step 17, d_loss: 0.19721384346485138, g_loss: 7.522553443908691\n","Epoch 78/4000, Step 18, d_loss: 0.41523411870002747, g_loss: 8.447932243347168\n","Epoch 78/4000, Step 19, d_loss: 0.01815619505941868, g_loss: 6.104454040527344\n","Epoch 78/4000, Step 20, d_loss: 0.2937706708908081, g_loss: 3.2559516429901123\n","Epoch 78/4000, Step 21, d_loss: 0.006349598988890648, g_loss: 4.165660858154297\n","Epoch 78/4000, Step 22, d_loss: 0.15754668414592743, g_loss: 2.1114730834960938\n","Epoch 78/4000, Step 23, d_loss: 0.016717320308089256, g_loss: 2.943511962890625\n","Epoch 78/4000, Step 24, d_loss: 0.11110904067754745, g_loss: 4.49960994720459\n","Epoch 78/4000, Step 25, d_loss: 0.041277483105659485, g_loss: 4.816433429718018\n","Epoch 78/4000, Step 26, d_loss: 0.16362722218036652, g_loss: 4.3953962326049805\n","Epoch 78/4000, Step 27, d_loss: 0.1013878658413887, g_loss: 2.7313919067382812\n","Epoch 78/4000, Step 28, d_loss: 0.2574205696582794, g_loss: 2.9498236179351807\n","Epoch 78/4000, Step 29, d_loss: 0.2234412282705307, g_loss: 3.7204389572143555\n","Epoch 78/4000, Step 30, d_loss: 0.04956146329641342, g_loss: 1.2639589309692383\n","Epoch 78/4000, Step 31, d_loss: 0.28203773498535156, g_loss: 5.324682712554932\n","Epoch 78/4000, Step 32, d_loss: 0.07522844523191452, g_loss: 2.839249849319458\n","Epoch 78/4000, Step 33, d_loss: 0.19938582181930542, g_loss: 4.588972568511963\n","Epoch 78/4000, Step 34, d_loss: 0.09171472489833832, g_loss: 4.457024574279785\n","Epoch 78/4000, Step 35, d_loss: 0.09360985457897186, g_loss: 6.510836601257324\n","Epoch 78/4000, Step 36, d_loss: 0.012073858641088009, g_loss: 7.062833786010742\n","Epoch 78/4000, Step 37, d_loss: 0.013541054911911488, g_loss: 4.80386209487915\n","Epoch 78/4000, Step 38, d_loss: 0.011381808668375015, g_loss: 7.958416938781738\n","Epoch 78/4000, Step 39, d_loss: 0.0048056296072900295, g_loss: 7.239572048187256\n","Epoch 78/4000, Step 40, d_loss: 0.013375558890402317, g_loss: 7.998876094818115\n","Epoch 78/4000, Step 41, d_loss: 0.004469447303563356, g_loss: 5.97886848449707\n","Epoch 78/4000, Step 42, d_loss: 0.08995461463928223, g_loss: 10.744274139404297\n","Epoch 78/4000, Step 43, d_loss: 0.06608464568853378, g_loss: 6.618858814239502\n","Epoch 78/4000, Step 44, d_loss: 0.023482607677578926, g_loss: 4.574156284332275\n","Epoch 78/4000, Step 45, d_loss: 0.011834238655865192, g_loss: 6.0590410232543945\n","Epoch 78/4000, Step 46, d_loss: 0.06276853382587433, g_loss: 4.610929489135742\n","Epoch 78/4000, Step 47, d_loss: 0.008778667077422142, g_loss: 3.645341634750366\n","Epoch 78/4000, Step 48, d_loss: 0.0039048504550009966, g_loss: 2.4267477989196777\n","Epoch 78/4000, Step 49, d_loss: 0.32424962520599365, g_loss: 2.1781606674194336\n","Epoch 78/4000, Step 50, d_loss: 0.31973695755004883, g_loss: 7.223025321960449\n","Epoch 78/4000, Step 51, d_loss: 0.0821964293718338, g_loss: 6.6283650398254395\n","Epoch 78/4000, Step 52, d_loss: 0.2730102241039276, g_loss: 6.8389892578125\n","Epoch 78/4000, Step 53, d_loss: 0.0037147067487239838, g_loss: 8.458358764648438\n","Epoch 78/4000, Step 54, d_loss: 0.01501369383186102, g_loss: 10.130743980407715\n","Epoch 78/4000, Step 55, d_loss: 0.3188818395137787, g_loss: 7.605804443359375\n","Epoch 78/4000, Step 56, d_loss: 0.07131437957286835, g_loss: 6.3714094161987305\n","Epoch 78/4000, Step 57, d_loss: 0.011139719747006893, g_loss: 5.862143039703369\n","Epoch 78/4000, Step 58, d_loss: 0.012465953826904297, g_loss: 7.523632526397705\n","Epoch 78/4000, Step 59, d_loss: 0.051071297377347946, g_loss: 6.936703681945801\n","Epoch 78/4000, Step 60, d_loss: 0.02496403269469738, g_loss: 5.489250659942627\n","Epoch 78/4000, Step 61, d_loss: 0.3112512230873108, g_loss: 3.9881322383880615\n","Epoch 78/4000, Step 62, d_loss: 0.0527382455766201, g_loss: 6.226698398590088\n","Epoch 78/4000, Step 63, d_loss: 0.0026766913942992687, g_loss: 4.522738456726074\n","Epoch 78/4000, Step 64, d_loss: 0.010118627920746803, g_loss: 4.126391887664795\n","Epoch 78/4000, Step 65, d_loss: 0.1032743752002716, g_loss: 8.058719635009766\n","Epoch 78/4000, Step 66, d_loss: 0.032692503184080124, g_loss: 3.841506004333496\n","Epoch 78/4000, Step 67, d_loss: 0.014355501160025597, g_loss: 4.8958892822265625\n","Epoch 78/4000, Step 68, d_loss: 0.005394000094383955, g_loss: 2.4017715454101562\n","Epoch 78/4000, Step 69, d_loss: 0.08365843445062637, g_loss: 3.6699888706207275\n","Epoch 78/4000, Step 70, d_loss: 0.018757157027721405, g_loss: 4.700655937194824\n","Epoch 78/4000, Step 71, d_loss: 0.0630236268043518, g_loss: 2.9725184440612793\n","Epoch 78/4000, Step 72, d_loss: 0.024721406400203705, g_loss: 4.4348626136779785\n","Epoch 78/4000, Step 73, d_loss: 0.04320251941680908, g_loss: 4.050498962402344\n","Epoch 78/4000, Step 74, d_loss: 0.0907832682132721, g_loss: 3.1560561656951904\n","Epoch 78/4000, Step 75, d_loss: 0.08790506422519684, g_loss: 6.983083248138428\n","Epoch 78/4000, Step 76, d_loss: 0.0011926323641091585, g_loss: 3.443054437637329\n","Epoch 78/4000, Step 77, d_loss: 0.03272096812725067, g_loss: 5.48709774017334\n","Epoch 78/4000, Step 78, d_loss: 0.0076864855363965034, g_loss: 7.681484222412109\n","Epoch 78/4000, Step 79, d_loss: 0.01786664128303528, g_loss: 3.5858707427978516\n","Epoch 78/4000, Step 80, d_loss: 0.008063938468694687, g_loss: 3.463573694229126\n","Epoch 78/4000, Step 81, d_loss: 0.06711497157812119, g_loss: 8.456229209899902\n","Epoch 78/4000, Step 82, d_loss: 0.021554777398705482, g_loss: 8.74799633026123\n","Epoch 78/4000, Step 83, d_loss: 0.05840989574790001, g_loss: 6.175121307373047\n","Epoch 78/4000, Step 84, d_loss: 0.016674913465976715, g_loss: 7.134453296661377\n","Epoch 78/4000, Step 85, d_loss: 0.004873581696301699, g_loss: 5.010305881500244\n","Epoch 78/4000, Step 86, d_loss: 0.020406194031238556, g_loss: 6.905454635620117\n","Epoch 78/4000, Step 87, d_loss: 0.004019543994218111, g_loss: 7.640527248382568\n","Epoch 78/4000, Step 88, d_loss: 0.049142755568027496, g_loss: 4.73850679397583\n","Epoch 78/4000, Step 89, d_loss: 0.0006397435208782554, g_loss: 5.1650919914245605\n","Epoch 78/4000, Step 90, d_loss: 0.008418996818363667, g_loss: 7.1763482093811035\n","Epoch 78/4000, Step 91, d_loss: 0.018743200227618217, g_loss: 4.227294445037842\n","Epoch 78/4000, Step 92, d_loss: 0.02883816510438919, g_loss: 7.347538948059082\n","Epoch 78/4000, Step 93, d_loss: 0.040651582181453705, g_loss: 5.1312103271484375\n","Epoch 78/4000, Step 94, d_loss: 0.00982241053134203, g_loss: 6.619992733001709\n","Epoch 78/4000, Step 95, d_loss: 0.01743626967072487, g_loss: 6.776913642883301\n","Epoch 78/4000, Step 96, d_loss: 0.0031333952210843563, g_loss: 4.638512134552002\n","Epoch 78/4000, Step 97, d_loss: 0.004751001484692097, g_loss: 4.515561580657959\n","Epoch 78/4000, Step 98, d_loss: 0.0035140954423695803, g_loss: 5.697951316833496\n","Epoch 78/4000, Step 99, d_loss: 0.01577432081103325, g_loss: 8.943888664245605\n","Epoch 78/4000, Step 100, d_loss: 0.03572758287191391, g_loss: 4.914404392242432\n","Epoch 78/4000, Step 101, d_loss: 0.002960900543257594, g_loss: 6.159784317016602\n","Epoch 78/4000, Step 102, d_loss: 0.008705882355570793, g_loss: 4.771325588226318\n","Epoch 78/4000, Step 103, d_loss: 0.0377969816327095, g_loss: 6.40615177154541\n","Epoch 78/4000, Step 104, d_loss: 0.01592208817601204, g_loss: 5.277617454528809\n","Epoch 78/4000, Step 105, d_loss: 0.0021004867739975452, g_loss: 7.325811386108398\n","Epoch 78/4000, Step 106, d_loss: 0.006986008491367102, g_loss: 7.437662601470947\n","Epoch 78/4000, Step 107, d_loss: 0.003197359386831522, g_loss: 3.9093945026397705\n","Epoch 78/4000, Step 108, d_loss: 0.006721475627273321, g_loss: 5.232150554656982\n","Epoch 78/4000, Step 109, d_loss: 0.007880384102463722, g_loss: 3.366506814956665\n","Epoch 78/4000, Step 110, d_loss: 0.010302097536623478, g_loss: 6.740121364593506\n","Epoch 78/4000, Step 111, d_loss: 0.028408415615558624, g_loss: 5.599329471588135\n","Epoch 78/4000, Step 112, d_loss: 0.009699729271233082, g_loss: 7.597819805145264\n","Epoch 78/4000, Step 113, d_loss: 0.008800794370472431, g_loss: 4.073420524597168\n","Epoch 78/4000, Step 114, d_loss: 0.053151071071624756, g_loss: 7.931581020355225\n","Epoch 78/4000, Step 115, d_loss: 0.008495760150253773, g_loss: 4.78119421005249\n","Epoch 78/4000, Step 116, d_loss: 0.008083808235824108, g_loss: 3.6917426586151123\n","Epoch 78/4000, Step 117, d_loss: 0.011388582177460194, g_loss: 7.735625267028809\n","Epoch 78/4000, Step 118, d_loss: 0.0031286021694540977, g_loss: 5.9337544441223145\n","Epoch 78/4000, Step 119, d_loss: 0.0017541234847158194, g_loss: 8.122184753417969\n","Epoch 78/4000, Step 120, d_loss: 0.017179027199745178, g_loss: 3.9874231815338135\n","Epoch 78/4000, Step 121, d_loss: 0.0042517101392149925, g_loss: 4.152745723724365\n","Epoch 78/4000, Step 122, d_loss: 0.0020851437002420425, g_loss: 5.861247539520264\n","Epoch 78/4000, Step 123, d_loss: 0.012235663831233978, g_loss: 3.8428456783294678\n","Epoch 78/4000, Step 124, d_loss: 0.00100375444162637, g_loss: 5.2283453941345215\n","Epoch 78/4000, Step 125, d_loss: 0.022680213674902916, g_loss: 6.268710613250732\n","Epoch 78/4000, Step 126, d_loss: 0.003954891115427017, g_loss: 7.825181484222412\n","Epoch 78/4000, Step 127, d_loss: 0.014001882635056973, g_loss: 3.869504928588867\n","Epoch 78/4000, Step 128, d_loss: 0.00348015408962965, g_loss: 6.664613246917725\n","Epoch 78/4000, Step 129, d_loss: 0.006014036014676094, g_loss: 4.168478965759277\n","Epoch 78/4000, Step 130, d_loss: 0.003063205163925886, g_loss: 7.180822849273682\n","Epoch 78/4000, Step 131, d_loss: 0.002201683120802045, g_loss: 6.116507053375244\n","Epoch 78/4000, Step 132, d_loss: 0.017262326553463936, g_loss: 6.754629611968994\n","Epoch 78/4000, Step 133, d_loss: 0.0467529334127903, g_loss: 5.87957763671875\n","Epoch 78/4000, Step 134, d_loss: 0.0027312024030834436, g_loss: 10.050196647644043\n","Epoch 78/4000, Step 135, d_loss: 0.0010719387792050838, g_loss: 5.330951690673828\n","Epoch 78/4000, Step 136, d_loss: 0.0014667895156890154, g_loss: 7.4789910316467285\n","Epoch 78/4000, Step 137, d_loss: 0.02160556986927986, g_loss: 5.865703582763672\n","Epoch 78/4000, Step 138, d_loss: 0.0008561544818803668, g_loss: 7.840490341186523\n","Epoch 78/4000, Step 139, d_loss: 0.0045189522206783295, g_loss: 6.316807270050049\n","Epoch 78/4000, Step 140, d_loss: 0.006366755813360214, g_loss: 4.439157485961914\n","Epoch 78/4000, Step 141, d_loss: 0.007056629750877619, g_loss: 4.878556251525879\n","Epoch 78/4000, Step 142, d_loss: 0.004289754666388035, g_loss: 4.13538122177124\n","Epoch 78/4000, Step 143, d_loss: 0.005150315351784229, g_loss: 7.3052167892456055\n","Epoch 78/4000, Step 144, d_loss: 0.02858225256204605, g_loss: 4.858553409576416\n","Epoch 78/4000, Step 145, d_loss: 0.001674241153523326, g_loss: 6.636137962341309\n","Epoch 78/4000, Step 146, d_loss: 0.01082096341997385, g_loss: 3.5388307571411133\n","Epoch 78/4000, Step 147, d_loss: 0.002631089184433222, g_loss: 5.2909040451049805\n","Epoch 78/4000, Step 148, d_loss: 0.0027938028797507286, g_loss: 5.066379070281982\n","Epoch 78/4000, Step 149, d_loss: 0.0036390803288668394, g_loss: 5.748500347137451\n","Epoch 78/4000, Step 150, d_loss: 0.004374954383820295, g_loss: 4.748964309692383\n","Epoch 78/4000, Step 151, d_loss: 0.0035701850429177284, g_loss: 5.092940330505371\n","Epoch 78/4000, Step 152, d_loss: 0.00497933104634285, g_loss: 6.780303478240967\n","Epoch 78/4000, Step 153, d_loss: 0.02636643499135971, g_loss: 5.1684112548828125\n","Epoch 78/4000, Step 154, d_loss: 0.007220396306365728, g_loss: 6.745760917663574\n","Epoch 78/4000, Step 155, d_loss: 0.003924164921045303, g_loss: 7.634119987487793\n","Epoch 78/4000, Step 156, d_loss: 0.004117348697036505, g_loss: 5.551905632019043\n","Epoch 78/4000, Step 157, d_loss: 0.0038791107945144176, g_loss: 8.236988067626953\n","Epoch 78/4000, Step 158, d_loss: 0.00861163716763258, g_loss: 6.30005407333374\n","Epoch 78/4000, Step 159, d_loss: 0.007677757181227207, g_loss: 8.727524757385254\n","Epoch 78/4000, Step 160, d_loss: 0.005960012786090374, g_loss: 6.380613327026367\n","Epoch 78/4000, Step 161, d_loss: 0.00957315880805254, g_loss: 7.765683174133301\n","Epoch 78/4000, Step 162, d_loss: 0.003052887972444296, g_loss: 7.493574142456055\n","Epoch 78/4000, Step 163, d_loss: 0.005765758454799652, g_loss: 7.423686504364014\n","Epoch 78/4000, Step 164, d_loss: 0.017311260104179382, g_loss: 7.106355667114258\n","Epoch 78/4000, Step 165, d_loss: 0.015885164961218834, g_loss: 7.31644868850708\n","Epoch 78/4000, Step 166, d_loss: 0.0018021751893684268, g_loss: 9.579961776733398\n","Epoch 78/4000, Step 167, d_loss: 0.0025662952102720737, g_loss: 5.862051963806152\n","Epoch 78/4000, Step 168, d_loss: 0.009159021079540253, g_loss: 11.207146644592285\n","Epoch 78/4000, Step 169, d_loss: 0.004414343275129795, g_loss: 8.72144889831543\n","Epoch 78/4000, Step 170, d_loss: 0.0009938126895576715, g_loss: 5.850698471069336\n","Epoch 78/4000, Step 171, d_loss: 0.0027964019682258368, g_loss: 8.970220565795898\n","Epoch 78/4000, Step 172, d_loss: 0.007619846612215042, g_loss: 5.5792412757873535\n","Epoch 78/4000, Step 173, d_loss: 0.0031288648024201393, g_loss: 6.472191333770752\n","Epoch 78/4000, Step 174, d_loss: 0.018725648522377014, g_loss: 7.062027931213379\n","Epoch 78/4000, Step 175, d_loss: 0.006828479468822479, g_loss: 5.195119380950928\n","Epoch 78/4000, Step 176, d_loss: 0.0023883639369159937, g_loss: 7.44380521774292\n","Epoch 78/4000, Step 177, d_loss: 0.016195742413401604, g_loss: 7.536441326141357\n","Epoch 78/4000, Step 178, d_loss: 0.004244886338710785, g_loss: 5.997412204742432\n","Epoch 78/4000, Step 179, d_loss: 0.0043301354162395, g_loss: 6.176575183868408\n","Epoch 78/4000, Step 180, d_loss: 0.001960552064701915, g_loss: 3.268108606338501\n","Epoch 78/4000, Step 181, d_loss: 0.008279991336166859, g_loss: 9.447548866271973\n","Epoch 78/4000, Step 182, d_loss: 0.06277108192443848, g_loss: 3.98223876953125\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 79/4000, Step 1, d_loss: 0.007469738833606243, g_loss: 5.0881452560424805\n","Epoch 79/4000, Step 2, d_loss: 0.011655889451503754, g_loss: 8.77565860748291\n","Epoch 79/4000, Step 3, d_loss: 0.007557249162346125, g_loss: 5.79460334777832\n","Epoch 79/4000, Step 4, d_loss: 0.0013105273246765137, g_loss: 6.240525722503662\n","Epoch 79/4000, Step 5, d_loss: 0.0046026986092329025, g_loss: 4.087965488433838\n","Epoch 79/4000, Step 6, d_loss: 0.016996584832668304, g_loss: 7.369357109069824\n","Epoch 79/4000, Step 7, d_loss: 0.0046941121108829975, g_loss: 7.680845737457275\n","Epoch 79/4000, Step 8, d_loss: 0.019022373482584953, g_loss: 5.01619291305542\n","Epoch 79/4000, Step 9, d_loss: 0.009059125557541847, g_loss: 5.091771602630615\n","Epoch 79/4000, Step 10, d_loss: 0.029843173921108246, g_loss: 4.637382984161377\n","Epoch 79/4000, Step 11, d_loss: 0.011462881229817867, g_loss: 5.727027416229248\n","Epoch 79/4000, Step 12, d_loss: 0.0007451066630892456, g_loss: 5.977720260620117\n","Epoch 79/4000, Step 13, d_loss: 0.004065028391778469, g_loss: 5.382761478424072\n","Epoch 79/4000, Step 14, d_loss: 0.007158992812037468, g_loss: 6.233194351196289\n","Epoch 79/4000, Step 15, d_loss: 0.0016143453540280461, g_loss: 5.307171821594238\n","Epoch 79/4000, Step 16, d_loss: 0.007891356945037842, g_loss: 5.815605640411377\n","Epoch 79/4000, Step 17, d_loss: 0.018471065908670425, g_loss: 6.034679412841797\n","Epoch 79/4000, Step 18, d_loss: 0.012789205648005009, g_loss: 8.376428604125977\n","Epoch 79/4000, Step 19, d_loss: 0.0030493522062897682, g_loss: 6.21066427230835\n","Epoch 79/4000, Step 20, d_loss: 0.004671673756092787, g_loss: 3.995694398880005\n","Epoch 79/4000, Step 21, d_loss: 0.0048385062254965305, g_loss: 6.914921760559082\n","Epoch 79/4000, Step 22, d_loss: 0.00254492717795074, g_loss: 3.9985568523406982\n","Epoch 79/4000, Step 23, d_loss: 0.01665479689836502, g_loss: 5.435983657836914\n","Epoch 79/4000, Step 24, d_loss: 0.006311782635748386, g_loss: 6.967686176300049\n","Epoch 79/4000, Step 25, d_loss: 0.0018832471687346697, g_loss: 7.1427531242370605\n","Epoch 79/4000, Step 26, d_loss: 0.002692821901291609, g_loss: 8.062673568725586\n","Epoch 79/4000, Step 27, d_loss: 0.018607839941978455, g_loss: 5.052048683166504\n","Epoch 79/4000, Step 28, d_loss: 0.008295351639389992, g_loss: 5.853362083435059\n","Epoch 79/4000, Step 29, d_loss: 0.009260199032723904, g_loss: 5.955772399902344\n","Epoch 79/4000, Step 30, d_loss: 0.021603472530841827, g_loss: 5.563210964202881\n","Epoch 79/4000, Step 31, d_loss: 0.005043602082878351, g_loss: 5.707935810089111\n","Epoch 79/4000, Step 32, d_loss: 0.008522734977304935, g_loss: 4.87562894821167\n","Epoch 79/4000, Step 33, d_loss: 0.0085038673132658, g_loss: 6.779362201690674\n","Epoch 79/4000, Step 34, d_loss: 0.011960241012275219, g_loss: 6.284219741821289\n","Epoch 79/4000, Step 35, d_loss: 0.006941632367670536, g_loss: 9.717992782592773\n","Epoch 79/4000, Step 36, d_loss: 0.0039167460054159164, g_loss: 5.343694686889648\n","Epoch 79/4000, Step 37, d_loss: 0.004676312208175659, g_loss: 6.363014221191406\n","Epoch 79/4000, Step 38, d_loss: 0.005870359484106302, g_loss: 5.337096214294434\n","Epoch 79/4000, Step 39, d_loss: 0.007174767088145018, g_loss: 5.646824359893799\n","Epoch 79/4000, Step 40, d_loss: 0.001628883765079081, g_loss: 4.527445316314697\n","Epoch 79/4000, Step 41, d_loss: 0.005921831354498863, g_loss: 8.367265701293945\n","Epoch 79/4000, Step 42, d_loss: 0.009585944004356861, g_loss: 5.598920822143555\n","Epoch 79/4000, Step 43, d_loss: 0.00970332883298397, g_loss: 5.473097801208496\n","Epoch 79/4000, Step 44, d_loss: 0.00681920163333416, g_loss: 6.002124786376953\n","Epoch 79/4000, Step 45, d_loss: 0.004746744409203529, g_loss: 7.284762382507324\n","Epoch 79/4000, Step 46, d_loss: 0.0028918273746967316, g_loss: 7.062551498413086\n","Epoch 79/4000, Step 47, d_loss: 0.00407303124666214, g_loss: 5.994924545288086\n","Epoch 79/4000, Step 48, d_loss: 0.0005938519025221467, g_loss: 7.728866100311279\n","Epoch 79/4000, Step 49, d_loss: 0.002630384638905525, g_loss: 8.03453540802002\n","Epoch 79/4000, Step 50, d_loss: 0.0007427257369272411, g_loss: 7.5314106941223145\n","Epoch 79/4000, Step 51, d_loss: 0.0033479572739452124, g_loss: 6.874130725860596\n","Epoch 79/4000, Step 52, d_loss: 0.0005148320342414081, g_loss: 7.6969733238220215\n","Epoch 79/4000, Step 53, d_loss: 0.010031361132860184, g_loss: 5.862683296203613\n","Epoch 79/4000, Step 54, d_loss: 0.0036430798936635256, g_loss: 5.565249443054199\n","Epoch 79/4000, Step 55, d_loss: 0.0031272200867533684, g_loss: 5.727181911468506\n","Epoch 79/4000, Step 56, d_loss: 0.006892675533890724, g_loss: 7.419275760650635\n","Epoch 79/4000, Step 57, d_loss: 0.003247619606554508, g_loss: 5.7087507247924805\n","Epoch 79/4000, Step 58, d_loss: 0.006598942447453737, g_loss: 6.283891201019287\n","Epoch 79/4000, Step 59, d_loss: 0.0028359834104776382, g_loss: 6.35363245010376\n","Epoch 79/4000, Step 60, d_loss: 0.0010875609004870057, g_loss: 7.568132400512695\n","Epoch 79/4000, Step 61, d_loss: 0.011510496959090233, g_loss: 5.7798895835876465\n","Epoch 79/4000, Step 62, d_loss: 0.004822299815714359, g_loss: 6.2743353843688965\n","Epoch 79/4000, Step 63, d_loss: 0.0012021262664347887, g_loss: 8.885563850402832\n","Epoch 79/4000, Step 64, d_loss: 0.002165617188438773, g_loss: 6.4577717781066895\n","Epoch 79/4000, Step 65, d_loss: 0.011763113550841808, g_loss: 6.222318172454834\n","Epoch 79/4000, Step 66, d_loss: 0.015812203288078308, g_loss: 4.0131611824035645\n","Epoch 79/4000, Step 67, d_loss: 0.007322671823203564, g_loss: 6.543131351470947\n","Epoch 79/4000, Step 68, d_loss: 0.012108489871025085, g_loss: 6.6984663009643555\n","Epoch 79/4000, Step 69, d_loss: 0.004379766061902046, g_loss: 6.5726399421691895\n","Epoch 79/4000, Step 70, d_loss: 0.0061301905661821365, g_loss: 6.7334113121032715\n","Epoch 79/4000, Step 71, d_loss: 0.014547782950103283, g_loss: 6.335273265838623\n","Epoch 79/4000, Step 72, d_loss: 0.003064615884795785, g_loss: 6.286925315856934\n","Epoch 79/4000, Step 73, d_loss: 0.006574086379259825, g_loss: 8.218392372131348\n","Epoch 79/4000, Step 74, d_loss: 0.003693697042763233, g_loss: 5.733048915863037\n","Epoch 79/4000, Step 75, d_loss: 0.0060761128552258015, g_loss: 5.704401969909668\n","Epoch 79/4000, Step 76, d_loss: 0.0014946218580007553, g_loss: 9.385102272033691\n","Epoch 79/4000, Step 77, d_loss: 0.0024311281740665436, g_loss: 6.969799041748047\n","Epoch 79/4000, Step 78, d_loss: 0.0018689646385610104, g_loss: 6.149013996124268\n","Epoch 79/4000, Step 79, d_loss: 0.0021391035988926888, g_loss: 3.6803879737854004\n","Epoch 79/4000, Step 80, d_loss: 0.024590738117694855, g_loss: 6.678072929382324\n","Epoch 79/4000, Step 81, d_loss: 0.0013273388613015413, g_loss: 5.571520805358887\n","Epoch 79/4000, Step 82, d_loss: 0.002319319173693657, g_loss: 7.225725173950195\n","Epoch 79/4000, Step 83, d_loss: 0.006673261523246765, g_loss: 6.573730945587158\n","Epoch 79/4000, Step 84, d_loss: 0.0017516135703772306, g_loss: 7.1382575035095215\n","Epoch 79/4000, Step 85, d_loss: 0.01375177688896656, g_loss: 4.839036464691162\n","Epoch 79/4000, Step 86, d_loss: 0.008321833796799183, g_loss: 4.4458746910095215\n","Epoch 79/4000, Step 87, d_loss: 0.0038908133283257484, g_loss: 5.273784637451172\n","Epoch 79/4000, Step 88, d_loss: 0.003383179660886526, g_loss: 4.8890790939331055\n","Epoch 79/4000, Step 89, d_loss: 0.00441651651635766, g_loss: 3.8635945320129395\n","Epoch 79/4000, Step 90, d_loss: 0.0005438537336885929, g_loss: 10.153364181518555\n","Epoch 79/4000, Step 91, d_loss: 0.014727331697940826, g_loss: 8.626021385192871\n","Epoch 79/4000, Step 92, d_loss: 0.0009629105334170163, g_loss: 5.788287162780762\n","Epoch 79/4000, Step 93, d_loss: 0.003979517612606287, g_loss: 8.409096717834473\n","Epoch 79/4000, Step 94, d_loss: 0.010543614625930786, g_loss: 4.331653118133545\n","Epoch 79/4000, Step 95, d_loss: 0.006119487341493368, g_loss: 5.864073276519775\n","Epoch 79/4000, Step 96, d_loss: 0.003572702407836914, g_loss: 5.314452171325684\n","Epoch 79/4000, Step 97, d_loss: 0.00883507076650858, g_loss: 6.597530364990234\n","Epoch 79/4000, Step 98, d_loss: 0.004181206226348877, g_loss: 6.308919429779053\n","Epoch 79/4000, Step 99, d_loss: 0.0049551380798220634, g_loss: 4.152821063995361\n","Epoch 79/4000, Step 100, d_loss: 0.006520473398268223, g_loss: 7.040928840637207\n","Epoch 79/4000, Step 101, d_loss: 0.0006827531033195555, g_loss: 5.690103054046631\n","Epoch 79/4000, Step 102, d_loss: 0.0016408406663686037, g_loss: 6.861415863037109\n","Epoch 79/4000, Step 103, d_loss: 0.005927998572587967, g_loss: 5.15752649307251\n","Epoch 79/4000, Step 104, d_loss: 0.00574877904728055, g_loss: 8.383298873901367\n","Epoch 79/4000, Step 105, d_loss: 0.011556996032595634, g_loss: 6.712530136108398\n","Epoch 79/4000, Step 106, d_loss: 0.004466690123081207, g_loss: 6.772589206695557\n","Epoch 79/4000, Step 107, d_loss: 0.00349693326279521, g_loss: 8.200154304504395\n","Epoch 79/4000, Step 108, d_loss: 0.004607187118381262, g_loss: 7.322259426116943\n","Epoch 79/4000, Step 109, d_loss: 0.0007898256299085915, g_loss: 9.172924995422363\n","Epoch 79/4000, Step 110, d_loss: 0.0023703614715486765, g_loss: 6.9313740730285645\n","Epoch 79/4000, Step 111, d_loss: 0.002001119777560234, g_loss: 6.065241813659668\n","Epoch 79/4000, Step 112, d_loss: 0.002445569494739175, g_loss: 7.928832054138184\n","Epoch 79/4000, Step 113, d_loss: 0.0004720567958429456, g_loss: 5.912880897521973\n","Epoch 79/4000, Step 114, d_loss: 0.0009611794375814497, g_loss: 8.12162971496582\n","Epoch 79/4000, Step 115, d_loss: 0.0030556709971278906, g_loss: 7.910837650299072\n","Epoch 79/4000, Step 116, d_loss: 0.012211514636874199, g_loss: 7.328815460205078\n","Epoch 79/4000, Step 117, d_loss: 0.0010907050454989076, g_loss: 7.176000118255615\n","Epoch 79/4000, Step 118, d_loss: 0.006573699414730072, g_loss: 6.89539098739624\n","Epoch 79/4000, Step 119, d_loss: 0.002973796334117651, g_loss: 6.0556769371032715\n","Epoch 79/4000, Step 120, d_loss: 0.003269895678386092, g_loss: 7.762801170349121\n","Epoch 79/4000, Step 121, d_loss: 0.0012622143840417266, g_loss: 5.570656776428223\n","Epoch 79/4000, Step 122, d_loss: 0.001234274124726653, g_loss: 9.214194297790527\n","Epoch 79/4000, Step 123, d_loss: 0.002054760232567787, g_loss: 6.427164554595947\n","Epoch 79/4000, Step 124, d_loss: 0.0032516205683350563, g_loss: 6.858023166656494\n","Epoch 79/4000, Step 125, d_loss: 0.002953515388071537, g_loss: 6.961543083190918\n","Epoch 79/4000, Step 126, d_loss: 0.002748573198914528, g_loss: 5.424597263336182\n","Epoch 79/4000, Step 127, d_loss: 0.001622106647118926, g_loss: 7.082333087921143\n","Epoch 79/4000, Step 128, d_loss: 0.001400893903337419, g_loss: 6.4479851722717285\n","Epoch 79/4000, Step 129, d_loss: 0.005145433824509382, g_loss: 6.438745498657227\n","Epoch 79/4000, Step 130, d_loss: 0.0008548907935619354, g_loss: 8.821737289428711\n","Epoch 79/4000, Step 131, d_loss: 0.0031220675446093082, g_loss: 6.026634216308594\n","Epoch 79/4000, Step 132, d_loss: 0.0027618040330708027, g_loss: 6.147302150726318\n","Epoch 79/4000, Step 133, d_loss: 0.0010476666502654552, g_loss: 8.85027027130127\n","Epoch 79/4000, Step 134, d_loss: 0.014602704904973507, g_loss: 7.73616361618042\n","Epoch 79/4000, Step 135, d_loss: 0.0030645891092717648, g_loss: 7.8352742195129395\n","Epoch 79/4000, Step 136, d_loss: 0.005890889558941126, g_loss: 7.587010383605957\n","Epoch 79/4000, Step 137, d_loss: 0.007829234004020691, g_loss: 6.2830915451049805\n","Epoch 79/4000, Step 138, d_loss: 0.0004471014253795147, g_loss: 7.145460605621338\n","Epoch 79/4000, Step 139, d_loss: 0.002872900804504752, g_loss: 7.367627143859863\n","Epoch 79/4000, Step 140, d_loss: 0.001137416111305356, g_loss: 7.237310886383057\n","Epoch 79/4000, Step 141, d_loss: 0.002255707746371627, g_loss: 7.509122371673584\n","Epoch 79/4000, Step 142, d_loss: 0.0011916467919945717, g_loss: 6.034158229827881\n","Epoch 79/4000, Step 143, d_loss: 0.002236980712041259, g_loss: 5.736731052398682\n","Epoch 79/4000, Step 144, d_loss: 0.00881637167185545, g_loss: 8.512171745300293\n","Epoch 79/4000, Step 145, d_loss: 0.012291783466935158, g_loss: 6.391329288482666\n","Epoch 79/4000, Step 146, d_loss: 0.008928291499614716, g_loss: 7.521763801574707\n","Epoch 79/4000, Step 147, d_loss: 0.0014735362492501736, g_loss: 4.580231666564941\n","Epoch 79/4000, Step 148, d_loss: 0.0017613893141970038, g_loss: 9.418092727661133\n","Epoch 79/4000, Step 149, d_loss: 0.0036354572512209415, g_loss: 6.6516032218933105\n","Epoch 79/4000, Step 150, d_loss: 0.004610216710716486, g_loss: 6.512400150299072\n","Epoch 79/4000, Step 151, d_loss: 0.001900545903481543, g_loss: 6.456818103790283\n","Epoch 79/4000, Step 152, d_loss: 0.008978689089417458, g_loss: 6.08156156539917\n","Epoch 79/4000, Step 153, d_loss: 0.03350665420293808, g_loss: 6.43578577041626\n","Epoch 79/4000, Step 154, d_loss: 0.002793612191453576, g_loss: 5.899575233459473\n","Epoch 79/4000, Step 155, d_loss: 0.0007536500925198197, g_loss: 6.636089324951172\n","Epoch 79/4000, Step 156, d_loss: 0.0025898353196680546, g_loss: 7.191001892089844\n","Epoch 79/4000, Step 157, d_loss: 0.009382128715515137, g_loss: 6.2477946281433105\n","Epoch 79/4000, Step 158, d_loss: 0.010227968916296959, g_loss: 4.848682880401611\n","Epoch 79/4000, Step 159, d_loss: 0.0007799129816703498, g_loss: 6.322986125946045\n","Epoch 79/4000, Step 160, d_loss: 0.00800509937107563, g_loss: 5.729883670806885\n","Epoch 79/4000, Step 161, d_loss: 0.0018869778141379356, g_loss: 6.067067623138428\n","Epoch 79/4000, Step 162, d_loss: 0.015890859067440033, g_loss: 5.006776332855225\n","Epoch 79/4000, Step 163, d_loss: 0.008865483105182648, g_loss: 5.019589424133301\n","Epoch 79/4000, Step 164, d_loss: 0.008981248363852501, g_loss: 5.1100754737854\n","Epoch 79/4000, Step 165, d_loss: 0.0021964458283036947, g_loss: 6.21467924118042\n","Epoch 79/4000, Step 166, d_loss: 0.0010089854476973414, g_loss: 7.7457594871521\n","Epoch 79/4000, Step 167, d_loss: 0.004496968816965818, g_loss: 5.357170104980469\n","Epoch 79/4000, Step 168, d_loss: 0.0034312608186155558, g_loss: 6.241192817687988\n","Epoch 79/4000, Step 169, d_loss: 0.004776692949235439, g_loss: 7.654007434844971\n","Epoch 79/4000, Step 170, d_loss: 0.004984391387552023, g_loss: 6.1369781494140625\n","Epoch 79/4000, Step 171, d_loss: 0.005647379904985428, g_loss: 6.471158027648926\n","Epoch 79/4000, Step 172, d_loss: 0.012066038325428963, g_loss: 7.248109340667725\n","Epoch 79/4000, Step 173, d_loss: 0.004787384532392025, g_loss: 6.704343795776367\n","Epoch 79/4000, Step 174, d_loss: 0.0017318407772108912, g_loss: 5.812663555145264\n","Epoch 79/4000, Step 175, d_loss: 0.0031725415028631687, g_loss: 7.591801166534424\n","Epoch 79/4000, Step 176, d_loss: 0.007602518890053034, g_loss: 7.504639148712158\n","Epoch 79/4000, Step 177, d_loss: 0.002732228022068739, g_loss: 8.374981880187988\n","Epoch 79/4000, Step 178, d_loss: 0.0007531364681199193, g_loss: 6.765944957733154\n","Epoch 79/4000, Step 179, d_loss: 0.001997745595872402, g_loss: 7.573976516723633\n","Epoch 79/4000, Step 180, d_loss: 0.0007713594241067767, g_loss: 5.868007659912109\n","Epoch 79/4000, Step 181, d_loss: 0.02007542923092842, g_loss: 5.790830612182617\n","Epoch 79/4000, Step 182, d_loss: 0.0010082692606374621, g_loss: 5.2403059005737305\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 80/4000, Step 1, d_loss: 0.0008396164048463106, g_loss: 5.457889080047607\n","Epoch 80/4000, Step 2, d_loss: 0.0033988403156399727, g_loss: 7.254376411437988\n","Epoch 80/4000, Step 3, d_loss: 0.0037773039657622576, g_loss: 6.108012676239014\n","Epoch 80/4000, Step 4, d_loss: 0.00840051844716072, g_loss: 8.438369750976562\n","Epoch 80/4000, Step 5, d_loss: 0.007562852464616299, g_loss: 6.867114543914795\n","Epoch 80/4000, Step 6, d_loss: 0.0017843341920524836, g_loss: 4.780935287475586\n","Epoch 80/4000, Step 7, d_loss: 0.006465390790253878, g_loss: 9.496420860290527\n","Epoch 80/4000, Step 8, d_loss: 0.00495931738987565, g_loss: 5.092668056488037\n","Epoch 80/4000, Step 9, d_loss: 0.010344273410737514, g_loss: 6.107370376586914\n","Epoch 80/4000, Step 10, d_loss: 0.002347073284909129, g_loss: 4.926737308502197\n","Epoch 80/4000, Step 11, d_loss: 0.0002384302788414061, g_loss: 6.891499996185303\n","Epoch 80/4000, Step 12, d_loss: 0.0014979424886405468, g_loss: 4.39719295501709\n","Epoch 80/4000, Step 13, d_loss: 0.006577884778380394, g_loss: 6.12294340133667\n","Epoch 80/4000, Step 14, d_loss: 0.002573117148131132, g_loss: 5.644351005554199\n","Epoch 80/4000, Step 15, d_loss: 0.0031648408621549606, g_loss: 8.420625686645508\n","Epoch 80/4000, Step 16, d_loss: 0.0019730706699192524, g_loss: 7.031374454498291\n","Epoch 80/4000, Step 17, d_loss: 0.0022534788586199284, g_loss: 7.573766708374023\n","Epoch 80/4000, Step 18, d_loss: 0.0018833742942661047, g_loss: 7.511276721954346\n","Epoch 80/4000, Step 19, d_loss: 0.002671444322913885, g_loss: 7.736386775970459\n","Epoch 80/4000, Step 20, d_loss: 0.00381808215752244, g_loss: 7.316598415374756\n","Epoch 80/4000, Step 21, d_loss: 0.005857701413333416, g_loss: 6.8978753089904785\n","Epoch 80/4000, Step 22, d_loss: 0.013254384510219097, g_loss: 5.739662170410156\n","Epoch 80/4000, Step 23, d_loss: 0.010281167924404144, g_loss: 5.725570201873779\n","Epoch 80/4000, Step 24, d_loss: 0.0007240797858685255, g_loss: 5.569289207458496\n","Epoch 80/4000, Step 25, d_loss: 0.002531037200242281, g_loss: 9.520120620727539\n","Epoch 80/4000, Step 26, d_loss: 0.0003468834620434791, g_loss: 7.916996479034424\n","Epoch 80/4000, Step 27, d_loss: 0.006434595212340355, g_loss: 8.729504585266113\n","Epoch 80/4000, Step 28, d_loss: 0.005552779417484999, g_loss: 6.026566982269287\n","Epoch 80/4000, Step 29, d_loss: 0.0009411923820152879, g_loss: 5.159076690673828\n","Epoch 80/4000, Step 30, d_loss: 0.001195963704958558, g_loss: 5.640881061553955\n","Epoch 80/4000, Step 31, d_loss: 0.017560556530952454, g_loss: 5.966684818267822\n","Epoch 80/4000, Step 32, d_loss: 0.003336615627631545, g_loss: 6.865861892700195\n","Epoch 80/4000, Step 33, d_loss: 0.0029542632400989532, g_loss: 5.049598217010498\n","Epoch 80/4000, Step 34, d_loss: 0.0020861614029854536, g_loss: 7.2546234130859375\n","Epoch 80/4000, Step 35, d_loss: 0.0008573190425522625, g_loss: 5.611423969268799\n","Epoch 80/4000, Step 36, d_loss: 0.0013347298372536898, g_loss: 8.532365798950195\n","Epoch 80/4000, Step 37, d_loss: 0.0049169245176017284, g_loss: 5.369046688079834\n","Epoch 80/4000, Step 38, d_loss: 0.001406477065756917, g_loss: 6.841092109680176\n","Epoch 80/4000, Step 39, d_loss: 0.0040779500268399715, g_loss: 4.809844493865967\n","Epoch 80/4000, Step 40, d_loss: 0.00584813579916954, g_loss: 6.00046443939209\n","Epoch 80/4000, Step 41, d_loss: 0.002325601875782013, g_loss: 5.515879154205322\n","Epoch 80/4000, Step 42, d_loss: 0.0017952278722077608, g_loss: 4.190333843231201\n","Epoch 80/4000, Step 43, d_loss: 0.0024606927763670683, g_loss: 7.169445037841797\n","Epoch 80/4000, Step 44, d_loss: 0.01835205964744091, g_loss: 6.243651390075684\n","Epoch 80/4000, Step 45, d_loss: 0.007558558601886034, g_loss: 7.361508369445801\n","Epoch 80/4000, Step 46, d_loss: 0.0045044198632240295, g_loss: 9.402469635009766\n","Epoch 80/4000, Step 47, d_loss: 0.005424655973911285, g_loss: 6.071832180023193\n","Epoch 80/4000, Step 48, d_loss: 0.00568716786801815, g_loss: 5.517033576965332\n","Epoch 80/4000, Step 49, d_loss: 0.002310084644705057, g_loss: 5.8462677001953125\n","Epoch 80/4000, Step 50, d_loss: 0.0016907191602513194, g_loss: 6.027974605560303\n","Epoch 80/4000, Step 51, d_loss: 0.0031730257906019688, g_loss: 5.455849647521973\n","Epoch 80/4000, Step 52, d_loss: 0.005455441307276487, g_loss: 5.072087287902832\n","Epoch 80/4000, Step 53, d_loss: 0.0006888561183586717, g_loss: 6.568949222564697\n","Epoch 80/4000, Step 54, d_loss: 0.0014705575304105878, g_loss: 6.8047709465026855\n","Epoch 80/4000, Step 55, d_loss: 0.002278153784573078, g_loss: 5.528651237487793\n","Epoch 80/4000, Step 56, d_loss: 0.003940831869840622, g_loss: 9.458539962768555\n","Epoch 80/4000, Step 57, d_loss: 0.005714733153581619, g_loss: 9.34694766998291\n","Epoch 80/4000, Step 58, d_loss: 0.030546147376298904, g_loss: 4.3887810707092285\n","Epoch 80/4000, Step 59, d_loss: 0.0031311612110584974, g_loss: 6.69156551361084\n","Epoch 80/4000, Step 60, d_loss: 0.0044358475133776665, g_loss: 4.9839911460876465\n","Epoch 80/4000, Step 61, d_loss: 0.0017800343921408057, g_loss: 8.373136520385742\n","Epoch 80/4000, Step 62, d_loss: 0.000527748663444072, g_loss: 5.05153751373291\n","Epoch 80/4000, Step 63, d_loss: 0.0047472319565713406, g_loss: 5.811891078948975\n","Epoch 80/4000, Step 64, d_loss: 0.00992207694798708, g_loss: 7.7349395751953125\n","Epoch 80/4000, Step 65, d_loss: 0.005832475144416094, g_loss: 8.302793502807617\n","Epoch 80/4000, Step 66, d_loss: 0.0033508024644106627, g_loss: 7.207651615142822\n","Epoch 80/4000, Step 67, d_loss: 0.0024712129961699247, g_loss: 6.317724704742432\n","Epoch 80/4000, Step 68, d_loss: 0.0034907839726656675, g_loss: 5.581670761108398\n","Epoch 80/4000, Step 69, d_loss: 0.001275460934266448, g_loss: 7.002481460571289\n","Epoch 80/4000, Step 70, d_loss: 0.018684420734643936, g_loss: 7.093467712402344\n","Epoch 80/4000, Step 71, d_loss: 0.0016162748215720057, g_loss: 5.400097846984863\n","Epoch 80/4000, Step 72, d_loss: 0.0017786526586860418, g_loss: 5.8959269523620605\n","Epoch 80/4000, Step 73, d_loss: 0.003723457921296358, g_loss: 5.038202285766602\n","Epoch 80/4000, Step 74, d_loss: 0.0069029564037919044, g_loss: 5.219404697418213\n","Epoch 80/4000, Step 75, d_loss: 0.00403852853924036, g_loss: 5.40368127822876\n","Epoch 80/4000, Step 76, d_loss: 0.0017918205121532083, g_loss: 8.833921432495117\n","Epoch 80/4000, Step 77, d_loss: 0.006885147653520107, g_loss: 6.994685173034668\n","Epoch 80/4000, Step 78, d_loss: 0.012003013864159584, g_loss: 8.284499168395996\n","Epoch 80/4000, Step 79, d_loss: 0.006024910602718592, g_loss: 6.42005729675293\n","Epoch 80/4000, Step 80, d_loss: 0.035762637853622437, g_loss: 8.14898681640625\n","Epoch 80/4000, Step 81, d_loss: 0.004275244195014238, g_loss: 5.297226428985596\n","Epoch 80/4000, Step 82, d_loss: 0.006151892244815826, g_loss: 6.512172698974609\n","Epoch 80/4000, Step 83, d_loss: 0.0004498799389693886, g_loss: 3.7929890155792236\n","Epoch 80/4000, Step 84, d_loss: 0.025273596867918968, g_loss: 5.5381340980529785\n","Epoch 80/4000, Step 85, d_loss: 0.005306020379066467, g_loss: 7.799668788909912\n","Epoch 80/4000, Step 86, d_loss: 0.0028536112513393164, g_loss: 3.6706297397613525\n","Epoch 80/4000, Step 87, d_loss: 0.003275326918810606, g_loss: 6.457846164703369\n","Epoch 80/4000, Step 88, d_loss: 0.002125594299286604, g_loss: 6.213483810424805\n","Epoch 80/4000, Step 89, d_loss: 0.01281951554119587, g_loss: 6.540713310241699\n","Epoch 80/4000, Step 90, d_loss: 0.0018155949655920267, g_loss: 4.638303279876709\n","Epoch 80/4000, Step 91, d_loss: 0.0014379650820046663, g_loss: 7.188263893127441\n","Epoch 80/4000, Step 92, d_loss: 0.006615819409489632, g_loss: 5.233080863952637\n","Epoch 80/4000, Step 93, d_loss: 0.026392530649900436, g_loss: 8.760865211486816\n","Epoch 80/4000, Step 94, d_loss: 0.002714598085731268, g_loss: 5.680187225341797\n","Epoch 80/4000, Step 95, d_loss: 0.0010188838932663202, g_loss: 4.6449127197265625\n","Epoch 80/4000, Step 96, d_loss: 0.00940086878836155, g_loss: 6.243404388427734\n","Epoch 80/4000, Step 97, d_loss: 0.0011934360954910517, g_loss: 5.576458930969238\n","Epoch 80/4000, Step 98, d_loss: 0.0068342238664627075, g_loss: 5.074329376220703\n","Epoch 80/4000, Step 99, d_loss: 0.004978586453944445, g_loss: 6.267635345458984\n","Epoch 80/4000, Step 100, d_loss: 0.0075807124376297, g_loss: 7.6503143310546875\n","Epoch 80/4000, Step 101, d_loss: 0.0012321040267124772, g_loss: 5.707197666168213\n","Epoch 80/4000, Step 102, d_loss: 0.005784061271697283, g_loss: 6.658878326416016\n","Epoch 80/4000, Step 103, d_loss: 0.004692315589636564, g_loss: 6.7778778076171875\n","Epoch 80/4000, Step 104, d_loss: 0.00673105102032423, g_loss: 8.067249298095703\n","Epoch 80/4000, Step 105, d_loss: 0.007800656370818615, g_loss: 6.047386646270752\n","Epoch 80/4000, Step 106, d_loss: 0.0011302679777145386, g_loss: 4.2257080078125\n","Epoch 80/4000, Step 107, d_loss: 0.00733321625739336, g_loss: 4.848495006561279\n","Epoch 80/4000, Step 108, d_loss: 0.0009029752109199762, g_loss: 6.650607585906982\n","Epoch 80/4000, Step 109, d_loss: 0.011940634809434414, g_loss: 6.841806888580322\n","Epoch 80/4000, Step 110, d_loss: 0.007329859770834446, g_loss: 6.387783050537109\n","Epoch 80/4000, Step 111, d_loss: 0.0063147712498903275, g_loss: 5.0215559005737305\n","Epoch 80/4000, Step 112, d_loss: 0.0023731300607323647, g_loss: 3.9206700325012207\n","Epoch 80/4000, Step 113, d_loss: 0.0025727935135364532, g_loss: 4.752849578857422\n","Epoch 80/4000, Step 114, d_loss: 0.0007551513845100999, g_loss: 6.671789646148682\n","Epoch 80/4000, Step 115, d_loss: 0.0037107558455318213, g_loss: 6.649322986602783\n","Epoch 80/4000, Step 116, d_loss: 0.001555676688440144, g_loss: 5.543621063232422\n","Epoch 80/4000, Step 117, d_loss: 0.004321618936955929, g_loss: 5.241456985473633\n","Epoch 80/4000, Step 118, d_loss: 0.0025825651828199625, g_loss: 6.664679050445557\n","Epoch 80/4000, Step 119, d_loss: 0.0013543310342356563, g_loss: 5.3067708015441895\n","Epoch 80/4000, Step 120, d_loss: 0.0036777076311409473, g_loss: 5.785500526428223\n","Epoch 80/4000, Step 121, d_loss: 0.04677150398492813, g_loss: 6.097044944763184\n","Epoch 80/4000, Step 122, d_loss: 0.0006941995234228671, g_loss: 5.986094951629639\n","Epoch 80/4000, Step 123, d_loss: 0.004230151884257793, g_loss: 4.997117519378662\n","Epoch 80/4000, Step 124, d_loss: 0.008977684192359447, g_loss: 6.803194046020508\n","Epoch 80/4000, Step 125, d_loss: 0.009080072864890099, g_loss: 6.23906135559082\n","Epoch 80/4000, Step 126, d_loss: 0.0017149748746305704, g_loss: 5.761409759521484\n","Epoch 80/4000, Step 127, d_loss: 0.005127317737787962, g_loss: 7.108394622802734\n","Epoch 80/4000, Step 128, d_loss: 0.003973392304033041, g_loss: 6.901447296142578\n","Epoch 80/4000, Step 129, d_loss: 0.014163995161652565, g_loss: 5.950173377990723\n","Epoch 80/4000, Step 130, d_loss: 0.007989320904016495, g_loss: 8.007552146911621\n","Epoch 80/4000, Step 131, d_loss: 0.005518880672752857, g_loss: 7.05219841003418\n","Epoch 80/4000, Step 132, d_loss: 0.0023084040731191635, g_loss: 5.589878559112549\n","Epoch 80/4000, Step 133, d_loss: 0.0029762075282633305, g_loss: 8.049670219421387\n","Epoch 80/4000, Step 134, d_loss: 0.0018045189790427685, g_loss: 6.622334957122803\n","Epoch 80/4000, Step 135, d_loss: 0.004640597850084305, g_loss: 4.4930548667907715\n","Epoch 80/4000, Step 136, d_loss: 0.0019568197894841433, g_loss: 4.606773376464844\n","Epoch 80/4000, Step 137, d_loss: 0.0011048623127862811, g_loss: 5.117076396942139\n","Epoch 80/4000, Step 138, d_loss: 0.00338094518519938, g_loss: 8.175207138061523\n","Epoch 80/4000, Step 139, d_loss: 0.00457315007224679, g_loss: 7.007433891296387\n","Epoch 80/4000, Step 140, d_loss: 0.0030873077921569347, g_loss: 7.493760585784912\n","Epoch 80/4000, Step 141, d_loss: 0.007940692827105522, g_loss: 6.461035251617432\n","Epoch 80/4000, Step 142, d_loss: 0.02209750935435295, g_loss: 8.611037254333496\n","Epoch 80/4000, Step 143, d_loss: 0.0038914659526199102, g_loss: 7.536747932434082\n","Epoch 80/4000, Step 144, d_loss: 0.02879047952592373, g_loss: 5.284618377685547\n","Epoch 80/4000, Step 145, d_loss: 0.005854379385709763, g_loss: 6.756411552429199\n","Epoch 80/4000, Step 146, d_loss: 0.0035390076227486134, g_loss: 10.495743751525879\n","Epoch 80/4000, Step 147, d_loss: 0.0008885596762411296, g_loss: 6.602752685546875\n","Epoch 80/4000, Step 148, d_loss: 0.0019638794474303722, g_loss: 5.097970485687256\n","Epoch 80/4000, Step 149, d_loss: 0.0005285617080517113, g_loss: 6.479875087738037\n","Epoch 80/4000, Step 150, d_loss: 0.02110815979540348, g_loss: 6.479482650756836\n","Epoch 80/4000, Step 151, d_loss: 0.002738184528425336, g_loss: 5.1881914138793945\n","Epoch 80/4000, Step 152, d_loss: 0.006589096039533615, g_loss: 8.978063583374023\n","Epoch 80/4000, Step 153, d_loss: 0.0018311756430193782, g_loss: 4.773801326751709\n","Epoch 80/4000, Step 154, d_loss: 0.001593366265296936, g_loss: 3.67257022857666\n","Epoch 80/4000, Step 155, d_loss: 0.012922914698719978, g_loss: 6.253469944000244\n","Epoch 80/4000, Step 156, d_loss: 0.0011783388908952475, g_loss: 6.368655204772949\n","Epoch 80/4000, Step 157, d_loss: 0.010814094915986061, g_loss: 6.302772521972656\n","Epoch 80/4000, Step 158, d_loss: 0.018738672137260437, g_loss: 6.476940631866455\n","Epoch 80/4000, Step 159, d_loss: 0.00658955704420805, g_loss: 5.86427640914917\n","Epoch 80/4000, Step 160, d_loss: 0.0031726949382573366, g_loss: 6.792602062225342\n","Epoch 80/4000, Step 161, d_loss: 0.005270335823297501, g_loss: 6.314569473266602\n","Epoch 80/4000, Step 162, d_loss: 0.006444920785725117, g_loss: 7.446462631225586\n","Epoch 80/4000, Step 163, d_loss: 0.010884852148592472, g_loss: 7.215629577636719\n","Epoch 80/4000, Step 164, d_loss: 0.002231031656265259, g_loss: 5.54416561126709\n","Epoch 80/4000, Step 165, d_loss: 0.0037881373427808285, g_loss: 7.483401298522949\n","Epoch 80/4000, Step 166, d_loss: 0.008367874659597874, g_loss: 7.1428542137146\n","Epoch 80/4000, Step 167, d_loss: 0.0040380870923399925, g_loss: 4.921209812164307\n","Epoch 80/4000, Step 168, d_loss: 0.007328817620873451, g_loss: 5.702239513397217\n","Epoch 80/4000, Step 169, d_loss: 0.00452734949067235, g_loss: 7.160117149353027\n","Epoch 80/4000, Step 170, d_loss: 0.0007735322578810155, g_loss: 9.81496810913086\n","Epoch 80/4000, Step 171, d_loss: 0.0020107286982238293, g_loss: 6.4456682205200195\n","Epoch 80/4000, Step 172, d_loss: 0.01232953555881977, g_loss: 6.322707176208496\n","Epoch 80/4000, Step 173, d_loss: 0.0024720090441405773, g_loss: 5.172731399536133\n","Epoch 80/4000, Step 174, d_loss: 0.006947854999452829, g_loss: 6.8306474685668945\n","Epoch 80/4000, Step 175, d_loss: 0.004648988600820303, g_loss: 6.3181915283203125\n","Epoch 80/4000, Step 176, d_loss: 0.0014195111580193043, g_loss: 7.341864109039307\n","Epoch 80/4000, Step 177, d_loss: 0.0028070693369954824, g_loss: 5.398550510406494\n","Epoch 80/4000, Step 178, d_loss: 0.0018659259658306837, g_loss: 6.652513027191162\n","Epoch 80/4000, Step 179, d_loss: 0.012917832471430302, g_loss: 7.405742645263672\n","Epoch 80/4000, Step 180, d_loss: 0.0028027535881847143, g_loss: 5.65799617767334\n","Epoch 80/4000, Step 181, d_loss: 0.0011827487032860518, g_loss: 4.956896781921387\n","Epoch 80/4000, Step 182, d_loss: 0.010217823088169098, g_loss: 7.562550067901611\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 81/4000, Step 1, d_loss: 0.0065313223749399185, g_loss: 7.204953670501709\n","Epoch 81/4000, Step 2, d_loss: 0.016701769083738327, g_loss: 6.982811450958252\n","Epoch 81/4000, Step 3, d_loss: 0.0009514953708276153, g_loss: 5.556617736816406\n","Epoch 81/4000, Step 4, d_loss: 0.004413149319589138, g_loss: 5.757872104644775\n","Epoch 81/4000, Step 5, d_loss: 0.004914065822958946, g_loss: 5.374764442443848\n","Epoch 81/4000, Step 6, d_loss: 0.0029537086375057697, g_loss: 3.8346385955810547\n","Epoch 81/4000, Step 7, d_loss: 0.016410240903496742, g_loss: 8.414424896240234\n","Epoch 81/4000, Step 8, d_loss: 0.000715998699888587, g_loss: 7.232648849487305\n","Epoch 81/4000, Step 9, d_loss: 0.0004728437343146652, g_loss: 5.216923236846924\n","Epoch 81/4000, Step 10, d_loss: 0.001579274539835751, g_loss: 5.250044822692871\n","Epoch 81/4000, Step 11, d_loss: 0.00265010935254395, g_loss: 8.965167045593262\n","Epoch 81/4000, Step 12, d_loss: 0.005278799682855606, g_loss: 6.1113739013671875\n","Epoch 81/4000, Step 13, d_loss: 0.0036389660090208054, g_loss: 5.410885810852051\n","Epoch 81/4000, Step 14, d_loss: 0.0005928619066253304, g_loss: 6.249607086181641\n","Epoch 81/4000, Step 15, d_loss: 0.027327634394168854, g_loss: 5.213183879852295\n","Epoch 81/4000, Step 16, d_loss: 0.001529567874968052, g_loss: 5.221458911895752\n","Epoch 81/4000, Step 17, d_loss: 0.0044193207286298275, g_loss: 6.1131134033203125\n","Epoch 81/4000, Step 18, d_loss: 0.008469180203974247, g_loss: 5.097109794616699\n","Epoch 81/4000, Step 19, d_loss: 0.015674708411097527, g_loss: 6.748824119567871\n","Epoch 81/4000, Step 20, d_loss: 0.0029301950708031654, g_loss: 5.136230945587158\n","Epoch 81/4000, Step 21, d_loss: 0.0014072813792154193, g_loss: 5.216431617736816\n","Epoch 81/4000, Step 22, d_loss: 0.0014180566649883986, g_loss: 6.019262313842773\n","Epoch 81/4000, Step 23, d_loss: 0.0010009644320234656, g_loss: 5.119125843048096\n","Epoch 81/4000, Step 24, d_loss: 0.0037084396462887526, g_loss: 6.117862224578857\n","Epoch 81/4000, Step 25, d_loss: 0.004095105919986963, g_loss: 5.3106465339660645\n","Epoch 81/4000, Step 26, d_loss: 0.0009081466123461723, g_loss: 6.205615520477295\n","Epoch 81/4000, Step 27, d_loss: 0.003216412616893649, g_loss: 7.284907817840576\n","Epoch 81/4000, Step 28, d_loss: 0.002430302556604147, g_loss: 5.833559036254883\n","Epoch 81/4000, Step 29, d_loss: 0.004734281450510025, g_loss: 4.819974899291992\n","Epoch 81/4000, Step 30, d_loss: 0.002465165453031659, g_loss: 4.333287239074707\n","Epoch 81/4000, Step 31, d_loss: 0.002696898765861988, g_loss: 6.823200225830078\n","Epoch 81/4000, Step 32, d_loss: 0.0007579432567581534, g_loss: 8.109453201293945\n","Epoch 81/4000, Step 33, d_loss: 0.018465284258127213, g_loss: 6.3147478103637695\n","Epoch 81/4000, Step 34, d_loss: 0.0007707450422458351, g_loss: 3.3446707725524902\n","Epoch 81/4000, Step 35, d_loss: 0.0020514088682830334, g_loss: 5.096904754638672\n","Epoch 81/4000, Step 36, d_loss: 0.007543216459453106, g_loss: 7.221461772918701\n","Epoch 81/4000, Step 37, d_loss: 0.0010328347561880946, g_loss: 5.384781360626221\n","Epoch 81/4000, Step 38, d_loss: 0.002296369057148695, g_loss: 3.352149248123169\n","Epoch 81/4000, Step 39, d_loss: 0.014628001488745213, g_loss: 6.279426574707031\n","Epoch 81/4000, Step 40, d_loss: 0.0003646075783763081, g_loss: 3.840229034423828\n","Epoch 81/4000, Step 41, d_loss: 0.003936012741178274, g_loss: 6.380001544952393\n","Epoch 81/4000, Step 42, d_loss: 0.0005997647531330585, g_loss: 7.077638149261475\n","Epoch 81/4000, Step 43, d_loss: 0.001092227059416473, g_loss: 6.530209064483643\n","Epoch 81/4000, Step 44, d_loss: 0.00989507231861353, g_loss: 6.224189758300781\n","Epoch 81/4000, Step 45, d_loss: 0.005856611765921116, g_loss: 5.8413238525390625\n","Epoch 81/4000, Step 46, d_loss: 0.0038187443278729916, g_loss: 6.735707759857178\n","Epoch 81/4000, Step 47, d_loss: 0.004827974364161491, g_loss: 9.284581184387207\n","Epoch 81/4000, Step 48, d_loss: 0.004901825450360775, g_loss: 7.053321838378906\n","Epoch 81/4000, Step 49, d_loss: 0.01257485430687666, g_loss: 7.913296222686768\n","Epoch 81/4000, Step 50, d_loss: 0.0014174866955727339, g_loss: 6.333192825317383\n","Epoch 81/4000, Step 51, d_loss: 0.006319195497781038, g_loss: 4.933473110198975\n","Epoch 81/4000, Step 52, d_loss: 0.002950325608253479, g_loss: 7.493790149688721\n","Epoch 81/4000, Step 53, d_loss: 0.008129877038300037, g_loss: 5.875955581665039\n","Epoch 81/4000, Step 54, d_loss: 0.016343848779797554, g_loss: 8.09069538116455\n","Epoch 81/4000, Step 55, d_loss: 0.03865155950188637, g_loss: 6.672806262969971\n","Epoch 81/4000, Step 56, d_loss: 0.0014733189018443227, g_loss: 8.091802597045898\n","Epoch 81/4000, Step 57, d_loss: 0.004214551765471697, g_loss: 7.164233684539795\n","Epoch 81/4000, Step 58, d_loss: 0.005633936263620853, g_loss: 6.0656914710998535\n","Epoch 81/4000, Step 59, d_loss: 0.0018704081885516644, g_loss: 7.973385334014893\n","Epoch 81/4000, Step 60, d_loss: 0.004580765962600708, g_loss: 8.244366645812988\n","Epoch 81/4000, Step 61, d_loss: 0.0012560441391542554, g_loss: 6.497566223144531\n","Epoch 81/4000, Step 62, d_loss: 0.0081340866163373, g_loss: 4.190608978271484\n","Epoch 81/4000, Step 63, d_loss: 0.003443970577791333, g_loss: 7.922731876373291\n","Epoch 81/4000, Step 64, d_loss: 0.004593810997903347, g_loss: 7.450932025909424\n","Epoch 81/4000, Step 65, d_loss: 0.000721324235200882, g_loss: 6.261272430419922\n","Epoch 81/4000, Step 66, d_loss: 0.009428326971828938, g_loss: 8.815332412719727\n","Epoch 81/4000, Step 67, d_loss: 0.01091973390430212, g_loss: 7.048895835876465\n","Epoch 81/4000, Step 68, d_loss: 0.007217923179268837, g_loss: 7.094758033752441\n","Epoch 81/4000, Step 69, d_loss: 0.0007965789991430938, g_loss: 6.498284816741943\n","Epoch 81/4000, Step 70, d_loss: 0.0007404008647426963, g_loss: 6.302508354187012\n","Epoch 81/4000, Step 71, d_loss: 0.0014668621588498354, g_loss: 6.445857048034668\n","Epoch 81/4000, Step 72, d_loss: 0.007958480156958103, g_loss: 5.966770172119141\n","Epoch 81/4000, Step 73, d_loss: 0.031428128480911255, g_loss: 7.983766555786133\n","Epoch 81/4000, Step 74, d_loss: 0.0022018791642040014, g_loss: 5.609959125518799\n","Epoch 81/4000, Step 75, d_loss: 0.021449346095323563, g_loss: 6.182541847229004\n","Epoch 81/4000, Step 76, d_loss: 0.005461118649691343, g_loss: 5.80808687210083\n","Epoch 81/4000, Step 77, d_loss: 0.0032873975578695536, g_loss: 6.740162372589111\n","Epoch 81/4000, Step 78, d_loss: 0.0042616524733603, g_loss: 5.367532730102539\n","Epoch 81/4000, Step 79, d_loss: 0.0009911691304296255, g_loss: 3.909022331237793\n","Epoch 81/4000, Step 80, d_loss: 0.0023341283667832613, g_loss: 7.779670715332031\n","Epoch 81/4000, Step 81, d_loss: 0.006208506412804127, g_loss: 4.803130149841309\n","Epoch 81/4000, Step 82, d_loss: 0.0009327091975137591, g_loss: 5.077650547027588\n","Epoch 81/4000, Step 83, d_loss: 0.008894951082766056, g_loss: 7.249035358428955\n","Epoch 81/4000, Step 84, d_loss: 0.0008163531310856342, g_loss: 7.586724758148193\n","Epoch 81/4000, Step 85, d_loss: 0.01418735459446907, g_loss: 7.440603256225586\n","Epoch 81/4000, Step 86, d_loss: 0.0025284206494688988, g_loss: 5.826360702514648\n","Epoch 81/4000, Step 87, d_loss: 0.0025857724249362946, g_loss: 4.519980430603027\n","Epoch 81/4000, Step 88, d_loss: 0.0015121086034923792, g_loss: 4.681797504425049\n","Epoch 81/4000, Step 89, d_loss: 0.004267676267772913, g_loss: 4.547287940979004\n","Epoch 81/4000, Step 90, d_loss: 0.006865155883133411, g_loss: 7.854492664337158\n","Epoch 81/4000, Step 91, d_loss: 0.0013927239924669266, g_loss: 8.864068031311035\n","Epoch 81/4000, Step 92, d_loss: 0.0040564038790762424, g_loss: 6.5938568115234375\n","Epoch 81/4000, Step 93, d_loss: 0.002671431517228484, g_loss: 4.698249816894531\n","Epoch 81/4000, Step 94, d_loss: 0.00048128311755135655, g_loss: 5.813920497894287\n","Epoch 81/4000, Step 95, d_loss: 0.002850967925041914, g_loss: 10.384109497070312\n","Epoch 81/4000, Step 96, d_loss: 0.00025573448510840535, g_loss: 5.941216468811035\n","Epoch 81/4000, Step 97, d_loss: 0.0007796863210387528, g_loss: 4.553746223449707\n","Epoch 81/4000, Step 98, d_loss: 0.012419324368238449, g_loss: 8.872557640075684\n","Epoch 81/4000, Step 99, d_loss: 0.006312443874776363, g_loss: 7.742323398590088\n","Epoch 81/4000, Step 100, d_loss: 0.013707198202610016, g_loss: 5.544483184814453\n","Epoch 81/4000, Step 101, d_loss: 0.007332584820687771, g_loss: 8.649282455444336\n","Epoch 81/4000, Step 102, d_loss: 0.011387530714273453, g_loss: 9.232564926147461\n","Epoch 81/4000, Step 103, d_loss: 0.0023717174772173166, g_loss: 8.760013580322266\n","Epoch 81/4000, Step 104, d_loss: 0.004861790686845779, g_loss: 6.77140474319458\n","Epoch 81/4000, Step 105, d_loss: 0.029159167781472206, g_loss: 6.799626350402832\n","Epoch 81/4000, Step 106, d_loss: 0.003913631662726402, g_loss: 8.986080169677734\n","Epoch 81/4000, Step 107, d_loss: 0.004222989082336426, g_loss: 6.148147106170654\n","Epoch 81/4000, Step 108, d_loss: 0.0033575703855603933, g_loss: 6.301440238952637\n","Epoch 81/4000, Step 109, d_loss: 0.0037002312019467354, g_loss: 6.786930084228516\n","Epoch 81/4000, Step 110, d_loss: 0.003405155846849084, g_loss: 4.483671188354492\n","Epoch 81/4000, Step 111, d_loss: 0.0009956758003681898, g_loss: 5.625973224639893\n","Epoch 81/4000, Step 112, d_loss: 0.0009676021873019636, g_loss: 6.301621913909912\n","Epoch 81/4000, Step 113, d_loss: 0.0031572410371154547, g_loss: 8.423830032348633\n","Epoch 81/4000, Step 114, d_loss: 0.0033738312777131796, g_loss: 7.79930305480957\n","Epoch 81/4000, Step 115, d_loss: 0.0009573675342835486, g_loss: 7.013851642608643\n","Epoch 81/4000, Step 116, d_loss: 0.01702495664358139, g_loss: 4.495824813842773\n","Epoch 81/4000, Step 117, d_loss: 0.0021520217414945364, g_loss: 7.49100923538208\n","Epoch 81/4000, Step 118, d_loss: 0.0026582207065075636, g_loss: 8.65494441986084\n","Epoch 81/4000, Step 119, d_loss: 0.0010931226424872875, g_loss: 8.815094947814941\n","Epoch 81/4000, Step 120, d_loss: 0.0037497454322874546, g_loss: 5.45690393447876\n","Epoch 81/4000, Step 121, d_loss: 0.0018383702263236046, g_loss: 6.912729263305664\n","Epoch 81/4000, Step 122, d_loss: 0.007889245636761189, g_loss: 7.2166008949279785\n","Epoch 81/4000, Step 123, d_loss: 0.005740030203014612, g_loss: 5.804746150970459\n","Epoch 81/4000, Step 124, d_loss: 0.001772304531186819, g_loss: 6.914572238922119\n","Epoch 81/4000, Step 125, d_loss: 0.001465821755118668, g_loss: 8.513222694396973\n","Epoch 81/4000, Step 126, d_loss: 0.000775991240516305, g_loss: 6.202561855316162\n","Epoch 81/4000, Step 127, d_loss: 0.007617761380970478, g_loss: 10.79771614074707\n","Epoch 81/4000, Step 128, d_loss: 0.0064412569627165794, g_loss: 6.525045394897461\n","Epoch 81/4000, Step 129, d_loss: 0.003620327450335026, g_loss: 6.093330383300781\n","Epoch 81/4000, Step 130, d_loss: 0.003109571523964405, g_loss: 6.774394989013672\n","Epoch 81/4000, Step 131, d_loss: 0.0032730402890592813, g_loss: 7.993577003479004\n","Epoch 81/4000, Step 132, d_loss: 0.0031935246661305428, g_loss: 6.208083152770996\n","Epoch 81/4000, Step 133, d_loss: 0.001977611565962434, g_loss: 4.031375408172607\n","Epoch 81/4000, Step 134, d_loss: 0.029841935262084007, g_loss: 4.436066150665283\n","Epoch 81/4000, Step 135, d_loss: 0.0022568553686141968, g_loss: 6.667860984802246\n","Epoch 81/4000, Step 136, d_loss: 0.0003543272614479065, g_loss: 3.700040340423584\n","Epoch 81/4000, Step 137, d_loss: 0.0006853678496554494, g_loss: 4.361551284790039\n","Epoch 81/4000, Step 138, d_loss: 0.005026170052587986, g_loss: 5.052152156829834\n","Epoch 81/4000, Step 139, d_loss: 0.004563911352306604, g_loss: 8.297643661499023\n","Epoch 81/4000, Step 140, d_loss: 0.004281270783394575, g_loss: 5.808452129364014\n","Epoch 81/4000, Step 141, d_loss: 0.0018384007271379232, g_loss: 5.8260955810546875\n","Epoch 81/4000, Step 142, d_loss: 0.030925080180168152, g_loss: 7.068287372589111\n","Epoch 81/4000, Step 143, d_loss: 0.00903974287211895, g_loss: 8.610116958618164\n","Epoch 81/4000, Step 144, d_loss: 0.016227297484874725, g_loss: 5.442867279052734\n","Epoch 81/4000, Step 145, d_loss: 0.004115702118724585, g_loss: 7.703559398651123\n","Epoch 81/4000, Step 146, d_loss: 0.00044307566713541746, g_loss: 7.375002384185791\n","Epoch 81/4000, Step 147, d_loss: 0.0035842820070683956, g_loss: 3.5548219680786133\n","Epoch 81/4000, Step 148, d_loss: 0.016044113785028458, g_loss: 3.3896777629852295\n","Epoch 81/4000, Step 149, d_loss: 0.0017555730883032084, g_loss: 7.716885089874268\n","Epoch 81/4000, Step 150, d_loss: 0.06968901306390762, g_loss: 6.153839588165283\n","Epoch 81/4000, Step 151, d_loss: 0.012815327383577824, g_loss: 7.327808856964111\n","Epoch 81/4000, Step 152, d_loss: 0.006528252735733986, g_loss: 5.8645148277282715\n","Epoch 81/4000, Step 153, d_loss: 0.05573942884802818, g_loss: 5.513237476348877\n","Epoch 81/4000, Step 154, d_loss: 0.03856739401817322, g_loss: 5.644246578216553\n","Epoch 81/4000, Step 155, d_loss: 0.0016119477804750204, g_loss: 6.695443630218506\n","Epoch 81/4000, Step 156, d_loss: 0.005269227549433708, g_loss: 3.1233742237091064\n","Epoch 81/4000, Step 157, d_loss: 0.02146829478442669, g_loss: 4.830979824066162\n","Epoch 81/4000, Step 158, d_loss: 0.005991438403725624, g_loss: 5.562957763671875\n","Epoch 81/4000, Step 159, d_loss: 0.052128903567790985, g_loss: 6.2645463943481445\n","Epoch 81/4000, Step 160, d_loss: 0.008949076756834984, g_loss: 4.729782581329346\n","Epoch 81/4000, Step 161, d_loss: 0.0035644243471324444, g_loss: 3.3541207313537598\n","Epoch 81/4000, Step 162, d_loss: 0.04439057037234306, g_loss: 5.110551834106445\n","Epoch 81/4000, Step 163, d_loss: 0.009160120040178299, g_loss: 3.9189209938049316\n","Epoch 81/4000, Step 164, d_loss: 0.022538529708981514, g_loss: 5.977441787719727\n","Epoch 81/4000, Step 165, d_loss: 0.0037678470835089684, g_loss: 4.969210147857666\n","Epoch 81/4000, Step 166, d_loss: 0.007647735998034477, g_loss: 3.3754680156707764\n","Epoch 81/4000, Step 167, d_loss: 0.013027388602495193, g_loss: 5.200347900390625\n","Epoch 81/4000, Step 168, d_loss: 0.053768500685691833, g_loss: 5.908016681671143\n","Epoch 81/4000, Step 169, d_loss: 0.00988049991428852, g_loss: 5.364137649536133\n","Epoch 81/4000, Step 170, d_loss: 0.01785075105726719, g_loss: 6.731510162353516\n","Epoch 81/4000, Step 171, d_loss: 0.011777663603425026, g_loss: 5.548557281494141\n","Epoch 81/4000, Step 172, d_loss: 0.0011355574242770672, g_loss: 6.41100549697876\n","Epoch 81/4000, Step 173, d_loss: 0.0008641532622277737, g_loss: 8.024606704711914\n","Epoch 81/4000, Step 174, d_loss: 0.004055686295032501, g_loss: 4.152937889099121\n","Epoch 81/4000, Step 175, d_loss: 0.003641136921942234, g_loss: 5.458938121795654\n","Epoch 81/4000, Step 176, d_loss: 0.0027787990402430296, g_loss: 6.473034381866455\n","Epoch 81/4000, Step 177, d_loss: 0.004986320622265339, g_loss: 7.345238208770752\n","Epoch 81/4000, Step 178, d_loss: 0.004796359688043594, g_loss: 7.0156450271606445\n","Epoch 81/4000, Step 179, d_loss: 0.007034638896584511, g_loss: 10.335484504699707\n","Epoch 81/4000, Step 180, d_loss: 0.004780713003128767, g_loss: 7.250118732452393\n","Epoch 81/4000, Step 181, d_loss: 0.0012087223585695028, g_loss: 9.5966215133667\n","Epoch 81/4000, Step 182, d_loss: 0.2558523416519165, g_loss: 7.165804862976074\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 82/4000, Step 1, d_loss: 0.0009159630280919373, g_loss: 5.086078643798828\n","Epoch 82/4000, Step 2, d_loss: 0.003632107051089406, g_loss: 4.839149475097656\n","Epoch 82/4000, Step 3, d_loss: 0.03243435174226761, g_loss: 5.780430316925049\n","Epoch 82/4000, Step 4, d_loss: 0.0039488268084824085, g_loss: 4.899519443511963\n","Epoch 82/4000, Step 5, d_loss: 0.09663330763578415, g_loss: 6.664048194885254\n","Epoch 82/4000, Step 6, d_loss: 0.00918935053050518, g_loss: 4.87412691116333\n","Epoch 82/4000, Step 7, d_loss: 0.1963997483253479, g_loss: 2.909050703048706\n","Epoch 82/4000, Step 8, d_loss: 0.20176316797733307, g_loss: 5.104748249053955\n","Epoch 82/4000, Step 9, d_loss: 0.022923413664102554, g_loss: 4.776138782501221\n","Epoch 82/4000, Step 10, d_loss: 0.07487644255161285, g_loss: 3.3818788528442383\n","Epoch 82/4000, Step 11, d_loss: 0.11706183105707169, g_loss: 6.390474319458008\n","Epoch 82/4000, Step 12, d_loss: 0.2332949936389923, g_loss: 3.521822214126587\n","Epoch 82/4000, Step 13, d_loss: 0.031662892550230026, g_loss: 4.636689186096191\n","Epoch 82/4000, Step 14, d_loss: 0.06843553483486176, g_loss: 6.125410079956055\n","Epoch 82/4000, Step 15, d_loss: 0.0443405918776989, g_loss: 4.2612690925598145\n","Epoch 82/4000, Step 16, d_loss: 0.03216433897614479, g_loss: 4.676238059997559\n","Epoch 82/4000, Step 17, d_loss: 0.15433433651924133, g_loss: 7.3541765213012695\n","Epoch 82/4000, Step 18, d_loss: 0.05741002783179283, g_loss: 4.772459983825684\n","Epoch 82/4000, Step 19, d_loss: 0.013197438791394234, g_loss: 8.018457412719727\n","Epoch 82/4000, Step 20, d_loss: 0.021657560020685196, g_loss: 4.901514053344727\n","Epoch 82/4000, Step 21, d_loss: 0.007998162880539894, g_loss: 4.012646198272705\n","Epoch 82/4000, Step 22, d_loss: 0.019779697060585022, g_loss: 5.880608558654785\n","Epoch 82/4000, Step 23, d_loss: 0.025645479559898376, g_loss: 6.564361572265625\n","Epoch 82/4000, Step 24, d_loss: 0.02009924314916134, g_loss: 6.167632102966309\n","Epoch 82/4000, Step 25, d_loss: 0.03959331661462784, g_loss: 8.487442016601562\n","Epoch 82/4000, Step 26, d_loss: 0.005884778685867786, g_loss: 10.533268928527832\n","Epoch 82/4000, Step 27, d_loss: 0.01593039557337761, g_loss: 7.172880172729492\n","Epoch 82/4000, Step 28, d_loss: 0.012331970036029816, g_loss: 5.391880989074707\n","Epoch 82/4000, Step 29, d_loss: 0.008301611058413982, g_loss: 6.344446182250977\n","Epoch 82/4000, Step 30, d_loss: 0.1270204782485962, g_loss: 5.618069648742676\n","Epoch 82/4000, Step 31, d_loss: 0.02234610915184021, g_loss: 8.31845474243164\n","Epoch 82/4000, Step 32, d_loss: 0.0032143699936568737, g_loss: 6.9846343994140625\n","Epoch 82/4000, Step 33, d_loss: 0.004786087200045586, g_loss: 5.227626800537109\n","Epoch 82/4000, Step 34, d_loss: 0.004245676100254059, g_loss: 7.3291850090026855\n","Epoch 82/4000, Step 35, d_loss: 0.009082069620490074, g_loss: 5.506385326385498\n","Epoch 82/4000, Step 36, d_loss: 0.004987987689673901, g_loss: 8.927703857421875\n","Epoch 82/4000, Step 37, d_loss: 0.009179022163152695, g_loss: 8.757912635803223\n","Epoch 82/4000, Step 38, d_loss: 0.0462520457804203, g_loss: 5.119082450866699\n","Epoch 82/4000, Step 39, d_loss: 0.04703819006681442, g_loss: 2.4795937538146973\n","Epoch 82/4000, Step 40, d_loss: 0.04387388005852699, g_loss: 3.8166708946228027\n","Epoch 82/4000, Step 41, d_loss: 0.03416508063673973, g_loss: 6.266534805297852\n","Epoch 82/4000, Step 42, d_loss: 0.03558137267827988, g_loss: 6.516926288604736\n","Epoch 82/4000, Step 43, d_loss: 0.007563757244497538, g_loss: 10.490410804748535\n","Epoch 82/4000, Step 44, d_loss: 0.1028292179107666, g_loss: 5.002810478210449\n","Epoch 82/4000, Step 45, d_loss: 0.004705119878053665, g_loss: 4.119234561920166\n","Epoch 82/4000, Step 46, d_loss: 0.03270658105611801, g_loss: 5.034161567687988\n","Epoch 82/4000, Step 47, d_loss: 0.03230082988739014, g_loss: 5.345795154571533\n","Epoch 82/4000, Step 48, d_loss: 0.03165239840745926, g_loss: 6.910053253173828\n","Epoch 82/4000, Step 49, d_loss: 0.009528044611215591, g_loss: 5.888894081115723\n","Epoch 82/4000, Step 50, d_loss: 0.16067378222942352, g_loss: 5.089043140411377\n","Epoch 82/4000, Step 51, d_loss: 0.001017740461975336, g_loss: 3.8309977054595947\n","Epoch 82/4000, Step 52, d_loss: 0.08252052962779999, g_loss: 5.971687316894531\n","Epoch 82/4000, Step 53, d_loss: 0.006252557039260864, g_loss: 3.0508344173431396\n","Epoch 82/4000, Step 54, d_loss: 0.0854177474975586, g_loss: 6.651198863983154\n","Epoch 82/4000, Step 55, d_loss: 0.04872032627463341, g_loss: 4.458298206329346\n","Epoch 82/4000, Step 56, d_loss: 0.03193189948797226, g_loss: 5.507397651672363\n","Epoch 82/4000, Step 57, d_loss: 0.0029315357096493244, g_loss: 7.68915319442749\n","Epoch 82/4000, Step 58, d_loss: 0.07241177558898926, g_loss: 4.546485424041748\n","Epoch 82/4000, Step 59, d_loss: 0.04309237003326416, g_loss: 5.603881359100342\n","Epoch 82/4000, Step 60, d_loss: 0.03843136131763458, g_loss: 4.804691791534424\n","Epoch 82/4000, Step 61, d_loss: 0.050035975873470306, g_loss: 9.203219413757324\n","Epoch 82/4000, Step 62, d_loss: 0.014013797044754028, g_loss: 5.552270889282227\n","Epoch 82/4000, Step 63, d_loss: 0.0020711603574454784, g_loss: 5.756838321685791\n","Epoch 82/4000, Step 64, d_loss: 0.012767930515110493, g_loss: 4.135763168334961\n","Epoch 82/4000, Step 65, d_loss: 0.03665393963456154, g_loss: 4.77365779876709\n","Epoch 82/4000, Step 66, d_loss: 0.03700682148337364, g_loss: 4.009613990783691\n","Epoch 82/4000, Step 67, d_loss: 0.0019678203389048576, g_loss: 3.531832695007324\n","Epoch 82/4000, Step 68, d_loss: 0.004809364676475525, g_loss: 4.156237602233887\n","Epoch 82/4000, Step 69, d_loss: 0.008702972903847694, g_loss: 4.774051189422607\n","Epoch 82/4000, Step 70, d_loss: 0.04995128884911537, g_loss: 5.840182304382324\n","Epoch 82/4000, Step 71, d_loss: 0.01180899515748024, g_loss: 9.40520191192627\n","Epoch 82/4000, Step 72, d_loss: 0.03701068088412285, g_loss: 4.620596885681152\n","Epoch 82/4000, Step 73, d_loss: 0.06724825501441956, g_loss: 6.243981838226318\n","Epoch 82/4000, Step 74, d_loss: 0.003245133673772216, g_loss: 4.528188705444336\n","Epoch 82/4000, Step 75, d_loss: 0.02114485576748848, g_loss: 4.6105122566223145\n","Epoch 82/4000, Step 76, d_loss: 0.03596791625022888, g_loss: 9.30433464050293\n","Epoch 82/4000, Step 77, d_loss: 0.0034349444322288036, g_loss: 5.445865631103516\n","Epoch 82/4000, Step 78, d_loss: 0.001937361084856093, g_loss: 5.603812217712402\n","Epoch 82/4000, Step 79, d_loss: 0.03049813210964203, g_loss: 4.310825824737549\n","Epoch 82/4000, Step 80, d_loss: 0.048609524965286255, g_loss: 6.096970081329346\n","Epoch 82/4000, Step 81, d_loss: 0.0028918138705193996, g_loss: 5.573556423187256\n","Epoch 82/4000, Step 82, d_loss: 0.034348320215940475, g_loss: 6.8769330978393555\n","Epoch 82/4000, Step 83, d_loss: 0.011700030416250229, g_loss: 4.347504615783691\n","Epoch 82/4000, Step 84, d_loss: 0.009384386241436005, g_loss: 5.3206305503845215\n","Epoch 82/4000, Step 85, d_loss: 0.001230696216225624, g_loss: 4.855387210845947\n","Epoch 82/4000, Step 86, d_loss: 0.003471365198493004, g_loss: 7.154361724853516\n","Epoch 82/4000, Step 87, d_loss: 0.006884376518428326, g_loss: 6.574648857116699\n","Epoch 82/4000, Step 88, d_loss: 0.006762969307601452, g_loss: 7.15189266204834\n","Epoch 82/4000, Step 89, d_loss: 0.008536968380212784, g_loss: 5.132916450500488\n","Epoch 82/4000, Step 90, d_loss: 0.008337223902344704, g_loss: 5.830512523651123\n","Epoch 82/4000, Step 91, d_loss: 0.011808259412646294, g_loss: 8.845931053161621\n","Epoch 82/4000, Step 92, d_loss: 0.0006196582107804716, g_loss: 5.06156587600708\n","Epoch 82/4000, Step 93, d_loss: 0.009433509781956673, g_loss: 4.395008087158203\n","Epoch 82/4000, Step 94, d_loss: 0.009002195671200752, g_loss: 4.316420555114746\n","Epoch 82/4000, Step 95, d_loss: 0.0036341077648103237, g_loss: 4.2621941566467285\n","Epoch 82/4000, Step 96, d_loss: 0.01185888983309269, g_loss: 4.105194568634033\n","Epoch 82/4000, Step 97, d_loss: 0.03571378439664841, g_loss: 5.55011510848999\n","Epoch 82/4000, Step 98, d_loss: 0.007548498921096325, g_loss: 4.787490367889404\n","Epoch 82/4000, Step 99, d_loss: 0.011349440552294254, g_loss: 4.488999366760254\n","Epoch 82/4000, Step 100, d_loss: 0.00666035246104002, g_loss: 4.889310359954834\n","Epoch 82/4000, Step 101, d_loss: 0.009487459436058998, g_loss: 5.137148380279541\n","Epoch 82/4000, Step 102, d_loss: 0.001956129679456353, g_loss: 4.354299068450928\n","Epoch 82/4000, Step 103, d_loss: 0.011353068985044956, g_loss: 3.4608120918273926\n","Epoch 82/4000, Step 104, d_loss: 0.015189159661531448, g_loss: 4.585601329803467\n","Epoch 82/4000, Step 105, d_loss: 0.0015626245876774192, g_loss: 6.970508098602295\n","Epoch 82/4000, Step 106, d_loss: 0.004984462168067694, g_loss: 5.1290788650512695\n","Epoch 82/4000, Step 107, d_loss: 0.01300811767578125, g_loss: 8.089728355407715\n","Epoch 82/4000, Step 108, d_loss: 0.021444005891680717, g_loss: 5.182961940765381\n","Epoch 82/4000, Step 109, d_loss: 0.003097470384091139, g_loss: 7.857546806335449\n","Epoch 82/4000, Step 110, d_loss: 0.020335625857114792, g_loss: 5.427155494689941\n","Epoch 82/4000, Step 111, d_loss: 0.010981185361742973, g_loss: 7.947424411773682\n","Epoch 82/4000, Step 112, d_loss: 0.0027352236211299896, g_loss: 5.289193630218506\n","Epoch 82/4000, Step 113, d_loss: 0.005130832549184561, g_loss: 7.691747665405273\n","Epoch 82/4000, Step 114, d_loss: 0.014289602637290955, g_loss: 5.0419182777404785\n","Epoch 82/4000, Step 115, d_loss: 0.009862488135695457, g_loss: 5.976466655731201\n","Epoch 82/4000, Step 116, d_loss: 0.0010339957661926746, g_loss: 9.532438278198242\n","Epoch 82/4000, Step 117, d_loss: 0.002616757294163108, g_loss: 4.999231815338135\n","Epoch 82/4000, Step 118, d_loss: 0.0027399170212447643, g_loss: 4.741445064544678\n","Epoch 82/4000, Step 119, d_loss: 0.018884574994444847, g_loss: 5.350358963012695\n","Epoch 82/4000, Step 120, d_loss: 0.002410733141005039, g_loss: 4.88943338394165\n","Epoch 82/4000, Step 121, d_loss: 0.001318103400990367, g_loss: 6.949336528778076\n","Epoch 82/4000, Step 122, d_loss: 0.0039742873050272465, g_loss: 7.450977325439453\n","Epoch 82/4000, Step 123, d_loss: 0.001410759869031608, g_loss: 5.087341785430908\n","Epoch 82/4000, Step 124, d_loss: 0.010508602485060692, g_loss: 5.0517377853393555\n","Epoch 82/4000, Step 125, d_loss: 0.021962318569421768, g_loss: 4.629915237426758\n","Epoch 82/4000, Step 126, d_loss: 0.0013471522834151983, g_loss: 5.511207580566406\n","Epoch 82/4000, Step 127, d_loss: 0.003178773447871208, g_loss: 7.028130054473877\n","Epoch 82/4000, Step 128, d_loss: 0.0041885958053171635, g_loss: 5.4643120765686035\n","Epoch 82/4000, Step 129, d_loss: 0.0018718709470704198, g_loss: 7.560654163360596\n","Epoch 82/4000, Step 130, d_loss: 0.0019090971909463406, g_loss: 8.284887313842773\n","Epoch 82/4000, Step 131, d_loss: 0.012521530501544476, g_loss: 6.221179962158203\n","Epoch 82/4000, Step 132, d_loss: 0.024287836626172066, g_loss: 7.027955532073975\n","Epoch 82/4000, Step 133, d_loss: 0.003079765709117055, g_loss: 5.016763687133789\n","Epoch 82/4000, Step 134, d_loss: 0.01415198016911745, g_loss: 9.316548347473145\n","Epoch 82/4000, Step 135, d_loss: 0.006490980740636587, g_loss: 5.003913879394531\n","Epoch 82/4000, Step 136, d_loss: 0.002722232136875391, g_loss: 5.807775974273682\n","Epoch 82/4000, Step 137, d_loss: 0.0013707487378269434, g_loss: 7.278957843780518\n","Epoch 82/4000, Step 138, d_loss: 0.03562528267502785, g_loss: 6.255712985992432\n","Epoch 82/4000, Step 139, d_loss: 0.003747320733964443, g_loss: 4.752244472503662\n","Epoch 82/4000, Step 140, d_loss: 0.013307643122971058, g_loss: 5.559047222137451\n","Epoch 82/4000, Step 141, d_loss: 0.01050055492669344, g_loss: 4.18462610244751\n","Epoch 82/4000, Step 142, d_loss: 0.009333026595413685, g_loss: 9.036829948425293\n","Epoch 82/4000, Step 143, d_loss: 0.003461614018306136, g_loss: 5.77359676361084\n","Epoch 82/4000, Step 144, d_loss: 0.004014382138848305, g_loss: 6.638866424560547\n","Epoch 82/4000, Step 145, d_loss: 0.0015863376902416348, g_loss: 8.733217239379883\n","Epoch 82/4000, Step 146, d_loss: 0.002088823588564992, g_loss: 9.169998168945312\n","Epoch 82/4000, Step 147, d_loss: 0.001160452258773148, g_loss: 4.327822208404541\n","Epoch 82/4000, Step 148, d_loss: 0.08813570439815521, g_loss: 7.903103351593018\n","Epoch 82/4000, Step 149, d_loss: 0.0019762483425438404, g_loss: 5.796188831329346\n","Epoch 82/4000, Step 150, d_loss: 0.00833890587091446, g_loss: 5.885985851287842\n","Epoch 82/4000, Step 151, d_loss: 0.0025006821379065514, g_loss: 5.152632236480713\n","Epoch 82/4000, Step 152, d_loss: 0.005901273339986801, g_loss: 5.763793468475342\n","Epoch 82/4000, Step 153, d_loss: 0.02769014984369278, g_loss: 6.422489166259766\n","Epoch 82/4000, Step 154, d_loss: 0.02242722176015377, g_loss: 3.2747652530670166\n","Epoch 82/4000, Step 155, d_loss: 0.02939189039170742, g_loss: 4.040508270263672\n","Epoch 82/4000, Step 156, d_loss: 0.0012966961367055774, g_loss: 7.501464366912842\n","Epoch 82/4000, Step 157, d_loss: 0.006885501090437174, g_loss: 5.451033115386963\n","Epoch 82/4000, Step 158, d_loss: 0.003933523781597614, g_loss: 4.583983421325684\n","Epoch 82/4000, Step 159, d_loss: 0.010677901096642017, g_loss: 5.2786970138549805\n","Epoch 82/4000, Step 160, d_loss: 0.005195873323827982, g_loss: 9.378957748413086\n","Epoch 82/4000, Step 161, d_loss: 0.00036115964758209884, g_loss: 3.0711419582366943\n","Epoch 82/4000, Step 162, d_loss: 0.005796163342893124, g_loss: 4.580341339111328\n","Epoch 82/4000, Step 163, d_loss: 0.002170157851651311, g_loss: 5.8029375076293945\n","Epoch 82/4000, Step 164, d_loss: 0.0012445885222405195, g_loss: 7.503323554992676\n","Epoch 82/4000, Step 165, d_loss: 0.02033849060535431, g_loss: 6.265120983123779\n","Epoch 82/4000, Step 166, d_loss: 0.0036396775394678116, g_loss: 6.71259880065918\n","Epoch 82/4000, Step 167, d_loss: 0.010277063585817814, g_loss: 5.806610107421875\n","Epoch 82/4000, Step 168, d_loss: 0.001339414739049971, g_loss: 5.3020429611206055\n","Epoch 82/4000, Step 169, d_loss: 0.012160224840044975, g_loss: 6.595148086547852\n","Epoch 82/4000, Step 170, d_loss: 0.007467402145266533, g_loss: 5.16612434387207\n","Epoch 82/4000, Step 171, d_loss: 0.002063696039840579, g_loss: 4.350420951843262\n","Epoch 82/4000, Step 172, d_loss: 0.007343355566263199, g_loss: 3.592787504196167\n","Epoch 82/4000, Step 173, d_loss: 0.009024756029248238, g_loss: 4.99960994720459\n","Epoch 82/4000, Step 174, d_loss: 0.007774888072162867, g_loss: 5.1248555183410645\n","Epoch 82/4000, Step 175, d_loss: 0.0004278230480849743, g_loss: 3.6964118480682373\n","Epoch 82/4000, Step 176, d_loss: 0.0016344609903171659, g_loss: 6.473008632659912\n","Epoch 82/4000, Step 177, d_loss: 0.007791206240653992, g_loss: 12.23337459564209\n","Epoch 82/4000, Step 178, d_loss: 0.012098846957087517, g_loss: 6.466843128204346\n","Epoch 82/4000, Step 179, d_loss: 0.011269857175648212, g_loss: 6.9602370262146\n","Epoch 82/4000, Step 180, d_loss: 0.004859945736825466, g_loss: 7.184816360473633\n","Epoch 82/4000, Step 181, d_loss: 0.0033133591059595346, g_loss: 10.081116676330566\n","Epoch 82/4000, Step 182, d_loss: 0.008463636972010136, g_loss: 10.216484069824219\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 83/4000, Step 1, d_loss: 0.027340859174728394, g_loss: 5.545268535614014\n","Epoch 83/4000, Step 2, d_loss: 0.002612458076328039, g_loss: 6.735487461090088\n","Epoch 83/4000, Step 3, d_loss: 0.00472249835729599, g_loss: 4.2816386222839355\n","Epoch 83/4000, Step 4, d_loss: 0.00447250297293067, g_loss: 8.922525405883789\n","Epoch 83/4000, Step 5, d_loss: 0.0006649871938861907, g_loss: 6.090119361877441\n","Epoch 83/4000, Step 6, d_loss: 0.01807170920073986, g_loss: 6.228696346282959\n","Epoch 83/4000, Step 7, d_loss: 0.00683236587792635, g_loss: 8.438109397888184\n","Epoch 83/4000, Step 8, d_loss: 0.007453080732375383, g_loss: 5.295534610748291\n","Epoch 83/4000, Step 9, d_loss: 0.009842694737017155, g_loss: 6.27664852142334\n","Epoch 83/4000, Step 10, d_loss: 0.0013543306849896908, g_loss: 6.308423042297363\n","Epoch 83/4000, Step 11, d_loss: 0.00374354375526309, g_loss: 8.382102966308594\n","Epoch 83/4000, Step 12, d_loss: 0.013605588115751743, g_loss: 6.789121627807617\n","Epoch 83/4000, Step 13, d_loss: 0.05342326685786247, g_loss: 7.2137556076049805\n","Epoch 83/4000, Step 14, d_loss: 0.006431786809116602, g_loss: 6.241873741149902\n","Epoch 83/4000, Step 15, d_loss: 0.003663035575300455, g_loss: 6.217519760131836\n","Epoch 83/4000, Step 16, d_loss: 0.01849370449781418, g_loss: 4.984058856964111\n","Epoch 83/4000, Step 17, d_loss: 0.008604185655713081, g_loss: 5.907702922821045\n","Epoch 83/4000, Step 18, d_loss: 0.0073892888613045216, g_loss: 6.916004180908203\n","Epoch 83/4000, Step 19, d_loss: 0.0007077180780470371, g_loss: 6.541693210601807\n","Epoch 83/4000, Step 20, d_loss: 0.004373356234282255, g_loss: 7.603801250457764\n","Epoch 83/4000, Step 21, d_loss: 0.006638699676841497, g_loss: 5.748342990875244\n","Epoch 83/4000, Step 22, d_loss: 0.005604512989521027, g_loss: 6.183971405029297\n","Epoch 83/4000, Step 23, d_loss: 0.013183888047933578, g_loss: 8.805105209350586\n","Epoch 83/4000, Step 24, d_loss: 0.001990529475733638, g_loss: 7.664355754852295\n","Epoch 83/4000, Step 25, d_loss: 0.005400448106229305, g_loss: 6.4135870933532715\n","Epoch 83/4000, Step 26, d_loss: 0.0013972311280667782, g_loss: 9.433148384094238\n","Epoch 83/4000, Step 27, d_loss: 0.0045127952471375465, g_loss: 7.323629379272461\n","Epoch 83/4000, Step 28, d_loss: 0.021815069019794464, g_loss: 5.829466342926025\n","Epoch 83/4000, Step 29, d_loss: 0.0034583150409162045, g_loss: 4.408927917480469\n","Epoch 83/4000, Step 30, d_loss: 0.000972913927398622, g_loss: 7.186378002166748\n","Epoch 83/4000, Step 31, d_loss: 0.01242996659129858, g_loss: 5.834249973297119\n","Epoch 83/4000, Step 32, d_loss: 0.005784172564744949, g_loss: 9.311722755432129\n","Epoch 83/4000, Step 33, d_loss: 0.002953780349344015, g_loss: 6.09860372543335\n","Epoch 83/4000, Step 34, d_loss: 0.007947500795125961, g_loss: 4.347014427185059\n","Epoch 83/4000, Step 35, d_loss: 0.007671915926039219, g_loss: 7.721743583679199\n","Epoch 83/4000, Step 36, d_loss: 0.004088575020432472, g_loss: 6.493755340576172\n","Epoch 83/4000, Step 37, d_loss: 0.0010005796793848276, g_loss: 6.742724895477295\n","Epoch 83/4000, Step 38, d_loss: 0.02446626126766205, g_loss: 5.805310249328613\n","Epoch 83/4000, Step 39, d_loss: 0.006169011816382408, g_loss: 9.347658157348633\n","Epoch 83/4000, Step 40, d_loss: 0.004205642733722925, g_loss: 5.440852165222168\n","Epoch 83/4000, Step 41, d_loss: 0.0017368481494486332, g_loss: 9.17300033569336\n","Epoch 83/4000, Step 42, d_loss: 0.0019469852559268475, g_loss: 7.444427013397217\n","Epoch 83/4000, Step 43, d_loss: 0.007158546708524227, g_loss: 6.04622745513916\n","Epoch 83/4000, Step 44, d_loss: 0.01011333055794239, g_loss: 6.873353481292725\n","Epoch 83/4000, Step 45, d_loss: 0.0064112283289432526, g_loss: 6.703749656677246\n","Epoch 83/4000, Step 46, d_loss: 0.002499848138540983, g_loss: 7.001522541046143\n","Epoch 83/4000, Step 47, d_loss: 0.0023705826606601477, g_loss: 6.204823970794678\n","Epoch 83/4000, Step 48, d_loss: 0.03243706747889519, g_loss: 6.62582540512085\n","Epoch 83/4000, Step 49, d_loss: 0.0014761332422494888, g_loss: 5.749190330505371\n","Epoch 83/4000, Step 50, d_loss: 0.016627144068479538, g_loss: 3.8520538806915283\n","Epoch 83/4000, Step 51, d_loss: 0.002454614033922553, g_loss: 6.566487789154053\n","Epoch 83/4000, Step 52, d_loss: 0.007323489524424076, g_loss: 5.790065765380859\n","Epoch 83/4000, Step 53, d_loss: 0.023859048262238503, g_loss: 6.001842021942139\n","Epoch 83/4000, Step 54, d_loss: 0.004495254717767239, g_loss: 4.44126558303833\n","Epoch 83/4000, Step 55, d_loss: 0.013116011396050453, g_loss: 7.578140735626221\n","Epoch 83/4000, Step 56, d_loss: 0.0031525876838713884, g_loss: 7.47996711730957\n","Epoch 83/4000, Step 57, d_loss: 0.007391962222754955, g_loss: 6.467853546142578\n","Epoch 83/4000, Step 58, d_loss: 0.0037506232038140297, g_loss: 8.844976425170898\n","Epoch 83/4000, Step 59, d_loss: 0.014229672029614449, g_loss: 4.083174228668213\n","Epoch 83/4000, Step 60, d_loss: 0.0018284614197909832, g_loss: 5.1741719245910645\n","Epoch 83/4000, Step 61, d_loss: 0.014012418687343597, g_loss: 4.077949047088623\n","Epoch 83/4000, Step 62, d_loss: 0.00478828139603138, g_loss: 5.375556945800781\n","Epoch 83/4000, Step 63, d_loss: 0.002640322782099247, g_loss: 5.476267337799072\n","Epoch 83/4000, Step 64, d_loss: 0.002805354306474328, g_loss: 6.696165084838867\n","Epoch 83/4000, Step 65, d_loss: 0.020451068878173828, g_loss: 3.638179302215576\n","Epoch 83/4000, Step 66, d_loss: 0.013833282515406609, g_loss: 7.206969261169434\n","Epoch 83/4000, Step 67, d_loss: 0.008452652022242546, g_loss: 6.063632965087891\n","Epoch 83/4000, Step 68, d_loss: 0.00039500673301517963, g_loss: 8.018173217773438\n","Epoch 83/4000, Step 69, d_loss: 0.0045446730218827724, g_loss: 9.839292526245117\n","Epoch 83/4000, Step 70, d_loss: 0.006931730546057224, g_loss: 4.979519844055176\n","Epoch 83/4000, Step 71, d_loss: 0.02134346030652523, g_loss: 5.8521809577941895\n","Epoch 83/4000, Step 72, d_loss: 0.005151644349098206, g_loss: 5.662047863006592\n","Epoch 83/4000, Step 73, d_loss: 0.0015924896579235792, g_loss: 6.330134868621826\n","Epoch 83/4000, Step 74, d_loss: 0.0031671219039708376, g_loss: 7.542296409606934\n","Epoch 83/4000, Step 75, d_loss: 0.007271320093423128, g_loss: 6.260432720184326\n","Epoch 83/4000, Step 76, d_loss: 0.014308120124042034, g_loss: 7.606913089752197\n","Epoch 83/4000, Step 77, d_loss: 0.0009395384695380926, g_loss: 7.816319465637207\n","Epoch 83/4000, Step 78, d_loss: 0.002760885749012232, g_loss: 5.595729351043701\n","Epoch 83/4000, Step 79, d_loss: 0.042679041624069214, g_loss: 5.903491973876953\n","Epoch 83/4000, Step 80, d_loss: 0.004103871062397957, g_loss: 7.508963108062744\n","Epoch 83/4000, Step 81, d_loss: 0.01114120613783598, g_loss: 4.8520283699035645\n","Epoch 83/4000, Step 82, d_loss: 0.009275821037590504, g_loss: 7.904740810394287\n","Epoch 83/4000, Step 83, d_loss: 0.00499653210863471, g_loss: 8.816472053527832\n","Epoch 83/4000, Step 84, d_loss: 0.010295901447534561, g_loss: 7.127996444702148\n","Epoch 83/4000, Step 85, d_loss: 0.003056055400520563, g_loss: 8.272600173950195\n","Epoch 83/4000, Step 86, d_loss: 0.006396489683538675, g_loss: 5.2921881675720215\n","Epoch 83/4000, Step 87, d_loss: 0.007223545573651791, g_loss: 5.410048007965088\n","Epoch 83/4000, Step 88, d_loss: 0.004669788293540478, g_loss: 5.877817153930664\n","Epoch 83/4000, Step 89, d_loss: 0.017456844449043274, g_loss: 6.229827404022217\n","Epoch 83/4000, Step 90, d_loss: 0.002853069920092821, g_loss: 7.66524076461792\n","Epoch 83/4000, Step 91, d_loss: 0.0007302499143406749, g_loss: 8.342650413513184\n","Epoch 83/4000, Step 92, d_loss: 0.0022134357132017612, g_loss: 4.5860676765441895\n","Epoch 83/4000, Step 93, d_loss: 0.0015238894848152995, g_loss: 8.554943084716797\n","Epoch 83/4000, Step 94, d_loss: 0.011699501425027847, g_loss: 6.295888423919678\n","Epoch 83/4000, Step 95, d_loss: 0.005059187766164541, g_loss: 4.807330131530762\n","Epoch 83/4000, Step 96, d_loss: 0.0006647672853432596, g_loss: 6.615361213684082\n","Epoch 83/4000, Step 97, d_loss: 0.000747572397813201, g_loss: 10.406196594238281\n","Epoch 83/4000, Step 98, d_loss: 0.009446482174098492, g_loss: 7.032888412475586\n","Epoch 83/4000, Step 99, d_loss: 0.003975790459662676, g_loss: 5.876882076263428\n","Epoch 83/4000, Step 100, d_loss: 0.0017151564825326204, g_loss: 4.316586971282959\n","Epoch 83/4000, Step 101, d_loss: 0.004764007870107889, g_loss: 7.174802780151367\n","Epoch 83/4000, Step 102, d_loss: 0.009441466070711613, g_loss: 4.920790195465088\n","Epoch 83/4000, Step 103, d_loss: 0.004343309905380011, g_loss: 9.082395553588867\n","Epoch 83/4000, Step 104, d_loss: 0.0007714661187492311, g_loss: 5.818371772766113\n","Epoch 83/4000, Step 105, d_loss: 0.0022039783652871847, g_loss: 6.981374263763428\n","Epoch 83/4000, Step 106, d_loss: 0.006965160835534334, g_loss: 7.3882575035095215\n","Epoch 83/4000, Step 107, d_loss: 0.0007420265465043485, g_loss: 4.175589084625244\n","Epoch 83/4000, Step 108, d_loss: 0.0024837085511535406, g_loss: 4.757792949676514\n","Epoch 83/4000, Step 109, d_loss: 0.002561668399721384, g_loss: 6.47383975982666\n","Epoch 83/4000, Step 110, d_loss: 0.014967918395996094, g_loss: 6.233178615570068\n","Epoch 83/4000, Step 111, d_loss: 0.0031689293682575226, g_loss: 6.839940071105957\n","Epoch 83/4000, Step 112, d_loss: 0.0047838701866567135, g_loss: 4.12067174911499\n","Epoch 83/4000, Step 113, d_loss: 0.009562766179442406, g_loss: 4.251168727874756\n","Epoch 83/4000, Step 114, d_loss: 0.00887411180883646, g_loss: 5.891291618347168\n","Epoch 83/4000, Step 115, d_loss: 0.005624766461551189, g_loss: 5.9210357666015625\n","Epoch 83/4000, Step 116, d_loss: 0.0014519616961479187, g_loss: 6.28662109375\n","Epoch 83/4000, Step 117, d_loss: 0.005765434354543686, g_loss: 5.635965347290039\n","Epoch 83/4000, Step 118, d_loss: 0.003269416280090809, g_loss: 7.262918472290039\n","Epoch 83/4000, Step 119, d_loss: 0.010273975320160389, g_loss: 8.249065399169922\n","Epoch 83/4000, Step 120, d_loss: 0.014687796123325825, g_loss: 6.849580764770508\n","Epoch 83/4000, Step 121, d_loss: 0.0032428281847387552, g_loss: 9.469249725341797\n","Epoch 83/4000, Step 122, d_loss: 0.013039285317063332, g_loss: 6.790137767791748\n","Epoch 83/4000, Step 123, d_loss: 0.01519439835101366, g_loss: 7.653203010559082\n","Epoch 83/4000, Step 124, d_loss: 0.005866594612598419, g_loss: 3.986751079559326\n","Epoch 83/4000, Step 125, d_loss: 0.0022608665749430656, g_loss: 5.44429874420166\n","Epoch 83/4000, Step 126, d_loss: 0.004054793156683445, g_loss: 10.25866985321045\n","Epoch 83/4000, Step 127, d_loss: 0.011948563158512115, g_loss: 7.154790878295898\n","Epoch 83/4000, Step 128, d_loss: 0.003608094295486808, g_loss: 5.4640655517578125\n","Epoch 83/4000, Step 129, d_loss: 0.004971443209797144, g_loss: 7.341230869293213\n","Epoch 83/4000, Step 130, d_loss: 0.017794158309698105, g_loss: 5.936077117919922\n","Epoch 83/4000, Step 131, d_loss: 0.001965437550097704, g_loss: 10.183065414428711\n","Epoch 83/4000, Step 132, d_loss: 0.0074805328622460365, g_loss: 7.5937395095825195\n","Epoch 83/4000, Step 133, d_loss: 0.0007713540690019727, g_loss: 6.492937088012695\n","Epoch 83/4000, Step 134, d_loss: 0.005848645698279142, g_loss: 7.448825359344482\n","Epoch 83/4000, Step 135, d_loss: 0.006774864625185728, g_loss: 8.41375732421875\n","Epoch 83/4000, Step 136, d_loss: 0.006114073097705841, g_loss: 5.375243186950684\n","Epoch 83/4000, Step 137, d_loss: 0.0014956621453166008, g_loss: 8.789761543273926\n","Epoch 83/4000, Step 138, d_loss: 0.003983878996223211, g_loss: 4.418481349945068\n","Epoch 83/4000, Step 139, d_loss: 0.0036959280259907246, g_loss: 5.010590553283691\n","Epoch 83/4000, Step 140, d_loss: 0.0037252826150506735, g_loss: 4.835958003997803\n","Epoch 83/4000, Step 141, d_loss: 0.0012142850318923593, g_loss: 4.621925354003906\n","Epoch 83/4000, Step 142, d_loss: 0.0003179880150128156, g_loss: 4.463708400726318\n","Epoch 83/4000, Step 143, d_loss: 0.0020134032238274813, g_loss: 6.040580749511719\n","Epoch 83/4000, Step 144, d_loss: 0.0015772897750139236, g_loss: 6.006644248962402\n","Epoch 83/4000, Step 145, d_loss: 0.012971363961696625, g_loss: 6.101812362670898\n","Epoch 83/4000, Step 146, d_loss: 0.005446540657430887, g_loss: 7.228851795196533\n","Epoch 83/4000, Step 147, d_loss: 0.009304836392402649, g_loss: 7.13569450378418\n","Epoch 83/4000, Step 148, d_loss: 0.0037315457593649626, g_loss: 6.750232696533203\n","Epoch 83/4000, Step 149, d_loss: 0.018283089622855186, g_loss: 5.7634735107421875\n","Epoch 83/4000, Step 150, d_loss: 0.0008099724072962999, g_loss: 6.427055358886719\n","Epoch 83/4000, Step 151, d_loss: 0.001349382451735437, g_loss: 6.126641273498535\n","Epoch 83/4000, Step 152, d_loss: 0.011065546423196793, g_loss: 5.670021057128906\n","Epoch 83/4000, Step 153, d_loss: 0.0042241765186190605, g_loss: 8.059626579284668\n","Epoch 83/4000, Step 154, d_loss: 0.004074637778103352, g_loss: 6.21690034866333\n","Epoch 83/4000, Step 155, d_loss: 0.00395476259291172, g_loss: 5.337649822235107\n","Epoch 83/4000, Step 156, d_loss: 0.00198915577493608, g_loss: 6.499687671661377\n","Epoch 83/4000, Step 157, d_loss: 0.00810365378856659, g_loss: 6.111480712890625\n","Epoch 83/4000, Step 158, d_loss: 0.002725865226238966, g_loss: 5.587996959686279\n","Epoch 83/4000, Step 159, d_loss: 0.0005452327895909548, g_loss: 7.826642036437988\n","Epoch 83/4000, Step 160, d_loss: 0.0038000638596713543, g_loss: 4.733680248260498\n","Epoch 83/4000, Step 161, d_loss: 0.0039899651892483234, g_loss: 5.448929786682129\n","Epoch 83/4000, Step 162, d_loss: 0.00330493226647377, g_loss: 4.919045448303223\n","Epoch 83/4000, Step 163, d_loss: 0.0018170143011957407, g_loss: 9.590563774108887\n","Epoch 83/4000, Step 164, d_loss: 0.001842380384914577, g_loss: 10.56495189666748\n","Epoch 83/4000, Step 165, d_loss: 0.005819395650178194, g_loss: 4.799776077270508\n","Epoch 83/4000, Step 166, d_loss: 0.0011093999492004514, g_loss: 6.68283224105835\n","Epoch 83/4000, Step 167, d_loss: 0.0020710548851639032, g_loss: 4.9288787841796875\n","Epoch 83/4000, Step 168, d_loss: 0.010464257560670376, g_loss: 7.990015029907227\n","Epoch 83/4000, Step 169, d_loss: 0.006237640976905823, g_loss: 7.023369789123535\n","Epoch 83/4000, Step 170, d_loss: 0.004993559792637825, g_loss: 7.004676342010498\n","Epoch 83/4000, Step 171, d_loss: 0.0007788180373609066, g_loss: 8.116418838500977\n","Epoch 83/4000, Step 172, d_loss: 0.0022385152988135815, g_loss: 9.775497436523438\n","Epoch 83/4000, Step 173, d_loss: 0.004414170049130917, g_loss: 6.941930770874023\n","Epoch 83/4000, Step 174, d_loss: 0.0003900041920132935, g_loss: 7.303695201873779\n","Epoch 83/4000, Step 175, d_loss: 0.010974006727337837, g_loss: 7.3689446449279785\n","Epoch 83/4000, Step 176, d_loss: 0.005621827207505703, g_loss: 7.89508581161499\n","Epoch 83/4000, Step 177, d_loss: 0.007372672203928232, g_loss: 9.652948379516602\n","Epoch 83/4000, Step 178, d_loss: 0.00308221485465765, g_loss: 8.944422721862793\n","Epoch 83/4000, Step 179, d_loss: 0.002004863228648901, g_loss: 9.497363090515137\n","Epoch 83/4000, Step 180, d_loss: 0.0025666095316410065, g_loss: 7.654694557189941\n","Epoch 83/4000, Step 181, d_loss: 0.0030656345188617706, g_loss: 6.9519853591918945\n","Epoch 83/4000, Step 182, d_loss: 0.00396034074947238, g_loss: 6.8290510177612305\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 84/4000, Step 1, d_loss: 0.0036508566699922085, g_loss: 5.693211555480957\n","Epoch 84/4000, Step 2, d_loss: 0.005813483148813248, g_loss: 7.833690166473389\n","Epoch 84/4000, Step 3, d_loss: 0.0016297921538352966, g_loss: 9.694729804992676\n","Epoch 84/4000, Step 4, d_loss: 0.001218640012666583, g_loss: 7.418432235717773\n","Epoch 84/4000, Step 5, d_loss: 0.007040831260383129, g_loss: 9.136073112487793\n","Epoch 84/4000, Step 6, d_loss: 0.06840021908283234, g_loss: 6.988083362579346\n","Epoch 84/4000, Step 7, d_loss: 0.005130958743393421, g_loss: 5.938102722167969\n","Epoch 84/4000, Step 8, d_loss: 0.003605015343055129, g_loss: 8.538824081420898\n","Epoch 84/4000, Step 9, d_loss: 0.0016355166444554925, g_loss: 6.7254228591918945\n","Epoch 84/4000, Step 10, d_loss: 0.002054589567705989, g_loss: 5.816390514373779\n","Epoch 84/4000, Step 11, d_loss: 0.006480679381638765, g_loss: 4.926464080810547\n","Epoch 84/4000, Step 12, d_loss: 0.002304050140082836, g_loss: 6.45429801940918\n","Epoch 84/4000, Step 13, d_loss: 0.0007570755551569164, g_loss: 4.191603660583496\n","Epoch 84/4000, Step 14, d_loss: 0.008645350113511086, g_loss: 6.137770175933838\n","Epoch 84/4000, Step 15, d_loss: 0.0033777477219700813, g_loss: 5.724972724914551\n","Epoch 84/4000, Step 16, d_loss: 0.014654413796961308, g_loss: 6.482639789581299\n","Epoch 84/4000, Step 17, d_loss: 0.007257289253175259, g_loss: 5.429034233093262\n","Epoch 84/4000, Step 18, d_loss: 0.012401975691318512, g_loss: 2.527269124984741\n","Epoch 84/4000, Step 19, d_loss: 0.01870378665626049, g_loss: 5.940865993499756\n","Epoch 84/4000, Step 20, d_loss: 0.022591320797801018, g_loss: 6.601063251495361\n","Epoch 84/4000, Step 21, d_loss: 0.004728829022496939, g_loss: 5.222850799560547\n","Epoch 84/4000, Step 22, d_loss: 0.00994410552084446, g_loss: 4.231016159057617\n","Epoch 84/4000, Step 23, d_loss: 0.010433485731482506, g_loss: 9.027155876159668\n","Epoch 84/4000, Step 24, d_loss: 0.00888646487146616, g_loss: 6.403379440307617\n","Epoch 84/4000, Step 25, d_loss: 0.00331699475646019, g_loss: 3.8930745124816895\n","Epoch 84/4000, Step 26, d_loss: 0.0013739274581894279, g_loss: 6.2480621337890625\n","Epoch 84/4000, Step 27, d_loss: 0.0007800778839737177, g_loss: 5.756989479064941\n","Epoch 84/4000, Step 28, d_loss: 0.0056119016371667385, g_loss: 5.961608409881592\n","Epoch 84/4000, Step 29, d_loss: 0.0007296239491552114, g_loss: 6.290089130401611\n","Epoch 84/4000, Step 30, d_loss: 0.001790714100934565, g_loss: 5.411221981048584\n","Epoch 84/4000, Step 31, d_loss: 0.0003758453531190753, g_loss: 7.252551555633545\n","Epoch 84/4000, Step 32, d_loss: 0.0011466359719634056, g_loss: 6.750130653381348\n","Epoch 84/4000, Step 33, d_loss: 0.0036103632301092148, g_loss: 7.70106840133667\n","Epoch 84/4000, Step 34, d_loss: 0.000719692965503782, g_loss: 7.772311687469482\n","Epoch 84/4000, Step 35, d_loss: 0.011437778361141682, g_loss: 8.563844680786133\n","Epoch 84/4000, Step 36, d_loss: 0.0019403807818889618, g_loss: 7.301879405975342\n","Epoch 84/4000, Step 37, d_loss: 0.07351179420948029, g_loss: 4.895425319671631\n","Epoch 84/4000, Step 38, d_loss: 0.004256857093423605, g_loss: 7.205628395080566\n","Epoch 84/4000, Step 39, d_loss: 0.0076527162455022335, g_loss: 4.973313331604004\n","Epoch 84/4000, Step 40, d_loss: 0.0027449489571154118, g_loss: 5.207584381103516\n","Epoch 84/4000, Step 41, d_loss: 0.009813974611461163, g_loss: 8.484831809997559\n","Epoch 84/4000, Step 42, d_loss: 0.00620862003415823, g_loss: 9.88254165649414\n","Epoch 84/4000, Step 43, d_loss: 0.05454287678003311, g_loss: 3.426414966583252\n","Epoch 84/4000, Step 44, d_loss: 0.06038496270775795, g_loss: 6.61048698425293\n","Epoch 84/4000, Step 45, d_loss: 0.0058127352967858315, g_loss: 5.548280239105225\n","Epoch 84/4000, Step 46, d_loss: 0.1330573409795761, g_loss: 5.244961738586426\n","Epoch 84/4000, Step 47, d_loss: 0.01780731789767742, g_loss: 4.082566738128662\n","Epoch 84/4000, Step 48, d_loss: 0.006452898494899273, g_loss: 5.452442646026611\n","Epoch 84/4000, Step 49, d_loss: 0.002063766121864319, g_loss: 8.07002067565918\n","Epoch 84/4000, Step 50, d_loss: 0.004356664139777422, g_loss: 9.78285026550293\n","Epoch 84/4000, Step 51, d_loss: 0.009012266993522644, g_loss: 8.152791976928711\n","Epoch 84/4000, Step 52, d_loss: 0.008890609256923199, g_loss: 9.320751190185547\n","Epoch 84/4000, Step 53, d_loss: 0.004184423945844173, g_loss: 8.580805778503418\n","Epoch 84/4000, Step 54, d_loss: 0.11505714803934097, g_loss: 6.954469680786133\n","Epoch 84/4000, Step 55, d_loss: 0.07480698078870773, g_loss: 7.148993968963623\n","Epoch 84/4000, Step 56, d_loss: 0.002129515167325735, g_loss: 3.6821584701538086\n","Epoch 84/4000, Step 57, d_loss: 0.0022559156641364098, g_loss: 8.021490097045898\n","Epoch 84/4000, Step 58, d_loss: 0.033586807548999786, g_loss: 3.2234015464782715\n","Epoch 84/4000, Step 59, d_loss: 0.10900867730379105, g_loss: 2.7811429500579834\n","Epoch 84/4000, Step 60, d_loss: 0.22057552635669708, g_loss: 3.3459694385528564\n","Epoch 84/4000, Step 61, d_loss: 0.027809254825115204, g_loss: 7.070324420928955\n","Epoch 84/4000, Step 62, d_loss: 0.0827445313334465, g_loss: 5.978278636932373\n","Epoch 84/4000, Step 63, d_loss: 0.03704465180635452, g_loss: 3.6099135875701904\n","Epoch 84/4000, Step 64, d_loss: 0.021060628816485405, g_loss: 4.41673469543457\n","Epoch 84/4000, Step 65, d_loss: 0.004333506338298321, g_loss: 3.4219090938568115\n","Epoch 84/4000, Step 66, d_loss: 0.012236316688358784, g_loss: 9.230477333068848\n","Epoch 84/4000, Step 67, d_loss: 0.011958075687289238, g_loss: 6.549732208251953\n","Epoch 84/4000, Step 68, d_loss: 0.004164533223956823, g_loss: 2.221369743347168\n","Epoch 84/4000, Step 69, d_loss: 0.007652506232261658, g_loss: 6.184664726257324\n","Epoch 84/4000, Step 70, d_loss: 0.0032728812657296658, g_loss: 5.699893951416016\n","Epoch 84/4000, Step 71, d_loss: 0.011255014687776566, g_loss: 4.079038143157959\n","Epoch 84/4000, Step 72, d_loss: 0.1149854063987732, g_loss: 6.371929168701172\n","Epoch 84/4000, Step 73, d_loss: 0.014432830736041069, g_loss: 3.0978007316589355\n","Epoch 84/4000, Step 74, d_loss: 0.010809002444148064, g_loss: 8.271682739257812\n","Epoch 84/4000, Step 75, d_loss: 0.05362459644675255, g_loss: 4.589052200317383\n","Epoch 84/4000, Step 76, d_loss: 0.09447216987609863, g_loss: 7.699120044708252\n","Epoch 84/4000, Step 77, d_loss: 0.018886685371398926, g_loss: 6.9315619468688965\n","Epoch 84/4000, Step 78, d_loss: 0.06855214387178421, g_loss: 4.933165073394775\n","Epoch 84/4000, Step 79, d_loss: 0.0015047653578221798, g_loss: 3.3541500568389893\n","Epoch 84/4000, Step 80, d_loss: 0.01505950279533863, g_loss: 3.4515581130981445\n","Epoch 84/4000, Step 81, d_loss: 0.02111067995429039, g_loss: 4.615920543670654\n","Epoch 84/4000, Step 82, d_loss: 0.0022117197513580322, g_loss: 2.7166082859039307\n","Epoch 84/4000, Step 83, d_loss: 0.0025992821902036667, g_loss: 4.718873500823975\n","Epoch 84/4000, Step 84, d_loss: 0.05859921872615814, g_loss: 2.8305835723876953\n","Epoch 84/4000, Step 85, d_loss: 0.023983528837561607, g_loss: 4.503509044647217\n","Epoch 84/4000, Step 86, d_loss: 0.04864537715911865, g_loss: 5.116626739501953\n","Epoch 84/4000, Step 87, d_loss: 0.01845249906182289, g_loss: 6.004580974578857\n","Epoch 84/4000, Step 88, d_loss: 0.002835034392774105, g_loss: 5.028814792633057\n","Epoch 84/4000, Step 89, d_loss: 0.006426343694329262, g_loss: 4.464182376861572\n","Epoch 84/4000, Step 90, d_loss: 0.004676565062254667, g_loss: 6.184194564819336\n","Epoch 84/4000, Step 91, d_loss: 0.04188225045800209, g_loss: 5.543033599853516\n","Epoch 84/4000, Step 92, d_loss: 0.0028240866959095, g_loss: 4.796102046966553\n","Epoch 84/4000, Step 93, d_loss: 0.008003534749150276, g_loss: 3.354740858078003\n","Epoch 84/4000, Step 94, d_loss: 0.05210323631763458, g_loss: 8.357498168945312\n","Epoch 84/4000, Step 95, d_loss: 0.005242363549768925, g_loss: 3.687664747238159\n","Epoch 84/4000, Step 96, d_loss: 0.004040195606648922, g_loss: 7.090073585510254\n","Epoch 84/4000, Step 97, d_loss: 0.0028516233433037996, g_loss: 4.326944828033447\n","Epoch 84/4000, Step 98, d_loss: 0.02142716571688652, g_loss: 3.0443549156188965\n","Epoch 84/4000, Step 99, d_loss: 0.0027823911514133215, g_loss: 7.035915851593018\n","Epoch 84/4000, Step 100, d_loss: 0.026277322322130203, g_loss: 3.651641607284546\n","Epoch 84/4000, Step 101, d_loss: 0.0049117919988930225, g_loss: 6.512683391571045\n","Epoch 84/4000, Step 102, d_loss: 0.006861486006528139, g_loss: 4.79032039642334\n","Epoch 84/4000, Step 103, d_loss: 0.002151621738448739, g_loss: 6.582661151885986\n","Epoch 84/4000, Step 104, d_loss: 0.0011137122055515647, g_loss: 6.472686290740967\n","Epoch 84/4000, Step 105, d_loss: 0.02392113208770752, g_loss: 6.815688610076904\n","Epoch 84/4000, Step 106, d_loss: 0.009320534765720367, g_loss: 3.9615285396575928\n","Epoch 84/4000, Step 107, d_loss: 0.0019415756687521935, g_loss: 2.813904047012329\n","Epoch 84/4000, Step 108, d_loss: 0.001220089616253972, g_loss: 6.7717413902282715\n","Epoch 84/4000, Step 109, d_loss: 0.017334412783384323, g_loss: 6.244258880615234\n","Epoch 84/4000, Step 110, d_loss: 0.0020664462354034185, g_loss: 4.2923431396484375\n","Epoch 84/4000, Step 111, d_loss: 0.003025538520887494, g_loss: 5.928154468536377\n","Epoch 84/4000, Step 112, d_loss: 0.0012164243962615728, g_loss: 6.091660022735596\n","Epoch 84/4000, Step 113, d_loss: 0.006874258629977703, g_loss: 6.23232364654541\n","Epoch 84/4000, Step 114, d_loss: 0.004457989241927862, g_loss: 4.703400135040283\n","Epoch 84/4000, Step 115, d_loss: 0.0017069275490939617, g_loss: 4.730997562408447\n","Epoch 84/4000, Step 116, d_loss: 0.02760298177599907, g_loss: 7.869965076446533\n","Epoch 84/4000, Step 117, d_loss: 0.008700345642864704, g_loss: 6.218182563781738\n","Epoch 84/4000, Step 118, d_loss: 0.007117611821740866, g_loss: 5.262576103210449\n","Epoch 84/4000, Step 119, d_loss: 0.01317102462053299, g_loss: 4.1536664962768555\n","Epoch 84/4000, Step 120, d_loss: 0.008517717942595482, g_loss: 9.943302154541016\n","Epoch 84/4000, Step 121, d_loss: 0.013103142380714417, g_loss: 4.806581974029541\n","Epoch 84/4000, Step 122, d_loss: 0.003232632065191865, g_loss: 5.192846298217773\n","Epoch 84/4000, Step 123, d_loss: 0.00621374137699604, g_loss: 8.332402229309082\n","Epoch 84/4000, Step 124, d_loss: 0.002197208348661661, g_loss: 6.164429664611816\n","Epoch 84/4000, Step 125, d_loss: 0.009230580180883408, g_loss: 8.785765647888184\n","Epoch 84/4000, Step 126, d_loss: 0.02775660529732704, g_loss: 8.317015647888184\n","Epoch 84/4000, Step 127, d_loss: 0.014318756759166718, g_loss: 7.44875955581665\n","Epoch 84/4000, Step 128, d_loss: 0.0008317138999700546, g_loss: 7.514841556549072\n","Epoch 84/4000, Step 129, d_loss: 0.0025929189287126064, g_loss: 8.702938079833984\n","Epoch 84/4000, Step 130, d_loss: 0.011573228985071182, g_loss: 5.44218111038208\n","Epoch 84/4000, Step 131, d_loss: 0.023801114410161972, g_loss: 8.97739315032959\n","Epoch 84/4000, Step 132, d_loss: 0.0022348198108375072, g_loss: 8.585831642150879\n","Epoch 84/4000, Step 133, d_loss: 0.019057845696806908, g_loss: 9.321535110473633\n","Epoch 84/4000, Step 134, d_loss: 0.003133830614387989, g_loss: 5.60277795791626\n","Epoch 84/4000, Step 135, d_loss: 0.011391188949346542, g_loss: 4.747458457946777\n","Epoch 84/4000, Step 136, d_loss: 0.0016845527570694685, g_loss: 4.077919006347656\n","Epoch 84/4000, Step 137, d_loss: 0.0005554679082706571, g_loss: 3.8058245182037354\n","Epoch 84/4000, Step 138, d_loss: 0.0012606782838702202, g_loss: 7.386133193969727\n","Epoch 84/4000, Step 139, d_loss: 0.00368278450332582, g_loss: 6.481849193572998\n","Epoch 84/4000, Step 140, d_loss: 0.0007193872006610036, g_loss: 3.794151544570923\n","Epoch 84/4000, Step 141, d_loss: 0.018435895442962646, g_loss: 7.938069820404053\n","Epoch 84/4000, Step 142, d_loss: 0.010050365701317787, g_loss: 5.262922763824463\n","Epoch 84/4000, Step 143, d_loss: 0.0033630910329520702, g_loss: 7.098714828491211\n","Epoch 84/4000, Step 144, d_loss: 0.042729586362838745, g_loss: 8.56644058227539\n","Epoch 84/4000, Step 145, d_loss: 0.007728139869868755, g_loss: 10.645317077636719\n","Epoch 84/4000, Step 146, d_loss: 0.0011184936156496406, g_loss: 4.839768886566162\n","Epoch 84/4000, Step 147, d_loss: 0.00567210279405117, g_loss: 4.908626079559326\n","Epoch 84/4000, Step 148, d_loss: 0.0021384565625339746, g_loss: 4.5549421310424805\n","Epoch 84/4000, Step 149, d_loss: 0.004435023758560419, g_loss: 4.926296234130859\n","Epoch 84/4000, Step 150, d_loss: 0.006614746060222387, g_loss: 6.882431983947754\n","Epoch 84/4000, Step 151, d_loss: 0.0023279914166778326, g_loss: 7.025774955749512\n","Epoch 84/4000, Step 152, d_loss: 0.0038596256636083126, g_loss: 6.304727077484131\n","Epoch 84/4000, Step 153, d_loss: 0.01313882227987051, g_loss: 4.933083534240723\n","Epoch 84/4000, Step 154, d_loss: 0.013120576739311218, g_loss: 9.577832221984863\n","Epoch 84/4000, Step 155, d_loss: 0.004720563068985939, g_loss: 9.916666984558105\n","Epoch 84/4000, Step 156, d_loss: 0.003683012444525957, g_loss: 4.024537086486816\n","Epoch 84/4000, Step 157, d_loss: 0.002894507721066475, g_loss: 6.7369866371154785\n","Epoch 84/4000, Step 158, d_loss: 0.000687961932271719, g_loss: 7.381781578063965\n","Epoch 84/4000, Step 159, d_loss: 0.012506346218287945, g_loss: 5.654870986938477\n","Epoch 84/4000, Step 160, d_loss: 0.0016610408201813698, g_loss: 6.590940952301025\n","Epoch 84/4000, Step 161, d_loss: 0.007463518530130386, g_loss: 9.150871276855469\n","Epoch 84/4000, Step 162, d_loss: 0.005011776927858591, g_loss: 7.435730457305908\n","Epoch 84/4000, Step 163, d_loss: 0.01271839439868927, g_loss: 4.079473972320557\n","Epoch 84/4000, Step 164, d_loss: 0.00045590667286887765, g_loss: 6.245736598968506\n","Epoch 84/4000, Step 165, d_loss: 0.0141008161008358, g_loss: 10.416860580444336\n","Epoch 84/4000, Step 166, d_loss: 0.0008368344279006124, g_loss: 7.523024082183838\n","Epoch 84/4000, Step 167, d_loss: 0.0008669397211633623, g_loss: 6.146925449371338\n","Epoch 84/4000, Step 168, d_loss: 0.0007015314768068492, g_loss: 9.50163459777832\n","Epoch 84/4000, Step 169, d_loss: 0.024001114070415497, g_loss: 5.26083517074585\n","Epoch 84/4000, Step 170, d_loss: 0.005819197744131088, g_loss: 8.03144645690918\n","Epoch 84/4000, Step 171, d_loss: 0.006733683869242668, g_loss: 6.479541778564453\n","Epoch 84/4000, Step 172, d_loss: 0.002589968964457512, g_loss: 7.269070148468018\n","Epoch 84/4000, Step 173, d_loss: 0.003594504203647375, g_loss: 4.071781158447266\n","Epoch 84/4000, Step 174, d_loss: 0.0013217838713899255, g_loss: 5.860929489135742\n","Epoch 84/4000, Step 175, d_loss: 0.0014060649555176497, g_loss: 9.54098129272461\n","Epoch 84/4000, Step 176, d_loss: 0.013007307425141335, g_loss: 6.406540393829346\n","Epoch 84/4000, Step 177, d_loss: 0.0025564217939972878, g_loss: 7.6591715812683105\n","Epoch 84/4000, Step 178, d_loss: 0.0024111452512443066, g_loss: 5.25758171081543\n","Epoch 84/4000, Step 179, d_loss: 0.002448704792186618, g_loss: 6.023990631103516\n","Epoch 84/4000, Step 180, d_loss: 0.0009609229164198041, g_loss: 6.910026550292969\n","Epoch 84/4000, Step 181, d_loss: 0.004982163198292255, g_loss: 5.782686710357666\n","Epoch 84/4000, Step 182, d_loss: 3.3698713779449463, g_loss: 7.2624711990356445\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 85/4000, Step 1, d_loss: 0.04546446353197098, g_loss: 4.144789695739746\n","Epoch 85/4000, Step 2, d_loss: 0.11542239785194397, g_loss: 1.5432380437850952\n","Epoch 85/4000, Step 3, d_loss: 0.24041567742824554, g_loss: 1.379401683807373\n","Epoch 85/4000, Step 4, d_loss: 0.17996171116828918, g_loss: 1.703625202178955\n","Epoch 85/4000, Step 5, d_loss: 0.08582724630832672, g_loss: 0.14628866314888\n","Epoch 85/4000, Step 6, d_loss: 0.2091771960258484, g_loss: 0.4590156078338623\n","Epoch 85/4000, Step 7, d_loss: 2.7334868907928467, g_loss: 2.663515567779541\n","Epoch 85/4000, Step 8, d_loss: 0.39385300874710083, g_loss: 6.727004528045654\n","Epoch 85/4000, Step 9, d_loss: 0.17023895680904388, g_loss: 7.589892387390137\n","Epoch 85/4000, Step 10, d_loss: 0.5541237592697144, g_loss: 6.819180965423584\n","Epoch 85/4000, Step 11, d_loss: 0.9500184059143066, g_loss: 2.207415819168091\n","Epoch 85/4000, Step 12, d_loss: 0.17455726861953735, g_loss: 4.83796501159668\n","Epoch 85/4000, Step 13, d_loss: 0.213846817612648, g_loss: 8.111307144165039\n","Epoch 85/4000, Step 14, d_loss: 0.229797825217247, g_loss: 3.967803478240967\n","Epoch 85/4000, Step 15, d_loss: 0.3922120928764343, g_loss: 2.039595603942871\n","Epoch 85/4000, Step 16, d_loss: 0.12579752504825592, g_loss: 0.42659229040145874\n","Epoch 85/4000, Step 17, d_loss: 0.22824998199939728, g_loss: 0.062837153673172\n","Epoch 85/4000, Step 18, d_loss: 0.3576224446296692, g_loss: 2.4967081546783447\n","Epoch 85/4000, Step 19, d_loss: 1.0351718664169312, g_loss: 0.24835610389709473\n","Epoch 85/4000, Step 20, d_loss: 0.7184658646583557, g_loss: 1.4232383966445923\n","Epoch 85/4000, Step 21, d_loss: 0.039175961166620255, g_loss: 3.952305793762207\n","Epoch 85/4000, Step 22, d_loss: 0.09795604646205902, g_loss: 2.8888041973114014\n","Epoch 85/4000, Step 23, d_loss: 0.31615975499153137, g_loss: 3.1278858184814453\n","Epoch 85/4000, Step 24, d_loss: 0.6967187523841858, g_loss: 3.2259294986724854\n","Epoch 85/4000, Step 25, d_loss: 0.5148003697395325, g_loss: 2.5898280143737793\n","Epoch 85/4000, Step 26, d_loss: 0.23680835962295532, g_loss: 4.366158485412598\n","Epoch 85/4000, Step 27, d_loss: 0.02553544193506241, g_loss: 0.4954635798931122\n","Epoch 85/4000, Step 28, d_loss: 0.14977282285690308, g_loss: 0.5332424640655518\n","Epoch 85/4000, Step 29, d_loss: 0.7035374641418457, g_loss: 1.1959470510482788\n","Epoch 85/4000, Step 30, d_loss: 0.9880722761154175, g_loss: 5.6189751625061035\n","Epoch 85/4000, Step 31, d_loss: 0.36036667227745056, g_loss: 0.9460548162460327\n","Epoch 85/4000, Step 32, d_loss: 0.189795583486557, g_loss: 5.437965393066406\n","Epoch 85/4000, Step 33, d_loss: 0.3093152344226837, g_loss: 4.165882110595703\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print (device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 32\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 200\n","\n","# Size of feature maps in discriminator\n","ndf = 1200\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='G:/My Drive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/ag2/',\n","                              transform=transforms.Compose([\n","                              transforms.Resize(image_size),\n","                              transforms.CenterCrop(image_size),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                              ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # input is ``(nc) x 64 x 64``\n","            nn.Conv2d(3, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.2),\n","            nn.Conv2d(ndf , ndf * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 2, ndf, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ndf),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(ndf, 1, 3, 1, 0, bias=False),\n","\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input).squeeze()\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='checkpointDZ.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","            save_image(samples, f'G:/My Drive/Colab Notebooks/dandies/new_dandies/dZoo2_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","                current_batch_size = images.size(0)\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((current_batch_size,), 1.0))\n","\n","\n","\n","\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","\n","                d_loss_real = self.loss(real_outputs.squeeze(0), real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(current_batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images).view(-1).squeeze()\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointDZ.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(4000, dataloader, resume=False)"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7TZvDVQTtN6","executionInfo":{"status":"ok","timestamp":1690605787603,"user_tz":240,"elapsed":19,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"}},"outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1GN2L9UPYR6TCzDsqMBaHwwc6PkLlmsyg","authorship_tag":"ABX9TyMwfxsdOMefUWtbtlTQSmot"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}