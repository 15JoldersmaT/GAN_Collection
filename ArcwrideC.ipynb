{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"8eeb04bb-008e-410d-d3db-8028753aca77"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","test\n","=> loading checkpoint 'checkpointC.pth'\n","=> loaded checkpoint 'checkpointC.pth' (epoch 91)\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 92/2000, Step 1, d_loss: 0.3454250395298004, g_loss: 4.630792617797852\n","Epoch 92/2000, Step 2, d_loss: 0.36729857325553894, g_loss: 5.792097091674805\n","Epoch 92/2000, Step 3, d_loss: 0.3931560218334198, g_loss: 5.037808418273926\n","Epoch 92/2000, Step 4, d_loss: 0.4994896650314331, g_loss: 4.085577487945557\n","Epoch 92/2000, Step 5, d_loss: 0.37737131118774414, g_loss: 3.9561519622802734\n","Epoch 92/2000, Step 6, d_loss: 0.35775595903396606, g_loss: 3.5633347034454346\n","Epoch 92/2000, Step 7, d_loss: 0.38570278882980347, g_loss: 3.839994430541992\n","Epoch 92/2000, Step 8, d_loss: 0.39342570304870605, g_loss: 3.4786500930786133\n","Epoch 92/2000, Step 9, d_loss: 0.40222984552383423, g_loss: 4.313447952270508\n","Epoch 92/2000, Step 10, d_loss: 0.3964149057865143, g_loss: 2.850257635116577\n","Epoch 92/2000, Step 11, d_loss: 0.3594900369644165, g_loss: 3.30694317817688\n","Epoch 92/2000, Step 12, d_loss: 0.39526087045669556, g_loss: 4.713199138641357\n","Epoch 92/2000, Step 13, d_loss: 0.3498438596725464, g_loss: 6.33360481262207\n","Epoch 92/2000, Step 14, d_loss: 0.3450551927089691, g_loss: 5.609093189239502\n","Epoch 92/2000, Step 15, d_loss: 0.4023532271385193, g_loss: 4.71923303604126\n","Epoch 92/2000, Step 16, d_loss: 0.3636032044887543, g_loss: 3.3188514709472656\n","Epoch 92/2000, Step 17, d_loss: 0.40634092688560486, g_loss: 5.264647006988525\n","Epoch 92/2000, Step 18, d_loss: 0.3450140357017517, g_loss: 8.595405578613281\n","Epoch 92/2000, Step 19, d_loss: 0.36260467767715454, g_loss: 3.8214480876922607\n","Epoch 92/2000, Step 20, d_loss: 0.3828279674053192, g_loss: 2.9668684005737305\n","Epoch 92/2000, Step 21, d_loss: 0.4121297001838684, g_loss: 3.8703479766845703\n","Epoch 92/2000, Step 22, d_loss: 0.38754817843437195, g_loss: 4.773373603820801\n","Epoch 92/2000, Step 23, d_loss: 0.3598414361476898, g_loss: 5.311311721801758\n","Epoch 92/2000, Step 24, d_loss: 0.3637535274028778, g_loss: 5.072623252868652\n","Epoch 92/2000, Step 25, d_loss: 0.35776710510253906, g_loss: 4.716443061828613\n","Epoch 92/2000, Step 26, d_loss: 0.39663010835647583, g_loss: 3.6175715923309326\n","Epoch 92/2000, Step 27, d_loss: 0.350414901971817, g_loss: 5.246720790863037\n","Epoch 92/2000, Step 28, d_loss: 0.35745665431022644, g_loss: 4.886893272399902\n","Epoch 92/2000, Step 29, d_loss: 0.3482123017311096, g_loss: 4.7669572830200195\n","Epoch 92/2000, Step 30, d_loss: 0.38072195649147034, g_loss: 4.784299850463867\n","Epoch 92/2000, Step 31, d_loss: 0.40343478322029114, g_loss: 4.5492095947265625\n","Epoch 92/2000, Step 32, d_loss: 0.3833654224872589, g_loss: 4.607828140258789\n","Epoch 92/2000, Step 33, d_loss: 0.4282897710800171, g_loss: 3.5438365936279297\n","Epoch 92/2000, Step 34, d_loss: 0.3512459397315979, g_loss: 6.877525806427002\n","Epoch 92/2000, Step 35, d_loss: 0.3656997084617615, g_loss: 6.449462890625\n","Epoch 92/2000, Step 36, d_loss: 0.4481959044933319, g_loss: 5.216311931610107\n","Epoch 92/2000, Step 37, d_loss: 0.38912636041641235, g_loss: 4.459142208099365\n","Epoch 92/2000, Step 38, d_loss: 0.38091379404067993, g_loss: 2.1303322315216064\n","Epoch 92/2000, Step 39, d_loss: 0.5287562012672424, g_loss: 5.506410598754883\n","Epoch 92/2000, Step 40, d_loss: 0.3849060833454132, g_loss: 3.0094974040985107\n","Epoch 92/2000, Step 41, d_loss: 0.685133695602417, g_loss: 5.569491386413574\n","Epoch 92/2000, Step 42, d_loss: 0.37931200861930847, g_loss: 3.1925148963928223\n","Epoch 92/2000, Step 43, d_loss: 0.3599414527416229, g_loss: 4.5220465660095215\n","Epoch 92/2000, Step 44, d_loss: 0.35340604186058044, g_loss: 3.880781888961792\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 93/2000, Step 1, d_loss: 0.34779611229896545, g_loss: 3.56646466255188\n","Epoch 93/2000, Step 2, d_loss: 0.35809525847435, g_loss: 4.552416801452637\n","Epoch 93/2000, Step 3, d_loss: 0.3542693853378296, g_loss: 6.03469181060791\n","Epoch 93/2000, Step 4, d_loss: 0.37619245052337646, g_loss: 4.673128604888916\n","Epoch 93/2000, Step 5, d_loss: 0.4028933346271515, g_loss: 5.91769552230835\n","Epoch 93/2000, Step 6, d_loss: 0.36572813987731934, g_loss: 4.28022575378418\n","Epoch 93/2000, Step 7, d_loss: 0.35567769408226013, g_loss: 5.948916912078857\n","Epoch 93/2000, Step 8, d_loss: 0.3795185983181, g_loss: 3.907935857772827\n","Epoch 93/2000, Step 9, d_loss: 0.38898858428001404, g_loss: 3.4016294479370117\n","Epoch 93/2000, Step 10, d_loss: 0.401535302400589, g_loss: 3.6890056133270264\n","Epoch 93/2000, Step 11, d_loss: 0.47742611169815063, g_loss: 3.988299608230591\n","Epoch 93/2000, Step 12, d_loss: 0.3819551467895508, g_loss: 3.6942899227142334\n","Epoch 93/2000, Step 13, d_loss: 0.4134507477283478, g_loss: 3.1980395317077637\n","Epoch 93/2000, Step 14, d_loss: 0.38245952129364014, g_loss: 5.494253635406494\n","Epoch 93/2000, Step 15, d_loss: 0.44984832406044006, g_loss: 4.0957746505737305\n","Epoch 93/2000, Step 16, d_loss: 0.3584904074668884, g_loss: 2.055614471435547\n","Epoch 93/2000, Step 17, d_loss: 0.3746890127658844, g_loss: 3.684819221496582\n","Epoch 93/2000, Step 18, d_loss: 0.42501893639564514, g_loss: 3.9561550617218018\n","Epoch 93/2000, Step 19, d_loss: 0.6521717309951782, g_loss: 5.316155910491943\n","Epoch 93/2000, Step 20, d_loss: 0.4608317017555237, g_loss: 4.965906620025635\n","Epoch 93/2000, Step 21, d_loss: 0.3746643662452698, g_loss: 7.9309983253479\n","Epoch 93/2000, Step 22, d_loss: 0.390912264585495, g_loss: 5.146788120269775\n","Epoch 93/2000, Step 23, d_loss: 0.3634997606277466, g_loss: 4.764287948608398\n","Epoch 93/2000, Step 24, d_loss: 0.44359055161476135, g_loss: 4.75154447555542\n","Epoch 93/2000, Step 25, d_loss: 0.49510499835014343, g_loss: 2.1494576930999756\n","Epoch 93/2000, Step 26, d_loss: 0.36856913566589355, g_loss: 3.639305591583252\n","Epoch 93/2000, Step 27, d_loss: 0.3942866027355194, g_loss: 3.1705007553100586\n","Epoch 93/2000, Step 28, d_loss: 0.44049736857414246, g_loss: 1.5003584623336792\n","Epoch 93/2000, Step 29, d_loss: 0.487295538187027, g_loss: 1.3278709650039673\n","Epoch 93/2000, Step 30, d_loss: 0.627774178981781, g_loss: 3.572067975997925\n","Epoch 93/2000, Step 31, d_loss: 0.40659818053245544, g_loss: 3.8932929039001465\n","Epoch 93/2000, Step 32, d_loss: 0.4363248348236084, g_loss: 2.2719640731811523\n","Epoch 93/2000, Step 33, d_loss: 0.3836279511451721, g_loss: 4.907716751098633\n","Epoch 93/2000, Step 34, d_loss: 0.37204933166503906, g_loss: 3.707474946975708\n","Epoch 93/2000, Step 35, d_loss: 0.46477800607681274, g_loss: 5.8130621910095215\n","Epoch 93/2000, Step 36, d_loss: 0.48643720149993896, g_loss: 4.774906635284424\n","Epoch 93/2000, Step 37, d_loss: 0.48913082480430603, g_loss: 1.10482656955719\n","Epoch 93/2000, Step 38, d_loss: 0.41946929693222046, g_loss: 7.4260358810424805\n","Epoch 93/2000, Step 39, d_loss: 0.38406166434288025, g_loss: 1.6955970525741577\n","Epoch 93/2000, Step 40, d_loss: 0.4225844442844391, g_loss: 4.403968334197998\n","Epoch 93/2000, Step 41, d_loss: 0.3636315166950226, g_loss: 4.160773277282715\n","Epoch 93/2000, Step 42, d_loss: 0.4669024646282196, g_loss: 6.881723403930664\n","Epoch 93/2000, Step 43, d_loss: 0.40434321761131287, g_loss: 3.954281806945801\n","Epoch 93/2000, Step 44, d_loss: 0.42494744062423706, g_loss: 2.8295040130615234\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 94/2000, Step 1, d_loss: 0.4525466859340668, g_loss: 4.60716438293457\n","Epoch 94/2000, Step 2, d_loss: 0.4288093149662018, g_loss: 3.888913631439209\n","Epoch 94/2000, Step 3, d_loss: 0.3784598410129547, g_loss: 3.439535140991211\n","Epoch 94/2000, Step 4, d_loss: 0.364464670419693, g_loss: 5.128152370452881\n","Epoch 94/2000, Step 5, d_loss: 0.546181321144104, g_loss: 4.00980806350708\n","Epoch 94/2000, Step 6, d_loss: 0.3799295127391815, g_loss: 2.2465057373046875\n","Epoch 94/2000, Step 7, d_loss: 0.38086676597595215, g_loss: 4.577073097229004\n","Epoch 94/2000, Step 8, d_loss: 0.3850453794002533, g_loss: 3.3437507152557373\n","Epoch 94/2000, Step 9, d_loss: 0.43708693981170654, g_loss: 3.865387439727783\n","Epoch 94/2000, Step 10, d_loss: 0.46020370721817017, g_loss: 4.170064449310303\n","Epoch 94/2000, Step 11, d_loss: 0.3908940553665161, g_loss: 6.003015041351318\n","Epoch 94/2000, Step 12, d_loss: 0.3836197853088379, g_loss: 3.3616180419921875\n","Epoch 94/2000, Step 13, d_loss: 0.4029175937175751, g_loss: 4.946429252624512\n","Epoch 94/2000, Step 14, d_loss: 0.39905303716659546, g_loss: 6.067519664764404\n","Epoch 94/2000, Step 15, d_loss: 0.37902048230171204, g_loss: 4.184078693389893\n","Epoch 94/2000, Step 16, d_loss: 0.6709927916526794, g_loss: 3.864743232727051\n","Epoch 94/2000, Step 17, d_loss: 0.38641881942749023, g_loss: 1.7315804958343506\n","Epoch 94/2000, Step 18, d_loss: 0.44328486919403076, g_loss: 2.7885589599609375\n","Epoch 94/2000, Step 19, d_loss: 0.4982678294181824, g_loss: 2.5322320461273193\n","Epoch 94/2000, Step 20, d_loss: 0.6857567429542542, g_loss: 3.395721673965454\n","Epoch 94/2000, Step 21, d_loss: 0.40360164642333984, g_loss: 5.069467544555664\n","Epoch 94/2000, Step 22, d_loss: 0.4921230673789978, g_loss: 4.622866630554199\n","Epoch 94/2000, Step 23, d_loss: 0.3950996696949005, g_loss: 5.508861064910889\n","Epoch 94/2000, Step 24, d_loss: 0.39740195870399475, g_loss: 6.105112075805664\n","Epoch 94/2000, Step 25, d_loss: 0.4642390012741089, g_loss: 2.328230142593384\n","Epoch 94/2000, Step 26, d_loss: 0.4086155295372009, g_loss: 4.537325859069824\n","Epoch 94/2000, Step 27, d_loss: 0.3702707290649414, g_loss: 4.143263816833496\n","Epoch 94/2000, Step 28, d_loss: 0.4510396122932434, g_loss: 3.8206281661987305\n","Epoch 94/2000, Step 29, d_loss: 0.4231122136116028, g_loss: 4.950656414031982\n","Epoch 94/2000, Step 30, d_loss: 0.55158531665802, g_loss: 3.1617627143859863\n","Epoch 94/2000, Step 31, d_loss: 0.37759965658187866, g_loss: 4.504929065704346\n","Epoch 94/2000, Step 32, d_loss: 0.3635043799877167, g_loss: 3.990854263305664\n","Epoch 94/2000, Step 33, d_loss: 0.47880786657333374, g_loss: 6.050555229187012\n","Epoch 94/2000, Step 34, d_loss: 0.45687466859817505, g_loss: 5.709174633026123\n","Epoch 94/2000, Step 35, d_loss: 0.5011460781097412, g_loss: 5.800773620605469\n","Epoch 94/2000, Step 36, d_loss: 0.388784795999527, g_loss: 4.487476825714111\n","Epoch 94/2000, Step 37, d_loss: 0.38516512513160706, g_loss: 3.7168374061584473\n","Epoch 94/2000, Step 38, d_loss: 0.3831566572189331, g_loss: 2.906672954559326\n","Epoch 94/2000, Step 39, d_loss: 0.4497719407081604, g_loss: 3.9221413135528564\n","Epoch 94/2000, Step 40, d_loss: 0.3806622624397278, g_loss: 2.890244483947754\n","Epoch 94/2000, Step 41, d_loss: 0.41930216550827026, g_loss: 4.84678316116333\n","Epoch 94/2000, Step 42, d_loss: 0.43015095591545105, g_loss: 3.3620779514312744\n","Epoch 94/2000, Step 43, d_loss: 0.41075167059898376, g_loss: 6.267255783081055\n","Epoch 94/2000, Step 44, d_loss: 0.46834367513656616, g_loss: 4.097815036773682\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 95/2000, Step 1, d_loss: 0.4481947124004364, g_loss: 4.392517566680908\n","Epoch 95/2000, Step 2, d_loss: 0.4440896511077881, g_loss: 3.9796369075775146\n","Epoch 95/2000, Step 3, d_loss: 0.37452611327171326, g_loss: 4.140958786010742\n","Epoch 95/2000, Step 4, d_loss: 0.4846561849117279, g_loss: 3.6991753578186035\n","Epoch 95/2000, Step 5, d_loss: 0.627335786819458, g_loss: 2.8130433559417725\n","Epoch 95/2000, Step 6, d_loss: 0.4061790108680725, g_loss: 4.569186687469482\n","Epoch 95/2000, Step 7, d_loss: 0.3767061233520508, g_loss: 6.507969379425049\n","Epoch 95/2000, Step 8, d_loss: 0.39924734830856323, g_loss: 5.640733242034912\n","Epoch 95/2000, Step 9, d_loss: 0.40431347489356995, g_loss: 5.999768257141113\n","Epoch 95/2000, Step 10, d_loss: 0.5077170133590698, g_loss: 5.048114776611328\n","Epoch 95/2000, Step 11, d_loss: 0.5617057681083679, g_loss: 4.190727233886719\n","Epoch 95/2000, Step 12, d_loss: 0.44060784578323364, g_loss: 3.7586379051208496\n","Epoch 95/2000, Step 13, d_loss: 0.4294138252735138, g_loss: 3.887458562850952\n","Epoch 95/2000, Step 14, d_loss: 0.4285297989845276, g_loss: 4.892655372619629\n","Epoch 95/2000, Step 15, d_loss: 0.43101629614830017, g_loss: 4.690087795257568\n","Epoch 95/2000, Step 16, d_loss: 0.40299707651138306, g_loss: 3.9919345378875732\n","Epoch 95/2000, Step 17, d_loss: 0.385368287563324, g_loss: 2.79766845703125\n","Epoch 95/2000, Step 18, d_loss: 0.4169546961784363, g_loss: 3.9531311988830566\n","Epoch 95/2000, Step 19, d_loss: 0.41986575722694397, g_loss: 3.799067974090576\n","Epoch 95/2000, Step 20, d_loss: 0.3654058575630188, g_loss: 5.731789588928223\n","Epoch 95/2000, Step 21, d_loss: 0.4141680598258972, g_loss: 4.707186698913574\n","Epoch 95/2000, Step 22, d_loss: 0.36603647470474243, g_loss: 4.941803932189941\n","Epoch 95/2000, Step 23, d_loss: 0.3897664546966553, g_loss: 4.075632095336914\n","Epoch 95/2000, Step 24, d_loss: 0.40594708919525146, g_loss: 6.628662586212158\n","Epoch 95/2000, Step 25, d_loss: 0.36550095677375793, g_loss: 5.027707576751709\n","Epoch 95/2000, Step 26, d_loss: 0.37591609358787537, g_loss: 5.923580169677734\n","Epoch 95/2000, Step 27, d_loss: 0.394010066986084, g_loss: 5.0208845138549805\n","Epoch 95/2000, Step 28, d_loss: 0.3577527105808258, g_loss: 4.250575065612793\n","Epoch 95/2000, Step 29, d_loss: 0.4077792465686798, g_loss: 3.6469619274139404\n","Epoch 95/2000, Step 30, d_loss: 0.4162091314792633, g_loss: 3.736855983734131\n","Epoch 95/2000, Step 31, d_loss: 0.4408330023288727, g_loss: 2.2706851959228516\n","Epoch 95/2000, Step 32, d_loss: 0.4627600610256195, g_loss: 5.756377696990967\n","Epoch 95/2000, Step 33, d_loss: 0.5261600613594055, g_loss: 4.889915943145752\n","Epoch 95/2000, Step 34, d_loss: 0.3464285433292389, g_loss: 4.532623291015625\n","Epoch 95/2000, Step 35, d_loss: 0.5511540174484253, g_loss: 5.223840236663818\n","Epoch 95/2000, Step 36, d_loss: 0.35460206866264343, g_loss: 3.140960454940796\n","Epoch 95/2000, Step 37, d_loss: 0.47343847155570984, g_loss: 4.169074058532715\n","Epoch 95/2000, Step 38, d_loss: 0.3702775835990906, g_loss: 3.8575308322906494\n","Epoch 95/2000, Step 39, d_loss: 0.4407450258731842, g_loss: 4.9738006591796875\n","Epoch 95/2000, Step 40, d_loss: 0.39574381709098816, g_loss: 3.921682834625244\n","Epoch 95/2000, Step 41, d_loss: 0.3658651113510132, g_loss: 5.159597873687744\n","Epoch 95/2000, Step 42, d_loss: 0.4268907904624939, g_loss: 6.418087482452393\n","Epoch 95/2000, Step 43, d_loss: 0.36250218749046326, g_loss: 4.08169412612915\n","Epoch 95/2000, Step 44, d_loss: 0.9584483504295349, g_loss: 3.2807319164276123\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 96/2000, Step 1, d_loss: 0.3918256163597107, g_loss: 4.190606594085693\n","Epoch 96/2000, Step 2, d_loss: 0.5078510642051697, g_loss: 3.777430772781372\n","Epoch 96/2000, Step 3, d_loss: 0.5512466430664062, g_loss: 2.4015815258026123\n","Epoch 96/2000, Step 4, d_loss: 0.4681054353713989, g_loss: 2.739337205886841\n","Epoch 96/2000, Step 5, d_loss: 0.48166975378990173, g_loss: 2.177635669708252\n","Epoch 96/2000, Step 6, d_loss: 0.49677562713623047, g_loss: 5.36163330078125\n","Epoch 96/2000, Step 7, d_loss: 0.41058236360549927, g_loss: 5.8141937255859375\n","Epoch 96/2000, Step 8, d_loss: 0.3813025951385498, g_loss: 5.856912136077881\n","Epoch 96/2000, Step 9, d_loss: 0.47365671396255493, g_loss: 5.012617111206055\n","Epoch 96/2000, Step 10, d_loss: 0.4234572649002075, g_loss: 4.779868125915527\n","Epoch 96/2000, Step 11, d_loss: 0.39297908544540405, g_loss: 5.47284460067749\n","Epoch 96/2000, Step 12, d_loss: 0.35648053884506226, g_loss: 3.107682943344116\n","Epoch 96/2000, Step 13, d_loss: 0.40853118896484375, g_loss: 4.640450477600098\n","Epoch 96/2000, Step 14, d_loss: 0.4348647892475128, g_loss: 3.320549249649048\n","Epoch 96/2000, Step 15, d_loss: 0.4871380925178528, g_loss: 4.049814701080322\n","Epoch 96/2000, Step 16, d_loss: 0.3824474513530731, g_loss: 5.3200554847717285\n","Epoch 96/2000, Step 17, d_loss: 0.35925984382629395, g_loss: 4.304863929748535\n","Epoch 96/2000, Step 18, d_loss: 0.3762742280960083, g_loss: 4.321225643157959\n","Epoch 96/2000, Step 19, d_loss: 0.38653475046157837, g_loss: 6.875298023223877\n","Epoch 96/2000, Step 20, d_loss: 0.38888323307037354, g_loss: 4.785473823547363\n","Epoch 96/2000, Step 21, d_loss: 0.38710886240005493, g_loss: 4.4410176277160645\n","Epoch 96/2000, Step 22, d_loss: 0.3785436451435089, g_loss: 6.186681747436523\n","Epoch 96/2000, Step 23, d_loss: 0.3786846101284027, g_loss: 3.5468547344207764\n","Epoch 96/2000, Step 24, d_loss: 0.40327322483062744, g_loss: 2.2805986404418945\n","Epoch 96/2000, Step 25, d_loss: 0.3828429877758026, g_loss: 3.826779842376709\n","Epoch 96/2000, Step 26, d_loss: 0.385383278131485, g_loss: 3.6305336952209473\n","Epoch 96/2000, Step 27, d_loss: 0.3692963123321533, g_loss: 4.755654811859131\n","Epoch 96/2000, Step 28, d_loss: 0.3844338357448578, g_loss: 6.435355186462402\n","Epoch 96/2000, Step 29, d_loss: 0.3640168607234955, g_loss: 5.45509672164917\n","Epoch 96/2000, Step 30, d_loss: 0.3849226236343384, g_loss: 5.3656325340271\n","Epoch 96/2000, Step 31, d_loss: 0.3584965765476227, g_loss: 5.24797248840332\n","Epoch 96/2000, Step 32, d_loss: 0.37089425325393677, g_loss: 6.319497108459473\n","Epoch 96/2000, Step 33, d_loss: 0.42107927799224854, g_loss: 4.967134475708008\n","Epoch 96/2000, Step 34, d_loss: 0.35230788588523865, g_loss: 6.350754261016846\n","Epoch 96/2000, Step 35, d_loss: 0.41468536853790283, g_loss: 5.707286357879639\n","Epoch 96/2000, Step 36, d_loss: 0.3771211504936218, g_loss: 5.020666599273682\n","Epoch 96/2000, Step 37, d_loss: 0.3738693296909332, g_loss: 4.004356861114502\n","Epoch 96/2000, Step 38, d_loss: 0.3936822712421417, g_loss: 6.01893424987793\n","Epoch 96/2000, Step 39, d_loss: 0.42991554737091064, g_loss: 4.107015132904053\n","Epoch 96/2000, Step 40, d_loss: 0.4132048785686493, g_loss: 4.457615852355957\n","Epoch 96/2000, Step 41, d_loss: 0.3843894302845001, g_loss: 2.1477320194244385\n","Epoch 96/2000, Step 42, d_loss: 0.42941948771476746, g_loss: 6.710180759429932\n","Epoch 96/2000, Step 43, d_loss: 0.35893669724464417, g_loss: 5.505299091339111\n","Epoch 96/2000, Step 44, d_loss: 0.3691924214363098, g_loss: 3.194821834564209\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 97/2000, Step 1, d_loss: 0.42773038148880005, g_loss: 6.115684986114502\n","Epoch 97/2000, Step 2, d_loss: 0.3608676791191101, g_loss: 5.472942352294922\n","Epoch 97/2000, Step 3, d_loss: 0.36528319120407104, g_loss: 6.22944974899292\n","Epoch 97/2000, Step 4, d_loss: 0.45590656995773315, g_loss: 4.859334468841553\n","Epoch 97/2000, Step 5, d_loss: 0.4433650076389313, g_loss: 3.1225481033325195\n","Epoch 97/2000, Step 6, d_loss: 0.4065283238887787, g_loss: 3.6971452236175537\n","Epoch 97/2000, Step 7, d_loss: 0.37135884165763855, g_loss: 2.78058123588562\n","Epoch 97/2000, Step 8, d_loss: 0.38145574927330017, g_loss: 2.5250778198242188\n","Epoch 97/2000, Step 9, d_loss: 0.43117251992225647, g_loss: 1.974621057510376\n","Epoch 97/2000, Step 10, d_loss: 0.44713276624679565, g_loss: 2.7832493782043457\n","Epoch 97/2000, Step 11, d_loss: 0.43349316716194153, g_loss: 3.321563720703125\n","Epoch 97/2000, Step 12, d_loss: 0.3935447037220001, g_loss: 5.0560173988342285\n","Epoch 97/2000, Step 13, d_loss: 0.40296250581741333, g_loss: 4.642587184906006\n","Epoch 97/2000, Step 14, d_loss: 0.37854960560798645, g_loss: 3.966390371322632\n","Epoch 97/2000, Step 15, d_loss: 0.3976810872554779, g_loss: 4.469804763793945\n","Epoch 97/2000, Step 16, d_loss: 0.36019402742385864, g_loss: 6.3957014083862305\n","Epoch 97/2000, Step 17, d_loss: 0.4515131115913391, g_loss: 4.465729713439941\n","Epoch 97/2000, Step 18, d_loss: 0.39343464374542236, g_loss: 5.345666885375977\n","Epoch 97/2000, Step 19, d_loss: 0.398880273103714, g_loss: 3.5453710556030273\n","Epoch 97/2000, Step 20, d_loss: 0.6059662699699402, g_loss: 4.318714141845703\n","Epoch 97/2000, Step 21, d_loss: 0.390591025352478, g_loss: 5.088937759399414\n","Epoch 97/2000, Step 22, d_loss: 0.37325534224510193, g_loss: 5.575304985046387\n","Epoch 97/2000, Step 23, d_loss: 0.4230872690677643, g_loss: 3.5299811363220215\n","Epoch 97/2000, Step 24, d_loss: 0.35481980443000793, g_loss: 5.497856616973877\n","Epoch 97/2000, Step 25, d_loss: 0.35534772276878357, g_loss: 4.515334129333496\n","Epoch 97/2000, Step 26, d_loss: 0.37524479627609253, g_loss: 5.2748823165893555\n","Epoch 97/2000, Step 27, d_loss: 0.3844016492366791, g_loss: 4.790114402770996\n","Epoch 97/2000, Step 28, d_loss: 0.3488260507583618, g_loss: 4.040764808654785\n","Epoch 97/2000, Step 29, d_loss: 0.42177295684814453, g_loss: 4.4280171394348145\n","Epoch 97/2000, Step 30, d_loss: 0.36622002720832825, g_loss: 5.446754455566406\n","Epoch 97/2000, Step 31, d_loss: 0.3831866979598999, g_loss: 6.16146183013916\n","Epoch 97/2000, Step 32, d_loss: 0.37396639585494995, g_loss: 6.122439384460449\n","Epoch 97/2000, Step 33, d_loss: 0.3763464689254761, g_loss: 5.496946811676025\n","Epoch 97/2000, Step 34, d_loss: 0.3875311315059662, g_loss: 4.769443988800049\n","Epoch 97/2000, Step 35, d_loss: 0.3591372072696686, g_loss: 5.248261451721191\n","Epoch 97/2000, Step 36, d_loss: 0.38669532537460327, g_loss: 4.802726745605469\n","Epoch 97/2000, Step 37, d_loss: 0.38940122723579407, g_loss: 3.3440585136413574\n","Epoch 97/2000, Step 38, d_loss: 0.4207276403903961, g_loss: 6.298428535461426\n","Epoch 97/2000, Step 39, d_loss: 0.36938560009002686, g_loss: 2.796046495437622\n","Epoch 97/2000, Step 40, d_loss: 0.3780810832977295, g_loss: 6.721399784088135\n","Epoch 97/2000, Step 41, d_loss: 0.37697547674179077, g_loss: 5.475969314575195\n","Epoch 97/2000, Step 42, d_loss: 0.3954146206378937, g_loss: 5.646636486053467\n","Epoch 97/2000, Step 43, d_loss: 0.3657093942165375, g_loss: 4.192862033843994\n","Epoch 97/2000, Step 44, d_loss: 0.4932011067867279, g_loss: 3.826767921447754\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 98/2000, Step 1, d_loss: 0.3863656222820282, g_loss: 3.237363576889038\n","Epoch 98/2000, Step 2, d_loss: 0.3669242560863495, g_loss: 5.298797130584717\n","Epoch 98/2000, Step 3, d_loss: 0.41654807329177856, g_loss: 5.537368297576904\n","Epoch 98/2000, Step 4, d_loss: 0.39195868372917175, g_loss: 3.7658848762512207\n","Epoch 98/2000, Step 5, d_loss: 0.36833107471466064, g_loss: 4.584615230560303\n","Epoch 98/2000, Step 6, d_loss: 0.39631617069244385, g_loss: 6.648807048797607\n","Epoch 98/2000, Step 7, d_loss: 0.38868919014930725, g_loss: 4.409598350524902\n","Epoch 98/2000, Step 8, d_loss: 0.3630080819129944, g_loss: 5.58357048034668\n","Epoch 98/2000, Step 9, d_loss: 0.3552854061126709, g_loss: 3.85676908493042\n","Epoch 98/2000, Step 10, d_loss: 0.3636447489261627, g_loss: 3.443589925765991\n","Epoch 98/2000, Step 11, d_loss: 0.36345601081848145, g_loss: 5.187770843505859\n","Epoch 98/2000, Step 12, d_loss: 0.3521789312362671, g_loss: 5.324147701263428\n","Epoch 98/2000, Step 13, d_loss: 0.35612672567367554, g_loss: 5.2074761390686035\n","Epoch 98/2000, Step 14, d_loss: 0.4253052771091461, g_loss: 4.009059906005859\n","Epoch 98/2000, Step 15, d_loss: 0.4076485335826874, g_loss: 6.53388786315918\n","Epoch 98/2000, Step 16, d_loss: 0.36510440707206726, g_loss: 4.539880752563477\n","Epoch 98/2000, Step 17, d_loss: 0.36490926146507263, g_loss: 4.9598236083984375\n","Epoch 98/2000, Step 18, d_loss: 0.3929809629917145, g_loss: 5.255535125732422\n","Epoch 98/2000, Step 19, d_loss: 0.365070104598999, g_loss: 4.6779704093933105\n","Epoch 98/2000, Step 20, d_loss: 0.3715130090713501, g_loss: 6.674975395202637\n","Epoch 98/2000, Step 21, d_loss: 0.3667186200618744, g_loss: 8.5421724319458\n","Epoch 98/2000, Step 22, d_loss: 0.41713279485702515, g_loss: 4.0203728675842285\n","Epoch 98/2000, Step 23, d_loss: 0.3657762408256531, g_loss: 6.853869438171387\n","Epoch 98/2000, Step 24, d_loss: 0.350696325302124, g_loss: 4.0394287109375\n","Epoch 98/2000, Step 25, d_loss: 0.5157772302627563, g_loss: 4.113903045654297\n","Epoch 98/2000, Step 26, d_loss: 0.3799761235713959, g_loss: 4.9767632484436035\n","Epoch 98/2000, Step 27, d_loss: 0.36064067482948303, g_loss: 4.643995761871338\n","Epoch 98/2000, Step 28, d_loss: 0.3743979334831238, g_loss: 4.171825408935547\n","Epoch 98/2000, Step 29, d_loss: 0.3929060697555542, g_loss: 4.173273086547852\n","Epoch 98/2000, Step 30, d_loss: 0.3495563268661499, g_loss: 5.844208717346191\n","Epoch 98/2000, Step 31, d_loss: 0.36623889207839966, g_loss: 3.67891526222229\n","Epoch 98/2000, Step 32, d_loss: 0.4218484163284302, g_loss: 3.7662274837493896\n","Epoch 98/2000, Step 33, d_loss: 0.37341269850730896, g_loss: 1.6716574430465698\n","Epoch 98/2000, Step 34, d_loss: 0.3633541762828827, g_loss: 5.180144309997559\n","Epoch 98/2000, Step 35, d_loss: 0.3599069118499756, g_loss: 3.0001933574676514\n","Epoch 98/2000, Step 36, d_loss: 0.389965683221817, g_loss: 5.920206069946289\n","Epoch 98/2000, Step 37, d_loss: 0.4610292911529541, g_loss: 4.828522682189941\n","Epoch 98/2000, Step 38, d_loss: 0.4365508258342743, g_loss: 3.9200632572174072\n","Epoch 98/2000, Step 39, d_loss: 0.3506084680557251, g_loss: 4.194698810577393\n","Epoch 98/2000, Step 40, d_loss: 0.37736546993255615, g_loss: 4.982361316680908\n","Epoch 98/2000, Step 41, d_loss: 0.36783653497695923, g_loss: 7.553339004516602\n","Epoch 98/2000, Step 42, d_loss: 0.3967217803001404, g_loss: 6.563229560852051\n","Epoch 98/2000, Step 43, d_loss: 0.5149147510528564, g_loss: 5.3662943840026855\n","Epoch 98/2000, Step 44, d_loss: 0.5577815771102905, g_loss: 5.4193220138549805\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 99/2000, Step 1, d_loss: 0.35706228017807007, g_loss: 3.139836549758911\n","Epoch 99/2000, Step 2, d_loss: 0.3714067041873932, g_loss: 5.155007362365723\n","Epoch 99/2000, Step 3, d_loss: 0.6305041909217834, g_loss: 2.2347500324249268\n","Epoch 99/2000, Step 4, d_loss: 0.46401548385620117, g_loss: 2.33632493019104\n","Epoch 99/2000, Step 5, d_loss: 0.5691534280776978, g_loss: 3.900515079498291\n","Epoch 99/2000, Step 6, d_loss: 0.4008411169052124, g_loss: 5.332629203796387\n","Epoch 99/2000, Step 7, d_loss: 0.365337997674942, g_loss: 5.203989028930664\n","Epoch 99/2000, Step 8, d_loss: 0.39984485507011414, g_loss: 6.26365852355957\n","Epoch 99/2000, Step 9, d_loss: 0.4354560971260071, g_loss: 7.03007173538208\n","Epoch 99/2000, Step 10, d_loss: 0.7856454253196716, g_loss: 3.7628097534179688\n","Epoch 99/2000, Step 11, d_loss: 0.37131980061531067, g_loss: 3.1108126640319824\n","Epoch 99/2000, Step 12, d_loss: 0.47802507877349854, g_loss: 2.4959771633148193\n","Epoch 99/2000, Step 13, d_loss: 0.6267216205596924, g_loss: 2.9842047691345215\n","Epoch 99/2000, Step 14, d_loss: 0.44309788942337036, g_loss: 4.290576457977295\n","Epoch 99/2000, Step 15, d_loss: 0.4483807384967804, g_loss: 3.0562968254089355\n","Epoch 99/2000, Step 16, d_loss: 0.3969414532184601, g_loss: 2.6625285148620605\n","Epoch 99/2000, Step 17, d_loss: 0.38049963116645813, g_loss: 5.902551174163818\n","Epoch 99/2000, Step 18, d_loss: 0.4484162926673889, g_loss: 4.421286106109619\n","Epoch 99/2000, Step 19, d_loss: 0.3808862864971161, g_loss: 4.10935640335083\n","Epoch 99/2000, Step 20, d_loss: 0.36121341586112976, g_loss: 3.5789668560028076\n","Epoch 99/2000, Step 21, d_loss: 0.365134060382843, g_loss: 5.09162712097168\n","Epoch 99/2000, Step 22, d_loss: 0.3541201055049896, g_loss: 3.830639600753784\n","Epoch 99/2000, Step 23, d_loss: 0.3799677789211273, g_loss: 4.219603061676025\n","Epoch 99/2000, Step 24, d_loss: 0.40427476167678833, g_loss: 2.2717034816741943\n","Epoch 99/2000, Step 25, d_loss: 0.4009556770324707, g_loss: 5.504098415374756\n","Epoch 99/2000, Step 26, d_loss: 0.4180164933204651, g_loss: 4.764895439147949\n","Epoch 99/2000, Step 27, d_loss: 0.36196738481521606, g_loss: 4.967309474945068\n","Epoch 99/2000, Step 28, d_loss: 0.3896532356739044, g_loss: 4.417208194732666\n","Epoch 99/2000, Step 29, d_loss: 0.47140371799468994, g_loss: 3.8258867263793945\n","Epoch 99/2000, Step 30, d_loss: 0.742473840713501, g_loss: 4.035437107086182\n","Epoch 99/2000, Step 31, d_loss: 0.4631924033164978, g_loss: 4.271821975708008\n","Epoch 99/2000, Step 32, d_loss: 0.4074733555316925, g_loss: 2.9443142414093018\n","Epoch 99/2000, Step 33, d_loss: 0.4247373938560486, g_loss: 3.245441198348999\n","Epoch 99/2000, Step 34, d_loss: 0.38790661096572876, g_loss: 3.2295613288879395\n","Epoch 99/2000, Step 35, d_loss: 0.6219494938850403, g_loss: 2.2188022136688232\n","Epoch 99/2000, Step 36, d_loss: 0.42508721351623535, g_loss: 4.22470235824585\n","Epoch 99/2000, Step 37, d_loss: 0.4472852349281311, g_loss: 3.7745020389556885\n","Epoch 99/2000, Step 38, d_loss: 0.3708494007587433, g_loss: 3.8291800022125244\n","Epoch 99/2000, Step 39, d_loss: 0.45669692754745483, g_loss: 4.366878509521484\n","Epoch 99/2000, Step 40, d_loss: 0.4167490601539612, g_loss: 5.624867916107178\n","Epoch 99/2000, Step 41, d_loss: 0.4291442632675171, g_loss: 4.65855598449707\n","Epoch 99/2000, Step 42, d_loss: 0.3783075511455536, g_loss: 3.887903928756714\n","Epoch 99/2000, Step 43, d_loss: 0.40155482292175293, g_loss: 3.246990919113159\n","Epoch 99/2000, Step 44, d_loss: 0.3964073061943054, g_loss: 2.6540091037750244\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 100/2000, Step 1, d_loss: 0.4074851870536804, g_loss: 3.501359224319458\n","Epoch 100/2000, Step 2, d_loss: 0.4954332113265991, g_loss: 1.7198541164398193\n","Epoch 100/2000, Step 3, d_loss: 0.49180442094802856, g_loss: 2.7950170040130615\n","Epoch 100/2000, Step 4, d_loss: 0.3604798913002014, g_loss: 4.602619171142578\n","Epoch 100/2000, Step 5, d_loss: 0.4060516357421875, g_loss: 5.361373424530029\n","Epoch 100/2000, Step 6, d_loss: 0.36853891611099243, g_loss: 5.636385440826416\n","Epoch 100/2000, Step 7, d_loss: 0.3521498441696167, g_loss: 6.392577171325684\n","Epoch 100/2000, Step 8, d_loss: 0.5878660678863525, g_loss: 4.122652530670166\n","Epoch 100/2000, Step 9, d_loss: 0.44862130284309387, g_loss: 4.197231769561768\n","Epoch 100/2000, Step 10, d_loss: 0.3764297068119049, g_loss: 3.8197033405303955\n","Epoch 100/2000, Step 11, d_loss: 0.38195449113845825, g_loss: 3.810420274734497\n","Epoch 100/2000, Step 12, d_loss: 0.49294182658195496, g_loss: 4.582360744476318\n","Epoch 100/2000, Step 13, d_loss: 0.5466628670692444, g_loss: 4.151613712310791\n","Epoch 100/2000, Step 14, d_loss: 0.3944857120513916, g_loss: 4.73501443862915\n","Epoch 100/2000, Step 15, d_loss: 0.38010552525520325, g_loss: 3.918442964553833\n","Epoch 100/2000, Step 16, d_loss: 0.42141854763031006, g_loss: 4.009395599365234\n","Epoch 100/2000, Step 17, d_loss: 0.4385802745819092, g_loss: 4.055843830108643\n","Epoch 100/2000, Step 18, d_loss: 0.3541626036167145, g_loss: 5.884704113006592\n","Epoch 100/2000, Step 19, d_loss: 0.515783965587616, g_loss: 5.3137617111206055\n","Epoch 100/2000, Step 20, d_loss: 0.40219250321388245, g_loss: 3.130441904067993\n","Epoch 100/2000, Step 21, d_loss: 0.48832467198371887, g_loss: 4.079229831695557\n","Epoch 100/2000, Step 22, d_loss: 0.3886760473251343, g_loss: 3.3521533012390137\n","Epoch 100/2000, Step 23, d_loss: 0.40661731362342834, g_loss: 5.716592311859131\n","Epoch 100/2000, Step 24, d_loss: 0.4158479571342468, g_loss: 4.120469093322754\n","Epoch 100/2000, Step 25, d_loss: 0.3879045248031616, g_loss: 5.2612714767456055\n","Epoch 100/2000, Step 26, d_loss: 0.34539899230003357, g_loss: 5.598500728607178\n","Epoch 100/2000, Step 27, d_loss: 0.42725640535354614, g_loss: 2.3968164920806885\n","Epoch 100/2000, Step 28, d_loss: 0.40033775568008423, g_loss: 6.426671028137207\n","Epoch 100/2000, Step 29, d_loss: 0.3828790783882141, g_loss: 4.5567755699157715\n","Epoch 100/2000, Step 30, d_loss: 0.3893050253391266, g_loss: 5.848033905029297\n","Epoch 100/2000, Step 31, d_loss: 0.386788934469223, g_loss: 2.240934371948242\n","Epoch 100/2000, Step 32, d_loss: 0.3728677034378052, g_loss: 3.8912546634674072\n","Epoch 100/2000, Step 33, d_loss: 0.4069543778896332, g_loss: 4.66654109954834\n","Epoch 100/2000, Step 34, d_loss: 0.4019225537776947, g_loss: 4.912683486938477\n","Epoch 100/2000, Step 35, d_loss: 0.3882826566696167, g_loss: 5.146651744842529\n","Epoch 100/2000, Step 36, d_loss: 0.4529513120651245, g_loss: 6.1476593017578125\n","Epoch 100/2000, Step 37, d_loss: 0.43267375230789185, g_loss: 5.7086029052734375\n","Epoch 100/2000, Step 38, d_loss: 0.3730805516242981, g_loss: 7.488001823425293\n","Epoch 100/2000, Step 39, d_loss: 0.4583783745765686, g_loss: 5.834805965423584\n","Epoch 100/2000, Step 40, d_loss: 0.3628355860710144, g_loss: 3.1700239181518555\n","Epoch 100/2000, Step 41, d_loss: 0.40315598249435425, g_loss: 5.477500915527344\n","Epoch 100/2000, Step 42, d_loss: 0.4275984764099121, g_loss: 4.813366889953613\n","Epoch 100/2000, Step 43, d_loss: 0.3617570400238037, g_loss: 4.326475143432617\n","Epoch 100/2000, Step 44, d_loss: 0.3810977041721344, g_loss: 3.6349363327026367\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 101/2000, Step 1, d_loss: 0.4061070382595062, g_loss: 3.698505401611328\n","Epoch 101/2000, Step 2, d_loss: 0.4272100329399109, g_loss: 3.0093226432800293\n","Epoch 101/2000, Step 3, d_loss: 0.4212859272956848, g_loss: 3.118497133255005\n","Epoch 101/2000, Step 4, d_loss: 0.4560355842113495, g_loss: 7.631236553192139\n","Epoch 101/2000, Step 5, d_loss: 0.36718282103538513, g_loss: 3.3162827491760254\n","Epoch 101/2000, Step 6, d_loss: 0.37416332960128784, g_loss: 6.632394790649414\n","Epoch 101/2000, Step 7, d_loss: 0.43780362606048584, g_loss: 4.863234043121338\n","Epoch 101/2000, Step 8, d_loss: 0.42235812544822693, g_loss: 5.841292858123779\n","Epoch 101/2000, Step 9, d_loss: 0.3732256591320038, g_loss: 3.768372058868408\n","Epoch 101/2000, Step 10, d_loss: 0.40796127915382385, g_loss: 3.2650110721588135\n","Epoch 101/2000, Step 11, d_loss: 0.37227219343185425, g_loss: 4.064119815826416\n","Epoch 101/2000, Step 12, d_loss: 0.3675515055656433, g_loss: 5.968044281005859\n","Epoch 101/2000, Step 13, d_loss: 0.37910568714141846, g_loss: 4.847662925720215\n","Epoch 101/2000, Step 14, d_loss: 0.3813616931438446, g_loss: 5.587845325469971\n","Epoch 101/2000, Step 15, d_loss: 0.37533921003341675, g_loss: 2.1189396381378174\n","Epoch 101/2000, Step 16, d_loss: 0.37235498428344727, g_loss: 4.0597825050354\n","Epoch 101/2000, Step 17, d_loss: 0.5079014897346497, g_loss: 5.501401424407959\n","Epoch 101/2000, Step 18, d_loss: 0.3735589385032654, g_loss: 4.058009624481201\n","Epoch 101/2000, Step 19, d_loss: 0.39082071185112, g_loss: 7.94295072555542\n","Epoch 101/2000, Step 20, d_loss: 0.3593483865261078, g_loss: 3.88431715965271\n","Epoch 101/2000, Step 21, d_loss: 0.37747427821159363, g_loss: 4.107887268066406\n","Epoch 101/2000, Step 22, d_loss: 0.49001041054725647, g_loss: 2.9340033531188965\n","Epoch 101/2000, Step 23, d_loss: 0.38895508646965027, g_loss: 3.749420166015625\n","Epoch 101/2000, Step 24, d_loss: 0.3502832353115082, g_loss: 4.689450263977051\n","Epoch 101/2000, Step 25, d_loss: 0.36979711055755615, g_loss: 6.147580146789551\n","Epoch 101/2000, Step 26, d_loss: 0.4042089581489563, g_loss: 3.3869924545288086\n","Epoch 101/2000, Step 27, d_loss: 0.4045964777469635, g_loss: 4.629781246185303\n","Epoch 101/2000, Step 28, d_loss: 0.4236399233341217, g_loss: 3.6718218326568604\n","Epoch 101/2000, Step 29, d_loss: 0.38488829135894775, g_loss: 4.933655738830566\n","Epoch 101/2000, Step 30, d_loss: 0.38312721252441406, g_loss: 4.660743236541748\n","Epoch 101/2000, Step 31, d_loss: 0.4038237929344177, g_loss: 5.445802688598633\n","Epoch 101/2000, Step 32, d_loss: 0.6879785060882568, g_loss: 3.8956942558288574\n","Epoch 101/2000, Step 33, d_loss: 0.3992215394973755, g_loss: 5.338727951049805\n","Epoch 101/2000, Step 34, d_loss: 0.41977977752685547, g_loss: 5.589203834533691\n","Epoch 101/2000, Step 35, d_loss: 0.40709811449050903, g_loss: 4.84967565536499\n","Epoch 101/2000, Step 36, d_loss: 0.4495428204536438, g_loss: 2.199852466583252\n","Epoch 101/2000, Step 37, d_loss: 0.4113676846027374, g_loss: 6.178753852844238\n","Epoch 101/2000, Step 38, d_loss: 0.36877480149269104, g_loss: 4.2906341552734375\n","Epoch 101/2000, Step 39, d_loss: 0.3964388370513916, g_loss: 4.1059184074401855\n","Epoch 101/2000, Step 40, d_loss: 0.38090869784355164, g_loss: 1.9832305908203125\n","Epoch 101/2000, Step 41, d_loss: 0.3922315537929535, g_loss: 1.5872071981430054\n","Epoch 101/2000, Step 42, d_loss: 0.3623219430446625, g_loss: 5.976146697998047\n","Epoch 101/2000, Step 43, d_loss: 0.4265584647655487, g_loss: 4.025947093963623\n","Epoch 101/2000, Step 44, d_loss: 0.47309210896492004, g_loss: 4.532225131988525\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 102/2000, Step 1, d_loss: 0.37106427550315857, g_loss: 5.799898147583008\n","Epoch 102/2000, Step 2, d_loss: 0.36970648169517517, g_loss: 5.047760009765625\n","Epoch 102/2000, Step 3, d_loss: 0.7770220637321472, g_loss: 6.209717750549316\n","Epoch 102/2000, Step 4, d_loss: 0.4422132074832916, g_loss: 6.117997169494629\n","Epoch 102/2000, Step 5, d_loss: 0.3734351396560669, g_loss: 3.279609441757202\n","Epoch 102/2000, Step 6, d_loss: 0.37862592935562134, g_loss: 2.4146432876586914\n","Epoch 102/2000, Step 7, d_loss: 0.462232768535614, g_loss: 2.267874240875244\n","Epoch 102/2000, Step 8, d_loss: 0.5034604072570801, g_loss: 1.1560139656066895\n","Epoch 102/2000, Step 9, d_loss: 0.532386302947998, g_loss: 4.156975746154785\n","Epoch 102/2000, Step 10, d_loss: 0.38113677501678467, g_loss: 4.0508341789245605\n","Epoch 102/2000, Step 11, d_loss: 0.4463088810443878, g_loss: 7.049155235290527\n","Epoch 102/2000, Step 12, d_loss: 0.5077143907546997, g_loss: 4.185922145843506\n","Epoch 102/2000, Step 13, d_loss: 0.506682813167572, g_loss: 4.82373046875\n","Epoch 102/2000, Step 14, d_loss: 0.44869115948677063, g_loss: 3.265556573867798\n","Epoch 102/2000, Step 15, d_loss: 0.4248654544353485, g_loss: 4.021273136138916\n","Epoch 102/2000, Step 16, d_loss: 0.45046812295913696, g_loss: 4.038102149963379\n","Epoch 102/2000, Step 17, d_loss: 0.4085978865623474, g_loss: 3.155959129333496\n","Epoch 102/2000, Step 18, d_loss: 0.48905202746391296, g_loss: 3.238802671432495\n","Epoch 102/2000, Step 19, d_loss: 0.4098767936229706, g_loss: 3.0940966606140137\n","Epoch 102/2000, Step 20, d_loss: 0.4270808696746826, g_loss: 4.73120641708374\n","Epoch 102/2000, Step 21, d_loss: 0.4175688624382019, g_loss: 6.009748935699463\n","Epoch 102/2000, Step 22, d_loss: 0.4131959080696106, g_loss: 3.8434038162231445\n","Epoch 102/2000, Step 23, d_loss: 0.36520540714263916, g_loss: 5.950000762939453\n","Epoch 102/2000, Step 24, d_loss: 0.3733368515968323, g_loss: 5.984661102294922\n","Epoch 102/2000, Step 25, d_loss: 0.49024173617362976, g_loss: 4.39560079574585\n","Epoch 102/2000, Step 26, d_loss: 0.355997771024704, g_loss: 5.3622918128967285\n","Epoch 102/2000, Step 27, d_loss: 0.39040642976760864, g_loss: 5.025842666625977\n","Epoch 102/2000, Step 28, d_loss: 0.38895896077156067, g_loss: 4.075490951538086\n","Epoch 102/2000, Step 29, d_loss: 0.48693180084228516, g_loss: 2.687116861343384\n","Epoch 102/2000, Step 30, d_loss: 0.3994283676147461, g_loss: 5.12822151184082\n","Epoch 102/2000, Step 31, d_loss: 0.35979947447776794, g_loss: 3.7545735836029053\n","Epoch 102/2000, Step 32, d_loss: 0.3571602404117584, g_loss: 5.62822961807251\n","Epoch 102/2000, Step 33, d_loss: 0.49417054653167725, g_loss: 4.423998832702637\n","Epoch 102/2000, Step 34, d_loss: 0.3866906464099884, g_loss: 4.140109539031982\n","Epoch 102/2000, Step 35, d_loss: 0.3678039312362671, g_loss: 1.943381428718567\n","Epoch 102/2000, Step 36, d_loss: 0.3665599822998047, g_loss: 5.019372940063477\n","Epoch 102/2000, Step 37, d_loss: 0.4084734320640564, g_loss: 3.4452767372131348\n","Epoch 102/2000, Step 38, d_loss: 0.40897783637046814, g_loss: 4.9279656410217285\n","Epoch 102/2000, Step 39, d_loss: 0.3761683404445648, g_loss: 3.7487142086029053\n","Epoch 102/2000, Step 40, d_loss: 0.3552777171134949, g_loss: 5.1580810546875\n","Epoch 102/2000, Step 41, d_loss: 0.3640667200088501, g_loss: 3.946866035461426\n","Epoch 102/2000, Step 42, d_loss: 0.3958907723426819, g_loss: 5.22542142868042\n","Epoch 102/2000, Step 43, d_loss: 0.40530985593795776, g_loss: 5.582870006561279\n","Epoch 102/2000, Step 44, d_loss: 0.5208004713058472, g_loss: 4.368276119232178\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 103/2000, Step 1, d_loss: 0.36363112926483154, g_loss: 3.270378351211548\n","Epoch 103/2000, Step 2, d_loss: 0.5647158622741699, g_loss: 3.951169490814209\n","Epoch 103/2000, Step 3, d_loss: 0.3815348148345947, g_loss: 3.290771484375\n","Epoch 103/2000, Step 4, d_loss: 0.4295695424079895, g_loss: 2.0572831630706787\n","Epoch 103/2000, Step 5, d_loss: 0.42489543557167053, g_loss: 3.4389476776123047\n","Epoch 103/2000, Step 6, d_loss: 0.42258507013320923, g_loss: 6.4090046882629395\n","Epoch 103/2000, Step 7, d_loss: 0.48848778009414673, g_loss: 4.534850597381592\n","Epoch 103/2000, Step 8, d_loss: 0.3850457966327667, g_loss: 4.086150169372559\n","Epoch 103/2000, Step 9, d_loss: 0.39195170998573303, g_loss: 5.655203819274902\n","Epoch 103/2000, Step 10, d_loss: 0.3934430480003357, g_loss: 4.795042991638184\n","Epoch 103/2000, Step 11, d_loss: 0.40734371542930603, g_loss: 5.086524486541748\n","Epoch 103/2000, Step 12, d_loss: 0.37960389256477356, g_loss: 4.075748443603516\n","Epoch 103/2000, Step 13, d_loss: 0.4226549565792084, g_loss: 3.5180232524871826\n","Epoch 103/2000, Step 14, d_loss: 0.3535729646682739, g_loss: 3.937944173812866\n","Epoch 103/2000, Step 15, d_loss: 0.36990147829055786, g_loss: 3.342210054397583\n","Epoch 103/2000, Step 16, d_loss: 0.4495909512042999, g_loss: 3.5347352027893066\n","Epoch 103/2000, Step 17, d_loss: 0.4263363778591156, g_loss: 3.1697537899017334\n","Epoch 103/2000, Step 18, d_loss: 0.4653155207633972, g_loss: 4.575873374938965\n","Epoch 103/2000, Step 19, d_loss: 0.40684372186660767, g_loss: 4.026752471923828\n","Epoch 103/2000, Step 20, d_loss: 0.3712000846862793, g_loss: 4.913893699645996\n","Epoch 103/2000, Step 21, d_loss: 0.36513248085975647, g_loss: 5.776410102844238\n","Epoch 103/2000, Step 22, d_loss: 0.41540589928627014, g_loss: 4.390857219696045\n","Epoch 103/2000, Step 23, d_loss: 0.4460066556930542, g_loss: 3.022406816482544\n","Epoch 103/2000, Step 24, d_loss: 0.4458024799823761, g_loss: 5.187526702880859\n","Epoch 103/2000, Step 25, d_loss: 0.3777809143066406, g_loss: 2.7657155990600586\n","Epoch 103/2000, Step 26, d_loss: 0.3628441095352173, g_loss: 4.2337751388549805\n","Epoch 103/2000, Step 27, d_loss: 0.36376386880874634, g_loss: 4.162502288818359\n","Epoch 103/2000, Step 28, d_loss: 0.3641003966331482, g_loss: 5.532703876495361\n","Epoch 103/2000, Step 29, d_loss: 0.3738054633140564, g_loss: 4.504818439483643\n","Epoch 103/2000, Step 30, d_loss: 0.39852914214134216, g_loss: 5.042117595672607\n","Epoch 103/2000, Step 31, d_loss: 0.43981873989105225, g_loss: 3.3952059745788574\n","Epoch 103/2000, Step 32, d_loss: 0.3714282512664795, g_loss: 2.7492659091949463\n","Epoch 103/2000, Step 33, d_loss: 0.3643817603588104, g_loss: 3.321465253829956\n","Epoch 103/2000, Step 34, d_loss: 0.350618839263916, g_loss: 5.392709732055664\n","Epoch 103/2000, Step 35, d_loss: 0.3784290552139282, g_loss: 5.028019905090332\n","Epoch 103/2000, Step 36, d_loss: 0.451675146818161, g_loss: 4.82414436340332\n","Epoch 103/2000, Step 37, d_loss: 0.38550740480422974, g_loss: 4.5001301765441895\n","Epoch 103/2000, Step 38, d_loss: 0.48413699865341187, g_loss: 3.0742156505584717\n","Epoch 103/2000, Step 39, d_loss: 0.42252659797668457, g_loss: 4.149370193481445\n","Epoch 103/2000, Step 40, d_loss: 0.5560620427131653, g_loss: 4.141561985015869\n","Epoch 103/2000, Step 41, d_loss: 0.4113868772983551, g_loss: 2.611541271209717\n","Epoch 103/2000, Step 42, d_loss: 0.5579046607017517, g_loss: 2.6556971073150635\n","Epoch 103/2000, Step 43, d_loss: 0.41252774000167847, g_loss: 5.063575267791748\n","Epoch 103/2000, Step 44, d_loss: 0.37174493074417114, g_loss: 5.860518932342529\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 104/2000, Step 1, d_loss: 0.44311296939849854, g_loss: 6.102190971374512\n","Epoch 104/2000, Step 2, d_loss: 0.45968902111053467, g_loss: 5.5804619789123535\n","Epoch 104/2000, Step 3, d_loss: 0.5183383226394653, g_loss: 5.019742012023926\n","Epoch 104/2000, Step 4, d_loss: 0.4561367332935333, g_loss: 3.2780027389526367\n","Epoch 104/2000, Step 5, d_loss: 0.40089645981788635, g_loss: 5.5920233726501465\n","Epoch 104/2000, Step 6, d_loss: 0.38283106684684753, g_loss: 3.776322603225708\n","Epoch 104/2000, Step 7, d_loss: 0.4079616367816925, g_loss: 3.5743143558502197\n","Epoch 104/2000, Step 8, d_loss: 0.45820489525794983, g_loss: 2.160370349884033\n","Epoch 104/2000, Step 9, d_loss: 0.5782522559165955, g_loss: 3.4310343265533447\n","Epoch 104/2000, Step 10, d_loss: 0.43652158975601196, g_loss: 3.0152924060821533\n","Epoch 104/2000, Step 11, d_loss: 0.39408576488494873, g_loss: 4.404029369354248\n","Epoch 104/2000, Step 12, d_loss: 0.3635650873184204, g_loss: 5.370421886444092\n","Epoch 104/2000, Step 13, d_loss: 0.3870407044887543, g_loss: 5.2094316482543945\n","Epoch 104/2000, Step 14, d_loss: 0.41381797194480896, g_loss: 5.486132621765137\n","Epoch 104/2000, Step 15, d_loss: 0.5087334513664246, g_loss: 3.4186978340148926\n","Epoch 104/2000, Step 16, d_loss: 0.3698664605617523, g_loss: 3.801121711730957\n","Epoch 104/2000, Step 17, d_loss: 0.423265278339386, g_loss: 3.4692676067352295\n","Epoch 104/2000, Step 18, d_loss: 0.4482954740524292, g_loss: 2.7155401706695557\n","Epoch 104/2000, Step 19, d_loss: 0.4778505265712738, g_loss: 2.627748489379883\n","Epoch 104/2000, Step 20, d_loss: 0.4699888229370117, g_loss: 3.8889706134796143\n","Epoch 104/2000, Step 21, d_loss: 0.4153575599193573, g_loss: 4.541682720184326\n","Epoch 104/2000, Step 22, d_loss: 0.4293026924133301, g_loss: 2.241658926010132\n","Epoch 104/2000, Step 23, d_loss: 0.38413888216018677, g_loss: 3.7295308113098145\n","Epoch 104/2000, Step 24, d_loss: 0.39348286390304565, g_loss: 5.016967296600342\n","Epoch 104/2000, Step 25, d_loss: 0.36990952491760254, g_loss: 4.769537448883057\n","Epoch 104/2000, Step 26, d_loss: 0.3653075397014618, g_loss: 8.581414222717285\n","Epoch 104/2000, Step 27, d_loss: 0.3944843113422394, g_loss: 5.4386067390441895\n","Epoch 104/2000, Step 28, d_loss: 0.47328564524650574, g_loss: 3.77901554107666\n","Epoch 104/2000, Step 29, d_loss: 0.38987722992897034, g_loss: 4.6705522537231445\n","Epoch 104/2000, Step 30, d_loss: 0.3654237985610962, g_loss: 5.771634578704834\n","Epoch 104/2000, Step 31, d_loss: 0.37441426515579224, g_loss: 4.695541858673096\n","Epoch 104/2000, Step 32, d_loss: 0.4963376820087433, g_loss: 8.582931518554688\n","Epoch 104/2000, Step 33, d_loss: 0.41943734884262085, g_loss: 4.331396579742432\n","Epoch 104/2000, Step 34, d_loss: 0.45280787348747253, g_loss: 8.184877395629883\n","Epoch 104/2000, Step 35, d_loss: 0.419140100479126, g_loss: 5.3264336585998535\n","Epoch 104/2000, Step 36, d_loss: 0.3813593089580536, g_loss: 6.165532112121582\n","Epoch 104/2000, Step 37, d_loss: 0.3685387372970581, g_loss: 5.244438648223877\n","Epoch 104/2000, Step 38, d_loss: 0.38045305013656616, g_loss: 4.538146495819092\n","Epoch 104/2000, Step 39, d_loss: 0.4143297076225281, g_loss: 5.447347640991211\n","Epoch 104/2000, Step 40, d_loss: 0.3699122965335846, g_loss: 4.530231475830078\n","Epoch 104/2000, Step 41, d_loss: 0.3896796405315399, g_loss: 3.1190831661224365\n","Epoch 104/2000, Step 42, d_loss: 0.4255201518535614, g_loss: 4.6790289878845215\n","Epoch 104/2000, Step 43, d_loss: 0.40292268991470337, g_loss: 4.359104633331299\n","Epoch 104/2000, Step 44, d_loss: 0.4303005635738373, g_loss: 4.680354595184326\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 105/2000, Step 1, d_loss: 0.3976374864578247, g_loss: 5.616729259490967\n","Epoch 105/2000, Step 2, d_loss: 0.68860924243927, g_loss: 3.357391834259033\n","Epoch 105/2000, Step 3, d_loss: 0.40626418590545654, g_loss: 2.5514771938323975\n","Epoch 105/2000, Step 4, d_loss: 0.4021090269088745, g_loss: 2.594820261001587\n","Epoch 105/2000, Step 5, d_loss: 0.47785869240760803, g_loss: 1.9753996133804321\n","Epoch 105/2000, Step 6, d_loss: 0.48694950342178345, g_loss: 3.2615764141082764\n","Epoch 105/2000, Step 7, d_loss: 0.41297996044158936, g_loss: 5.295114517211914\n","Epoch 105/2000, Step 8, d_loss: 0.46130460500717163, g_loss: 4.120447158813477\n","Epoch 105/2000, Step 9, d_loss: 0.357997864484787, g_loss: 5.207430839538574\n","Epoch 105/2000, Step 10, d_loss: 0.4357920289039612, g_loss: 4.9315009117126465\n","Epoch 105/2000, Step 11, d_loss: 0.3656867444515228, g_loss: 4.7112135887146\n","Epoch 105/2000, Step 12, d_loss: 0.3778359889984131, g_loss: 2.843412160873413\n","Epoch 105/2000, Step 13, d_loss: 0.4568033218383789, g_loss: 1.6334013938903809\n","Epoch 105/2000, Step 14, d_loss: 0.41675615310668945, g_loss: 2.9171226024627686\n","Epoch 105/2000, Step 15, d_loss: 0.39924103021621704, g_loss: 5.150444507598877\n","Epoch 105/2000, Step 16, d_loss: 0.3683781921863556, g_loss: 5.315550327301025\n","Epoch 105/2000, Step 17, d_loss: 0.34655848145484924, g_loss: 6.055181503295898\n","Epoch 105/2000, Step 18, d_loss: 0.39324089884757996, g_loss: 3.9854369163513184\n","Epoch 105/2000, Step 19, d_loss: 0.3663707375526428, g_loss: 3.762349843978882\n","Epoch 105/2000, Step 20, d_loss: 0.35682883858680725, g_loss: 3.8913114070892334\n","Epoch 105/2000, Step 21, d_loss: 0.37380844354629517, g_loss: 5.579498767852783\n","Epoch 105/2000, Step 22, d_loss: 0.3857077360153198, g_loss: 4.607717037200928\n","Epoch 105/2000, Step 23, d_loss: 0.37166255712509155, g_loss: 3.562171220779419\n","Epoch 105/2000, Step 24, d_loss: 0.41775083541870117, g_loss: 5.139401435852051\n","Epoch 105/2000, Step 25, d_loss: 0.36935538053512573, g_loss: 3.9788365364074707\n","Epoch 105/2000, Step 26, d_loss: 0.40018993616104126, g_loss: 2.9280166625976562\n","Epoch 105/2000, Step 27, d_loss: 0.3454810380935669, g_loss: 5.398034572601318\n","Epoch 105/2000, Step 28, d_loss: 0.36813417077064514, g_loss: 4.112518310546875\n","Epoch 105/2000, Step 29, d_loss: 0.3963782787322998, g_loss: 4.240578651428223\n","Epoch 105/2000, Step 30, d_loss: 0.38954269886016846, g_loss: 3.8572230339050293\n","Epoch 105/2000, Step 31, d_loss: 0.36872944235801697, g_loss: 4.0511040687561035\n","Epoch 105/2000, Step 32, d_loss: 0.5317649841308594, g_loss: 4.503534317016602\n","Epoch 105/2000, Step 33, d_loss: 0.455501914024353, g_loss: 3.30527663230896\n","Epoch 105/2000, Step 34, d_loss: 0.4588281810283661, g_loss: 4.396846294403076\n","Epoch 105/2000, Step 35, d_loss: 0.39758482575416565, g_loss: 3.206059217453003\n","Epoch 105/2000, Step 36, d_loss: 0.36403775215148926, g_loss: 5.604545593261719\n","Epoch 105/2000, Step 37, d_loss: 0.38380032777786255, g_loss: 6.305785655975342\n","Epoch 105/2000, Step 38, d_loss: 0.3851519525051117, g_loss: 4.00749397277832\n","Epoch 105/2000, Step 39, d_loss: 0.40044862031936646, g_loss: 5.073543071746826\n","Epoch 105/2000, Step 40, d_loss: 0.35409101843833923, g_loss: 4.045987129211426\n","Epoch 105/2000, Step 41, d_loss: 0.5859372019767761, g_loss: 3.498202323913574\n","Epoch 105/2000, Step 42, d_loss: 0.4348553419113159, g_loss: 3.1833419799804688\n","Epoch 105/2000, Step 43, d_loss: 0.4197501242160797, g_loss: 3.017169952392578\n","Epoch 105/2000, Step 44, d_loss: 0.46121227741241455, g_loss: 3.591965675354004\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 106/2000, Step 1, d_loss: 0.4254339039325714, g_loss: 3.3428502082824707\n","Epoch 106/2000, Step 2, d_loss: 0.4139278829097748, g_loss: 2.5710349082946777\n","Epoch 106/2000, Step 3, d_loss: 0.3504377603530884, g_loss: 5.466284275054932\n","Epoch 106/2000, Step 4, d_loss: 0.3838331401348114, g_loss: 6.11008882522583\n","Epoch 106/2000, Step 5, d_loss: 0.45148247480392456, g_loss: 5.96002721786499\n","Epoch 106/2000, Step 6, d_loss: 0.4321293830871582, g_loss: 6.405779838562012\n","Epoch 106/2000, Step 7, d_loss: 0.35888659954071045, g_loss: 3.9159772396087646\n","Epoch 106/2000, Step 8, d_loss: 0.38151735067367554, g_loss: 4.359163284301758\n","Epoch 106/2000, Step 9, d_loss: 0.40678268671035767, g_loss: 3.726235866546631\n","Epoch 106/2000, Step 10, d_loss: 0.3815566897392273, g_loss: 3.9578914642333984\n","Epoch 106/2000, Step 11, d_loss: 0.3591386079788208, g_loss: 4.6885175704956055\n","Epoch 106/2000, Step 12, d_loss: 0.4657082259654999, g_loss: 5.124884128570557\n","Epoch 106/2000, Step 13, d_loss: 0.3798852264881134, g_loss: 4.527975082397461\n","Epoch 106/2000, Step 14, d_loss: 0.3505914509296417, g_loss: 5.4971184730529785\n","Epoch 106/2000, Step 15, d_loss: 0.3813299834728241, g_loss: 4.767289161682129\n","Epoch 106/2000, Step 16, d_loss: 0.38571831583976746, g_loss: 5.016057014465332\n","Epoch 106/2000, Step 17, d_loss: 0.3636489808559418, g_loss: 4.396464824676514\n","Epoch 106/2000, Step 18, d_loss: 0.3984190821647644, g_loss: 4.496469020843506\n","Epoch 106/2000, Step 19, d_loss: 0.4721842110157013, g_loss: 4.199507713317871\n","Epoch 106/2000, Step 20, d_loss: 0.36212024092674255, g_loss: 5.8395094871521\n","Epoch 106/2000, Step 21, d_loss: 0.3567694425582886, g_loss: 5.532193660736084\n","Epoch 106/2000, Step 22, d_loss: 0.35456690192222595, g_loss: 4.13011360168457\n","Epoch 106/2000, Step 23, d_loss: 0.3727402985095978, g_loss: 4.659456253051758\n","Epoch 106/2000, Step 24, d_loss: 0.3742753863334656, g_loss: 4.64221715927124\n","Epoch 106/2000, Step 25, d_loss: 0.3630933463573456, g_loss: 4.55018424987793\n","Epoch 106/2000, Step 26, d_loss: 0.3953094482421875, g_loss: 5.425395965576172\n","Epoch 106/2000, Step 27, d_loss: 0.3495282530784607, g_loss: 2.350131034851074\n","Epoch 106/2000, Step 28, d_loss: 0.4521104097366333, g_loss: 5.891550540924072\n","Epoch 106/2000, Step 29, d_loss: 0.48902398347854614, g_loss: 5.2320027351379395\n","Epoch 106/2000, Step 30, d_loss: 0.36015352606773376, g_loss: 5.348359107971191\n","Epoch 106/2000, Step 31, d_loss: 0.37526968121528625, g_loss: 5.478375434875488\n","Epoch 106/2000, Step 32, d_loss: 0.38560667634010315, g_loss: 7.489309787750244\n","Epoch 106/2000, Step 33, d_loss: 0.3846908509731293, g_loss: 2.189077615737915\n","Epoch 106/2000, Step 34, d_loss: 0.37301987409591675, g_loss: 4.2366204261779785\n","Epoch 106/2000, Step 35, d_loss: 0.37935590744018555, g_loss: 3.7522637844085693\n","Epoch 106/2000, Step 36, d_loss: 0.364643931388855, g_loss: 4.078898906707764\n","Epoch 106/2000, Step 37, d_loss: 0.3803623616695404, g_loss: 5.123594760894775\n","Epoch 106/2000, Step 38, d_loss: 0.4110393226146698, g_loss: 5.486948013305664\n","Epoch 106/2000, Step 39, d_loss: 0.37958797812461853, g_loss: 3.92592453956604\n","Epoch 106/2000, Step 40, d_loss: 0.37239134311676025, g_loss: 4.062228679656982\n","Epoch 106/2000, Step 41, d_loss: 0.3554554581642151, g_loss: 3.641368865966797\n","Epoch 106/2000, Step 42, d_loss: 0.40639474987983704, g_loss: 3.8368608951568604\n","Epoch 106/2000, Step 43, d_loss: 0.3796398639678955, g_loss: 4.087530136108398\n","Epoch 106/2000, Step 44, d_loss: 0.4163217544555664, g_loss: 4.176395416259766\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 107/2000, Step 1, d_loss: 0.4059126675128937, g_loss: 3.419712781906128\n","Epoch 107/2000, Step 2, d_loss: 0.3968343436717987, g_loss: 4.068686485290527\n","Epoch 107/2000, Step 3, d_loss: 0.41506311297416687, g_loss: 4.3268022537231445\n","Epoch 107/2000, Step 4, d_loss: 0.36614304780960083, g_loss: 3.275310754776001\n","Epoch 107/2000, Step 5, d_loss: 0.374024897813797, g_loss: 3.650885820388794\n","Epoch 107/2000, Step 6, d_loss: 0.39463135600090027, g_loss: 4.944190979003906\n","Epoch 107/2000, Step 7, d_loss: 0.35835447907447815, g_loss: 3.9351491928100586\n","Epoch 107/2000, Step 8, d_loss: 0.3733113706111908, g_loss: 5.302802562713623\n","Epoch 107/2000, Step 9, d_loss: 0.35453131794929504, g_loss: 4.213717937469482\n","Epoch 107/2000, Step 10, d_loss: 0.3706722557544708, g_loss: 5.4862589836120605\n","Epoch 107/2000, Step 11, d_loss: 0.3727172911167145, g_loss: 5.82658052444458\n","Epoch 107/2000, Step 12, d_loss: 0.36209237575531006, g_loss: 3.393986225128174\n","Epoch 107/2000, Step 13, d_loss: 0.3791011869907379, g_loss: 3.5502429008483887\n","Epoch 107/2000, Step 14, d_loss: 0.3761861324310303, g_loss: 5.120675086975098\n","Epoch 107/2000, Step 15, d_loss: 0.3511882722377777, g_loss: 3.3599162101745605\n","Epoch 107/2000, Step 16, d_loss: 0.34953680634498596, g_loss: 2.9744842052459717\n","Epoch 107/2000, Step 17, d_loss: 0.38309258222579956, g_loss: 3.9134106636047363\n","Epoch 107/2000, Step 18, d_loss: 0.3827548325061798, g_loss: 3.118089437484741\n","Epoch 107/2000, Step 19, d_loss: 0.7269550561904907, g_loss: 3.6487340927124023\n","Epoch 107/2000, Step 20, d_loss: 0.3861124813556671, g_loss: 5.642858982086182\n","Epoch 107/2000, Step 21, d_loss: 0.5133401155471802, g_loss: 5.0654826164245605\n","Epoch 107/2000, Step 22, d_loss: 0.45150575041770935, g_loss: 5.1889801025390625\n","Epoch 107/2000, Step 23, d_loss: 0.3607600927352905, g_loss: 6.270925998687744\n","Epoch 107/2000, Step 24, d_loss: 0.3888946771621704, g_loss: 3.8968937397003174\n","Epoch 107/2000, Step 25, d_loss: 0.4061034917831421, g_loss: 5.1791205406188965\n","Epoch 107/2000, Step 26, d_loss: 0.3888593018054962, g_loss: 1.1776392459869385\n","Epoch 107/2000, Step 27, d_loss: 0.4525468945503235, g_loss: 3.471120595932007\n","Epoch 107/2000, Step 28, d_loss: 0.6559277772903442, g_loss: 4.241879940032959\n","Epoch 107/2000, Step 29, d_loss: 0.3851216435432434, g_loss: 6.591000080108643\n","Epoch 107/2000, Step 30, d_loss: 0.48983028531074524, g_loss: 4.13217306137085\n","Epoch 107/2000, Step 31, d_loss: 0.3826701045036316, g_loss: 5.14754056930542\n","Epoch 107/2000, Step 32, d_loss: 0.39461174607276917, g_loss: 4.175806999206543\n","Epoch 107/2000, Step 33, d_loss: 0.5233587622642517, g_loss: 2.1168978214263916\n","Epoch 107/2000, Step 34, d_loss: 0.5338170528411865, g_loss: 3.47430419921875\n","Epoch 107/2000, Step 35, d_loss: 0.3704128861427307, g_loss: 3.5721211433410645\n","Epoch 107/2000, Step 36, d_loss: 0.3795534670352936, g_loss: 3.534165382385254\n","Epoch 107/2000, Step 37, d_loss: 0.5284640192985535, g_loss: 3.783557176589966\n","Epoch 107/2000, Step 38, d_loss: 0.47059017419815063, g_loss: 4.27638578414917\n","Epoch 107/2000, Step 39, d_loss: 0.4186836779117584, g_loss: 5.276407718658447\n","Epoch 107/2000, Step 40, d_loss: 0.37093785405158997, g_loss: 4.36603307723999\n","Epoch 107/2000, Step 41, d_loss: 0.35258054733276367, g_loss: 3.1965956687927246\n","Epoch 107/2000, Step 42, d_loss: 0.6416325569152832, g_loss: 5.001533508300781\n","Epoch 107/2000, Step 43, d_loss: 0.3674212098121643, g_loss: 3.2781448364257812\n","Epoch 107/2000, Step 44, d_loss: 0.7179694771766663, g_loss: 4.355088233947754\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 108/2000, Step 1, d_loss: 0.3599611520767212, g_loss: 4.320422172546387\n","Epoch 108/2000, Step 2, d_loss: 0.4232177436351776, g_loss: 1.6102159023284912\n","Epoch 108/2000, Step 3, d_loss: 0.3839646875858307, g_loss: 3.874467611312866\n","Epoch 108/2000, Step 4, d_loss: 0.38829725980758667, g_loss: 2.2765517234802246\n","Epoch 108/2000, Step 5, d_loss: 0.631717324256897, g_loss: 3.950629949569702\n","Epoch 108/2000, Step 6, d_loss: 0.40381595492362976, g_loss: 3.804356336593628\n","Epoch 108/2000, Step 7, d_loss: 0.48589271306991577, g_loss: 5.028027057647705\n","Epoch 108/2000, Step 8, d_loss: 0.4344809949398041, g_loss: 3.956061601638794\n","Epoch 108/2000, Step 9, d_loss: 0.983786940574646, g_loss: 4.688261032104492\n","Epoch 108/2000, Step 10, d_loss: 0.48437824845314026, g_loss: 2.950681447982788\n","Epoch 108/2000, Step 11, d_loss: 0.4443797469139099, g_loss: 2.5704689025878906\n","Epoch 108/2000, Step 12, d_loss: 0.5116595029830933, g_loss: 3.2936208248138428\n","Epoch 108/2000, Step 13, d_loss: 0.5381882190704346, g_loss: 4.017245292663574\n","Epoch 108/2000, Step 14, d_loss: 0.4436875581741333, g_loss: 3.815056800842285\n","Epoch 108/2000, Step 15, d_loss: 0.39568501710891724, g_loss: 2.372539758682251\n","Epoch 108/2000, Step 16, d_loss: 0.4537065029144287, g_loss: 4.024299144744873\n","Epoch 108/2000, Step 17, d_loss: 0.4997819662094116, g_loss: 3.4250943660736084\n","Epoch 108/2000, Step 18, d_loss: 0.38646721839904785, g_loss: 2.938723564147949\n","Epoch 108/2000, Step 19, d_loss: 0.42453455924987793, g_loss: 3.7928125858306885\n","Epoch 108/2000, Step 20, d_loss: 0.42728933691978455, g_loss: 5.0126633644104\n","Epoch 108/2000, Step 21, d_loss: 0.421649307012558, g_loss: 4.069634914398193\n","Epoch 108/2000, Step 22, d_loss: 0.4337412118911743, g_loss: 3.2584517002105713\n","Epoch 108/2000, Step 23, d_loss: 0.4081757366657257, g_loss: 4.481707572937012\n","Epoch 108/2000, Step 24, d_loss: 0.4299483895301819, g_loss: 4.659082412719727\n","Epoch 108/2000, Step 25, d_loss: 0.3833271265029907, g_loss: 5.998010158538818\n","Epoch 108/2000, Step 26, d_loss: 0.3938933312892914, g_loss: 5.333763599395752\n","Epoch 108/2000, Step 27, d_loss: 0.4493255615234375, g_loss: 3.47649884223938\n","Epoch 108/2000, Step 28, d_loss: 0.42020702362060547, g_loss: 3.049747943878174\n","Epoch 108/2000, Step 29, d_loss: 0.4020123779773712, g_loss: 4.704953193664551\n","Epoch 108/2000, Step 30, d_loss: 0.4603479504585266, g_loss: 5.4902873039245605\n","Epoch 108/2000, Step 31, d_loss: 0.39191150665283203, g_loss: 3.151564121246338\n","Epoch 108/2000, Step 32, d_loss: 0.43233561515808105, g_loss: 5.118993759155273\n","Epoch 108/2000, Step 33, d_loss: 0.48055821657180786, g_loss: 5.205033302307129\n","Epoch 108/2000, Step 34, d_loss: 0.4786737561225891, g_loss: 5.575933933258057\n","Epoch 108/2000, Step 35, d_loss: 0.38145697116851807, g_loss: 3.4987995624542236\n","Epoch 108/2000, Step 36, d_loss: 0.3761194050312042, g_loss: 3.4377574920654297\n","Epoch 108/2000, Step 37, d_loss: 0.40684396028518677, g_loss: 3.681562662124634\n","Epoch 108/2000, Step 38, d_loss: 0.4119574725627899, g_loss: 3.914746046066284\n","Epoch 108/2000, Step 39, d_loss: 0.5103726983070374, g_loss: 4.29812479019165\n","Epoch 108/2000, Step 40, d_loss: 0.3916678726673126, g_loss: 3.001303195953369\n","Epoch 108/2000, Step 41, d_loss: 0.36281460523605347, g_loss: 5.125251293182373\n","Epoch 108/2000, Step 42, d_loss: 0.4988420903682709, g_loss: 4.125133991241455\n","Epoch 108/2000, Step 43, d_loss: 0.41225665807724, g_loss: 4.504149913787842\n","Epoch 108/2000, Step 44, d_loss: 0.42517733573913574, g_loss: 3.1517372131347656\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 109/2000, Step 1, d_loss: 0.4888702630996704, g_loss: 4.92755126953125\n","Epoch 109/2000, Step 2, d_loss: 0.37856441736221313, g_loss: 2.9238178730010986\n","Epoch 109/2000, Step 3, d_loss: 0.39357489347457886, g_loss: 3.0610463619232178\n","Epoch 109/2000, Step 4, d_loss: 0.4251941442489624, g_loss: 4.242869853973389\n","Epoch 109/2000, Step 5, d_loss: 0.4008176624774933, g_loss: 3.959852933883667\n","Epoch 109/2000, Step 6, d_loss: 0.4229277968406677, g_loss: 5.232649803161621\n","Epoch 109/2000, Step 7, d_loss: 0.405584454536438, g_loss: 4.866068363189697\n","Epoch 109/2000, Step 8, d_loss: 0.39189374446868896, g_loss: 4.109658718109131\n","Epoch 109/2000, Step 9, d_loss: 0.3620515465736389, g_loss: 4.549697399139404\n","Epoch 109/2000, Step 10, d_loss: 0.39715349674224854, g_loss: 3.0068821907043457\n","Epoch 109/2000, Step 11, d_loss: 0.4354409873485565, g_loss: 5.237619876861572\n","Epoch 109/2000, Step 12, d_loss: 0.4745851755142212, g_loss: 4.626498222351074\n","Epoch 109/2000, Step 13, d_loss: 0.3985499441623688, g_loss: 5.223858833312988\n","Epoch 109/2000, Step 14, d_loss: 0.5055592060089111, g_loss: 5.227606296539307\n","Epoch 109/2000, Step 15, d_loss: 0.46111515164375305, g_loss: 3.3702504634857178\n","Epoch 109/2000, Step 16, d_loss: 0.43461474776268005, g_loss: 4.640862464904785\n","Epoch 109/2000, Step 17, d_loss: 0.43824976682662964, g_loss: 5.31278133392334\n","Epoch 109/2000, Step 18, d_loss: 0.5804286003112793, g_loss: 3.6557657718658447\n","Epoch 109/2000, Step 19, d_loss: 0.4003228545188904, g_loss: 3.1532838344573975\n","Epoch 109/2000, Step 20, d_loss: 0.47076815366744995, g_loss: 4.546156883239746\n","Epoch 109/2000, Step 21, d_loss: 0.44245558977127075, g_loss: 4.4168782234191895\n","Epoch 109/2000, Step 22, d_loss: 0.4006969630718231, g_loss: 6.179174423217773\n","Epoch 109/2000, Step 23, d_loss: 0.44259387254714966, g_loss: 4.569619655609131\n","Epoch 109/2000, Step 24, d_loss: 0.44449856877326965, g_loss: 3.319570541381836\n","Epoch 109/2000, Step 25, d_loss: 0.36637014150619507, g_loss: 5.085454940795898\n","Epoch 109/2000, Step 26, d_loss: 0.3750816583633423, g_loss: 3.883258819580078\n","Epoch 109/2000, Step 27, d_loss: 0.44196617603302, g_loss: 3.8586843013763428\n","Epoch 109/2000, Step 28, d_loss: 0.5076978206634521, g_loss: 5.127434253692627\n","Epoch 109/2000, Step 29, d_loss: 0.6454557180404663, g_loss: 4.0250630378723145\n","Epoch 109/2000, Step 30, d_loss: 0.48624515533447266, g_loss: 3.896517753601074\n","Epoch 109/2000, Step 31, d_loss: 0.45897403359413147, g_loss: 4.518946647644043\n","Epoch 109/2000, Step 32, d_loss: 0.5174410939216614, g_loss: 4.607613563537598\n","Epoch 109/2000, Step 33, d_loss: 0.38929226994514465, g_loss: 3.0641651153564453\n","Epoch 109/2000, Step 34, d_loss: 0.3668833374977112, g_loss: 6.650330066680908\n","Epoch 109/2000, Step 35, d_loss: 0.3989773094654083, g_loss: 7.576224327087402\n","Epoch 109/2000, Step 36, d_loss: 0.5656440258026123, g_loss: 5.573995113372803\n","Epoch 109/2000, Step 37, d_loss: 0.44964054226875305, g_loss: 2.8129258155822754\n","Epoch 109/2000, Step 38, d_loss: 0.41300344467163086, g_loss: 5.096907138824463\n","Epoch 109/2000, Step 39, d_loss: 0.406989187002182, g_loss: 3.9388632774353027\n","Epoch 109/2000, Step 40, d_loss: 0.5130330920219421, g_loss: 3.2213706970214844\n","Epoch 109/2000, Step 41, d_loss: 0.39533036947250366, g_loss: 6.271369457244873\n","Epoch 109/2000, Step 42, d_loss: 0.42835748195648193, g_loss: 5.231180191040039\n","Epoch 109/2000, Step 43, d_loss: 0.38022661209106445, g_loss: 6.023448944091797\n","Epoch 109/2000, Step 44, d_loss: 0.3777807354927063, g_loss: 4.278277397155762\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 110/2000, Step 1, d_loss: 0.40273135900497437, g_loss: 4.049483776092529\n","Epoch 110/2000, Step 2, d_loss: 0.39932557940483093, g_loss: 4.745609283447266\n","Epoch 110/2000, Step 3, d_loss: 0.38073402643203735, g_loss: 5.048733711242676\n","Epoch 110/2000, Step 4, d_loss: 0.3838551640510559, g_loss: 3.6826159954071045\n","Epoch 110/2000, Step 5, d_loss: 0.44644519686698914, g_loss: 4.29538631439209\n","Epoch 110/2000, Step 6, d_loss: 0.38986504077911377, g_loss: 3.7314980030059814\n","Epoch 110/2000, Step 7, d_loss: 0.4227665662765503, g_loss: 5.273054599761963\n","Epoch 110/2000, Step 8, d_loss: 0.4286194145679474, g_loss: 5.007565498352051\n","Epoch 110/2000, Step 9, d_loss: 0.41178008913993835, g_loss: 4.6936187744140625\n","Epoch 110/2000, Step 10, d_loss: 0.37640026211738586, g_loss: 3.6898279190063477\n","Epoch 110/2000, Step 11, d_loss: 0.3823224604129791, g_loss: 4.480999946594238\n","Epoch 110/2000, Step 12, d_loss: 0.38498634099960327, g_loss: 4.540858268737793\n","Epoch 110/2000, Step 13, d_loss: 0.4950868487358093, g_loss: 3.0989255905151367\n","Epoch 110/2000, Step 14, d_loss: 0.3483707010746002, g_loss: 4.378474712371826\n","Epoch 110/2000, Step 15, d_loss: 0.3587352931499481, g_loss: 3.8558223247528076\n","Epoch 110/2000, Step 16, d_loss: 0.40619832277297974, g_loss: 4.1121296882629395\n","Epoch 110/2000, Step 17, d_loss: 0.42602744698524475, g_loss: 2.3534140586853027\n","Epoch 110/2000, Step 18, d_loss: 0.4589858055114746, g_loss: 3.2823357582092285\n","Epoch 110/2000, Step 19, d_loss: 0.41430068016052246, g_loss: 3.7711546421051025\n","Epoch 110/2000, Step 20, d_loss: 0.4128583073616028, g_loss: 5.061182975769043\n","Epoch 110/2000, Step 21, d_loss: 0.3822207450866699, g_loss: 5.166841983795166\n","Epoch 110/2000, Step 22, d_loss: 0.4384428858757019, g_loss: 4.861616611480713\n","Epoch 110/2000, Step 23, d_loss: 0.4147407114505768, g_loss: 5.174886703491211\n","Epoch 110/2000, Step 24, d_loss: 0.38791924715042114, g_loss: 4.6048407554626465\n","Epoch 110/2000, Step 25, d_loss: 0.36952587962150574, g_loss: 4.081024169921875\n","Epoch 110/2000, Step 26, d_loss: 0.3762412965297699, g_loss: 4.406380653381348\n","Epoch 110/2000, Step 27, d_loss: 0.49732765555381775, g_loss: 4.404906749725342\n","Epoch 110/2000, Step 28, d_loss: 0.43292906880378723, g_loss: 2.8294026851654053\n","Epoch 110/2000, Step 29, d_loss: 0.4107292890548706, g_loss: 3.287320137023926\n","Epoch 110/2000, Step 30, d_loss: 0.4029957354068756, g_loss: 3.8587214946746826\n","Epoch 110/2000, Step 31, d_loss: 0.45673978328704834, g_loss: 3.188812732696533\n","Epoch 110/2000, Step 32, d_loss: 0.3865838050842285, g_loss: 3.3292174339294434\n","Epoch 110/2000, Step 33, d_loss: 0.3736903965473175, g_loss: 3.6854088306427\n","Epoch 110/2000, Step 34, d_loss: 0.3905761241912842, g_loss: 5.639344692230225\n","Epoch 110/2000, Step 35, d_loss: 0.39014220237731934, g_loss: 5.459622859954834\n","Epoch 110/2000, Step 36, d_loss: 0.3996971845626831, g_loss: 4.798844337463379\n","Epoch 110/2000, Step 37, d_loss: 0.45373573899269104, g_loss: 3.6474640369415283\n","Epoch 110/2000, Step 38, d_loss: 0.42346256971359253, g_loss: 3.6210086345672607\n","Epoch 110/2000, Step 39, d_loss: 0.49272000789642334, g_loss: 3.6086184978485107\n","Epoch 110/2000, Step 40, d_loss: 0.3726280629634857, g_loss: 3.2392702102661133\n","Epoch 110/2000, Step 41, d_loss: 0.45899444818496704, g_loss: 4.356070041656494\n","Epoch 110/2000, Step 42, d_loss: 0.3802698254585266, g_loss: 2.8323616981506348\n","Epoch 110/2000, Step 43, d_loss: 0.3809763789176941, g_loss: 3.668030261993408\n","Epoch 110/2000, Step 44, d_loss: 0.4176276922225952, g_loss: 3.606534719467163\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 111/2000, Step 1, d_loss: 0.36464428901672363, g_loss: 4.154968738555908\n","Epoch 111/2000, Step 2, d_loss: 0.42993679642677307, g_loss: 4.787832260131836\n","Epoch 111/2000, Step 3, d_loss: 0.42694246768951416, g_loss: 3.2203445434570312\n","Epoch 111/2000, Step 4, d_loss: 0.39969223737716675, g_loss: 1.7245213985443115\n","Epoch 111/2000, Step 5, d_loss: 0.41846802830696106, g_loss: 3.3883249759674072\n","Epoch 111/2000, Step 6, d_loss: 0.43100935220718384, g_loss: 2.827481508255005\n","Epoch 111/2000, Step 7, d_loss: 0.6277648210525513, g_loss: 3.8896849155426025\n","Epoch 111/2000, Step 8, d_loss: 0.4105571210384369, g_loss: 4.371302604675293\n","Epoch 111/2000, Step 9, d_loss: 0.5226935148239136, g_loss: 5.090710639953613\n","Epoch 111/2000, Step 10, d_loss: 0.4403395354747772, g_loss: 4.2226457595825195\n","Epoch 111/2000, Step 11, d_loss: 0.4130522310733795, g_loss: 5.224675178527832\n","Epoch 111/2000, Step 12, d_loss: 0.4000363051891327, g_loss: 7.034833908081055\n","Epoch 111/2000, Step 13, d_loss: 0.3966158628463745, g_loss: 3.235720634460449\n","Epoch 111/2000, Step 14, d_loss: 0.3960495889186859, g_loss: 1.584635615348816\n","Epoch 111/2000, Step 15, d_loss: 0.5316292643547058, g_loss: 3.1945343017578125\n","Epoch 111/2000, Step 16, d_loss: 0.42521554231643677, g_loss: 4.277899265289307\n","Epoch 111/2000, Step 17, d_loss: 0.46669310331344604, g_loss: 3.370527744293213\n","Epoch 111/2000, Step 18, d_loss: 0.36529192328453064, g_loss: 4.109888076782227\n","Epoch 111/2000, Step 19, d_loss: 0.40128499269485474, g_loss: 5.642669677734375\n","Epoch 111/2000, Step 20, d_loss: 0.48790228366851807, g_loss: 5.20845365524292\n","Epoch 111/2000, Step 21, d_loss: 0.37310856580734253, g_loss: 5.364100933074951\n","Epoch 111/2000, Step 22, d_loss: 0.4524071514606476, g_loss: 2.0295791625976562\n","Epoch 111/2000, Step 23, d_loss: 0.813322901725769, g_loss: 4.202556610107422\n","Epoch 111/2000, Step 24, d_loss: 0.4243503212928772, g_loss: 4.13529109954834\n","Epoch 111/2000, Step 25, d_loss: 0.4323434829711914, g_loss: 5.996693134307861\n","Epoch 111/2000, Step 26, d_loss: 0.43180423974990845, g_loss: 5.972679615020752\n","Epoch 111/2000, Step 27, d_loss: 0.36661478877067566, g_loss: 5.184432029724121\n","Epoch 111/2000, Step 28, d_loss: 0.4152219295501709, g_loss: 4.53242826461792\n","Epoch 111/2000, Step 29, d_loss: 0.49523308873176575, g_loss: 4.898321151733398\n","Epoch 111/2000, Step 30, d_loss: 0.5338891744613647, g_loss: 4.188157081604004\n","Epoch 111/2000, Step 31, d_loss: 0.4215271472930908, g_loss: 4.416638374328613\n","Epoch 111/2000, Step 32, d_loss: 0.3963102400302887, g_loss: 5.936972141265869\n","Epoch 111/2000, Step 33, d_loss: 0.4033450186252594, g_loss: 2.7451765537261963\n","Epoch 111/2000, Step 34, d_loss: 0.41447046399116516, g_loss: 4.28077507019043\n","Epoch 111/2000, Step 35, d_loss: 0.41320091485977173, g_loss: 5.7661542892456055\n","Epoch 111/2000, Step 36, d_loss: 0.3798450231552124, g_loss: 5.101900577545166\n","Epoch 111/2000, Step 37, d_loss: 0.42760175466537476, g_loss: 4.146603584289551\n","Epoch 111/2000, Step 38, d_loss: 0.36705484986305237, g_loss: 4.2157392501831055\n","Epoch 111/2000, Step 39, d_loss: 0.43228214979171753, g_loss: 2.8658530712127686\n","Epoch 111/2000, Step 40, d_loss: 0.4692792296409607, g_loss: 2.478262424468994\n","Epoch 111/2000, Step 41, d_loss: 0.39305418729782104, g_loss: 3.8505141735076904\n","Epoch 111/2000, Step 42, d_loss: 0.42603668570518494, g_loss: 4.724981784820557\n","Epoch 111/2000, Step 43, d_loss: 0.39392369985580444, g_loss: 3.7030467987060547\n","Epoch 111/2000, Step 44, d_loss: 0.37387004494667053, g_loss: 4.634108543395996\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 112/2000, Step 1, d_loss: 0.36209315061569214, g_loss: 3.015308141708374\n","Epoch 112/2000, Step 2, d_loss: 0.4613635540008545, g_loss: 1.6360912322998047\n","Epoch 112/2000, Step 3, d_loss: 0.43577373027801514, g_loss: 2.865699052810669\n","Epoch 112/2000, Step 4, d_loss: 0.37842950224876404, g_loss: 2.8278748989105225\n","Epoch 112/2000, Step 5, d_loss: 0.5874738693237305, g_loss: 3.313308000564575\n","Epoch 112/2000, Step 6, d_loss: 0.39505189657211304, g_loss: 4.446703910827637\n","Epoch 112/2000, Step 7, d_loss: 0.4161941707134247, g_loss: 3.206923007965088\n","Epoch 112/2000, Step 8, d_loss: 0.36372193694114685, g_loss: 4.067661762237549\n","Epoch 112/2000, Step 9, d_loss: 0.44351834058761597, g_loss: 3.086153984069824\n","Epoch 112/2000, Step 10, d_loss: 0.3927527964115143, g_loss: 4.154618740081787\n","Epoch 112/2000, Step 11, d_loss: 0.5213937163352966, g_loss: 4.4559783935546875\n","Epoch 112/2000, Step 12, d_loss: 0.38661250472068787, g_loss: 4.1272664070129395\n","Epoch 112/2000, Step 13, d_loss: 0.3771131932735443, g_loss: 3.934009552001953\n","Epoch 112/2000, Step 14, d_loss: 0.4738319218158722, g_loss: 5.220371246337891\n","Epoch 112/2000, Step 15, d_loss: 0.37580227851867676, g_loss: 4.869526386260986\n","Epoch 112/2000, Step 16, d_loss: 0.3524571359157562, g_loss: 3.7432844638824463\n","Epoch 112/2000, Step 17, d_loss: 0.36744678020477295, g_loss: 4.961124897003174\n","Epoch 112/2000, Step 18, d_loss: 0.39918389916419983, g_loss: 2.5404603481292725\n","Epoch 112/2000, Step 19, d_loss: 0.3985796868801117, g_loss: 2.23697566986084\n","Epoch 112/2000, Step 20, d_loss: 0.4439651370048523, g_loss: 4.344904899597168\n","Epoch 112/2000, Step 21, d_loss: 0.41644999384880066, g_loss: 3.023846387863159\n","Epoch 112/2000, Step 22, d_loss: 0.3572981059551239, g_loss: 4.383042335510254\n","Epoch 112/2000, Step 23, d_loss: 0.41720688343048096, g_loss: 3.4380593299865723\n","Epoch 112/2000, Step 24, d_loss: 0.38511115312576294, g_loss: 3.6357436180114746\n","Epoch 112/2000, Step 25, d_loss: 0.38522693514823914, g_loss: 3.776689291000366\n","Epoch 112/2000, Step 26, d_loss: 0.40321484208106995, g_loss: 8.366348266601562\n","Epoch 112/2000, Step 27, d_loss: 0.3580548167228699, g_loss: 4.149393558502197\n","Epoch 112/2000, Step 28, d_loss: 0.44087451696395874, g_loss: 5.8184590339660645\n","Epoch 112/2000, Step 29, d_loss: 0.3635094463825226, g_loss: 3.4311017990112305\n","Epoch 112/2000, Step 30, d_loss: 0.3505561351776123, g_loss: 3.9024429321289062\n","Epoch 112/2000, Step 31, d_loss: 0.37649887800216675, g_loss: 5.7689619064331055\n","Epoch 112/2000, Step 32, d_loss: 0.4334409236907959, g_loss: 3.0324058532714844\n","Epoch 112/2000, Step 33, d_loss: 0.5156428217887878, g_loss: 6.456809997558594\n","Epoch 112/2000, Step 34, d_loss: 0.4324933886528015, g_loss: 4.007175445556641\n","Epoch 112/2000, Step 35, d_loss: 0.3973303437232971, g_loss: 2.811047315597534\n","Epoch 112/2000, Step 36, d_loss: 0.4148452877998352, g_loss: 3.7383570671081543\n","Epoch 112/2000, Step 37, d_loss: 0.4538145065307617, g_loss: 3.9272122383117676\n","Epoch 112/2000, Step 38, d_loss: 0.39709869027137756, g_loss: 5.405569553375244\n","Epoch 112/2000, Step 39, d_loss: 0.4233943819999695, g_loss: 2.95800518989563\n","Epoch 112/2000, Step 40, d_loss: 0.4087773859500885, g_loss: 4.241147994995117\n","Epoch 112/2000, Step 41, d_loss: 0.3662171959877014, g_loss: 4.7519426345825195\n","Epoch 112/2000, Step 42, d_loss: 0.45671117305755615, g_loss: 5.068801403045654\n","Epoch 112/2000, Step 43, d_loss: 0.35967400670051575, g_loss: 4.823846817016602\n","Epoch 112/2000, Step 44, d_loss: 0.46991589665412903, g_loss: 3.8070363998413086\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 113/2000, Step 1, d_loss: 0.37510421872138977, g_loss: 4.906616687774658\n","Epoch 113/2000, Step 2, d_loss: 0.3705064356327057, g_loss: 6.504312515258789\n","Epoch 113/2000, Step 3, d_loss: 0.38962802290916443, g_loss: 4.3898606300354\n","Epoch 113/2000, Step 4, d_loss: 0.38425037264823914, g_loss: 4.450414180755615\n","Epoch 113/2000, Step 5, d_loss: 0.45098352432250977, g_loss: 2.1322615146636963\n","Epoch 113/2000, Step 6, d_loss: 0.3782314658164978, g_loss: 3.4340717792510986\n","Epoch 113/2000, Step 7, d_loss: 0.3751222789287567, g_loss: 2.6647067070007324\n","Epoch 113/2000, Step 8, d_loss: 0.4004006087779999, g_loss: 2.6253111362457275\n","Epoch 113/2000, Step 9, d_loss: 0.3843514323234558, g_loss: 3.8380794525146484\n","Epoch 113/2000, Step 10, d_loss: 0.3763939142227173, g_loss: 2.859565258026123\n","Epoch 113/2000, Step 11, d_loss: 0.3736846446990967, g_loss: 3.081317663192749\n","Epoch 113/2000, Step 12, d_loss: 0.38294756412506104, g_loss: 4.550487041473389\n","Epoch 113/2000, Step 13, d_loss: 0.37163349986076355, g_loss: 3.9438564777374268\n","Epoch 113/2000, Step 14, d_loss: 0.4169398546218872, g_loss: 3.864509344100952\n","Epoch 113/2000, Step 15, d_loss: 0.3540874123573303, g_loss: 3.706690788269043\n","Epoch 113/2000, Step 16, d_loss: 0.5600212216377258, g_loss: 4.401530742645264\n","Epoch 113/2000, Step 17, d_loss: 0.3569220006465912, g_loss: 3.935349225997925\n","Epoch 113/2000, Step 18, d_loss: 0.39738571643829346, g_loss: 3.460178852081299\n","Epoch 113/2000, Step 19, d_loss: 0.4235486388206482, g_loss: 5.390332221984863\n","Epoch 113/2000, Step 20, d_loss: 0.4739547371864319, g_loss: 2.353504180908203\n","Epoch 113/2000, Step 21, d_loss: 0.43624693155288696, g_loss: 3.752002000808716\n","Epoch 113/2000, Step 22, d_loss: 0.43686339259147644, g_loss: 5.863705635070801\n","Epoch 113/2000, Step 23, d_loss: 0.377388060092926, g_loss: 5.238187789916992\n","Epoch 113/2000, Step 24, d_loss: 0.36971306800842285, g_loss: 5.701984405517578\n","Epoch 113/2000, Step 25, d_loss: 0.3917272090911865, g_loss: 5.777157306671143\n","Epoch 113/2000, Step 26, d_loss: 0.4350375831127167, g_loss: 5.516191005706787\n","Epoch 113/2000, Step 27, d_loss: 0.4210120439529419, g_loss: 5.914139747619629\n","Epoch 113/2000, Step 28, d_loss: 0.4987379312515259, g_loss: 3.1000564098358154\n","Epoch 113/2000, Step 29, d_loss: 0.36257269978523254, g_loss: 3.1518502235412598\n","Epoch 113/2000, Step 30, d_loss: 0.5622255206108093, g_loss: 2.654689311981201\n","Epoch 113/2000, Step 31, d_loss: 0.5915380716323853, g_loss: 6.92366361618042\n","Epoch 113/2000, Step 32, d_loss: 0.47037985920906067, g_loss: 3.456258535385132\n","Epoch 113/2000, Step 33, d_loss: 0.3965527415275574, g_loss: 4.384400844573975\n","Epoch 113/2000, Step 34, d_loss: 0.40678882598876953, g_loss: 4.681811332702637\n","Epoch 113/2000, Step 35, d_loss: 0.3973860740661621, g_loss: 5.272336006164551\n","Epoch 113/2000, Step 36, d_loss: 0.3853951096534729, g_loss: 5.850778102874756\n","Epoch 113/2000, Step 37, d_loss: 0.4295480251312256, g_loss: 3.1294209957122803\n","Epoch 113/2000, Step 38, d_loss: 0.40205055475234985, g_loss: 4.058895111083984\n","Epoch 113/2000, Step 39, d_loss: 0.5055445432662964, g_loss: 3.39833927154541\n","Epoch 113/2000, Step 40, d_loss: 0.694007158279419, g_loss: 3.5737342834472656\n","Epoch 113/2000, Step 41, d_loss: 0.46827030181884766, g_loss: 1.1773854494094849\n","Epoch 113/2000, Step 42, d_loss: 0.4333704113960266, g_loss: 3.167771339416504\n","Epoch 113/2000, Step 43, d_loss: 0.40458548069000244, g_loss: 2.2973670959472656\n","Epoch 113/2000, Step 44, d_loss: 0.3894190192222595, g_loss: 3.4963276386260986\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 114/2000, Step 1, d_loss: 0.49995386600494385, g_loss: 3.866652727127075\n","Epoch 114/2000, Step 2, d_loss: 0.4000687301158905, g_loss: 5.561148166656494\n","Epoch 114/2000, Step 3, d_loss: 0.3473065495491028, g_loss: 4.705336570739746\n","Epoch 114/2000, Step 4, d_loss: 0.4296412169933319, g_loss: 4.3866286277771\n","Epoch 114/2000, Step 5, d_loss: 0.49653351306915283, g_loss: 2.7663514614105225\n","Epoch 114/2000, Step 6, d_loss: 0.39403337240219116, g_loss: 4.3495283126831055\n","Epoch 114/2000, Step 7, d_loss: 0.3978135585784912, g_loss: 2.698701858520508\n","Epoch 114/2000, Step 8, d_loss: 0.4106247127056122, g_loss: 2.7138659954071045\n","Epoch 114/2000, Step 9, d_loss: 0.45505911111831665, g_loss: 6.324156284332275\n","Epoch 114/2000, Step 10, d_loss: 0.40321868658065796, g_loss: 6.72675895690918\n","Epoch 114/2000, Step 11, d_loss: 0.5294123291969299, g_loss: 3.1168453693389893\n","Epoch 114/2000, Step 12, d_loss: 0.4543360769748688, g_loss: 2.83662486076355\n","Epoch 114/2000, Step 13, d_loss: 0.42006343603134155, g_loss: 3.4361143112182617\n","Epoch 114/2000, Step 14, d_loss: 0.4049586355686188, g_loss: 3.4865410327911377\n","Epoch 114/2000, Step 15, d_loss: 0.4329904317855835, g_loss: 3.2305634021759033\n","Epoch 114/2000, Step 16, d_loss: 0.7156227231025696, g_loss: 3.667377233505249\n","Epoch 114/2000, Step 17, d_loss: 0.5155701041221619, g_loss: 4.592158794403076\n","Epoch 114/2000, Step 18, d_loss: 0.3826068341732025, g_loss: 4.666622161865234\n","Epoch 114/2000, Step 19, d_loss: 0.4481833875179291, g_loss: 6.903057098388672\n","Epoch 114/2000, Step 20, d_loss: 0.5167970061302185, g_loss: 6.050840377807617\n","Epoch 114/2000, Step 21, d_loss: 0.4142242968082428, g_loss: 6.525594711303711\n","Epoch 114/2000, Step 22, d_loss: 0.5464296340942383, g_loss: 3.4084672927856445\n","Epoch 114/2000, Step 23, d_loss: 0.4373852610588074, g_loss: 0.9196226596832275\n","Epoch 114/2000, Step 24, d_loss: 0.4601125717163086, g_loss: 1.0182043313980103\n","Epoch 114/2000, Step 25, d_loss: 0.4508630633354187, g_loss: 1.6107840538024902\n","Epoch 114/2000, Step 26, d_loss: 0.4247153401374817, g_loss: 2.216111898422241\n","Epoch 114/2000, Step 27, d_loss: 0.7214342355728149, g_loss: 4.171083450317383\n","Epoch 114/2000, Step 28, d_loss: 0.4484189748764038, g_loss: 4.8949666023254395\n","Epoch 114/2000, Step 29, d_loss: 0.4502410292625427, g_loss: 5.364332675933838\n","Epoch 114/2000, Step 30, d_loss: 0.5897741317749023, g_loss: 7.015183448791504\n","Epoch 114/2000, Step 31, d_loss: 0.41270601749420166, g_loss: 5.271817207336426\n","Epoch 114/2000, Step 32, d_loss: 0.43052220344543457, g_loss: 5.87142276763916\n","Epoch 114/2000, Step 33, d_loss: 0.40017014741897583, g_loss: 4.707771301269531\n","Epoch 114/2000, Step 34, d_loss: 0.39487534761428833, g_loss: 3.650104522705078\n","Epoch 114/2000, Step 35, d_loss: 0.4403417408466339, g_loss: 5.132999897003174\n","Epoch 114/2000, Step 36, d_loss: 0.44516000151634216, g_loss: 3.6048929691314697\n","Epoch 114/2000, Step 37, d_loss: 0.4066505432128906, g_loss: 2.6283929347991943\n","Epoch 114/2000, Step 38, d_loss: 0.44262006878852844, g_loss: 3.5247857570648193\n","Epoch 114/2000, Step 39, d_loss: 0.42612695693969727, g_loss: 4.803014755249023\n","Epoch 114/2000, Step 40, d_loss: 0.3812333345413208, g_loss: 4.78787899017334\n","Epoch 114/2000, Step 41, d_loss: 0.4221499264240265, g_loss: 6.646243095397949\n","Epoch 114/2000, Step 42, d_loss: 0.5167010426521301, g_loss: 5.562459468841553\n","Epoch 114/2000, Step 43, d_loss: 0.4142354130744934, g_loss: 4.842422962188721\n","Epoch 114/2000, Step 44, d_loss: 0.36594367027282715, g_loss: 2.650371551513672\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 115/2000, Step 1, d_loss: 0.4616960287094116, g_loss: 3.766721725463867\n","Epoch 115/2000, Step 2, d_loss: 0.40881824493408203, g_loss: 5.532461166381836\n","Epoch 115/2000, Step 3, d_loss: 0.4102931618690491, g_loss: 2.805488348007202\n","Epoch 115/2000, Step 4, d_loss: 0.5371906757354736, g_loss: 3.4469008445739746\n","Epoch 115/2000, Step 5, d_loss: 0.37232330441474915, g_loss: 5.427432537078857\n","Epoch 115/2000, Step 6, d_loss: 0.4462917745113373, g_loss: 3.810079336166382\n","Epoch 115/2000, Step 7, d_loss: 0.427203893661499, g_loss: 4.271454334259033\n","Epoch 115/2000, Step 8, d_loss: 0.4235589802265167, g_loss: 3.6786835193634033\n","Epoch 115/2000, Step 9, d_loss: 0.44704410433769226, g_loss: 4.688208103179932\n","Epoch 115/2000, Step 10, d_loss: 0.4422891438007355, g_loss: 3.600635051727295\n","Epoch 115/2000, Step 11, d_loss: 0.4382830858230591, g_loss: 2.8524930477142334\n","Epoch 115/2000, Step 12, d_loss: 0.619564414024353, g_loss: 3.12848162651062\n","Epoch 115/2000, Step 13, d_loss: 0.43760445713996887, g_loss: 3.6233887672424316\n","Epoch 115/2000, Step 14, d_loss: 0.41921037435531616, g_loss: 6.292756080627441\n","Epoch 115/2000, Step 15, d_loss: 0.4718709886074066, g_loss: 5.778689861297607\n","Epoch 115/2000, Step 16, d_loss: 0.36996927857398987, g_loss: 3.3528640270233154\n","Epoch 115/2000, Step 17, d_loss: 0.37527263164520264, g_loss: 4.178010940551758\n","Epoch 115/2000, Step 18, d_loss: 0.49415671825408936, g_loss: 3.509993076324463\n","Epoch 115/2000, Step 19, d_loss: 0.38348981738090515, g_loss: 4.218697547912598\n","Epoch 115/2000, Step 20, d_loss: 0.3917980492115021, g_loss: 3.5635931491851807\n","Epoch 115/2000, Step 21, d_loss: 0.39741918444633484, g_loss: 2.288118362426758\n","Epoch 115/2000, Step 22, d_loss: 0.4849088191986084, g_loss: 5.896058559417725\n","Epoch 115/2000, Step 23, d_loss: 0.4632928669452667, g_loss: 2.9256675243377686\n","Epoch 115/2000, Step 24, d_loss: 0.39730143547058105, g_loss: 5.335341930389404\n","Epoch 115/2000, Step 25, d_loss: 0.39219793677330017, g_loss: 4.08603572845459\n","Epoch 115/2000, Step 26, d_loss: 0.3666594922542572, g_loss: 5.7456488609313965\n","Epoch 115/2000, Step 27, d_loss: 0.4531526267528534, g_loss: 5.036111831665039\n","Epoch 115/2000, Step 28, d_loss: 0.4127444326877594, g_loss: 4.987485408782959\n","Epoch 115/2000, Step 29, d_loss: 0.3823400139808655, g_loss: 4.206904888153076\n","Epoch 115/2000, Step 30, d_loss: 0.5822681188583374, g_loss: 3.8982319831848145\n","Epoch 115/2000, Step 31, d_loss: 0.41930732131004333, g_loss: 3.850109815597534\n","Epoch 115/2000, Step 32, d_loss: 0.6348441243171692, g_loss: 4.950107097625732\n","Epoch 115/2000, Step 33, d_loss: 0.6382404565811157, g_loss: 4.442951679229736\n","Epoch 115/2000, Step 34, d_loss: 0.5712178945541382, g_loss: 4.436792373657227\n","Epoch 115/2000, Step 35, d_loss: 0.4362994432449341, g_loss: 4.792105674743652\n","Epoch 115/2000, Step 36, d_loss: 0.4364338517189026, g_loss: 6.171715259552002\n","Epoch 115/2000, Step 37, d_loss: 0.42108821868896484, g_loss: 6.1091532707214355\n","Epoch 115/2000, Step 38, d_loss: 0.4244394600391388, g_loss: 5.549507141113281\n","Epoch 115/2000, Step 39, d_loss: 0.489112913608551, g_loss: 3.1385996341705322\n","Epoch 115/2000, Step 40, d_loss: 0.4131585955619812, g_loss: 5.36139440536499\n","Epoch 115/2000, Step 41, d_loss: 0.437170147895813, g_loss: 2.44580340385437\n","Epoch 115/2000, Step 42, d_loss: 0.4495442807674408, g_loss: 3.1140029430389404\n","Epoch 115/2000, Step 43, d_loss: 0.4472120404243469, g_loss: 2.3969085216522217\n","Epoch 115/2000, Step 44, d_loss: 1.0013463497161865, g_loss: 6.890635013580322\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 116/2000, Step 1, d_loss: 1.0814764499664307, g_loss: 6.632584095001221\n","Epoch 116/2000, Step 2, d_loss: 0.4726129174232483, g_loss: 3.9693522453308105\n","Epoch 116/2000, Step 3, d_loss: 0.9463604092597961, g_loss: 5.362634658813477\n","Epoch 116/2000, Step 4, d_loss: 0.39033016562461853, g_loss: 2.5850584506988525\n","Epoch 116/2000, Step 5, d_loss: 0.47181665897369385, g_loss: 4.20225191116333\n","Epoch 116/2000, Step 6, d_loss: 0.564628005027771, g_loss: 1.847387671470642\n","Epoch 116/2000, Step 7, d_loss: 0.8725177645683289, g_loss: 3.589063882827759\n","Epoch 116/2000, Step 8, d_loss: 0.6610690355300903, g_loss: 3.4475886821746826\n","Epoch 116/2000, Step 9, d_loss: 0.7540507912635803, g_loss: 3.517040967941284\n","Epoch 116/2000, Step 10, d_loss: 0.7173442840576172, g_loss: 3.1704859733581543\n","Epoch 116/2000, Step 11, d_loss: 0.5222818851470947, g_loss: 2.2314629554748535\n","Epoch 116/2000, Step 12, d_loss: 0.6143272519111633, g_loss: 2.903517484664917\n","Epoch 116/2000, Step 13, d_loss: 0.491485059261322, g_loss: 3.4307405948638916\n","Epoch 116/2000, Step 14, d_loss: 0.7180933356285095, g_loss: 3.2800309658050537\n","Epoch 116/2000, Step 15, d_loss: 0.5432629585266113, g_loss: 3.0443918704986572\n","Epoch 116/2000, Step 16, d_loss: 0.5282310843467712, g_loss: 4.201376914978027\n","Epoch 116/2000, Step 17, d_loss: 0.5547433495521545, g_loss: 2.673356771469116\n","Epoch 116/2000, Step 18, d_loss: 0.4807508587837219, g_loss: 2.6018002033233643\n","Epoch 116/2000, Step 19, d_loss: 0.600031316280365, g_loss: 2.431096315383911\n","Epoch 116/2000, Step 20, d_loss: 0.48017334938049316, g_loss: 1.929764986038208\n","Epoch 116/2000, Step 21, d_loss: 0.49244919419288635, g_loss: 1.1203885078430176\n","Epoch 116/2000, Step 22, d_loss: 0.3914150297641754, g_loss: 2.703308582305908\n","Epoch 116/2000, Step 23, d_loss: 0.4018482565879822, g_loss: 0.9563764929771423\n","Epoch 116/2000, Step 24, d_loss: 0.4400280714035034, g_loss: 0.8180646300315857\n","Epoch 116/2000, Step 25, d_loss: 0.49262890219688416, g_loss: 2.8311245441436768\n","Epoch 116/2000, Step 26, d_loss: 0.5850241184234619, g_loss: 3.5435025691986084\n","Epoch 116/2000, Step 27, d_loss: 0.4423406422138214, g_loss: 5.362741470336914\n","Epoch 116/2000, Step 28, d_loss: 0.6885604858398438, g_loss: 3.170315742492676\n","Epoch 116/2000, Step 29, d_loss: 0.5560282468795776, g_loss: 3.996746778488159\n","Epoch 116/2000, Step 30, d_loss: 0.4948696494102478, g_loss: 3.5079689025878906\n","Epoch 116/2000, Step 31, d_loss: 0.40918469429016113, g_loss: 2.986445665359497\n","Epoch 116/2000, Step 32, d_loss: 0.5423157215118408, g_loss: 3.520185947418213\n","Epoch 116/2000, Step 33, d_loss: 0.4490979015827179, g_loss: 3.8609256744384766\n","Epoch 116/2000, Step 34, d_loss: 0.44629615545272827, g_loss: 2.6444461345672607\n","Epoch 116/2000, Step 35, d_loss: 0.49005040526390076, g_loss: 3.553534984588623\n","Epoch 116/2000, Step 36, d_loss: 0.3804289400577545, g_loss: 3.8981375694274902\n","Epoch 116/2000, Step 37, d_loss: 0.5101320743560791, g_loss: 3.1903247833251953\n","Epoch 116/2000, Step 38, d_loss: 0.4340069890022278, g_loss: 3.9919159412384033\n","Epoch 116/2000, Step 39, d_loss: 0.38587820529937744, g_loss: 3.1686058044433594\n","Epoch 116/2000, Step 40, d_loss: 0.3831220269203186, g_loss: 5.429753303527832\n","Epoch 116/2000, Step 41, d_loss: 0.4066000282764435, g_loss: 3.7516164779663086\n","Epoch 116/2000, Step 42, d_loss: 0.45332658290863037, g_loss: 2.672884464263916\n","Epoch 116/2000, Step 43, d_loss: 0.42917221784591675, g_loss: 3.0738184452056885\n","Epoch 116/2000, Step 44, d_loss: 0.4267345666885376, g_loss: 2.5063443183898926\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 117/2000, Step 1, d_loss: 0.3843732476234436, g_loss: 3.1372010707855225\n","Epoch 117/2000, Step 2, d_loss: 0.3965272009372711, g_loss: 2.782870292663574\n","Epoch 117/2000, Step 3, d_loss: 0.3816748857498169, g_loss: 3.54720139503479\n","Epoch 117/2000, Step 4, d_loss: 0.6949460506439209, g_loss: 6.814871311187744\n","Epoch 117/2000, Step 5, d_loss: 0.5356596112251282, g_loss: 3.5319294929504395\n","Epoch 117/2000, Step 6, d_loss: 0.44191259145736694, g_loss: 4.96444034576416\n","Epoch 117/2000, Step 7, d_loss: 0.4016696810722351, g_loss: 3.8397278785705566\n","Epoch 117/2000, Step 8, d_loss: 0.4012775421142578, g_loss: 3.61564040184021\n","Epoch 117/2000, Step 9, d_loss: 0.38840872049331665, g_loss: 3.573655366897583\n","Epoch 117/2000, Step 10, d_loss: 0.39254218339920044, g_loss: 3.056230068206787\n","Epoch 117/2000, Step 11, d_loss: 0.38539808988571167, g_loss: 3.2588305473327637\n","Epoch 117/2000, Step 12, d_loss: 0.36750560998916626, g_loss: 4.60276460647583\n","Epoch 117/2000, Step 13, d_loss: 0.40346434712409973, g_loss: 2.3862156867980957\n","Epoch 117/2000, Step 14, d_loss: 0.3620665967464447, g_loss: 1.4818764925003052\n","Epoch 117/2000, Step 15, d_loss: 0.3943980932235718, g_loss: 4.510985374450684\n","Epoch 117/2000, Step 16, d_loss: 0.3887317180633545, g_loss: 3.2200047969818115\n","Epoch 117/2000, Step 17, d_loss: 0.41195061802864075, g_loss: 2.7392404079437256\n","Epoch 117/2000, Step 18, d_loss: 0.47174856066703796, g_loss: 4.172359466552734\n","Epoch 117/2000, Step 19, d_loss: 0.4061579704284668, g_loss: 4.604493618011475\n","Epoch 117/2000, Step 20, d_loss: 0.3751848340034485, g_loss: 3.0081825256347656\n","Epoch 117/2000, Step 21, d_loss: 0.4072737693786621, g_loss: 5.386799335479736\n","Epoch 117/2000, Step 22, d_loss: 0.37923067808151245, g_loss: 5.928502082824707\n","Epoch 117/2000, Step 23, d_loss: 0.37249788641929626, g_loss: 4.492915630340576\n","Epoch 117/2000, Step 24, d_loss: 0.6242150068283081, g_loss: 3.8344926834106445\n","Epoch 117/2000, Step 25, d_loss: 0.4268815517425537, g_loss: 3.462412118911743\n","Epoch 117/2000, Step 26, d_loss: 0.3765559196472168, g_loss: 5.521144390106201\n","Epoch 117/2000, Step 27, d_loss: 0.43664878606796265, g_loss: 4.446106910705566\n","Epoch 117/2000, Step 28, d_loss: 0.38340163230895996, g_loss: 5.355174541473389\n","Epoch 117/2000, Step 29, d_loss: 0.4597957134246826, g_loss: 5.574642658233643\n","Epoch 117/2000, Step 30, d_loss: 0.35747578740119934, g_loss: 3.3713488578796387\n","Epoch 117/2000, Step 31, d_loss: 0.4275578260421753, g_loss: 3.6250381469726562\n","Epoch 117/2000, Step 32, d_loss: 0.37045755982398987, g_loss: 4.690641403198242\n","Epoch 117/2000, Step 33, d_loss: 0.40006738901138306, g_loss: 5.755932807922363\n","Epoch 117/2000, Step 34, d_loss: 0.424740195274353, g_loss: 4.92202091217041\n","Epoch 117/2000, Step 35, d_loss: 0.4548019468784332, g_loss: 2.9614551067352295\n","Epoch 117/2000, Step 36, d_loss: 0.38102126121520996, g_loss: 3.479318618774414\n","Epoch 117/2000, Step 37, d_loss: 0.3871378004550934, g_loss: 2.1313695907592773\n","Epoch 117/2000, Step 38, d_loss: 0.3779493570327759, g_loss: 5.014946937561035\n","Epoch 117/2000, Step 39, d_loss: 0.5083974003791809, g_loss: 6.184264659881592\n","Epoch 117/2000, Step 40, d_loss: 0.4307928681373596, g_loss: 2.096306800842285\n","Epoch 117/2000, Step 41, d_loss: 0.4961870312690735, g_loss: 3.684803009033203\n","Epoch 117/2000, Step 42, d_loss: 0.40683189034461975, g_loss: 3.9836525917053223\n","Epoch 117/2000, Step 43, d_loss: 0.5018740892410278, g_loss: 3.1518757343292236\n","Epoch 117/2000, Step 44, d_loss: 0.41804853081703186, g_loss: 3.803705930709839\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 118/2000, Step 1, d_loss: 0.3785080909729004, g_loss: 4.28167200088501\n","Epoch 118/2000, Step 2, d_loss: 0.3607249855995178, g_loss: 4.675731658935547\n","Epoch 118/2000, Step 3, d_loss: 0.5628608465194702, g_loss: 5.900784969329834\n","Epoch 118/2000, Step 4, d_loss: 0.3570456802845001, g_loss: 6.028632164001465\n","Epoch 118/2000, Step 5, d_loss: 0.3798655867576599, g_loss: 6.152637481689453\n","Epoch 118/2000, Step 6, d_loss: 0.3687707781791687, g_loss: 4.537092208862305\n","Epoch 118/2000, Step 7, d_loss: 0.396509051322937, g_loss: 3.576307535171509\n","Epoch 118/2000, Step 8, d_loss: 0.41650745272636414, g_loss: 3.2317748069763184\n","Epoch 118/2000, Step 9, d_loss: 0.3882248103618622, g_loss: 3.159976005554199\n","Epoch 118/2000, Step 10, d_loss: 0.42817842960357666, g_loss: 2.8757410049438477\n","Epoch 118/2000, Step 11, d_loss: 0.4271744191646576, g_loss: 5.753574848175049\n","Epoch 118/2000, Step 12, d_loss: 0.40406396985054016, g_loss: 3.404254674911499\n","Epoch 118/2000, Step 13, d_loss: 0.3839741349220276, g_loss: 2.131234884262085\n","Epoch 118/2000, Step 14, d_loss: 0.40768590569496155, g_loss: 3.6827590465545654\n","Epoch 118/2000, Step 15, d_loss: 0.46060866117477417, g_loss: 5.106490135192871\n","Epoch 118/2000, Step 16, d_loss: 0.36742547154426575, g_loss: 5.416991710662842\n","Epoch 118/2000, Step 17, d_loss: 0.3490028977394104, g_loss: 4.684647083282471\n","Epoch 118/2000, Step 18, d_loss: 0.46975335478782654, g_loss: 5.855750560760498\n","Epoch 118/2000, Step 19, d_loss: 0.40068960189819336, g_loss: 4.8951497077941895\n","Epoch 118/2000, Step 20, d_loss: 0.5245141983032227, g_loss: 5.289330005645752\n","Epoch 118/2000, Step 21, d_loss: 0.3525838255882263, g_loss: 3.1574718952178955\n","Epoch 118/2000, Step 22, d_loss: 0.3999691605567932, g_loss: 2.607978582382202\n","Epoch 118/2000, Step 23, d_loss: 0.4921693205833435, g_loss: 4.094761371612549\n","Epoch 118/2000, Step 24, d_loss: 0.3688829243183136, g_loss: 3.5282211303710938\n","Epoch 118/2000, Step 25, d_loss: 0.40165719389915466, g_loss: 3.1651275157928467\n","Epoch 118/2000, Step 26, d_loss: 0.4103236794471741, g_loss: 5.609548091888428\n","Epoch 118/2000, Step 27, d_loss: 0.42157983779907227, g_loss: 4.001300811767578\n","Epoch 118/2000, Step 28, d_loss: 0.36951184272766113, g_loss: 3.1487033367156982\n","Epoch 118/2000, Step 29, d_loss: 0.4866379201412201, g_loss: 5.166555881500244\n","Epoch 118/2000, Step 30, d_loss: 0.3750144839286804, g_loss: 5.907402038574219\n","Epoch 118/2000, Step 31, d_loss: 0.35454261302948, g_loss: 5.218181133270264\n","Epoch 118/2000, Step 32, d_loss: 0.3850080966949463, g_loss: 3.2400381565093994\n","Epoch 118/2000, Step 33, d_loss: 0.37713056802749634, g_loss: 5.437932968139648\n","Epoch 118/2000, Step 34, d_loss: 0.48370033502578735, g_loss: 3.0299572944641113\n","Epoch 118/2000, Step 35, d_loss: 0.34976547956466675, g_loss: 4.0870184898376465\n","Epoch 118/2000, Step 36, d_loss: 0.4136759638786316, g_loss: 4.657442092895508\n","Epoch 118/2000, Step 37, d_loss: 0.34202858805656433, g_loss: 3.77004075050354\n","Epoch 118/2000, Step 38, d_loss: 0.4071897268295288, g_loss: 5.074827671051025\n","Epoch 118/2000, Step 39, d_loss: 0.37914136052131653, g_loss: 5.341236114501953\n","Epoch 118/2000, Step 40, d_loss: 0.37621891498565674, g_loss: 2.7528133392333984\n","Epoch 118/2000, Step 41, d_loss: 0.525378942489624, g_loss: 5.788502216339111\n","Epoch 118/2000, Step 42, d_loss: 0.5371261239051819, g_loss: 4.520422458648682\n","Epoch 118/2000, Step 43, d_loss: 0.39124730229377747, g_loss: 4.8396806716918945\n","Epoch 118/2000, Step 44, d_loss: 0.49210992455482483, g_loss: 5.420620441436768\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 119/2000, Step 1, d_loss: 0.3634774088859558, g_loss: 3.558229446411133\n","Epoch 119/2000, Step 2, d_loss: 0.40101557970046997, g_loss: 5.711081027984619\n","Epoch 119/2000, Step 3, d_loss: 0.4921761453151703, g_loss: 3.500917673110962\n","Epoch 119/2000, Step 4, d_loss: 0.39307618141174316, g_loss: 4.343939304351807\n","Epoch 119/2000, Step 5, d_loss: 0.4457095265388489, g_loss: 5.062257766723633\n","Epoch 119/2000, Step 6, d_loss: 0.4170069098472595, g_loss: 3.7094359397888184\n","Epoch 119/2000, Step 7, d_loss: 0.3909304141998291, g_loss: 4.376856327056885\n","Epoch 119/2000, Step 8, d_loss: 0.39363932609558105, g_loss: 3.5926849842071533\n","Epoch 119/2000, Step 9, d_loss: 0.45701879262924194, g_loss: 7.840910911560059\n","Epoch 119/2000, Step 10, d_loss: 0.4076858460903168, g_loss: 4.771732807159424\n","Epoch 119/2000, Step 11, d_loss: 0.4473212957382202, g_loss: 2.868901252746582\n","Epoch 119/2000, Step 12, d_loss: 0.3716751039028168, g_loss: 3.8424201011657715\n","Epoch 119/2000, Step 13, d_loss: 0.3860592544078827, g_loss: 2.4636921882629395\n","Epoch 119/2000, Step 14, d_loss: 0.39516812562942505, g_loss: 3.270315408706665\n","Epoch 119/2000, Step 15, d_loss: 0.37515226006507874, g_loss: 4.462906360626221\n","Epoch 119/2000, Step 16, d_loss: 0.4073108732700348, g_loss: 4.32899808883667\n","Epoch 119/2000, Step 17, d_loss: 0.4254932999610901, g_loss: 5.432027816772461\n","Epoch 119/2000, Step 18, d_loss: 0.40286028385162354, g_loss: 5.696263313293457\n","Epoch 119/2000, Step 19, d_loss: 0.3801282048225403, g_loss: 5.14329195022583\n","Epoch 119/2000, Step 20, d_loss: 0.36777058243751526, g_loss: 6.93290901184082\n","Epoch 119/2000, Step 21, d_loss: 0.3582859933376312, g_loss: 4.791520118713379\n","Epoch 119/2000, Step 22, d_loss: 0.37736085057258606, g_loss: 5.84378719329834\n","Epoch 119/2000, Step 23, d_loss: 0.38276615738868713, g_loss: 6.188138008117676\n","Epoch 119/2000, Step 24, d_loss: 0.5208175182342529, g_loss: 6.40056848526001\n","Epoch 119/2000, Step 25, d_loss: 0.3579823970794678, g_loss: 5.924117088317871\n","Epoch 119/2000, Step 26, d_loss: 0.42975538969039917, g_loss: 3.7690083980560303\n","Epoch 119/2000, Step 27, d_loss: 0.3575800657272339, g_loss: 4.5566864013671875\n","Epoch 119/2000, Step 28, d_loss: 0.376823753118515, g_loss: 4.554859161376953\n","Epoch 119/2000, Step 29, d_loss: 0.3680543005466461, g_loss: 3.3535685539245605\n","Epoch 119/2000, Step 30, d_loss: 0.5275585651397705, g_loss: 3.721230983734131\n","Epoch 119/2000, Step 31, d_loss: 0.6016793251037598, g_loss: 3.240668773651123\n","Epoch 119/2000, Step 32, d_loss: 0.37038108706474304, g_loss: 6.684048652648926\n","Epoch 119/2000, Step 33, d_loss: 0.38153210282325745, g_loss: 5.444134712219238\n","Epoch 119/2000, Step 34, d_loss: 0.4092685282230377, g_loss: 6.349460124969482\n","Epoch 119/2000, Step 35, d_loss: 0.39197733998298645, g_loss: 5.071180820465088\n","Epoch 119/2000, Step 36, d_loss: 0.3687843382358551, g_loss: 6.645956993103027\n","Epoch 119/2000, Step 37, d_loss: 0.45276492834091187, g_loss: 5.970218658447266\n","Epoch 119/2000, Step 38, d_loss: 0.3935508728027344, g_loss: 6.043167591094971\n","Epoch 119/2000, Step 39, d_loss: 0.3863328695297241, g_loss: 3.9874792098999023\n","Epoch 119/2000, Step 40, d_loss: 0.4151294231414795, g_loss: 2.342527151107788\n","Epoch 119/2000, Step 41, d_loss: 0.4268873929977417, g_loss: 2.702233076095581\n","Epoch 119/2000, Step 42, d_loss: 0.4323713779449463, g_loss: 3.2095820903778076\n","Epoch 119/2000, Step 43, d_loss: 0.3953324258327484, g_loss: 5.168459415435791\n","Epoch 119/2000, Step 44, d_loss: 0.3719582259654999, g_loss: 5.988897800445557\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 120/2000, Step 1, d_loss: 0.35792961716651917, g_loss: 4.678826808929443\n","Epoch 120/2000, Step 2, d_loss: 0.36941179633140564, g_loss: 4.1809306144714355\n","Epoch 120/2000, Step 3, d_loss: 0.42570042610168457, g_loss: 4.633456707000732\n","Epoch 120/2000, Step 4, d_loss: 0.3707113265991211, g_loss: 4.174297332763672\n","Epoch 120/2000, Step 5, d_loss: 0.46842432022094727, g_loss: 6.103829860687256\n","Epoch 120/2000, Step 6, d_loss: 0.38837283849716187, g_loss: 2.8299789428710938\n","Epoch 120/2000, Step 7, d_loss: 0.6641033887863159, g_loss: 2.8881421089172363\n","Epoch 120/2000, Step 8, d_loss: 0.4461843967437744, g_loss: 5.87918758392334\n","Epoch 120/2000, Step 9, d_loss: 0.3952113389968872, g_loss: 3.4730350971221924\n","Epoch 120/2000, Step 10, d_loss: 0.36916583776474, g_loss: 6.042293071746826\n","Epoch 120/2000, Step 11, d_loss: 0.3827877342700958, g_loss: 5.699856281280518\n","Epoch 120/2000, Step 12, d_loss: 0.4995381236076355, g_loss: 3.570570707321167\n","Epoch 120/2000, Step 13, d_loss: 0.3978070318698883, g_loss: 4.503998756408691\n","Epoch 120/2000, Step 14, d_loss: 0.3437701165676117, g_loss: 4.441714763641357\n","Epoch 120/2000, Step 15, d_loss: 0.41551318764686584, g_loss: 4.330501556396484\n","Epoch 120/2000, Step 16, d_loss: 0.40611210465431213, g_loss: 3.8109889030456543\n","Epoch 120/2000, Step 17, d_loss: 0.43014073371887207, g_loss: 4.613729000091553\n","Epoch 120/2000, Step 18, d_loss: 0.41091984510421753, g_loss: 3.522786855697632\n","Epoch 120/2000, Step 19, d_loss: 0.5063891410827637, g_loss: 2.755197525024414\n","Epoch 120/2000, Step 20, d_loss: 0.45412129163742065, g_loss: 4.306995868682861\n","Epoch 120/2000, Step 21, d_loss: 0.36124590039253235, g_loss: 5.297555923461914\n","Epoch 120/2000, Step 22, d_loss: 0.36057835817337036, g_loss: 6.873158931732178\n","Epoch 120/2000, Step 23, d_loss: 0.38879284262657166, g_loss: 5.756452560424805\n","Epoch 120/2000, Step 24, d_loss: 0.40320712327957153, g_loss: 6.026695728302002\n","Epoch 120/2000, Step 25, d_loss: 0.34895044565200806, g_loss: 4.594693183898926\n","Epoch 120/2000, Step 26, d_loss: 0.3602379858493805, g_loss: 4.365813732147217\n","Epoch 120/2000, Step 27, d_loss: 0.3979339301586151, g_loss: 5.1974639892578125\n","Epoch 120/2000, Step 28, d_loss: 0.3586272895336151, g_loss: 3.620863199234009\n","Epoch 120/2000, Step 29, d_loss: 0.4059225916862488, g_loss: 5.268013954162598\n","Epoch 120/2000, Step 30, d_loss: 0.3932819962501526, g_loss: 4.023615837097168\n","Epoch 120/2000, Step 31, d_loss: 0.37991729378700256, g_loss: 3.3482208251953125\n","Epoch 120/2000, Step 32, d_loss: 0.35162922739982605, g_loss: 6.167900085449219\n","Epoch 120/2000, Step 33, d_loss: 0.37351882457733154, g_loss: 5.513819694519043\n","Epoch 120/2000, Step 34, d_loss: 0.35661137104034424, g_loss: 5.171366214752197\n","Epoch 120/2000, Step 35, d_loss: 0.36708247661590576, g_loss: 4.174314498901367\n","Epoch 120/2000, Step 36, d_loss: 0.3613569736480713, g_loss: 6.365550518035889\n","Epoch 120/2000, Step 37, d_loss: 0.35867318511009216, g_loss: 4.7641921043396\n","Epoch 120/2000, Step 38, d_loss: 0.39196184277534485, g_loss: 3.417304754257202\n","Epoch 120/2000, Step 39, d_loss: 0.36282259225845337, g_loss: 4.182107925415039\n","Epoch 120/2000, Step 40, d_loss: 0.4014638364315033, g_loss: 4.438319683074951\n","Epoch 120/2000, Step 41, d_loss: 0.5120172500610352, g_loss: 5.694164752960205\n","Epoch 120/2000, Step 42, d_loss: 0.34147438406944275, g_loss: 4.411434173583984\n","Epoch 120/2000, Step 43, d_loss: 0.3645024597644806, g_loss: 3.114872932434082\n","Epoch 120/2000, Step 44, d_loss: 0.35514435172080994, g_loss: 5.0158586502075195\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 121/2000, Step 1, d_loss: 0.3814459443092346, g_loss: 6.395472526550293\n","Epoch 121/2000, Step 2, d_loss: 0.38341325521469116, g_loss: 4.451117992401123\n","Epoch 121/2000, Step 3, d_loss: 0.42283886671066284, g_loss: 5.251443862915039\n","Epoch 121/2000, Step 4, d_loss: 0.35818785429000854, g_loss: 3.8821985721588135\n","Epoch 121/2000, Step 5, d_loss: 0.3586166501045227, g_loss: 5.698407173156738\n","Epoch 121/2000, Step 6, d_loss: 0.47420549392700195, g_loss: 3.664963960647583\n","Epoch 121/2000, Step 7, d_loss: 0.37069952487945557, g_loss: 4.080567836761475\n","Epoch 121/2000, Step 8, d_loss: 0.5053343176841736, g_loss: 5.461736679077148\n","Epoch 121/2000, Step 9, d_loss: 0.36310863494873047, g_loss: 3.2208828926086426\n","Epoch 121/2000, Step 10, d_loss: 0.360421746969223, g_loss: 5.960779190063477\n","Epoch 121/2000, Step 11, d_loss: 0.41411203145980835, g_loss: 6.685669898986816\n","Epoch 121/2000, Step 12, d_loss: 0.36400696635246277, g_loss: 3.905764579772949\n","Epoch 121/2000, Step 13, d_loss: 0.4900663495063782, g_loss: 4.600497722625732\n","Epoch 121/2000, Step 14, d_loss: 0.3808332085609436, g_loss: 3.6489224433898926\n","Epoch 121/2000, Step 15, d_loss: 0.39832794666290283, g_loss: 5.86392879486084\n","Epoch 121/2000, Step 16, d_loss: 0.3793739974498749, g_loss: 4.837904453277588\n","Epoch 121/2000, Step 17, d_loss: 0.4417339563369751, g_loss: 5.824270248413086\n","Epoch 121/2000, Step 18, d_loss: 0.38018766045570374, g_loss: 4.224822998046875\n","Epoch 121/2000, Step 19, d_loss: 0.34578680992126465, g_loss: 3.6738905906677246\n","Epoch 121/2000, Step 20, d_loss: 0.3783684968948364, g_loss: 3.8641631603240967\n","Epoch 121/2000, Step 21, d_loss: 0.37856683135032654, g_loss: 5.944500923156738\n","Epoch 121/2000, Step 22, d_loss: 0.35349175333976746, g_loss: 6.120102405548096\n","Epoch 121/2000, Step 23, d_loss: 0.36175408959388733, g_loss: 2.7278831005096436\n","Epoch 121/2000, Step 24, d_loss: 0.4192862808704376, g_loss: 3.027981758117676\n","Epoch 121/2000, Step 25, d_loss: 0.350254088640213, g_loss: 4.023362159729004\n","Epoch 121/2000, Step 26, d_loss: 0.4048049747943878, g_loss: 6.815326690673828\n","Epoch 121/2000, Step 27, d_loss: 0.42974910140037537, g_loss: 7.209334373474121\n","Epoch 121/2000, Step 28, d_loss: 0.5105074048042297, g_loss: 4.902928352355957\n","Epoch 121/2000, Step 29, d_loss: 0.41697433590888977, g_loss: 3.1057631969451904\n","Epoch 121/2000, Step 30, d_loss: 0.42862218618392944, g_loss: 3.0977838039398193\n","Epoch 121/2000, Step 31, d_loss: 0.6669158339500427, g_loss: 5.493675708770752\n","Epoch 121/2000, Step 32, d_loss: 0.42640963196754456, g_loss: 4.60793924331665\n","Epoch 121/2000, Step 33, d_loss: 0.47416412830352783, g_loss: 5.101370811462402\n","Epoch 121/2000, Step 34, d_loss: 0.6126739978790283, g_loss: 5.105521202087402\n","Epoch 121/2000, Step 35, d_loss: 0.35385793447494507, g_loss: 3.448312759399414\n","Epoch 121/2000, Step 36, d_loss: 0.41024839878082275, g_loss: 4.04660177230835\n","Epoch 121/2000, Step 37, d_loss: 0.39448103308677673, g_loss: 7.518054008483887\n","Epoch 121/2000, Step 38, d_loss: 0.4300054609775543, g_loss: 5.313525199890137\n","Epoch 121/2000, Step 39, d_loss: 0.41812244057655334, g_loss: 2.4373462200164795\n","Epoch 121/2000, Step 40, d_loss: 0.4233846366405487, g_loss: 2.6934072971343994\n","Epoch 121/2000, Step 41, d_loss: 0.4597920775413513, g_loss: 4.629422187805176\n","Epoch 121/2000, Step 42, d_loss: 0.3908069133758545, g_loss: 4.807175159454346\n","Epoch 121/2000, Step 43, d_loss: 0.4116378128528595, g_loss: 3.766118288040161\n","Epoch 121/2000, Step 44, d_loss: 0.41377657651901245, g_loss: 4.0311665534973145\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 122/2000, Step 1, d_loss: 0.3791457712650299, g_loss: 5.3915815353393555\n","Epoch 122/2000, Step 2, d_loss: 0.38625985383987427, g_loss: 5.68482780456543\n","Epoch 122/2000, Step 3, d_loss: 0.4220772981643677, g_loss: 5.853630542755127\n","Epoch 122/2000, Step 4, d_loss: 0.44174012541770935, g_loss: 5.285916328430176\n","Epoch 122/2000, Step 5, d_loss: 0.3868947923183441, g_loss: 2.7269887924194336\n","Epoch 122/2000, Step 6, d_loss: 0.39464500546455383, g_loss: 6.159366607666016\n","Epoch 122/2000, Step 7, d_loss: 0.43553686141967773, g_loss: 4.307013511657715\n","Epoch 122/2000, Step 8, d_loss: 0.5064132809638977, g_loss: 4.057448387145996\n","Epoch 122/2000, Step 9, d_loss: 0.49620309472084045, g_loss: 4.482028007507324\n","Epoch 122/2000, Step 10, d_loss: 0.38386282324790955, g_loss: 5.738598823547363\n","Epoch 122/2000, Step 11, d_loss: 0.3842458724975586, g_loss: 3.713925838470459\n","Epoch 122/2000, Step 12, d_loss: 0.3911106586456299, g_loss: 4.633747100830078\n","Epoch 122/2000, Step 13, d_loss: 0.42162585258483887, g_loss: 6.1412434577941895\n","Epoch 122/2000, Step 14, d_loss: 0.4023902416229248, g_loss: 7.242555141448975\n","Epoch 122/2000, Step 15, d_loss: 0.3886854946613312, g_loss: 3.2132556438446045\n","Epoch 122/2000, Step 16, d_loss: 0.5059988498687744, g_loss: 3.37986421585083\n","Epoch 122/2000, Step 17, d_loss: 0.3810088038444519, g_loss: 5.937695503234863\n","Epoch 122/2000, Step 18, d_loss: 0.4036426544189453, g_loss: 3.6746370792388916\n","Epoch 122/2000, Step 19, d_loss: 0.432555228471756, g_loss: 5.2075324058532715\n","Epoch 122/2000, Step 20, d_loss: 0.42215442657470703, g_loss: 5.8117594718933105\n","Epoch 122/2000, Step 21, d_loss: 0.42256641387939453, g_loss: 4.235805988311768\n","Epoch 122/2000, Step 22, d_loss: 0.4572679102420807, g_loss: 5.6864495277404785\n","Epoch 122/2000, Step 23, d_loss: 0.40899181365966797, g_loss: 6.324102878570557\n","Epoch 122/2000, Step 24, d_loss: 0.3731599748134613, g_loss: 1.9273325204849243\n","Epoch 122/2000, Step 25, d_loss: 0.4172113835811615, g_loss: 2.1598782539367676\n","Epoch 122/2000, Step 26, d_loss: 0.4118160605430603, g_loss: 4.996546745300293\n","Epoch 122/2000, Step 27, d_loss: 0.48891976475715637, g_loss: 2.41922926902771\n","Epoch 122/2000, Step 28, d_loss: 0.35503658652305603, g_loss: 2.889528274536133\n","Epoch 122/2000, Step 29, d_loss: 0.36986637115478516, g_loss: 3.342538595199585\n","Epoch 122/2000, Step 30, d_loss: 0.3639379143714905, g_loss: 4.864531993865967\n","Epoch 122/2000, Step 31, d_loss: 0.3939906060695648, g_loss: 3.7040419578552246\n","Epoch 122/2000, Step 32, d_loss: 0.5111227631568909, g_loss: 6.38848352432251\n","Epoch 122/2000, Step 33, d_loss: 0.41946735978126526, g_loss: 4.953061103820801\n","Epoch 122/2000, Step 34, d_loss: 0.4829641282558441, g_loss: 4.526396751403809\n","Epoch 122/2000, Step 35, d_loss: 0.37846827507019043, g_loss: 6.013197898864746\n","Epoch 122/2000, Step 36, d_loss: 0.35296431183815, g_loss: 5.804582118988037\n","Epoch 122/2000, Step 37, d_loss: 0.3706727623939514, g_loss: 2.734501838684082\n","Epoch 122/2000, Step 38, d_loss: 0.4061426520347595, g_loss: 5.361913681030273\n","Epoch 122/2000, Step 39, d_loss: 0.44151774048805237, g_loss: 3.4846110343933105\n","Epoch 122/2000, Step 40, d_loss: 0.37574613094329834, g_loss: 5.571738243103027\n","Epoch 122/2000, Step 41, d_loss: 0.3733536899089813, g_loss: 5.641604423522949\n","Epoch 122/2000, Step 42, d_loss: 0.41395577788352966, g_loss: 3.470924139022827\n","Epoch 122/2000, Step 43, d_loss: 0.4442274570465088, g_loss: 4.2101640701293945\n","Epoch 122/2000, Step 44, d_loss: 0.5302512645721436, g_loss: 3.7664988040924072\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 123/2000, Step 1, d_loss: 0.4037192463874817, g_loss: 4.071267604827881\n","Epoch 123/2000, Step 2, d_loss: 0.37670400738716125, g_loss: 3.8119847774505615\n","Epoch 123/2000, Step 3, d_loss: 0.407988041639328, g_loss: 5.09392786026001\n","Epoch 123/2000, Step 4, d_loss: 0.48061829805374146, g_loss: 6.950613975524902\n","Epoch 123/2000, Step 5, d_loss: 0.3716059923171997, g_loss: 4.28108549118042\n","Epoch 123/2000, Step 6, d_loss: 0.3745522201061249, g_loss: 8.564088821411133\n","Epoch 123/2000, Step 7, d_loss: 0.9856758117675781, g_loss: 4.250843524932861\n","Epoch 123/2000, Step 8, d_loss: 0.35518428683280945, g_loss: 2.758089780807495\n","Epoch 123/2000, Step 9, d_loss: 0.3653962016105652, g_loss: 2.825144052505493\n","Epoch 123/2000, Step 10, d_loss: 0.4518059194087982, g_loss: 2.696124315261841\n","Epoch 123/2000, Step 11, d_loss: 0.6168027520179749, g_loss: 4.302761554718018\n","Epoch 123/2000, Step 12, d_loss: 0.40051794052124023, g_loss: 2.7712411880493164\n","Epoch 123/2000, Step 13, d_loss: 0.550463080406189, g_loss: 4.395354747772217\n","Epoch 123/2000, Step 14, d_loss: 0.45078760385513306, g_loss: 3.8525149822235107\n","Epoch 123/2000, Step 15, d_loss: 0.4675845801830292, g_loss: 6.691382884979248\n","Epoch 123/2000, Step 16, d_loss: 0.49096497893333435, g_loss: 4.556689739227295\n","Epoch 123/2000, Step 17, d_loss: 0.3571804463863373, g_loss: 4.422811508178711\n","Epoch 123/2000, Step 18, d_loss: 0.37768054008483887, g_loss: 3.6271729469299316\n","Epoch 123/2000, Step 19, d_loss: 0.46632057428359985, g_loss: 3.702396869659424\n","Epoch 123/2000, Step 20, d_loss: 0.34801366925239563, g_loss: 4.382744789123535\n","Epoch 123/2000, Step 21, d_loss: 0.42673492431640625, g_loss: 3.5600099563598633\n","Epoch 123/2000, Step 22, d_loss: 0.547195553779602, g_loss: 2.9971354007720947\n","Epoch 123/2000, Step 23, d_loss: 0.38642317056655884, g_loss: 3.100125551223755\n","Epoch 123/2000, Step 24, d_loss: 0.5176835060119629, g_loss: 4.923502445220947\n","Epoch 123/2000, Step 25, d_loss: 0.450397789478302, g_loss: 3.961611270904541\n","Epoch 123/2000, Step 26, d_loss: 0.39975208044052124, g_loss: 3.6262810230255127\n","Epoch 123/2000, Step 27, d_loss: 0.3835943043231964, g_loss: 3.2813870906829834\n","Epoch 123/2000, Step 28, d_loss: 0.38647761940956116, g_loss: 3.150806427001953\n","Epoch 123/2000, Step 29, d_loss: 0.401204377412796, g_loss: 4.395356178283691\n","Epoch 123/2000, Step 30, d_loss: 0.407297819852829, g_loss: 4.005428314208984\n","Epoch 123/2000, Step 31, d_loss: 0.4553097188472748, g_loss: 4.688861846923828\n","Epoch 123/2000, Step 32, d_loss: 0.3766297996044159, g_loss: 6.389598846435547\n","Epoch 123/2000, Step 33, d_loss: 0.39584657549858093, g_loss: 4.018222808837891\n","Epoch 123/2000, Step 34, d_loss: 0.5402539372444153, g_loss: 3.675542116165161\n","Epoch 123/2000, Step 35, d_loss: 0.40669742226600647, g_loss: 3.7555723190307617\n","Epoch 123/2000, Step 36, d_loss: 0.4296345114707947, g_loss: 3.503976345062256\n","Epoch 123/2000, Step 37, d_loss: 0.39460691809654236, g_loss: 5.364883899688721\n","Epoch 123/2000, Step 38, d_loss: 0.48875996470451355, g_loss: 6.589142799377441\n","Epoch 123/2000, Step 39, d_loss: 0.3837215006351471, g_loss: 2.6907005310058594\n","Epoch 123/2000, Step 40, d_loss: 0.5121060013771057, g_loss: 2.9323227405548096\n","Epoch 123/2000, Step 41, d_loss: 0.4782989025115967, g_loss: 5.598145484924316\n","Epoch 123/2000, Step 42, d_loss: 0.47329193353652954, g_loss: 6.0423173904418945\n","Epoch 123/2000, Step 43, d_loss: 0.5625683069229126, g_loss: 6.1257219314575195\n","Epoch 123/2000, Step 44, d_loss: 0.7891536355018616, g_loss: 4.855426788330078\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 124/2000, Step 1, d_loss: 0.47133082151412964, g_loss: 5.000288486480713\n","Epoch 124/2000, Step 2, d_loss: 0.4881521463394165, g_loss: 5.989031791687012\n","Epoch 124/2000, Step 3, d_loss: 0.4067496955394745, g_loss: 4.794741630554199\n","Epoch 124/2000, Step 4, d_loss: 0.40166541934013367, g_loss: 3.0142879486083984\n","Epoch 124/2000, Step 5, d_loss: 0.4488617181777954, g_loss: 3.4191160202026367\n","Epoch 124/2000, Step 6, d_loss: 0.4231985807418823, g_loss: 5.006173610687256\n","Epoch 124/2000, Step 7, d_loss: 0.403058797121048, g_loss: 2.7137954235076904\n","Epoch 124/2000, Step 8, d_loss: 0.5172369480133057, g_loss: 3.4692254066467285\n","Epoch 124/2000, Step 9, d_loss: 0.5320984125137329, g_loss: 2.617144823074341\n","Epoch 124/2000, Step 10, d_loss: 0.49135011434555054, g_loss: 3.861793279647827\n","Epoch 124/2000, Step 11, d_loss: 0.39686721563339233, g_loss: 4.113302230834961\n","Epoch 124/2000, Step 12, d_loss: 0.43391039967536926, g_loss: 3.966061592102051\n","Epoch 124/2000, Step 13, d_loss: 0.39162760972976685, g_loss: 5.062861919403076\n","Epoch 124/2000, Step 14, d_loss: 0.3963690996170044, g_loss: 3.5751848220825195\n","Epoch 124/2000, Step 15, d_loss: 0.43696168065071106, g_loss: 5.0657267570495605\n","Epoch 124/2000, Step 16, d_loss: 0.42821893095970154, g_loss: 3.7805142402648926\n","Epoch 124/2000, Step 17, d_loss: 0.37693482637405396, g_loss: 4.115808010101318\n","Epoch 124/2000, Step 18, d_loss: 0.36270976066589355, g_loss: 2.9530885219573975\n","Epoch 124/2000, Step 19, d_loss: 0.3802923858165741, g_loss: 3.118016481399536\n","Epoch 124/2000, Step 20, d_loss: 0.3945174813270569, g_loss: 4.766902923583984\n","Epoch 124/2000, Step 21, d_loss: 0.4122321605682373, g_loss: 2.2406718730926514\n","Epoch 124/2000, Step 22, d_loss: 0.5183299779891968, g_loss: 4.646787166595459\n","Epoch 124/2000, Step 23, d_loss: 0.38984546065330505, g_loss: 6.620744228363037\n","Epoch 124/2000, Step 24, d_loss: 0.4376712143421173, g_loss: 5.712918758392334\n","Epoch 124/2000, Step 25, d_loss: 0.48001497983932495, g_loss: 3.750246286392212\n","Epoch 124/2000, Step 26, d_loss: 0.40828150510787964, g_loss: 5.781459331512451\n","Epoch 124/2000, Step 27, d_loss: 0.3797643184661865, g_loss: 2.315706491470337\n","Epoch 124/2000, Step 28, d_loss: 0.3937363624572754, g_loss: 3.6822359561920166\n","Epoch 124/2000, Step 29, d_loss: 0.3947366178035736, g_loss: 4.043086528778076\n","Epoch 124/2000, Step 30, d_loss: 0.39510685205459595, g_loss: 3.903838872909546\n","Epoch 124/2000, Step 31, d_loss: 0.44352197647094727, g_loss: 4.066514015197754\n","Epoch 124/2000, Step 32, d_loss: 0.44351428747177124, g_loss: 1.5291575193405151\n","Epoch 124/2000, Step 33, d_loss: 0.3961470127105713, g_loss: 4.770017623901367\n","Epoch 124/2000, Step 34, d_loss: 0.392387330532074, g_loss: 4.476560115814209\n","Epoch 124/2000, Step 35, d_loss: 0.4014018774032593, g_loss: 3.7210052013397217\n","Epoch 124/2000, Step 36, d_loss: 0.4839792549610138, g_loss: 4.4115309715271\n","Epoch 124/2000, Step 37, d_loss: 0.6937546133995056, g_loss: 2.9711625576019287\n","Epoch 124/2000, Step 38, d_loss: 0.4599543809890747, g_loss: 3.2923996448516846\n","Epoch 124/2000, Step 39, d_loss: 0.3831368088722229, g_loss: 1.6942503452301025\n","Epoch 124/2000, Step 40, d_loss: 0.46073779463768005, g_loss: 1.528968095779419\n","Epoch 124/2000, Step 41, d_loss: 0.8215963840484619, g_loss: 3.2063376903533936\n","Epoch 124/2000, Step 42, d_loss: 0.5098718404769897, g_loss: 5.615664958953857\n","Epoch 124/2000, Step 43, d_loss: 0.5512215495109558, g_loss: 5.399919033050537\n","Epoch 124/2000, Step 44, d_loss: 0.7096040844917297, g_loss: 3.766355276107788\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 125/2000, Step 1, d_loss: 0.42664656043052673, g_loss: 4.332945823669434\n","Epoch 125/2000, Step 2, d_loss: 0.49309518933296204, g_loss: 5.192002296447754\n","Epoch 125/2000, Step 3, d_loss: 0.5539040565490723, g_loss: 4.407439708709717\n","Epoch 125/2000, Step 4, d_loss: 0.5602611303329468, g_loss: 2.651179313659668\n","Epoch 125/2000, Step 5, d_loss: 0.4567585587501526, g_loss: 4.6458420753479\n","Epoch 125/2000, Step 6, d_loss: 0.6013350486755371, g_loss: 2.903540849685669\n","Epoch 125/2000, Step 7, d_loss: 0.4675758183002472, g_loss: 3.744189977645874\n","Epoch 125/2000, Step 8, d_loss: 0.6742083430290222, g_loss: 4.344346523284912\n","Epoch 125/2000, Step 9, d_loss: 0.4005682170391083, g_loss: 4.156623840332031\n","Epoch 125/2000, Step 10, d_loss: 0.4114980101585388, g_loss: 3.094639778137207\n","Epoch 125/2000, Step 11, d_loss: 0.5663304328918457, g_loss: 3.819592237472534\n","Epoch 125/2000, Step 12, d_loss: 0.49855461716651917, g_loss: 3.800163984298706\n","Epoch 125/2000, Step 13, d_loss: 0.44445693492889404, g_loss: 4.99514627456665\n","Epoch 125/2000, Step 14, d_loss: 0.4428902864456177, g_loss: 4.335546493530273\n","Epoch 125/2000, Step 15, d_loss: 0.4618402421474457, g_loss: 5.057718753814697\n","Epoch 125/2000, Step 16, d_loss: 0.4351043999195099, g_loss: 3.2922909259796143\n","Epoch 125/2000, Step 17, d_loss: 0.43577203154563904, g_loss: 2.815702438354492\n","Epoch 125/2000, Step 18, d_loss: 0.42511823773384094, g_loss: 3.3128139972686768\n","Epoch 125/2000, Step 19, d_loss: 0.4053530991077423, g_loss: 3.6164300441741943\n","Epoch 125/2000, Step 20, d_loss: 0.47985759377479553, g_loss: 4.338597774505615\n","Epoch 125/2000, Step 21, d_loss: 0.40490755438804626, g_loss: 4.078250408172607\n","Epoch 125/2000, Step 22, d_loss: 0.43759581446647644, g_loss: 2.7286458015441895\n","Epoch 125/2000, Step 23, d_loss: 0.4333650469779968, g_loss: 2.8930752277374268\n","Epoch 125/2000, Step 24, d_loss: 0.41724324226379395, g_loss: 4.054862022399902\n","Epoch 125/2000, Step 25, d_loss: 0.5122034549713135, g_loss: 6.401844501495361\n","Epoch 125/2000, Step 26, d_loss: 0.3857662081718445, g_loss: 3.1117336750030518\n","Epoch 125/2000, Step 27, d_loss: 0.438726544380188, g_loss: 3.275031566619873\n","Epoch 125/2000, Step 28, d_loss: 0.45687025785446167, g_loss: 8.492506980895996\n","Epoch 125/2000, Step 29, d_loss: 0.44124919176101685, g_loss: 3.478199005126953\n","Epoch 125/2000, Step 30, d_loss: 0.4331130385398865, g_loss: 3.3811769485473633\n","Epoch 125/2000, Step 31, d_loss: 0.40623047947883606, g_loss: 2.0911500453948975\n","Epoch 125/2000, Step 32, d_loss: 0.41296619176864624, g_loss: 3.0461111068725586\n","Epoch 125/2000, Step 33, d_loss: 0.4135509431362152, g_loss: 2.9990992546081543\n","Epoch 125/2000, Step 34, d_loss: 0.48625171184539795, g_loss: 1.5343011617660522\n","Epoch 125/2000, Step 35, d_loss: 0.4454798400402069, g_loss: 3.0330698490142822\n","Epoch 125/2000, Step 36, d_loss: 0.4093570113182068, g_loss: 2.7439868450164795\n","Epoch 125/2000, Step 37, d_loss: 0.3849819004535675, g_loss: 2.243685245513916\n","Epoch 125/2000, Step 38, d_loss: 0.591177761554718, g_loss: 6.6976494789123535\n","Epoch 125/2000, Step 39, d_loss: 0.435303270816803, g_loss: 4.788846492767334\n","Epoch 125/2000, Step 40, d_loss: 0.3877943158149719, g_loss: 8.331390380859375\n","Epoch 125/2000, Step 41, d_loss: 0.7573975920677185, g_loss: 4.181848049163818\n","Epoch 125/2000, Step 42, d_loss: 0.40834367275238037, g_loss: 3.89668345451355\n","Epoch 125/2000, Step 43, d_loss: 0.3830915689468384, g_loss: 5.883993148803711\n","Epoch 125/2000, Step 44, d_loss: 0.37981778383255005, g_loss: 3.0635926723480225\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 126/2000, Step 1, d_loss: 0.39253494143486023, g_loss: 3.2081358432769775\n","Epoch 126/2000, Step 2, d_loss: 0.5578441619873047, g_loss: 2.35007643699646\n","Epoch 126/2000, Step 3, d_loss: 0.39759987592697144, g_loss: 4.38297176361084\n","Epoch 126/2000, Step 4, d_loss: 0.3775385618209839, g_loss: 4.324156761169434\n","Epoch 126/2000, Step 5, d_loss: 0.4215623736381531, g_loss: 4.0873565673828125\n","Epoch 126/2000, Step 6, d_loss: 0.521065890789032, g_loss: 5.599641799926758\n","Epoch 126/2000, Step 7, d_loss: 0.5098642110824585, g_loss: 2.840580463409424\n","Epoch 126/2000, Step 8, d_loss: 0.4074510931968689, g_loss: 2.9162094593048096\n","Epoch 126/2000, Step 9, d_loss: 0.39760830998420715, g_loss: 3.8468682765960693\n","Epoch 126/2000, Step 10, d_loss: 0.4675266742706299, g_loss: 3.5553557872772217\n","Epoch 126/2000, Step 11, d_loss: 0.5760951042175293, g_loss: 2.259309768676758\n","Epoch 126/2000, Step 12, d_loss: 0.4375147223472595, g_loss: 5.215541362762451\n","Epoch 126/2000, Step 13, d_loss: 0.48581093549728394, g_loss: 4.394510746002197\n","Epoch 126/2000, Step 14, d_loss: 0.41450411081314087, g_loss: 4.622896194458008\n","Epoch 126/2000, Step 15, d_loss: 0.41572678089141846, g_loss: 5.738607406616211\n","Epoch 126/2000, Step 16, d_loss: 0.4310370683670044, g_loss: 5.570366382598877\n","Epoch 126/2000, Step 17, d_loss: 0.43006670475006104, g_loss: 5.261407852172852\n","Epoch 126/2000, Step 18, d_loss: 0.48991459608078003, g_loss: 3.1120595932006836\n","Epoch 126/2000, Step 19, d_loss: 0.3918970823287964, g_loss: 3.83808970451355\n","Epoch 126/2000, Step 20, d_loss: 0.424877792596817, g_loss: 4.071061611175537\n","Epoch 126/2000, Step 21, d_loss: 0.3686932623386383, g_loss: 2.9004769325256348\n","Epoch 126/2000, Step 22, d_loss: 0.5604497194290161, g_loss: 2.927708625793457\n","Epoch 126/2000, Step 23, d_loss: 0.5102138519287109, g_loss: 5.288298606872559\n","Epoch 126/2000, Step 24, d_loss: 0.4713928997516632, g_loss: 4.946888446807861\n","Epoch 126/2000, Step 25, d_loss: 0.3930829167366028, g_loss: 3.9450178146362305\n","Epoch 126/2000, Step 26, d_loss: 0.3572388291358948, g_loss: 5.1277008056640625\n","Epoch 126/2000, Step 27, d_loss: 0.37604275345802307, g_loss: 3.5500307083129883\n","Epoch 126/2000, Step 28, d_loss: 0.4736905097961426, g_loss: 3.172226667404175\n","Epoch 126/2000, Step 29, d_loss: 0.3730313777923584, g_loss: 4.294025421142578\n","Epoch 126/2000, Step 30, d_loss: 0.4124614894390106, g_loss: 5.009355545043945\n","Epoch 126/2000, Step 31, d_loss: 0.4447285532951355, g_loss: 5.129446983337402\n","Epoch 126/2000, Step 32, d_loss: 0.3545173406600952, g_loss: 4.309963226318359\n","Epoch 126/2000, Step 33, d_loss: 0.4826697111129761, g_loss: 3.50710391998291\n","Epoch 126/2000, Step 34, d_loss: 0.3832170069217682, g_loss: 4.3112921714782715\n","Epoch 126/2000, Step 35, d_loss: 0.3589468002319336, g_loss: 3.2374958992004395\n","Epoch 126/2000, Step 36, d_loss: 0.39447638392448425, g_loss: 3.021613597869873\n","Epoch 126/2000, Step 37, d_loss: 0.38715311884880066, g_loss: 3.3498990535736084\n","Epoch 126/2000, Step 38, d_loss: 0.36780208349227905, g_loss: 4.208276271820068\n","Epoch 126/2000, Step 39, d_loss: 0.3866356909275055, g_loss: 3.8159847259521484\n","Epoch 126/2000, Step 40, d_loss: 0.5358677506446838, g_loss: 1.4403189420700073\n","Epoch 126/2000, Step 41, d_loss: 0.39779481291770935, g_loss: 3.844960927963257\n","Epoch 126/2000, Step 42, d_loss: 0.4479597806930542, g_loss: 3.92808198928833\n","Epoch 126/2000, Step 43, d_loss: 0.37290623784065247, g_loss: 2.215029001235962\n","Epoch 126/2000, Step 44, d_loss: 0.4084433615207672, g_loss: 5.852797985076904\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 127/2000, Step 1, d_loss: 0.356427401304245, g_loss: 5.939851760864258\n","Epoch 127/2000, Step 2, d_loss: 0.40807271003723145, g_loss: 1.8944705724716187\n","Epoch 127/2000, Step 3, d_loss: 0.38225260376930237, g_loss: 5.6805739402771\n","Epoch 127/2000, Step 4, d_loss: 0.42811691761016846, g_loss: 3.052811622619629\n","Epoch 127/2000, Step 5, d_loss: 0.3864404559135437, g_loss: 4.299808025360107\n","Epoch 127/2000, Step 6, d_loss: 0.37011492252349854, g_loss: 3.8389508724212646\n","Epoch 127/2000, Step 7, d_loss: 0.3960878849029541, g_loss: 5.738903999328613\n","Epoch 127/2000, Step 8, d_loss: 0.38224393129348755, g_loss: 3.388040065765381\n","Epoch 127/2000, Step 9, d_loss: 0.3898886740207672, g_loss: 4.114904403686523\n","Epoch 127/2000, Step 10, d_loss: 0.39111751317977905, g_loss: 5.919847011566162\n","Epoch 127/2000, Step 11, d_loss: 0.3462614417076111, g_loss: 3.5473010540008545\n","Epoch 127/2000, Step 12, d_loss: 0.378230482339859, g_loss: 3.4249937534332275\n","Epoch 127/2000, Step 13, d_loss: 0.3679640591144562, g_loss: 5.491395950317383\n","Epoch 127/2000, Step 14, d_loss: 0.37496641278266907, g_loss: 6.006180763244629\n","Epoch 127/2000, Step 15, d_loss: 0.39214733242988586, g_loss: 4.928534030914307\n","Epoch 127/2000, Step 16, d_loss: 0.4024690091609955, g_loss: 6.718419075012207\n","Epoch 127/2000, Step 17, d_loss: 0.37736502289772034, g_loss: 4.020742893218994\n","Epoch 127/2000, Step 18, d_loss: 0.43607547879219055, g_loss: 4.803720951080322\n","Epoch 127/2000, Step 19, d_loss: 0.43566039204597473, g_loss: 3.738436460494995\n","Epoch 127/2000, Step 20, d_loss: 0.3806350529193878, g_loss: 2.8497838973999023\n","Epoch 127/2000, Step 21, d_loss: 0.4122965335845947, g_loss: 1.9075576066970825\n","Epoch 127/2000, Step 22, d_loss: 0.4171750247478485, g_loss: 4.1176557540893555\n","Epoch 127/2000, Step 23, d_loss: 0.3964425325393677, g_loss: 4.316643238067627\n","Epoch 127/2000, Step 24, d_loss: 0.35718002915382385, g_loss: 6.140636444091797\n","Epoch 127/2000, Step 25, d_loss: 0.3907059133052826, g_loss: 4.6665263175964355\n","Epoch 127/2000, Step 26, d_loss: 0.4176088571548462, g_loss: 4.710683345794678\n","Epoch 127/2000, Step 27, d_loss: 0.3467728793621063, g_loss: 5.587574005126953\n","Epoch 127/2000, Step 28, d_loss: 0.44613710045814514, g_loss: 2.3574471473693848\n","Epoch 127/2000, Step 29, d_loss: 0.4693397283554077, g_loss: 4.593132972717285\n","Epoch 127/2000, Step 30, d_loss: 0.3671219050884247, g_loss: 4.5497260093688965\n","Epoch 127/2000, Step 31, d_loss: 0.39006754755973816, g_loss: 2.215378999710083\n","Epoch 127/2000, Step 32, d_loss: 0.5542001724243164, g_loss: 2.522393226623535\n","Epoch 127/2000, Step 33, d_loss: 0.7247604131698608, g_loss: 2.2172818183898926\n","Epoch 127/2000, Step 34, d_loss: 0.383232057094574, g_loss: 3.4474427700042725\n","Epoch 127/2000, Step 35, d_loss: 0.3781886100769043, g_loss: 5.437889575958252\n","Epoch 127/2000, Step 36, d_loss: 0.652226984500885, g_loss: 7.847416400909424\n","Epoch 127/2000, Step 37, d_loss: 0.4388713836669922, g_loss: 4.736301422119141\n","Epoch 127/2000, Step 38, d_loss: 0.44701269268989563, g_loss: 5.228222846984863\n","Epoch 127/2000, Step 39, d_loss: 0.4707309901714325, g_loss: 5.753932476043701\n","Epoch 127/2000, Step 40, d_loss: 0.4410672187805176, g_loss: 5.340836048126221\n","Epoch 127/2000, Step 41, d_loss: 0.40449637174606323, g_loss: 3.1760263442993164\n","Epoch 127/2000, Step 42, d_loss: 0.38037922978401184, g_loss: 4.144829750061035\n","Epoch 127/2000, Step 43, d_loss: 0.3741375505924225, g_loss: 2.3419487476348877\n","Epoch 127/2000, Step 44, d_loss: 0.48538991808891296, g_loss: 1.7908332347869873\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 128/2000, Step 1, d_loss: 0.5148155093193054, g_loss: 3.396803379058838\n","Epoch 128/2000, Step 2, d_loss: 0.3917868137359619, g_loss: 2.715740203857422\n","Epoch 128/2000, Step 3, d_loss: 0.40115708112716675, g_loss: 2.051893949508667\n","Epoch 128/2000, Step 4, d_loss: 0.46926161646842957, g_loss: 4.409224033355713\n","Epoch 128/2000, Step 5, d_loss: 0.4102160334587097, g_loss: 5.29064416885376\n","Epoch 128/2000, Step 6, d_loss: 0.51104336977005, g_loss: 3.258462905883789\n","Epoch 128/2000, Step 7, d_loss: 0.4404304623603821, g_loss: 3.1360414028167725\n","Epoch 128/2000, Step 8, d_loss: 0.5783417820930481, g_loss: 1.8065696954727173\n","Epoch 128/2000, Step 9, d_loss: 1.0151317119598389, g_loss: 4.676746845245361\n","Epoch 128/2000, Step 10, d_loss: 0.3651576340198517, g_loss: 4.131845951080322\n","Epoch 128/2000, Step 11, d_loss: 0.7403324246406555, g_loss: 3.4982504844665527\n","Epoch 128/2000, Step 12, d_loss: 0.506659984588623, g_loss: 4.759529113769531\n","Epoch 128/2000, Step 13, d_loss: 0.36643368005752563, g_loss: 4.267570495605469\n","Epoch 128/2000, Step 14, d_loss: 0.48377957940101624, g_loss: 2.2259292602539062\n","Epoch 128/2000, Step 15, d_loss: 0.36999863386154175, g_loss: 4.0848236083984375\n","Epoch 128/2000, Step 16, d_loss: 0.4645117521286011, g_loss: 2.3169546127319336\n","Epoch 128/2000, Step 17, d_loss: 0.6780564188957214, g_loss: 3.766141653060913\n","Epoch 128/2000, Step 18, d_loss: 0.4529038667678833, g_loss: 3.9809436798095703\n","Epoch 128/2000, Step 19, d_loss: 0.4243725538253784, g_loss: 2.760263442993164\n","Epoch 128/2000, Step 20, d_loss: 0.43563663959503174, g_loss: 5.657715797424316\n","Epoch 128/2000, Step 21, d_loss: 0.42945873737335205, g_loss: 4.236294746398926\n","Epoch 128/2000, Step 22, d_loss: 0.43210113048553467, g_loss: 4.221244812011719\n","Epoch 128/2000, Step 23, d_loss: 0.43025413155555725, g_loss: 4.042667388916016\n","Epoch 128/2000, Step 24, d_loss: 0.43019717931747437, g_loss: 4.313717842102051\n","Epoch 128/2000, Step 25, d_loss: 0.3697580099105835, g_loss: 3.511432647705078\n","Epoch 128/2000, Step 26, d_loss: 0.5086315870285034, g_loss: 4.628152370452881\n","Epoch 128/2000, Step 27, d_loss: 0.48461732268333435, g_loss: 2.9236245155334473\n","Epoch 128/2000, Step 28, d_loss: 0.4028072655200958, g_loss: 2.9146835803985596\n","Epoch 128/2000, Step 29, d_loss: 0.4068521559238434, g_loss: 2.629878044128418\n","Epoch 128/2000, Step 30, d_loss: 0.3851175606250763, g_loss: 1.6733697652816772\n","Epoch 128/2000, Step 31, d_loss: 0.4958042502403259, g_loss: 3.4090065956115723\n","Epoch 128/2000, Step 32, d_loss: 0.44683679938316345, g_loss: 3.44122052192688\n","Epoch 128/2000, Step 33, d_loss: 0.40894997119903564, g_loss: 2.860227108001709\n","Epoch 128/2000, Step 34, d_loss: 0.37228450179100037, g_loss: 6.172348976135254\n","Epoch 128/2000, Step 35, d_loss: 0.5231272578239441, g_loss: 4.337619304656982\n","Epoch 128/2000, Step 36, d_loss: 0.39897361397743225, g_loss: 3.1991689205169678\n","Epoch 128/2000, Step 37, d_loss: 0.4376758635044098, g_loss: 5.03162145614624\n","Epoch 128/2000, Step 38, d_loss: 0.5122816562652588, g_loss: 3.9358339309692383\n","Epoch 128/2000, Step 39, d_loss: 0.41269034147262573, g_loss: 5.7732744216918945\n","Epoch 128/2000, Step 40, d_loss: 0.4882734417915344, g_loss: 3.7902677059173584\n","Epoch 128/2000, Step 41, d_loss: 0.4732598066329956, g_loss: 4.3025970458984375\n","Epoch 128/2000, Step 42, d_loss: 0.3755267858505249, g_loss: 5.973783493041992\n","Epoch 128/2000, Step 43, d_loss: 0.5823532342910767, g_loss: 4.331847667694092\n","Epoch 128/2000, Step 44, d_loss: 0.36021557450294495, g_loss: 3.68324875831604\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 129/2000, Step 1, d_loss: 0.4112379550933838, g_loss: 2.371307373046875\n","Epoch 129/2000, Step 2, d_loss: 0.6090649962425232, g_loss: 1.4431354999542236\n","Epoch 129/2000, Step 3, d_loss: 0.46538299322128296, g_loss: 4.753904819488525\n","Epoch 129/2000, Step 4, d_loss: 0.41443246603012085, g_loss: 4.54477071762085\n","Epoch 129/2000, Step 5, d_loss: 0.38230809569358826, g_loss: 6.591088771820068\n","Epoch 129/2000, Step 6, d_loss: 0.47205817699432373, g_loss: 6.959474086761475\n","Epoch 129/2000, Step 7, d_loss: 0.3805790841579437, g_loss: 6.780100345611572\n","Epoch 129/2000, Step 8, d_loss: 0.41838738322257996, g_loss: 4.799128532409668\n","Epoch 129/2000, Step 9, d_loss: 0.4348409175872803, g_loss: 1.9918290376663208\n","Epoch 129/2000, Step 10, d_loss: 0.4293426275253296, g_loss: 4.396797180175781\n","Epoch 129/2000, Step 11, d_loss: 0.4349859654903412, g_loss: 4.3026885986328125\n","Epoch 129/2000, Step 12, d_loss: 0.3662990629673004, g_loss: 3.1375646591186523\n","Epoch 129/2000, Step 13, d_loss: 0.5962810516357422, g_loss: 4.745774745941162\n","Epoch 129/2000, Step 14, d_loss: 0.5490128397941589, g_loss: 4.451544284820557\n","Epoch 129/2000, Step 15, d_loss: 0.3904116153717041, g_loss: 3.684587001800537\n","Epoch 129/2000, Step 16, d_loss: 0.4165096879005432, g_loss: 4.257308006286621\n","Epoch 129/2000, Step 17, d_loss: 0.5319868326187134, g_loss: 1.9097040891647339\n","Epoch 129/2000, Step 18, d_loss: 0.3843456506729126, g_loss: 5.560685157775879\n","Epoch 129/2000, Step 19, d_loss: 0.40657418966293335, g_loss: 3.9249513149261475\n","Epoch 129/2000, Step 20, d_loss: 0.4661170244216919, g_loss: 2.707029104232788\n","Epoch 129/2000, Step 21, d_loss: 0.4207766652107239, g_loss: 2.9046831130981445\n","Epoch 129/2000, Step 22, d_loss: 0.3514522314071655, g_loss: 3.4818689823150635\n","Epoch 129/2000, Step 23, d_loss: 0.595455527305603, g_loss: 4.257126331329346\n","Epoch 129/2000, Step 24, d_loss: 0.41400817036628723, g_loss: 3.6348953247070312\n","Epoch 129/2000, Step 25, d_loss: 0.3882041275501251, g_loss: 5.107824802398682\n","Epoch 129/2000, Step 26, d_loss: 0.38021522760391235, g_loss: 6.974316120147705\n","Epoch 129/2000, Step 27, d_loss: 0.41744792461395264, g_loss: 5.216980934143066\n","Epoch 129/2000, Step 28, d_loss: 0.4372595548629761, g_loss: 5.3581342697143555\n","Epoch 129/2000, Step 29, d_loss: 0.37190937995910645, g_loss: 3.5312225818634033\n","Epoch 129/2000, Step 30, d_loss: 0.3782047927379608, g_loss: 3.540466070175171\n","Epoch 129/2000, Step 31, d_loss: 0.4008983373641968, g_loss: 2.5690629482269287\n","Epoch 129/2000, Step 32, d_loss: 0.47346705198287964, g_loss: 3.4151949882507324\n","Epoch 129/2000, Step 33, d_loss: 0.4361191689968109, g_loss: 5.728743553161621\n","Epoch 129/2000, Step 34, d_loss: 0.3732425570487976, g_loss: 5.691120147705078\n","Epoch 129/2000, Step 35, d_loss: 0.3985961675643921, g_loss: 5.063526630401611\n","Epoch 129/2000, Step 36, d_loss: 0.43551304936408997, g_loss: 4.349969863891602\n","Epoch 129/2000, Step 37, d_loss: 0.44690555334091187, g_loss: 6.83121919631958\n","Epoch 129/2000, Step 38, d_loss: 0.3851892054080963, g_loss: 4.771203994750977\n","Epoch 129/2000, Step 39, d_loss: 0.44557899236679077, g_loss: 1.6600079536437988\n","Epoch 129/2000, Step 40, d_loss: 0.5006102323532104, g_loss: 1.4944077730178833\n","Epoch 129/2000, Step 41, d_loss: 0.4784313440322876, g_loss: 2.308370351791382\n","Epoch 129/2000, Step 42, d_loss: 0.40766429901123047, g_loss: 5.171921730041504\n","Epoch 129/2000, Step 43, d_loss: 0.41118744015693665, g_loss: 5.336009502410889\n","Epoch 129/2000, Step 44, d_loss: 0.48560672998428345, g_loss: 5.933224678039551\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 130/2000, Step 1, d_loss: 0.3945469260215759, g_loss: 4.862173080444336\n","Epoch 130/2000, Step 2, d_loss: 0.4112744629383087, g_loss: 5.090388298034668\n","Epoch 130/2000, Step 3, d_loss: 0.3732161223888397, g_loss: 6.257174491882324\n","Epoch 130/2000, Step 4, d_loss: 0.5108206272125244, g_loss: 4.929629802703857\n","Epoch 130/2000, Step 5, d_loss: 0.4056445062160492, g_loss: 4.860046863555908\n","Epoch 130/2000, Step 6, d_loss: 0.42229923605918884, g_loss: 5.073025703430176\n","Epoch 130/2000, Step 7, d_loss: 0.4183778464794159, g_loss: 4.642714023590088\n","Epoch 130/2000, Step 8, d_loss: 0.3813471794128418, g_loss: 3.6604397296905518\n","Epoch 130/2000, Step 9, d_loss: 0.3701515793800354, g_loss: 5.216999053955078\n","Epoch 130/2000, Step 10, d_loss: 0.3956853449344635, g_loss: 3.9981698989868164\n","Epoch 130/2000, Step 11, d_loss: 0.3817938566207886, g_loss: 4.690706729888916\n","Epoch 130/2000, Step 12, d_loss: 0.38676440715789795, g_loss: 3.1657986640930176\n","Epoch 130/2000, Step 13, d_loss: 0.38437968492507935, g_loss: 2.3193116188049316\n","Epoch 130/2000, Step 14, d_loss: 0.44056859612464905, g_loss: 3.16583251953125\n","Epoch 130/2000, Step 15, d_loss: 0.5161385536193848, g_loss: 4.73386812210083\n","Epoch 130/2000, Step 16, d_loss: 0.45651775598526, g_loss: 4.034370422363281\n","Epoch 130/2000, Step 17, d_loss: 0.5551442503929138, g_loss: 5.469188213348389\n","Epoch 130/2000, Step 18, d_loss: 0.37174633145332336, g_loss: 3.8660295009613037\n","Epoch 130/2000, Step 19, d_loss: 0.3579765558242798, g_loss: 4.941429138183594\n","Epoch 130/2000, Step 20, d_loss: 0.37097135186195374, g_loss: 5.4572319984436035\n","Epoch 130/2000, Step 21, d_loss: 0.3881376087665558, g_loss: 4.394758224487305\n","Epoch 130/2000, Step 22, d_loss: 0.4034350514411926, g_loss: 2.9622304439544678\n","Epoch 130/2000, Step 23, d_loss: 0.3594321310520172, g_loss: 4.151321887969971\n","Epoch 130/2000, Step 24, d_loss: 0.3912132680416107, g_loss: 2.0385608673095703\n","Epoch 130/2000, Step 25, d_loss: 0.3851146996021271, g_loss: 3.66738224029541\n","Epoch 130/2000, Step 26, d_loss: 0.46595385670661926, g_loss: 3.904391288757324\n","Epoch 130/2000, Step 27, d_loss: 0.4540047347545624, g_loss: 5.835476875305176\n","Epoch 130/2000, Step 28, d_loss: 0.3721723258495331, g_loss: 4.169968128204346\n","Epoch 130/2000, Step 29, d_loss: 0.3605477213859558, g_loss: 5.40770959854126\n","Epoch 130/2000, Step 30, d_loss: 0.3714492917060852, g_loss: 5.523662090301514\n","Epoch 130/2000, Step 31, d_loss: 0.39228197932243347, g_loss: 6.342875003814697\n","Epoch 130/2000, Step 32, d_loss: 0.37568458914756775, g_loss: 5.9910888671875\n","Epoch 130/2000, Step 33, d_loss: 0.3792702853679657, g_loss: 5.328058242797852\n","Epoch 130/2000, Step 34, d_loss: 0.5015247464179993, g_loss: 5.403409004211426\n","Epoch 130/2000, Step 35, d_loss: 0.3522277772426605, g_loss: 5.038597583770752\n","Epoch 130/2000, Step 36, d_loss: 0.3609602749347687, g_loss: 4.618828296661377\n","Epoch 130/2000, Step 37, d_loss: 0.5045053958892822, g_loss: 5.656067371368408\n","Epoch 130/2000, Step 38, d_loss: 0.3879988491535187, g_loss: 4.381462574005127\n","Epoch 130/2000, Step 39, d_loss: 0.37591317296028137, g_loss: 4.220378875732422\n","Epoch 130/2000, Step 40, d_loss: 0.37684738636016846, g_loss: 5.468088150024414\n","Epoch 130/2000, Step 41, d_loss: 0.4029308557510376, g_loss: 2.421832799911499\n","Epoch 130/2000, Step 42, d_loss: 0.3630937933921814, g_loss: 2.9225077629089355\n","Epoch 130/2000, Step 43, d_loss: 0.4319392740726471, g_loss: 4.079161167144775\n","Epoch 130/2000, Step 44, d_loss: 0.3990989327430725, g_loss: 7.4470744132995605\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 131/2000, Step 1, d_loss: 0.35668912529945374, g_loss: 4.59633207321167\n","Epoch 131/2000, Step 2, d_loss: 0.3979584574699402, g_loss: 5.236566543579102\n","Epoch 131/2000, Step 3, d_loss: 0.43617260456085205, g_loss: 6.077804088592529\n","Epoch 131/2000, Step 4, d_loss: 0.36023133993148804, g_loss: 4.848850250244141\n","Epoch 131/2000, Step 5, d_loss: 0.5933113098144531, g_loss: 6.336455345153809\n","Epoch 131/2000, Step 6, d_loss: 0.4196856915950775, g_loss: 5.382230758666992\n","Epoch 131/2000, Step 7, d_loss: 0.41717448830604553, g_loss: 4.676357746124268\n","Epoch 131/2000, Step 8, d_loss: 0.36478230357170105, g_loss: 5.499322414398193\n","Epoch 131/2000, Step 9, d_loss: 0.423120379447937, g_loss: 4.312549114227295\n","Epoch 131/2000, Step 10, d_loss: 0.375609427690506, g_loss: 5.637532711029053\n","Epoch 131/2000, Step 11, d_loss: 0.3823722004890442, g_loss: 5.359879493713379\n","Epoch 131/2000, Step 12, d_loss: 0.4038504362106323, g_loss: 4.470644474029541\n","Epoch 131/2000, Step 13, d_loss: 0.3581790328025818, g_loss: 5.097522258758545\n","Epoch 131/2000, Step 14, d_loss: 0.4331034719944, g_loss: 6.4555768966674805\n","Epoch 131/2000, Step 15, d_loss: 0.34867826104164124, g_loss: 6.343936920166016\n","Epoch 131/2000, Step 16, d_loss: 0.35268470644950867, g_loss: 3.358025312423706\n","Epoch 131/2000, Step 17, d_loss: 0.36584270000457764, g_loss: 4.026489734649658\n","Epoch 131/2000, Step 18, d_loss: 0.39787209033966064, g_loss: 4.782839298248291\n","Epoch 131/2000, Step 19, d_loss: 0.37341541051864624, g_loss: 4.355219841003418\n","Epoch 131/2000, Step 20, d_loss: 0.401003897190094, g_loss: 5.674053192138672\n","Epoch 131/2000, Step 21, d_loss: 0.7121822834014893, g_loss: 4.763530731201172\n","Epoch 131/2000, Step 22, d_loss: 0.36777031421661377, g_loss: 4.069344997406006\n","Epoch 131/2000, Step 23, d_loss: 0.3917860686779022, g_loss: 3.50610089302063\n","Epoch 131/2000, Step 24, d_loss: 0.3759028911590576, g_loss: 3.794008731842041\n","Epoch 131/2000, Step 25, d_loss: 0.39084720611572266, g_loss: 3.044830799102783\n","Epoch 131/2000, Step 26, d_loss: 0.5989514589309692, g_loss: 3.2981536388397217\n","Epoch 131/2000, Step 27, d_loss: 0.46845126152038574, g_loss: 3.8089611530303955\n","Epoch 131/2000, Step 28, d_loss: 0.7883887887001038, g_loss: 5.479568958282471\n","Epoch 131/2000, Step 29, d_loss: 0.413686603307724, g_loss: 5.382415771484375\n","Epoch 131/2000, Step 30, d_loss: 0.592034101486206, g_loss: 6.050657272338867\n","Epoch 131/2000, Step 31, d_loss: 0.36408066749572754, g_loss: 4.671457290649414\n","Epoch 131/2000, Step 32, d_loss: 0.7104688882827759, g_loss: 3.1706864833831787\n","Epoch 131/2000, Step 33, d_loss: 0.43002447485923767, g_loss: 3.5910117626190186\n","Epoch 131/2000, Step 34, d_loss: 0.36931121349334717, g_loss: 2.209080696105957\n","Epoch 131/2000, Step 35, d_loss: 0.38240933418273926, g_loss: 2.604565382003784\n","Epoch 131/2000, Step 36, d_loss: 0.4179776608943939, g_loss: 2.741266965866089\n","Epoch 131/2000, Step 37, d_loss: 0.7311816215515137, g_loss: 2.5778772830963135\n","Epoch 131/2000, Step 38, d_loss: 0.39940330386161804, g_loss: 5.636312961578369\n","Epoch 131/2000, Step 39, d_loss: 0.4064440131187439, g_loss: 4.326549530029297\n","Epoch 131/2000, Step 40, d_loss: 0.40944811701774597, g_loss: 5.230220794677734\n","Epoch 131/2000, Step 41, d_loss: 0.44365984201431274, g_loss: 5.583701133728027\n","Epoch 131/2000, Step 42, d_loss: 0.5030903816223145, g_loss: 4.372270107269287\n","Epoch 131/2000, Step 43, d_loss: 0.37214887142181396, g_loss: 4.840305328369141\n","Epoch 131/2000, Step 44, d_loss: 0.4053981602191925, g_loss: 3.6551685333251953\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 132/2000, Step 1, d_loss: 0.4657549262046814, g_loss: 2.130316734313965\n","Epoch 132/2000, Step 2, d_loss: 0.4758259057998657, g_loss: 3.5735092163085938\n","Epoch 132/2000, Step 3, d_loss: 0.48438069224357605, g_loss: 2.7748165130615234\n","Epoch 132/2000, Step 4, d_loss: 0.4880383610725403, g_loss: 3.002983808517456\n","Epoch 132/2000, Step 5, d_loss: 0.5343011617660522, g_loss: 3.8879973888397217\n","Epoch 132/2000, Step 6, d_loss: 0.5199813842773438, g_loss: 3.6807494163513184\n","Epoch 132/2000, Step 7, d_loss: 0.8179572820663452, g_loss: 5.0021891593933105\n","Epoch 132/2000, Step 8, d_loss: 0.44432926177978516, g_loss: 4.917360782623291\n","Epoch 132/2000, Step 9, d_loss: 0.5741878747940063, g_loss: 2.6309025287628174\n","Epoch 132/2000, Step 10, d_loss: 0.5457543730735779, g_loss: 2.717724323272705\n","Epoch 132/2000, Step 11, d_loss: 0.47305530309677124, g_loss: 2.6378753185272217\n","Epoch 132/2000, Step 12, d_loss: 0.49565422534942627, g_loss: 4.051239490509033\n","Epoch 132/2000, Step 13, d_loss: 0.46569815278053284, g_loss: 3.1903581619262695\n","Epoch 132/2000, Step 14, d_loss: 0.41094258427619934, g_loss: 4.691198348999023\n","Epoch 132/2000, Step 15, d_loss: 0.44013136625289917, g_loss: 6.881542682647705\n","Epoch 132/2000, Step 16, d_loss: 0.47958722710609436, g_loss: 3.041250705718994\n","Epoch 132/2000, Step 17, d_loss: 0.402996689081192, g_loss: 5.205147743225098\n","Epoch 132/2000, Step 18, d_loss: 0.36580151319503784, g_loss: 4.062648773193359\n","Epoch 132/2000, Step 19, d_loss: 0.620478093624115, g_loss: 2.3869736194610596\n","Epoch 132/2000, Step 20, d_loss: 0.45206713676452637, g_loss: 3.6155340671539307\n","Epoch 132/2000, Step 21, d_loss: 0.43808281421661377, g_loss: 1.4816519021987915\n","Epoch 132/2000, Step 22, d_loss: 0.448954313993454, g_loss: 4.673668384552002\n","Epoch 132/2000, Step 23, d_loss: 0.3685947060585022, g_loss: 3.0849015712738037\n","Epoch 132/2000, Step 24, d_loss: 0.3911408483982086, g_loss: 4.403956890106201\n","Epoch 132/2000, Step 25, d_loss: 0.3674181401729584, g_loss: 3.159329891204834\n","Epoch 132/2000, Step 26, d_loss: 0.3790959119796753, g_loss: 5.416876316070557\n","Epoch 132/2000, Step 27, d_loss: 0.4161743223667145, g_loss: 2.6444480419158936\n","Epoch 132/2000, Step 28, d_loss: 0.3791545629501343, g_loss: 3.8863303661346436\n","Epoch 132/2000, Step 29, d_loss: 0.39902520179748535, g_loss: 2.658348321914673\n","Epoch 132/2000, Step 30, d_loss: 0.41347983479499817, g_loss: 4.213342189788818\n","Epoch 132/2000, Step 31, d_loss: 0.3974398970603943, g_loss: 3.841595411300659\n","Epoch 132/2000, Step 32, d_loss: 0.4398137331008911, g_loss: 2.648057460784912\n","Epoch 132/2000, Step 33, d_loss: 0.3577146828174591, g_loss: 3.9968111515045166\n","Epoch 132/2000, Step 34, d_loss: 0.4127577245235443, g_loss: 6.664083003997803\n","Epoch 132/2000, Step 35, d_loss: 0.4326184093952179, g_loss: 5.251269817352295\n","Epoch 132/2000, Step 36, d_loss: 0.4243766963481903, g_loss: 3.256808042526245\n","Epoch 132/2000, Step 37, d_loss: 0.3736186623573303, g_loss: 4.37022066116333\n","Epoch 132/2000, Step 38, d_loss: 0.4439736008644104, g_loss: 3.1321170330047607\n","Epoch 132/2000, Step 39, d_loss: 0.4736774265766144, g_loss: 2.937422752380371\n","Epoch 132/2000, Step 40, d_loss: 0.4686703383922577, g_loss: 3.6594347953796387\n","Epoch 132/2000, Step 41, d_loss: 0.39247336983680725, g_loss: 4.493260383605957\n","Epoch 132/2000, Step 42, d_loss: 0.3825196325778961, g_loss: 5.791287899017334\n","Epoch 132/2000, Step 43, d_loss: 0.4176214039325714, g_loss: 4.152490615844727\n","Epoch 132/2000, Step 44, d_loss: 0.3747537434101105, g_loss: 2.7072598934173584\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 133/2000, Step 1, d_loss: 0.3658856451511383, g_loss: 3.705983877182007\n","Epoch 133/2000, Step 2, d_loss: 0.38921019434928894, g_loss: 5.082279205322266\n","Epoch 133/2000, Step 3, d_loss: 0.37745627760887146, g_loss: 4.732170581817627\n","Epoch 133/2000, Step 4, d_loss: 0.44552138447761536, g_loss: 2.11562442779541\n","Epoch 133/2000, Step 5, d_loss: 0.37180566787719727, g_loss: 3.5847582817077637\n","Epoch 133/2000, Step 6, d_loss: 0.40150874853134155, g_loss: 5.177799701690674\n","Epoch 133/2000, Step 7, d_loss: 0.4038136303424835, g_loss: 2.8912479877471924\n","Epoch 133/2000, Step 8, d_loss: 0.42527368664741516, g_loss: 3.2566065788269043\n","Epoch 133/2000, Step 9, d_loss: 0.4317721426486969, g_loss: 4.217587471008301\n","Epoch 133/2000, Step 10, d_loss: 0.44074171781539917, g_loss: 5.967339515686035\n","Epoch 133/2000, Step 11, d_loss: 0.3797478973865509, g_loss: 4.112563133239746\n","Epoch 133/2000, Step 12, d_loss: 0.44610509276390076, g_loss: 4.274575233459473\n","Epoch 133/2000, Step 13, d_loss: 0.3740021884441376, g_loss: 4.8298516273498535\n","Epoch 133/2000, Step 14, d_loss: 0.39639919996261597, g_loss: 5.908989429473877\n","Epoch 133/2000, Step 15, d_loss: 0.39379945397377014, g_loss: 3.2287795543670654\n","Epoch 133/2000, Step 16, d_loss: 0.49279147386550903, g_loss: 3.866584300994873\n","Epoch 133/2000, Step 17, d_loss: 0.37545493245124817, g_loss: 4.058837413787842\n","Epoch 133/2000, Step 18, d_loss: 0.3549416661262512, g_loss: 6.073032855987549\n","Epoch 133/2000, Step 19, d_loss: 0.5046190023422241, g_loss: 4.1002092361450195\n","Epoch 133/2000, Step 20, d_loss: 0.36620083451271057, g_loss: 3.2716498374938965\n","Epoch 133/2000, Step 21, d_loss: 0.37005218863487244, g_loss: 5.83448600769043\n","Epoch 133/2000, Step 22, d_loss: 0.3774081766605377, g_loss: 3.551522731781006\n","Epoch 133/2000, Step 23, d_loss: 0.3876381814479828, g_loss: 4.1611552238464355\n","Epoch 133/2000, Step 24, d_loss: 0.37871110439300537, g_loss: 5.127204418182373\n","Epoch 133/2000, Step 25, d_loss: 0.34741440415382385, g_loss: 4.012280464172363\n","Epoch 133/2000, Step 26, d_loss: 0.4008306562900543, g_loss: 4.203927040100098\n","Epoch 133/2000, Step 27, d_loss: 0.3992448151111603, g_loss: 4.1813836097717285\n","Epoch 133/2000, Step 28, d_loss: 0.3615351915359497, g_loss: 6.409976482391357\n","Epoch 133/2000, Step 29, d_loss: 0.41006046533584595, g_loss: 8.789607048034668\n","Epoch 133/2000, Step 30, d_loss: 0.35956552624702454, g_loss: 4.732761859893799\n","Epoch 133/2000, Step 31, d_loss: 0.3507721424102783, g_loss: 4.38035249710083\n","Epoch 133/2000, Step 32, d_loss: 0.4026646018028259, g_loss: 4.302183151245117\n","Epoch 133/2000, Step 33, d_loss: 0.3621441423892975, g_loss: 5.275786876678467\n","Epoch 133/2000, Step 34, d_loss: 0.38725027441978455, g_loss: 4.108479022979736\n","Epoch 133/2000, Step 35, d_loss: 0.3664292097091675, g_loss: 3.632901668548584\n","Epoch 133/2000, Step 36, d_loss: 0.3798946738243103, g_loss: 3.9203829765319824\n","Epoch 133/2000, Step 37, d_loss: 0.43130743503570557, g_loss: 5.864070415496826\n","Epoch 133/2000, Step 38, d_loss: 0.36952340602874756, g_loss: 4.614011764526367\n","Epoch 133/2000, Step 39, d_loss: 0.3894810378551483, g_loss: 5.784124374389648\n","Epoch 133/2000, Step 40, d_loss: 0.3523733615875244, g_loss: 4.248331546783447\n","Epoch 133/2000, Step 41, d_loss: 0.6533273458480835, g_loss: 1.836649775505066\n","Epoch 133/2000, Step 42, d_loss: 0.41629987955093384, g_loss: 2.374772548675537\n","Epoch 133/2000, Step 43, d_loss: 0.4109746813774109, g_loss: 3.4894800186157227\n","Epoch 133/2000, Step 44, d_loss: 0.5019855499267578, g_loss: 2.9853196144104004\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 134/2000, Step 1, d_loss: 0.4528636038303375, g_loss: 3.1315860748291016\n","Epoch 134/2000, Step 2, d_loss: 0.48028773069381714, g_loss: 3.721341133117676\n","Epoch 134/2000, Step 3, d_loss: 0.49546849727630615, g_loss: 4.4745988845825195\n","Epoch 134/2000, Step 4, d_loss: 0.37307652831077576, g_loss: 5.323606491088867\n","Epoch 134/2000, Step 5, d_loss: 0.42444396018981934, g_loss: 3.716644763946533\n","Epoch 134/2000, Step 6, d_loss: 0.3832317590713501, g_loss: 6.594109058380127\n","Epoch 134/2000, Step 7, d_loss: 0.3884911835193634, g_loss: 5.983612537384033\n","Epoch 134/2000, Step 8, d_loss: 0.43718278408050537, g_loss: 3.5970633029937744\n","Epoch 134/2000, Step 9, d_loss: 0.37171047925949097, g_loss: 3.6283671855926514\n","Epoch 134/2000, Step 10, d_loss: 0.4211887717247009, g_loss: 4.574058532714844\n","Epoch 134/2000, Step 11, d_loss: 0.3648183047771454, g_loss: 3.939807176589966\n","Epoch 134/2000, Step 12, d_loss: 0.39156943559646606, g_loss: 2.9328105449676514\n","Epoch 134/2000, Step 13, d_loss: 0.42495617270469666, g_loss: 3.8017351627349854\n","Epoch 134/2000, Step 14, d_loss: 0.3978985548019409, g_loss: 4.075173854827881\n","Epoch 134/2000, Step 15, d_loss: 0.3561214208602905, g_loss: 3.3477861881256104\n","Epoch 134/2000, Step 16, d_loss: 0.3647097051143646, g_loss: 4.514134883880615\n","Epoch 134/2000, Step 17, d_loss: 0.37209343910217285, g_loss: 3.7798097133636475\n","Epoch 134/2000, Step 18, d_loss: 0.4162152111530304, g_loss: 4.093034267425537\n","Epoch 134/2000, Step 19, d_loss: 0.37185201048851013, g_loss: 4.070048809051514\n","Epoch 134/2000, Step 20, d_loss: 0.37463414669036865, g_loss: 4.644253253936768\n","Epoch 134/2000, Step 21, d_loss: 0.3617109954357147, g_loss: 4.407805919647217\n","Epoch 134/2000, Step 22, d_loss: 0.5318381786346436, g_loss: 4.068179607391357\n","Epoch 134/2000, Step 23, d_loss: 0.42104512453079224, g_loss: 5.439830303192139\n","Epoch 134/2000, Step 24, d_loss: 0.41329580545425415, g_loss: 4.367987632751465\n","Epoch 134/2000, Step 25, d_loss: 0.38886088132858276, g_loss: 3.9791548252105713\n","Epoch 134/2000, Step 26, d_loss: 0.4139881730079651, g_loss: 5.905426979064941\n","Epoch 134/2000, Step 27, d_loss: 0.38785648345947266, g_loss: 3.164153575897217\n","Epoch 134/2000, Step 28, d_loss: 0.384168803691864, g_loss: 4.882339954376221\n","Epoch 134/2000, Step 29, d_loss: 0.41531744599342346, g_loss: 2.609168767929077\n","Epoch 134/2000, Step 30, d_loss: 0.36867430806159973, g_loss: 5.96162223815918\n","Epoch 134/2000, Step 31, d_loss: 0.35647523403167725, g_loss: 5.925461769104004\n","Epoch 134/2000, Step 32, d_loss: 0.3726017475128174, g_loss: 6.043478012084961\n","Epoch 134/2000, Step 33, d_loss: 0.4037093222141266, g_loss: 6.324033737182617\n","Epoch 134/2000, Step 34, d_loss: 0.46553748846054077, g_loss: 2.3929924964904785\n","Epoch 134/2000, Step 35, d_loss: 0.37099531292915344, g_loss: 4.114912509918213\n","Epoch 134/2000, Step 36, d_loss: 0.40148723125457764, g_loss: 7.68018102645874\n","Epoch 134/2000, Step 37, d_loss: 0.40861085057258606, g_loss: 5.523882865905762\n","Epoch 134/2000, Step 38, d_loss: 0.3932873606681824, g_loss: 5.632230281829834\n","Epoch 134/2000, Step 39, d_loss: 0.3795190155506134, g_loss: 3.695110559463501\n","Epoch 134/2000, Step 40, d_loss: 0.3763453960418701, g_loss: 7.590253829956055\n","Epoch 134/2000, Step 41, d_loss: 0.35671764612197876, g_loss: 6.225996971130371\n","Epoch 134/2000, Step 42, d_loss: 0.3555375039577484, g_loss: 4.10263729095459\n","Epoch 134/2000, Step 43, d_loss: 0.3498379588127136, g_loss: 6.1257781982421875\n","Epoch 134/2000, Step 44, d_loss: 0.38321787118911743, g_loss: 5.847005367279053\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 135/2000, Step 1, d_loss: 0.35118386149406433, g_loss: 6.152736186981201\n","Epoch 135/2000, Step 2, d_loss: 0.3682654798030853, g_loss: 5.676891803741455\n","Epoch 135/2000, Step 3, d_loss: 0.37666502594947815, g_loss: 4.302395820617676\n","Epoch 135/2000, Step 4, d_loss: 0.3750064969062805, g_loss: 3.763033151626587\n","Epoch 135/2000, Step 5, d_loss: 0.3562648594379425, g_loss: 3.917219638824463\n","Epoch 135/2000, Step 6, d_loss: 0.3550926744937897, g_loss: 4.91554594039917\n","Epoch 135/2000, Step 7, d_loss: 0.38531142473220825, g_loss: 2.791459560394287\n","Epoch 135/2000, Step 8, d_loss: 0.38914111256599426, g_loss: 2.715834856033325\n","Epoch 135/2000, Step 9, d_loss: 0.43358850479125977, g_loss: 3.8034815788269043\n","Epoch 135/2000, Step 10, d_loss: 0.3843676447868347, g_loss: 1.840439796447754\n","Epoch 135/2000, Step 11, d_loss: 0.36984214186668396, g_loss: 3.8719913959503174\n","Epoch 135/2000, Step 12, d_loss: 0.4007296860218048, g_loss: 3.4174978733062744\n","Epoch 135/2000, Step 13, d_loss: 0.3595164120197296, g_loss: 3.3628695011138916\n","Epoch 135/2000, Step 14, d_loss: 0.4063871204853058, g_loss: 3.7330057621002197\n","Epoch 135/2000, Step 15, d_loss: 0.3629680275917053, g_loss: 2.5766425132751465\n","Epoch 135/2000, Step 16, d_loss: 0.3501257002353668, g_loss: 4.285170078277588\n","Epoch 135/2000, Step 17, d_loss: 0.3640930652618408, g_loss: 2.784273147583008\n","Epoch 135/2000, Step 18, d_loss: 0.37424176931381226, g_loss: 5.716741561889648\n","Epoch 135/2000, Step 19, d_loss: 0.3712256848812103, g_loss: 3.8874332904815674\n","Epoch 135/2000, Step 20, d_loss: 0.3843507766723633, g_loss: 3.7797281742095947\n","Epoch 135/2000, Step 21, d_loss: 0.3634813725948334, g_loss: 3.246229887008667\n","Epoch 135/2000, Step 22, d_loss: 0.4661770164966583, g_loss: 2.725905418395996\n","Epoch 135/2000, Step 23, d_loss: 0.37520885467529297, g_loss: 3.748969316482544\n","Epoch 135/2000, Step 24, d_loss: 0.4200619161128998, g_loss: 3.571587324142456\n","Epoch 135/2000, Step 25, d_loss: 0.5394135117530823, g_loss: 3.914151668548584\n","Epoch 135/2000, Step 26, d_loss: 0.41369715332984924, g_loss: 4.43167781829834\n","Epoch 135/2000, Step 27, d_loss: 0.3656083643436432, g_loss: 3.3643460273742676\n","Epoch 135/2000, Step 28, d_loss: 0.4027620851993561, g_loss: 6.708646297454834\n","Epoch 135/2000, Step 29, d_loss: 0.34810805320739746, g_loss: 5.09382963180542\n","Epoch 135/2000, Step 30, d_loss: 0.3889044523239136, g_loss: 4.544124603271484\n","Epoch 135/2000, Step 31, d_loss: 0.4011164903640747, g_loss: 4.3216872215271\n","Epoch 135/2000, Step 32, d_loss: 0.6232163310050964, g_loss: 6.158154010772705\n","Epoch 135/2000, Step 33, d_loss: 0.34248557686805725, g_loss: 4.868969440460205\n","Epoch 135/2000, Step 34, d_loss: 0.399042010307312, g_loss: 3.0442488193511963\n","Epoch 135/2000, Step 35, d_loss: 0.41320642828941345, g_loss: 2.6815121173858643\n","Epoch 135/2000, Step 36, d_loss: 0.4636163115501404, g_loss: 4.555177211761475\n","Epoch 135/2000, Step 37, d_loss: 0.37357914447784424, g_loss: 4.450002670288086\n","Epoch 135/2000, Step 38, d_loss: 0.39266178011894226, g_loss: 4.362267971038818\n","Epoch 135/2000, Step 39, d_loss: 0.4941098093986511, g_loss: 5.714053153991699\n","Epoch 135/2000, Step 40, d_loss: 0.38058897852897644, g_loss: 6.726149559020996\n","Epoch 135/2000, Step 41, d_loss: 0.37196704745292664, g_loss: 6.464084625244141\n","Epoch 135/2000, Step 42, d_loss: 0.38199180364608765, g_loss: 5.039440155029297\n","Epoch 135/2000, Step 43, d_loss: 0.35357317328453064, g_loss: 6.178755760192871\n","Epoch 135/2000, Step 44, d_loss: 0.6869784593582153, g_loss: 5.152846336364746\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 136/2000, Step 1, d_loss: 0.4576290547847748, g_loss: 4.411905288696289\n","Epoch 136/2000, Step 2, d_loss: 0.39468878507614136, g_loss: 2.9268178939819336\n","Epoch 136/2000, Step 3, d_loss: 0.4642612934112549, g_loss: 2.4461703300476074\n","Epoch 136/2000, Step 4, d_loss: 0.6433851718902588, g_loss: 2.430983066558838\n","Epoch 136/2000, Step 5, d_loss: 0.4817783534526825, g_loss: 4.502377033233643\n","Epoch 136/2000, Step 6, d_loss: 0.40349531173706055, g_loss: 4.447876930236816\n","Epoch 136/2000, Step 7, d_loss: 0.39760497212409973, g_loss: 5.058133602142334\n","Epoch 136/2000, Step 8, d_loss: 0.3730736970901489, g_loss: 6.718625068664551\n","Epoch 136/2000, Step 9, d_loss: 0.47558271884918213, g_loss: 6.597930431365967\n","Epoch 136/2000, Step 10, d_loss: 0.42243966460227966, g_loss: 6.174460411071777\n","Epoch 136/2000, Step 11, d_loss: 0.525725245475769, g_loss: 3.903388261795044\n","Epoch 136/2000, Step 12, d_loss: 0.42453011870384216, g_loss: 3.905885934829712\n","Epoch 136/2000, Step 13, d_loss: 0.3689073920249939, g_loss: 5.211513996124268\n","Epoch 136/2000, Step 14, d_loss: 0.4753113090991974, g_loss: 4.485448837280273\n","Epoch 136/2000, Step 15, d_loss: 0.36061418056488037, g_loss: 5.718634605407715\n","Epoch 136/2000, Step 16, d_loss: 0.351030558347702, g_loss: 6.284737586975098\n","Epoch 136/2000, Step 17, d_loss: 0.37283405661582947, g_loss: 3.207087755203247\n","Epoch 136/2000, Step 18, d_loss: 0.4106476902961731, g_loss: 2.622479200363159\n","Epoch 136/2000, Step 19, d_loss: 0.4585409164428711, g_loss: 3.987274169921875\n","Epoch 136/2000, Step 20, d_loss: 0.35659685730934143, g_loss: 5.175719261169434\n","Epoch 136/2000, Step 21, d_loss: 0.4279669523239136, g_loss: 3.9290921688079834\n","Epoch 136/2000, Step 22, d_loss: 0.42756927013397217, g_loss: 4.709188938140869\n","Epoch 136/2000, Step 23, d_loss: 0.3644696772098541, g_loss: 5.902484893798828\n","Epoch 136/2000, Step 24, d_loss: 0.5147154927253723, g_loss: 6.244283199310303\n","Epoch 136/2000, Step 25, d_loss: 0.36562907695770264, g_loss: 4.513886451721191\n","Epoch 136/2000, Step 26, d_loss: 0.38901835680007935, g_loss: 4.555599212646484\n","Epoch 136/2000, Step 27, d_loss: 0.35442090034484863, g_loss: 7.596298694610596\n","Epoch 136/2000, Step 28, d_loss: 0.37466421723365784, g_loss: 4.295593738555908\n","Epoch 136/2000, Step 29, d_loss: 0.37379711866378784, g_loss: 4.071338653564453\n","Epoch 136/2000, Step 30, d_loss: 0.40904319286346436, g_loss: 5.929617881774902\n","Epoch 136/2000, Step 31, d_loss: 0.5399968028068542, g_loss: 2.2246875762939453\n","Epoch 136/2000, Step 32, d_loss: 0.3863999843597412, g_loss: 2.4206643104553223\n","Epoch 136/2000, Step 33, d_loss: 0.4302106499671936, g_loss: 2.7219440937042236\n","Epoch 136/2000, Step 34, d_loss: 0.4818597733974457, g_loss: 7.239836692810059\n","Epoch 136/2000, Step 35, d_loss: 0.40824565291404724, g_loss: 6.676605701446533\n","Epoch 136/2000, Step 36, d_loss: 0.3916233479976654, g_loss: 5.911617279052734\n","Epoch 136/2000, Step 37, d_loss: 0.36305445432662964, g_loss: 3.943007707595825\n","Epoch 136/2000, Step 38, d_loss: 0.3694283664226532, g_loss: 5.24184513092041\n","Epoch 136/2000, Step 39, d_loss: 0.3721016049385071, g_loss: 4.269193649291992\n","Epoch 136/2000, Step 40, d_loss: 0.4432497024536133, g_loss: 4.097873687744141\n","Epoch 136/2000, Step 41, d_loss: 0.3921741247177124, g_loss: 3.505044937133789\n","Epoch 136/2000, Step 42, d_loss: 0.5014065504074097, g_loss: 3.1090073585510254\n","Epoch 136/2000, Step 43, d_loss: 0.4635877311229706, g_loss: 4.150930404663086\n","Epoch 136/2000, Step 44, d_loss: 0.5286815166473389, g_loss: 3.284259080886841\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 137/2000, Step 1, d_loss: 0.41414499282836914, g_loss: 4.3543500900268555\n","Epoch 137/2000, Step 2, d_loss: 0.5824682712554932, g_loss: 7.00382661819458\n","Epoch 137/2000, Step 3, d_loss: 0.4024566411972046, g_loss: 5.267291069030762\n","Epoch 137/2000, Step 4, d_loss: 0.43827444314956665, g_loss: 6.644613265991211\n","Epoch 137/2000, Step 5, d_loss: 0.6271220445632935, g_loss: 6.847431659698486\n","Epoch 137/2000, Step 6, d_loss: 0.41433820128440857, g_loss: 3.5628762245178223\n","Epoch 137/2000, Step 7, d_loss: 0.3801526427268982, g_loss: 4.017828941345215\n","Epoch 137/2000, Step 8, d_loss: 0.3855150043964386, g_loss: 5.406632423400879\n","Epoch 137/2000, Step 9, d_loss: 0.467536062002182, g_loss: 4.474152565002441\n","Epoch 137/2000, Step 10, d_loss: 0.3925788998603821, g_loss: 4.096534252166748\n","Epoch 137/2000, Step 11, d_loss: 0.3643098473548889, g_loss: 2.8183393478393555\n","Epoch 137/2000, Step 12, d_loss: 0.3816598951816559, g_loss: 5.038127899169922\n","Epoch 137/2000, Step 13, d_loss: 0.38173913955688477, g_loss: 4.575860977172852\n","Epoch 137/2000, Step 14, d_loss: 0.37158605456352234, g_loss: 6.26146125793457\n","Epoch 137/2000, Step 15, d_loss: 0.39984166622161865, g_loss: 5.4700822830200195\n","Epoch 137/2000, Step 16, d_loss: 0.3627360761165619, g_loss: 4.808477878570557\n","Epoch 137/2000, Step 17, d_loss: 0.3450871706008911, g_loss: 5.533697605133057\n","Epoch 137/2000, Step 18, d_loss: 0.3660745322704315, g_loss: 5.182232856750488\n","Epoch 137/2000, Step 19, d_loss: 0.37132728099823, g_loss: 6.209971904754639\n","Epoch 137/2000, Step 20, d_loss: 0.5186296105384827, g_loss: 4.105831146240234\n","Epoch 137/2000, Step 21, d_loss: 0.3759920299053192, g_loss: 4.300708293914795\n","Epoch 137/2000, Step 22, d_loss: 0.36756467819213867, g_loss: 4.324612140655518\n","Epoch 137/2000, Step 23, d_loss: 0.3759942352771759, g_loss: 5.090933799743652\n","Epoch 137/2000, Step 24, d_loss: 0.38352006673812866, g_loss: 3.5954904556274414\n","Epoch 137/2000, Step 25, d_loss: 0.384925901889801, g_loss: 6.804594039916992\n","Epoch 137/2000, Step 26, d_loss: 0.3999926447868347, g_loss: 4.899170875549316\n","Epoch 137/2000, Step 27, d_loss: 0.37262216210365295, g_loss: 5.051340103149414\n","Epoch 137/2000, Step 28, d_loss: 0.38869667053222656, g_loss: 2.866036891937256\n","Epoch 137/2000, Step 29, d_loss: 0.4143162965774536, g_loss: 4.8388285636901855\n","Epoch 137/2000, Step 30, d_loss: 0.40449801087379456, g_loss: 4.856371879577637\n","Epoch 137/2000, Step 31, d_loss: 0.4165092408657074, g_loss: 4.529628753662109\n","Epoch 137/2000, Step 32, d_loss: 0.36240795254707336, g_loss: 4.727054119110107\n","Epoch 137/2000, Step 33, d_loss: 0.3849952220916748, g_loss: 4.147284030914307\n","Epoch 137/2000, Step 34, d_loss: 0.36733278632164, g_loss: 5.303005218505859\n","Epoch 137/2000, Step 35, d_loss: 0.36730140447616577, g_loss: 5.396300792694092\n","Epoch 137/2000, Step 36, d_loss: 0.3729676306247711, g_loss: 6.019282817840576\n","Epoch 137/2000, Step 37, d_loss: 0.459198921918869, g_loss: 5.51829195022583\n","Epoch 137/2000, Step 38, d_loss: 0.35985469818115234, g_loss: 4.92987060546875\n","Epoch 137/2000, Step 39, d_loss: 0.3674108684062958, g_loss: 5.468092441558838\n","Epoch 137/2000, Step 40, d_loss: 0.3783871531486511, g_loss: 4.6509785652160645\n","Epoch 137/2000, Step 41, d_loss: 0.3892851173877716, g_loss: 5.1140618324279785\n","Epoch 137/2000, Step 42, d_loss: 0.3891448378562927, g_loss: 3.875678539276123\n","Epoch 137/2000, Step 43, d_loss: 0.352289080619812, g_loss: 5.5642313957214355\n","Epoch 137/2000, Step 44, d_loss: 0.4037299156188965, g_loss: 4.004437446594238\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print (device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 64\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 32\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 120\n","\n","# Size of feature maps in discriminator\n","ndf = 280\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='G:/My Drive/Colab Notebooks/art77/Abstract_gallery/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/ag2/',\n","                              transform=transforms.Compose([\n","                              transforms.Resize(image_size),\n","                              transforms.CenterCrop(image_size),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                              ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # input is ``(nc) x 64 x 64``\n","            nn.Conv2d(3, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.2),\n","            nn.Conv2d(ndf , ndf * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","\n","            nn.Conv2d(ndf *4, ndf * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 2, 1, 3, 1, 0, bias=False),\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input).squeeze()\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0019)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='checkpointC.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","            save_image(samples, f'G:/My Drive/Colab Notebooks/dandies/new_dandies/ArcwrideC_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","                current_batch_size = images.size(0)\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((current_batch_size,), 0.9))\n","\n","\n","\n","\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","\n","                d_loss_real = self.loss(real_outputs.squeeze(0), real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(current_batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images).view(-1).squeeze()\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointC.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(2000, dataloader, resume=True)"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7TZvDVQTtN6","executionInfo":{"status":"ok","timestamp":1690605787603,"user_tz":240,"elapsed":19,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"}},"outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1GN2L9UPYR6TCzDsqMBaHwwc6PkLlmsyg","authorship_tag":"ABX9TyNsg+yM5zQKcqOju8PzXFkl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}