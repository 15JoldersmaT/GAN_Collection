{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"48523c66-74d4-41e6-b1cd-96342917f3da"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","test\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 1/2000, Step 1, d_loss: 1.401515245437622, g_loss: 1.6915977001190186\n","Epoch 1/2000, Step 2, d_loss: 1.8128535747528076, g_loss: 1.145408272743225\n","Epoch 1/2000, Step 3, d_loss: 1.438502550125122, g_loss: 0.6117903590202332\n","Epoch 1/2000, Step 4, d_loss: 1.301833152770996, g_loss: 0.4910530149936676\n","Epoch 1/2000, Step 5, d_loss: 1.3385894298553467, g_loss: 0.5866345763206482\n","Epoch 1/2000, Step 6, d_loss: 1.216804027557373, g_loss: 0.7269443869590759\n","Epoch 1/2000, Step 7, d_loss: 1.1115347146987915, g_loss: 0.9349708557128906\n","Epoch 1/2000, Step 8, d_loss: 1.005630612373352, g_loss: 1.122554063796997\n","Epoch 1/2000, Step 9, d_loss: 0.9789180755615234, g_loss: 1.240742564201355\n","Epoch 1/2000, Step 10, d_loss: 0.9566237330436707, g_loss: 1.2992559671401978\n","Epoch 1/2000, Step 11, d_loss: 0.887370228767395, g_loss: 1.281833529472351\n","Epoch 1/2000, Step 12, d_loss: 0.7726877331733704, g_loss: 1.2602052688598633\n","Epoch 1/2000, Step 13, d_loss: 0.7131521701812744, g_loss: 1.2287875413894653\n","Epoch 1/2000, Step 14, d_loss: 0.6374852657318115, g_loss: 1.2760872840881348\n","Epoch 1/2000, Step 15, d_loss: 0.5804896354675293, g_loss: 1.3112714290618896\n","Epoch 1/2000, Step 16, d_loss: 0.5084697604179382, g_loss: 1.4071964025497437\n","Epoch 1/2000, Step 17, d_loss: 0.4824661612510681, g_loss: 1.5905508995056152\n","Epoch 1/2000, Step 18, d_loss: 0.3834924101829529, g_loss: 1.7510462999343872\n","Epoch 1/2000, Step 19, d_loss: 0.3382798433303833, g_loss: 1.8691582679748535\n","Epoch 1/2000, Step 20, d_loss: 0.30353808403015137, g_loss: 1.9792650938034058\n","Epoch 1/2000, Step 21, d_loss: 0.2566000521183014, g_loss: 2.154539108276367\n","Epoch 1/2000, Step 22, d_loss: 0.23496857285499573, g_loss: 2.3152146339416504\n","Epoch 1/2000, Step 23, d_loss: 0.19792324304580688, g_loss: 2.4128165245056152\n","Epoch 1/2000, Step 24, d_loss: 0.24618013203144073, g_loss: 2.3657283782958984\n","Epoch 1/2000, Step 25, d_loss: 0.18148766458034515, g_loss: 2.4679102897644043\n","Epoch 1/2000, Step 26, d_loss: 0.20932307839393616, g_loss: 2.6665353775024414\n","Epoch 1/2000, Step 27, d_loss: 0.1262732744216919, g_loss: 2.8071682453155518\n","Epoch 1/2000, Step 28, d_loss: 0.12726454436779022, g_loss: 2.7704999446868896\n","Epoch 1/2000, Step 29, d_loss: 0.13579148054122925, g_loss: 2.8227930068969727\n","Epoch 1/2000, Step 30, d_loss: 0.10653668642044067, g_loss: 3.028653144836426\n","Epoch 1/2000, Step 31, d_loss: 0.08364701271057129, g_loss: 3.30637264251709\n","Epoch 1/2000, Step 32, d_loss: 0.07157294452190399, g_loss: 3.409977436065674\n","Epoch 1/2000, Step 33, d_loss: 0.07321441173553467, g_loss: 3.4500818252563477\n","Epoch 1/2000, Step 34, d_loss: 0.06681512296199799, g_loss: 3.391040325164795\n","Epoch 1/2000, Step 35, d_loss: 0.061099667102098465, g_loss: 3.4318418502807617\n","Epoch 1/2000, Step 36, d_loss: 0.051364071667194366, g_loss: 3.5381920337677\n","Epoch 1/2000, Step 37, d_loss: 0.050655439496040344, g_loss: 3.732041597366333\n","Epoch 1/2000, Step 38, d_loss: 0.040648553520441055, g_loss: 3.9122064113616943\n","Epoch 1/2000, Step 39, d_loss: 0.042439281940460205, g_loss: 4.098468780517578\n","Epoch 1/2000, Step 40, d_loss: 0.03130700811743736, g_loss: 4.166532039642334\n","Epoch 1/2000, Step 41, d_loss: 0.02905956469476223, g_loss: 4.256789207458496\n","Epoch 1/2000, Step 42, d_loss: 0.032839976251125336, g_loss: 4.215201377868652\n","Epoch 1/2000, Step 43, d_loss: 0.027857376262545586, g_loss: 4.229183673858643\n","Epoch 1/2000, Step 44, d_loss: 0.025112994015216827, g_loss: 4.391648769378662\n","Epoch 1/2000, Step 45, d_loss: 0.021531712263822556, g_loss: 4.533875465393066\n","Epoch 1/2000, Step 46, d_loss: 0.019133880734443665, g_loss: 4.6670732498168945\n","Epoch 1/2000, Step 47, d_loss: 0.018359892070293427, g_loss: 4.750590801239014\n","Epoch 1/2000, Step 48, d_loss: 0.017240967601537704, g_loss: 4.842645168304443\n","Epoch 1/2000, Step 49, d_loss: 0.015869222581386566, g_loss: 4.891781806945801\n","Epoch 1/2000, Step 50, d_loss: 0.013674549758434296, g_loss: 4.898109436035156\n","Epoch 1/2000, Step 51, d_loss: 0.013130455277860165, g_loss: 4.938623428344727\n","Epoch 1/2000, Step 52, d_loss: 0.013086214661598206, g_loss: 4.895022392272949\n","Epoch 1/2000, Step 53, d_loss: 0.011622466146945953, g_loss: 4.966472625732422\n","Epoch 1/2000, Step 54, d_loss: 0.012305976822972298, g_loss: 4.925661087036133\n","Epoch 1/2000, Step 55, d_loss: 0.010449828580021858, g_loss: 5.031860828399658\n","Epoch 1/2000, Step 56, d_loss: 0.013143683783710003, g_loss: 5.074826240539551\n","Epoch 1/2000, Step 57, d_loss: 0.012310151010751724, g_loss: 5.190897464752197\n","Epoch 1/2000, Step 58, d_loss: 0.009856447577476501, g_loss: 5.199239730834961\n","Epoch 1/2000, Step 59, d_loss: 0.010374238714575768, g_loss: 5.2869553565979\n","Epoch 1/2000, Step 60, d_loss: 0.012441529892385006, g_loss: 5.331451416015625\n","Epoch 1/2000, Step 61, d_loss: 0.01047961600124836, g_loss: 5.336444854736328\n","Epoch 1/2000, Step 62, d_loss: 0.009418518282473087, g_loss: 5.394067287445068\n","Epoch 1/2000, Step 63, d_loss: 0.008810648694634438, g_loss: 5.35598087310791\n","Epoch 1/2000, Step 64, d_loss: 0.007829100824892521, g_loss: 5.362850189208984\n","Epoch 1/2000, Step 65, d_loss: 0.010461113415658474, g_loss: 5.302161693572998\n","Epoch 1/2000, Step 66, d_loss: 0.008017715066671371, g_loss: 5.323265552520752\n","Epoch 1/2000, Step 67, d_loss: 0.008117537014186382, g_loss: 5.3626275062561035\n","Epoch 1/2000, Step 68, d_loss: 0.00820300355553627, g_loss: 5.390268802642822\n","Epoch 1/2000, Step 69, d_loss: 0.007149864453822374, g_loss: 5.455250263214111\n","Epoch 1/2000, Step 70, d_loss: 0.00640813447535038, g_loss: 5.4370293617248535\n","Epoch 1/2000, Step 71, d_loss: 0.006545627489686012, g_loss: 5.46366548538208\n","Epoch 1/2000, Step 72, d_loss: 0.006750626023858786, g_loss: 5.498712062835693\n","Epoch 1/2000, Step 73, d_loss: 0.007238060235977173, g_loss: 5.422729015350342\n","Epoch 1/2000, Step 74, d_loss: 0.006444512400776148, g_loss: 5.440450668334961\n","Epoch 1/2000, Step 75, d_loss: 0.006947798654437065, g_loss: 5.4990315437316895\n","Epoch 1/2000, Step 76, d_loss: 0.006778965704143047, g_loss: 5.584787368774414\n","Epoch 1/2000, Step 77, d_loss: 0.006005839444696903, g_loss: 5.552524566650391\n","Epoch 1/2000, Step 78, d_loss: 0.007970881648361683, g_loss: 5.650694847106934\n","Epoch 1/2000, Step 79, d_loss: 0.005819832906126976, g_loss: 5.627673625946045\n","Epoch 1/2000, Step 80, d_loss: 0.005277888849377632, g_loss: 5.600428581237793\n","Epoch 1/2000, Step 81, d_loss: 0.005236024037003517, g_loss: 5.645044803619385\n","Epoch 1/2000, Step 82, d_loss: 0.005440413020551205, g_loss: 5.766770362854004\n","Epoch 1/2000, Step 83, d_loss: 0.005639873910695314, g_loss: 5.786182880401611\n","Epoch 1/2000, Step 84, d_loss: 0.004810959566384554, g_loss: 5.78485107421875\n","Epoch 1/2000, Step 85, d_loss: 0.005172571167349815, g_loss: 5.774557113647461\n","Epoch 1/2000, Step 86, d_loss: 0.005485247820615768, g_loss: 5.847756385803223\n","Epoch 1/2000, Step 87, d_loss: 0.004814762622117996, g_loss: 5.891277313232422\n","Epoch 1/2000, Step 88, d_loss: 0.004424164071679115, g_loss: 5.933127403259277\n","Epoch 1/2000, Step 89, d_loss: 0.004585695452988148, g_loss: 5.887810707092285\n","Epoch 1/2000, Step 90, d_loss: 0.004650329705327749, g_loss: 5.922851085662842\n","Epoch 1/2000, Step 91, d_loss: 0.003935499116778374, g_loss: 5.950394153594971\n","Epoch 1/2000, Step 92, d_loss: 0.00462083937600255, g_loss: 5.940224647521973\n","Epoch 1/2000, Step 93, d_loss: 0.00390840508043766, g_loss: 5.998863697052002\n","Epoch 1/2000, Step 94, d_loss: 0.004207703284919262, g_loss: 5.959105968475342\n","Epoch 1/2000, Step 95, d_loss: 0.0038774607237428427, g_loss: 6.043531894683838\n","Epoch 1/2000, Step 96, d_loss: 0.003591804997995496, g_loss: 6.059791088104248\n","Epoch 1/2000, Step 97, d_loss: 0.006868466269224882, g_loss: 6.050380229949951\n","Epoch 1/2000, Step 98, d_loss: 0.0038635460659861565, g_loss: 6.0319037437438965\n","Epoch 1/2000, Step 99, d_loss: 0.004196717403829098, g_loss: 6.0288286209106445\n","Epoch 1/2000, Step 100, d_loss: 0.004600073676556349, g_loss: 6.091733932495117\n","Epoch 1/2000, Step 101, d_loss: 0.0037115311715751886, g_loss: 6.121592044830322\n","Epoch 1/2000, Step 102, d_loss: 0.004335151985287666, g_loss: 6.05387020111084\n","Epoch 1/2000, Step 103, d_loss: 0.004012571647763252, g_loss: 6.184182167053223\n","Epoch 1/2000, Step 104, d_loss: 0.003606682876124978, g_loss: 6.172252178192139\n","Epoch 1/2000, Step 105, d_loss: 0.003889400977641344, g_loss: 6.100859642028809\n","Epoch 1/2000, Step 106, d_loss: 0.0034077090676873922, g_loss: 6.171336650848389\n","Epoch 1/2000, Step 107, d_loss: 0.004031971096992493, g_loss: 6.2087178230285645\n","Epoch 1/2000, Step 108, d_loss: 0.0032615421805530787, g_loss: 6.192737102508545\n","Epoch 1/2000, Step 109, d_loss: 0.0038210744969546795, g_loss: 6.221779823303223\n","Epoch 1/2000, Step 110, d_loss: 0.003337766043841839, g_loss: 6.1619343757629395\n","Epoch 1/2000, Step 111, d_loss: 0.003098304383456707, g_loss: 6.215110778808594\n","Epoch 1/2000, Step 112, d_loss: 0.0031546675600111485, g_loss: 6.183167457580566\n","Epoch 1/2000, Step 113, d_loss: 0.003872858826071024, g_loss: 6.240596771240234\n","Epoch 1/2000, Step 114, d_loss: 0.0035088129807263613, g_loss: 6.238814353942871\n","Epoch 1/2000, Step 115, d_loss: 0.003540417179465294, g_loss: 6.2223896980285645\n","Epoch 1/2000, Step 116, d_loss: 0.003467405680567026, g_loss: 6.187544345855713\n","Epoch 1/2000, Step 117, d_loss: 0.0034148762933909893, g_loss: 6.210866928100586\n","Epoch 1/2000, Step 118, d_loss: 0.003127606585621834, g_loss: 6.278650760650635\n","Epoch 1/2000, Step 119, d_loss: 0.0031817308627068996, g_loss: 6.3116254806518555\n","Epoch 1/2000, Step 120, d_loss: 0.0030689567793160677, g_loss: 6.337767601013184\n","Epoch 1/2000, Step 121, d_loss: 0.00418337807059288, g_loss: 6.309216499328613\n","Epoch 1/2000, Step 122, d_loss: 0.0033855242654681206, g_loss: 6.339955806732178\n","Epoch 1/2000, Step 123, d_loss: 0.0034394427202641964, g_loss: 6.336380958557129\n","Epoch 1/2000, Step 124, d_loss: 0.003396560437977314, g_loss: 6.385385513305664\n","Epoch 1/2000, Step 125, d_loss: 0.002621237887069583, g_loss: 6.368647575378418\n","Epoch 1/2000, Step 126, d_loss: 0.0030856525991111994, g_loss: 6.359074592590332\n","Epoch 1/2000, Step 127, d_loss: 0.0030651777051389217, g_loss: 6.386884689331055\n","Epoch 1/2000, Step 128, d_loss: 0.0030389935709536076, g_loss: 6.431478500366211\n","Epoch 1/2000, Step 129, d_loss: 0.003220081329345703, g_loss: 6.480512619018555\n","Epoch 1/2000, Step 130, d_loss: 0.00278287916444242, g_loss: 6.487398624420166\n","Epoch 1/2000, Step 131, d_loss: 0.003234362229704857, g_loss: 6.35228967666626\n","Epoch 1/2000, Step 132, d_loss: 0.0032638413831591606, g_loss: 6.21763277053833\n","Epoch 1/2000, Step 133, d_loss: 0.0035485452972352505, g_loss: 6.079254627227783\n","Epoch 1/2000, Step 134, d_loss: 0.003103130031377077, g_loss: 6.117834568023682\n","Epoch 1/2000, Step 135, d_loss: 0.003222882281988859, g_loss: 6.122659683227539\n","Epoch 1/2000, Step 136, d_loss: 0.004187354352325201, g_loss: 6.170917987823486\n","Epoch 1/2000, Step 137, d_loss: 0.0032095536589622498, g_loss: 6.136086940765381\n","Epoch 1/2000, Step 138, d_loss: 0.003148897783830762, g_loss: 6.197889804840088\n","Epoch 1/2000, Step 139, d_loss: 0.0032953117042779922, g_loss: 6.224679946899414\n","Epoch 1/2000, Step 140, d_loss: 0.004896129015833139, g_loss: 6.230065822601318\n","Epoch 1/2000, Step 141, d_loss: 0.0031138211488723755, g_loss: 6.277582168579102\n","Epoch 1/2000, Step 142, d_loss: 0.002959820441901684, g_loss: 6.207835674285889\n","Epoch 1/2000, Step 143, d_loss: 0.0030304258689284325, g_loss: 6.3018798828125\n","Epoch 1/2000, Step 144, d_loss: 0.0032999259419739246, g_loss: 6.2456955909729\n","Epoch 1/2000, Step 145, d_loss: 0.0025782203301787376, g_loss: 6.351235866546631\n","Epoch 1/2000, Step 146, d_loss: 0.0026145102456212044, g_loss: 6.371317386627197\n","Epoch 1/2000, Step 147, d_loss: 0.0028435285203158855, g_loss: 6.3923211097717285\n","Epoch 1/2000, Step 148, d_loss: 0.0027494095265865326, g_loss: 6.374831199645996\n","Epoch 1/2000, Step 149, d_loss: 0.0026427623815834522, g_loss: 6.407894611358643\n","Epoch 1/2000, Step 150, d_loss: 0.0030755801126360893, g_loss: 6.400550365447998\n","Epoch 1/2000, Step 151, d_loss: 0.002574686426669359, g_loss: 6.44801664352417\n","Epoch 1/2000, Step 152, d_loss: 0.0027260538190603256, g_loss: 6.512513637542725\n","Epoch 1/2000, Step 153, d_loss: 0.0032073184847831726, g_loss: 6.482418060302734\n","Epoch 1/2000, Step 154, d_loss: 0.0035433864686638117, g_loss: 6.544440269470215\n","Epoch 1/2000, Step 155, d_loss: 0.0032928981818258762, g_loss: 6.321496486663818\n","Epoch 1/2000, Step 156, d_loss: 0.0027988916262984276, g_loss: 6.350334167480469\n","Epoch 1/2000, Step 157, d_loss: 0.0025669902097433805, g_loss: 6.442537307739258\n","Epoch 1/2000, Step 158, d_loss: 0.0024655433371663094, g_loss: 6.512203216552734\n","Epoch 1/2000, Step 159, d_loss: 0.0027251234278082848, g_loss: 6.512387752532959\n","Epoch 1/2000, Step 160, d_loss: 0.00282471114769578, g_loss: 6.494004726409912\n","Epoch 1/2000, Step 161, d_loss: 0.003609488718211651, g_loss: 6.5541205406188965\n","Epoch 1/2000, Step 162, d_loss: 0.002978416858240962, g_loss: 6.614803314208984\n","Epoch 1/2000, Step 163, d_loss: 0.0022111034486442804, g_loss: 6.547287464141846\n","Epoch 1/2000, Step 164, d_loss: 0.0022123984526842833, g_loss: 6.599267482757568\n","Epoch 1/2000, Step 165, d_loss: 0.0022199847735464573, g_loss: 6.617416858673096\n","Epoch 1/2000, Step 166, d_loss: 0.002349118236452341, g_loss: 6.616222381591797\n","Epoch 1/2000, Step 167, d_loss: 0.0023425209801644087, g_loss: 6.708615303039551\n","Epoch 1/2000, Step 168, d_loss: 0.00199528387747705, g_loss: 6.678789138793945\n","Epoch 1/2000, Step 169, d_loss: 0.0018629770493134856, g_loss: 6.7143754959106445\n","Epoch 1/2000, Step 170, d_loss: 0.0025352337397634983, g_loss: 6.658773422241211\n","Epoch 1/2000, Step 171, d_loss: 0.0024484908208251, g_loss: 6.647961616516113\n","Epoch 1/2000, Step 172, d_loss: 0.002137233270332217, g_loss: 6.617899417877197\n","Epoch 1/2000, Step 173, d_loss: 0.0019983970560133457, g_loss: 6.602209091186523\n","Epoch 1/2000, Step 174, d_loss: 0.00213578250259161, g_loss: 6.550275802612305\n","Epoch 1/2000, Step 175, d_loss: 0.0026577715761959553, g_loss: 6.236032009124756\n","Epoch 1/2000, Step 176, d_loss: 0.0029021152295172215, g_loss: 6.32584285736084\n","Epoch 1/2000, Step 177, d_loss: 0.002902666572481394, g_loss: 6.378951072692871\n","Epoch 1/2000, Step 178, d_loss: 0.002323389519006014, g_loss: 6.437193870544434\n","Epoch 1/2000, Step 179, d_loss: 0.0027052906807512045, g_loss: 6.429053783416748\n","Epoch 1/2000, Step 180, d_loss: 0.0023403712548315525, g_loss: 6.3847784996032715\n","Epoch 1/2000, Step 181, d_loss: 0.0024739615619182587, g_loss: 6.403025150299072\n","Epoch 1/2000, Step 182, d_loss: 0.0025786664336919785, g_loss: 6.411668300628662\n","Epoch 1/2000, Step 183, d_loss: 0.002285087015479803, g_loss: 6.504260540008545\n","Epoch 1/2000, Step 184, d_loss: 0.0022143744863569736, g_loss: 6.491453647613525\n","Epoch 1/2000, Step 185, d_loss: 0.0025372072122991085, g_loss: 6.517390251159668\n","Epoch 1/2000, Step 186, d_loss: 0.002258848864585161, g_loss: 6.496037483215332\n","Epoch 1/2000, Step 187, d_loss: 0.0023730522952973843, g_loss: 6.512579917907715\n","Epoch 1/2000, Step 188, d_loss: 0.002052436815574765, g_loss: 6.527285575866699\n","Epoch 1/2000, Step 189, d_loss: 0.002294509904459119, g_loss: 6.721696376800537\n","Epoch 1/2000, Step 190, d_loss: 0.0020997929386794567, g_loss: 6.679912567138672\n","Epoch 1/2000, Step 191, d_loss: 0.002081307116895914, g_loss: 6.69098424911499\n","Epoch 1/2000, Step 192, d_loss: 0.002774675376713276, g_loss: 6.675267219543457\n","Epoch 1/2000, Step 193, d_loss: 0.0020549220498651266, g_loss: 6.729348659515381\n","Epoch 1/2000, Step 194, d_loss: 0.001865361467935145, g_loss: 6.778285980224609\n","Epoch 1/2000, Step 195, d_loss: 0.001677997293882072, g_loss: 6.849013328552246\n","Epoch 1/2000, Step 196, d_loss: 0.0016943477094173431, g_loss: 6.908135414123535\n","Epoch 1/2000, Step 197, d_loss: 0.0015229502459987998, g_loss: 6.924848556518555\n","Epoch 1/2000, Step 198, d_loss: 0.0016929991543293, g_loss: 6.900783538818359\n","Epoch 1/2000, Step 199, d_loss: 0.0016207315493375063, g_loss: 6.948921203613281\n","Epoch 1/2000, Step 200, d_loss: 0.002341135870665312, g_loss: 6.955996990203857\n","Epoch 1/2000, Step 201, d_loss: 0.0016093002632260323, g_loss: 6.961607933044434\n","Epoch 1/2000, Step 202, d_loss: 0.0017565031303092837, g_loss: 6.900679588317871\n","Epoch 1/2000, Step 203, d_loss: 0.0016831991961225867, g_loss: 6.852353096008301\n","Epoch 1/2000, Step 204, d_loss: 0.0019966030959039927, g_loss: 6.89326286315918\n","Epoch 1/2000, Step 205, d_loss: 0.001997757703065872, g_loss: 6.870831489562988\n","Epoch 1/2000, Step 206, d_loss: 0.0016824257327243686, g_loss: 6.964994430541992\n","Epoch 1/2000, Step 207, d_loss: 0.0018157127778977156, g_loss: 6.985558032989502\n","Epoch 1/2000, Step 208, d_loss: 0.0017582045402377844, g_loss: 7.016635894775391\n","Epoch 1/2000, Step 209, d_loss: 0.001510479487478733, g_loss: 7.0273118019104\n","Epoch 1/2000, Step 210, d_loss: 0.001494091935455799, g_loss: 7.078706741333008\n","Epoch 1/2000, Step 211, d_loss: 0.0016390755772590637, g_loss: 7.130836486816406\n","Epoch 1/2000, Step 212, d_loss: 0.001593703287653625, g_loss: 7.156582832336426\n","Epoch 1/2000, Step 213, d_loss: 0.0014314164873212576, g_loss: 7.1557936668396\n","Epoch 1/2000, Step 214, d_loss: 0.0012953567784279585, g_loss: 7.135115146636963\n","Epoch 1/2000, Step 215, d_loss: 0.0015666624531149864, g_loss: 7.055115222930908\n","Epoch 1/2000, Step 216, d_loss: 0.0017470305319875479, g_loss: 6.877269268035889\n","Epoch 1/2000, Step 217, d_loss: 0.0018980230670422316, g_loss: 6.680449485778809\n","Epoch 1/2000, Step 218, d_loss: 0.001971741206943989, g_loss: 6.804810047149658\n","Epoch 1/2000, Step 219, d_loss: 0.0021859861444681883, g_loss: 6.848004341125488\n","Epoch 1/2000, Step 220, d_loss: 0.0019316867692396045, g_loss: 6.909041404724121\n","Epoch 1/2000, Step 221, d_loss: 0.0015364749124273658, g_loss: 7.030084609985352\n","Epoch 1/2000, Step 222, d_loss: 0.001714634709060192, g_loss: 7.044654369354248\n","Epoch 1/2000, Step 223, d_loss: 0.0015983833000063896, g_loss: 7.032896518707275\n","Epoch 1/2000, Step 224, d_loss: 0.0014176409458741546, g_loss: 7.028213024139404\n","Epoch 1/2000, Step 225, d_loss: 0.0016141351079568267, g_loss: 7.032358646392822\n","Epoch 1/2000, Step 226, d_loss: 0.001705844304524362, g_loss: 7.01880407333374\n","Epoch 1/2000, Step 227, d_loss: 0.0025216657668352127, g_loss: 6.9225335121154785\n","Epoch 1/2000, Step 228, d_loss: 0.0016096180770546198, g_loss: 6.7956953048706055\n","Epoch 1/2000, Step 229, d_loss: 0.0016201179241761565, g_loss: 6.830114364624023\n","Epoch 1/2000, Step 230, d_loss: 0.0018522351747378707, g_loss: 6.868305206298828\n","Epoch 1/2000, Step 231, d_loss: 0.0018089821096509695, g_loss: 6.911882400512695\n","Epoch 1/2000, Step 232, d_loss: 0.0019680773839354515, g_loss: 6.893180847167969\n","Epoch 1/2000, Step 233, d_loss: 0.0016800223384052515, g_loss: 6.955022811889648\n","Epoch 1/2000, Step 234, d_loss: 0.0015005044406279922, g_loss: 6.900104999542236\n","Epoch 1/2000, Step 235, d_loss: 0.001564903650432825, g_loss: 7.040696144104004\n","Epoch 1/2000, Step 236, d_loss: 0.0016932615544646978, g_loss: 7.155290126800537\n","Epoch 1/2000, Step 237, d_loss: 0.0018390000332146883, g_loss: 7.221410274505615\n","Epoch 1/2000, Step 238, d_loss: 0.001436089864000678, g_loss: 7.2063493728637695\n","Epoch 1/2000, Step 239, d_loss: 0.0012309907469898462, g_loss: 7.308979511260986\n","Epoch 1/2000, Step 240, d_loss: 0.0015562096377834678, g_loss: 7.320031642913818\n","Epoch 1/2000, Step 241, d_loss: 0.0011810739524662495, g_loss: 7.327268600463867\n","Epoch 1/2000, Step 242, d_loss: 0.0012032995000481606, g_loss: 7.318044662475586\n","Epoch 1/2000, Step 243, d_loss: 0.001008820254355669, g_loss: 7.397472381591797\n","Epoch 1/2000, Step 244, d_loss: 0.0012952412944287062, g_loss: 7.38872766494751\n","Epoch 1/2000, Step 245, d_loss: 0.001085276948288083, g_loss: 7.37141227722168\n","Epoch 1/2000, Step 246, d_loss: 0.0011475778883323073, g_loss: 7.336756229400635\n","Epoch 1/2000, Step 247, d_loss: 0.001077931490726769, g_loss: 7.383167743682861\n","Epoch 1/2000, Step 248, d_loss: 0.0010362828616052866, g_loss: 7.456939220428467\n","Epoch 1/2000, Step 249, d_loss: 0.0011234099511057138, g_loss: 7.478392124176025\n","Epoch 1/2000, Step 250, d_loss: 0.0010059862397611141, g_loss: 7.379877090454102\n","Epoch 1/2000, Step 251, d_loss: 0.0010009331163018942, g_loss: 7.367764472961426\n","Epoch 1/2000, Step 252, d_loss: 0.001178980222903192, g_loss: 7.432956218719482\n","Epoch 1/2000, Step 253, d_loss: 0.0011846874840557575, g_loss: 7.397927761077881\n","Epoch 1/2000, Step 254, d_loss: 0.0009290969464927912, g_loss: 7.3845415115356445\n","Epoch 1/2000, Step 255, d_loss: 0.0010173949413001537, g_loss: 7.489849090576172\n","Epoch 1/2000, Step 256, d_loss: 0.0010829035891219974, g_loss: 7.397604465484619\n","Epoch 1/2000, Step 257, d_loss: 0.0009815918747335672, g_loss: 7.404677867889404\n","Epoch 1/2000, Step 258, d_loss: 0.0010081629734486341, g_loss: 7.522276401519775\n","Epoch 1/2000, Step 259, d_loss: 0.0009625324746593833, g_loss: 7.505879878997803\n","Epoch 1/2000, Step 260, d_loss: 0.001172567019239068, g_loss: 7.497191905975342\n","Epoch 1/2000, Step 261, d_loss: 0.0010003835195675492, g_loss: 7.600434303283691\n","Epoch 1/2000, Step 262, d_loss: 0.0011788689298555255, g_loss: 7.5135393142700195\n","Epoch 1/2000, Step 263, d_loss: 0.0009653359884396195, g_loss: 7.560993671417236\n","Epoch 1/2000, Step 264, d_loss: 0.001338459551334381, g_loss: 7.636680603027344\n","Epoch 1/2000, Step 265, d_loss: 0.0009023251477628946, g_loss: 7.598444938659668\n","Epoch 1/2000, Step 266, d_loss: 0.000843000365421176, g_loss: 7.674820423126221\n","Epoch 1/2000, Step 267, d_loss: 0.0007926149992272258, g_loss: 7.606366157531738\n","Epoch 1/2000, Step 268, d_loss: 0.0009971685940399766, g_loss: 7.692744731903076\n","Epoch 1/2000, Step 269, d_loss: 0.0008195575792342424, g_loss: 7.683806896209717\n","Epoch 1/2000, Step 270, d_loss: 0.0008941784617491066, g_loss: 7.705533027648926\n","Epoch 1/2000, Step 271, d_loss: 0.0009830091148614883, g_loss: 7.661062240600586\n","Epoch 1/2000, Step 272, d_loss: 0.0007702299626544118, g_loss: 7.603239059448242\n","Epoch 1/2000, Step 273, d_loss: 0.0009710562881082296, g_loss: 7.643036365509033\n","Epoch 1/2000, Step 274, d_loss: 0.000993565539829433, g_loss: 7.560027599334717\n","Epoch 1/2000, Step 275, d_loss: 0.0008056263322941959, g_loss: 7.66425085067749\n","Epoch 1/2000, Step 276, d_loss: 0.0008193824323825538, g_loss: 7.717504978179932\n","Epoch 1/2000, Step 277, d_loss: 0.0009590421104803681, g_loss: 7.706595420837402\n","Epoch 1/2000, Step 278, d_loss: 0.0007161397370509803, g_loss: 7.716512680053711\n","Epoch 1/2000, Step 279, d_loss: 0.0011667771032080054, g_loss: 7.676555633544922\n","Epoch 1/2000, Step 280, d_loss: 0.0007590091554448009, g_loss: 7.724359512329102\n","Epoch 1/2000, Step 281, d_loss: 0.0008709696703590453, g_loss: 7.763523101806641\n","Epoch 1/2000, Step 282, d_loss: 0.0009530295501463115, g_loss: 7.717469692230225\n","Epoch 1/2000, Step 283, d_loss: 0.0007084081880748272, g_loss: 7.710890293121338\n","Epoch 1/2000, Step 284, d_loss: 0.0007978654466569424, g_loss: 7.697767734527588\n","Epoch 1/2000, Step 285, d_loss: 0.0007014689035713673, g_loss: 7.765569686889648\n","Epoch 1/2000, Step 286, d_loss: 0.0010289241326972842, g_loss: 7.768035411834717\n","Epoch 1/2000, Step 287, d_loss: 0.0008101614657789469, g_loss: 7.765307426452637\n","Epoch 1/2000, Step 288, d_loss: 0.000840206746943295, g_loss: 7.752994537353516\n","Epoch 1/2000, Step 289, d_loss: 0.0006648292765021324, g_loss: 7.730313777923584\n","Epoch 1/2000, Step 290, d_loss: 0.0007856048177927732, g_loss: 7.743296146392822\n","Epoch 1/2000, Step 291, d_loss: 0.0007292747613973916, g_loss: 7.783440113067627\n","Epoch 1/2000, Step 292, d_loss: 0.000677659991197288, g_loss: 7.770637035369873\n","Epoch 1/2000, Step 293, d_loss: 0.0007733518723398447, g_loss: 7.762570381164551\n","Epoch 1/2000, Step 294, d_loss: 0.0006632516742683947, g_loss: 7.784603595733643\n","Epoch 1/2000, Step 295, d_loss: 0.0007088573183864355, g_loss: 7.813227653503418\n","Epoch 1/2000, Step 296, d_loss: 0.0007455286104232073, g_loss: 7.7997894287109375\n","Epoch 1/2000, Step 297, d_loss: 0.0007438756292685866, g_loss: 7.779255390167236\n","Epoch 1/2000, Step 298, d_loss: 0.0007212251075543463, g_loss: 7.7997541427612305\n","Epoch 1/2000, Step 299, d_loss: 0.0006526136421598494, g_loss: 7.84149169921875\n","Epoch 1/2000, Step 300, d_loss: 0.0006479592411778867, g_loss: 7.830240249633789\n","Epoch 1/2000, Step 301, d_loss: 0.0006340384716168046, g_loss: 7.869419097900391\n","Epoch 1/2000, Step 302, d_loss: 0.0007037589093670249, g_loss: 7.870636940002441\n","Epoch 1/2000, Step 303, d_loss: 0.0007672955980524421, g_loss: 7.774369716644287\n","Epoch 1/2000, Step 304, d_loss: 0.0006654384778812528, g_loss: 7.862922668457031\n","Epoch 1/2000, Step 305, d_loss: 0.0007399815949611366, g_loss: 7.858836650848389\n","Epoch 1/2000, Step 306, d_loss: 0.0005907368031330407, g_loss: 7.861346244812012\n","Epoch 1/2000, Step 307, d_loss: 0.0007160487584769726, g_loss: 7.838322639465332\n","Epoch 1/2000, Step 308, d_loss: 0.0006449057254940271, g_loss: 7.875607490539551\n","Epoch 1/2000, Step 309, d_loss: 0.0006877488922327757, g_loss: 7.872120380401611\n","Epoch 1/2000, Step 310, d_loss: 0.0006571320118382573, g_loss: 7.911124229431152\n","Epoch 1/2000, Step 311, d_loss: 0.0009716101340018213, g_loss: 7.814943790435791\n","Epoch 1/2000, Step 312, d_loss: 0.0007037298055365682, g_loss: 7.898509502410889\n","Epoch 1/2000, Step 313, d_loss: 0.0006599288899451494, g_loss: 7.881220817565918\n","Epoch 1/2000, Step 314, d_loss: 0.000712525681592524, g_loss: 7.846852779388428\n","Epoch 1/2000, Step 315, d_loss: 0.0005560520803555846, g_loss: 7.880999565124512\n","Epoch 1/2000, Step 316, d_loss: 0.0005627803038805723, g_loss: 7.889106273651123\n","Epoch 1/2000, Step 317, d_loss: 0.0006948505179025233, g_loss: 7.8581013679504395\n","Epoch 1/2000, Step 318, d_loss: 0.0007916070753708482, g_loss: 7.884708881378174\n","Epoch 1/2000, Step 319, d_loss: 0.0006493876571767032, g_loss: 7.874738693237305\n","Epoch 1/2000, Step 320, d_loss: 0.000968209351412952, g_loss: 7.896244049072266\n","Epoch 1/2000, Step 321, d_loss: 0.0007148683653213084, g_loss: 7.867579936981201\n","Epoch 1/2000, Step 322, d_loss: 0.0006267742719501257, g_loss: 7.949254035949707\n","Epoch 1/2000, Step 323, d_loss: 0.0005313802394084632, g_loss: 7.892463684082031\n","Epoch 1/2000, Step 324, d_loss: 0.0007263769512064755, g_loss: 7.9143853187561035\n","Epoch 1/2000, Step 325, d_loss: 0.0005973570514470339, g_loss: 7.9370927810668945\n","Epoch 1/2000, Step 326, d_loss: 0.000611249590292573, g_loss: 7.914952278137207\n","Epoch 1/2000, Step 327, d_loss: 0.0006581754423677921, g_loss: 7.9597554206848145\n","Epoch 1/2000, Step 328, d_loss: 0.0006016960833221674, g_loss: 7.9533915519714355\n","Epoch 1/2000, Step 329, d_loss: 0.0005453214980661869, g_loss: 7.942224979400635\n","Epoch 1/2000, Step 330, d_loss: 0.0005813544848933816, g_loss: 7.9348554611206055\n","Epoch 1/2000, Step 331, d_loss: 0.0006165872910059988, g_loss: 7.9319987297058105\n","Epoch 1/2000, Step 332, d_loss: 0.0006835669046267867, g_loss: 7.925946235656738\n","Epoch 1/2000, Step 333, d_loss: 0.0007316069677472115, g_loss: 7.944276332855225\n","Epoch 1/2000, Step 334, d_loss: 0.0005709580145776272, g_loss: 7.953194618225098\n","Epoch 1/2000, Step 335, d_loss: 0.0005895477952435613, g_loss: 7.995142459869385\n","Epoch 1/2000, Step 336, d_loss: 0.0006650104187428951, g_loss: 8.008445739746094\n","Epoch 1/2000, Step 337, d_loss: 0.0005885148420929909, g_loss: 7.997555255889893\n","Epoch 1/2000, Step 338, d_loss: 0.0005548407789319754, g_loss: 8.008814811706543\n","Epoch 1/2000, Step 339, d_loss: 0.000534319959115237, g_loss: 7.979476451873779\n","Epoch 1/2000, Step 340, d_loss: 0.0005533669027499855, g_loss: 7.9517107009887695\n","Epoch 1/2000, Step 341, d_loss: 0.0005437658983282745, g_loss: 8.011018753051758\n","Epoch 1/2000, Step 342, d_loss: 0.0005297036259435117, g_loss: 8.005669593811035\n","Epoch 1/2000, Step 343, d_loss: 0.0005208380753174424, g_loss: 8.002476692199707\n","Epoch 1/2000, Step 344, d_loss: 0.0005359537899494171, g_loss: 7.98136568069458\n","Epoch 1/2000, Step 345, d_loss: 0.0005480675608851016, g_loss: 8.032872200012207\n","Epoch 1/2000, Step 346, d_loss: 0.0005286381347104907, g_loss: 7.979730606079102\n","Epoch 1/2000, Step 347, d_loss: 0.000530601479113102, g_loss: 8.014472007751465\n","Epoch 1/2000, Step 348, d_loss: 0.0005363661330193281, g_loss: 8.039953231811523\n","Epoch 1/2000, Step 349, d_loss: 0.0005122319562360644, g_loss: 8.067734718322754\n","Epoch 1/2000, Step 350, d_loss: 0.0005169741925783455, g_loss: 8.052520751953125\n","Epoch 1/2000, Step 351, d_loss: 0.0005348552367649972, g_loss: 8.046996116638184\n","Epoch 1/2000, Step 352, d_loss: 0.0004887672839686275, g_loss: 8.070272445678711\n","Epoch 1/2000, Step 353, d_loss: 0.0005438168300315738, g_loss: 8.04735279083252\n","Epoch 1/2000, Step 354, d_loss: 0.0005306510720402002, g_loss: 8.058358192443848\n","Epoch 1/2000, Step 355, d_loss: 0.0005186633206903934, g_loss: 8.07837963104248\n","Epoch 1/2000, Step 356, d_loss: 0.0005011190660297871, g_loss: 8.010906219482422\n","Epoch 1/2000, Step 357, d_loss: 0.0005121834110468626, g_loss: 8.063862800598145\n","Epoch 1/2000, Step 358, d_loss: 0.0004993103793822229, g_loss: 8.00256633758545\n","Epoch 1/2000, Step 359, d_loss: 0.0004568031872622669, g_loss: 8.093469619750977\n","Epoch 1/2000, Step 360, d_loss: 0.0005810318980365992, g_loss: 8.012996673583984\n","Epoch 1/2000, Step 361, d_loss: 0.00046901105088181794, g_loss: 8.020879745483398\n","Epoch 1/2000, Step 362, d_loss: 0.0005198310827836394, g_loss: 8.071967124938965\n","Epoch 1/2000, Step 363, d_loss: 0.0005727412644773722, g_loss: 8.040672302246094\n","Epoch 1/2000, Step 364, d_loss: 0.0004961627419106662, g_loss: 8.044126510620117\n","Epoch 1/2000, Step 365, d_loss: 0.0005319657502695918, g_loss: 7.928213596343994\n","Epoch 1/2000, Step 366, d_loss: 0.0005089952610433102, g_loss: 7.974979400634766\n","Epoch 1/2000, Step 367, d_loss: 0.0005295572918839753, g_loss: 7.94887638092041\n","Epoch 1/2000, Step 368, d_loss: 0.0005019726813770831, g_loss: 7.985505104064941\n","Epoch 1/2000, Step 369, d_loss: 0.0005207543144933879, g_loss: 7.916440486907959\n","Epoch 1/2000, Step 370, d_loss: 0.0005560772260650992, g_loss: 7.901845932006836\n","Epoch 1/2000, Step 371, d_loss: 0.000498080684337765, g_loss: 7.867878437042236\n","Epoch 1/2000, Step 372, d_loss: 0.0005139291170053184, g_loss: 7.917987823486328\n","Epoch 1/2000, Step 373, d_loss: 0.0005367946578189731, g_loss: 7.915162563323975\n","Epoch 1/2000, Step 374, d_loss: 0.0006034284015186131, g_loss: 7.894668102264404\n","Epoch 1/2000, Step 375, d_loss: 0.0006294770282693207, g_loss: 7.873059272766113\n","Epoch 1/2000, Step 376, d_loss: 0.0006284929695539176, g_loss: 7.8525710105896\n","Epoch 1/2000, Step 377, d_loss: 0.0005693176644854248, g_loss: 7.753796100616455\n","Epoch 1/2000, Step 378, d_loss: 0.0006463768077082932, g_loss: 7.880153656005859\n","Epoch 1/2000, Step 379, d_loss: 0.0006265846895985305, g_loss: 7.820141792297363\n","Epoch 1/2000, Step 380, d_loss: 0.0006136454176157713, g_loss: 7.852670669555664\n","Epoch 1/2000, Step 381, d_loss: 0.0005150837241671979, g_loss: 7.872597694396973\n","Epoch 1/2000, Step 382, d_loss: 0.0005599924479611218, g_loss: 7.881739139556885\n","Epoch 1/2000, Step 383, d_loss: 0.0005456786602735519, g_loss: 7.838291168212891\n","Epoch 1/2000, Step 384, d_loss: 0.0005638848524540663, g_loss: 7.884761333465576\n","Epoch 1/2000, Step 385, d_loss: 0.0007427105447277427, g_loss: 7.881750106811523\n","Epoch 1/2000, Step 386, d_loss: 0.0007560903904959559, g_loss: 7.863889694213867\n","Epoch 1/2000, Step 387, d_loss: 0.0005490545881912112, g_loss: 7.922386646270752\n","Epoch 1/2000, Step 388, d_loss: 0.0006648609996773303, g_loss: 7.838421821594238\n","Epoch 1/2000, Step 389, d_loss: 0.0005463152192533016, g_loss: 7.894371509552002\n","Epoch 1/2000, Step 390, d_loss: 0.0005925767472945154, g_loss: 7.881571292877197\n","Epoch 1/2000, Step 391, d_loss: 0.0005625333869829774, g_loss: 7.889957427978516\n","Epoch 1/2000, Step 392, d_loss: 0.000575860554818064, g_loss: 7.948662757873535\n","Epoch 1/2000, Step 393, d_loss: 0.0005890008760616183, g_loss: 7.878068447113037\n","Epoch 1/2000, Step 394, d_loss: 0.0005314667942002416, g_loss: 7.942502021789551\n","Epoch 1/2000, Step 395, d_loss: 0.0005604465259239078, g_loss: 7.813168525695801\n","Epoch 1/2000, Step 396, d_loss: 0.0005356055917218328, g_loss: 7.907866477966309\n","Epoch 1/2000, Step 397, d_loss: 0.000491769693326205, g_loss: 7.940616130828857\n","Epoch 1/2000, Step 398, d_loss: 0.0005224634078331292, g_loss: 7.998512268066406\n","Epoch 1/2000, Step 399, d_loss: 0.0005460368702188134, g_loss: 7.988490581512451\n","Epoch 1/2000, Step 400, d_loss: 0.0004765708581544459, g_loss: 7.921868324279785\n","Epoch 1/2000, Step 401, d_loss: 0.0006849843775853515, g_loss: 8.010700225830078\n","Epoch 1/2000, Step 402, d_loss: 0.0006015572580508888, g_loss: 8.00576114654541\n","Epoch 1/2000, Step 403, d_loss: 0.0005115764797665179, g_loss: 7.95919132232666\n","Epoch 1/2000, Step 404, d_loss: 0.0004954153555445373, g_loss: 7.940360069274902\n","Epoch 1/2000, Step 405, d_loss: 0.0005586905172094703, g_loss: 7.9743332862854\n","Epoch 1/2000, Step 406, d_loss: 0.0005428388831205666, g_loss: 7.976683139801025\n","Epoch 1/2000, Step 407, d_loss: 0.0006399350240826607, g_loss: 7.990314483642578\n","Epoch 1/2000, Step 408, d_loss: 0.00047848070971667767, g_loss: 7.99430513381958\n","Epoch 1/2000, Step 409, d_loss: 0.0005511163035407662, g_loss: 8.004866600036621\n","Epoch 1/2000, Step 410, d_loss: 0.000571159936953336, g_loss: 7.994418144226074\n","Epoch 1/2000, Step 411, d_loss: 0.0004918695776723325, g_loss: 7.976053714752197\n","Epoch 1/2000, Step 412, d_loss: 0.0004942202940583229, g_loss: 8.012839317321777\n","Epoch 1/2000, Step 413, d_loss: 0.0005001922836527228, g_loss: 7.99574613571167\n","Epoch 1/2000, Step 414, d_loss: 0.0005061239353381097, g_loss: 8.025653839111328\n","Epoch 1/2000, Step 415, d_loss: 0.0004796002758666873, g_loss: 7.9720940589904785\n","Epoch 1/2000, Step 416, d_loss: 0.000568716786801815, g_loss: 7.988242149353027\n","Epoch 1/2000, Step 417, d_loss: 0.0005640265881083906, g_loss: 8.003280639648438\n","Epoch 1/2000, Step 418, d_loss: 0.0005293134017847478, g_loss: 8.02314567565918\n","Epoch 1/2000, Step 419, d_loss: 0.0005514941294677556, g_loss: 8.116823196411133\n","Epoch 1/2000, Step 420, d_loss: 0.00043764719157479703, g_loss: 8.04863166809082\n","Epoch 1/2000, Step 421, d_loss: 0.0005231829127296805, g_loss: 7.973166465759277\n","Epoch 1/2000, Step 422, d_loss: 0.0004606846487149596, g_loss: 8.158468246459961\n","Epoch 1/2000, Step 423, d_loss: 0.0004922553198412061, g_loss: 8.116127014160156\n","Epoch 1/2000, Step 424, d_loss: 0.00042652024421840906, g_loss: 8.135904312133789\n","Epoch 1/2000, Step 425, d_loss: 0.0004540150403045118, g_loss: 8.166147232055664\n","Epoch 1/2000, Step 426, d_loss: 0.0005347812548279762, g_loss: 8.060783386230469\n","Epoch 1/2000, Step 427, d_loss: 0.00047459598863497376, g_loss: 8.099133491516113\n","Epoch 1/2000, Step 428, d_loss: 0.0004411234403960407, g_loss: 8.056676864624023\n","Epoch 1/2000, Step 429, d_loss: 0.0004906259709969163, g_loss: 8.035431861877441\n","Epoch 1/2000, Step 430, d_loss: 0.00048318965127691627, g_loss: 8.071223258972168\n","Epoch 1/2000, Step 431, d_loss: 0.0005672330735251307, g_loss: 8.07331657409668\n","Epoch 1/2000, Step 432, d_loss: 0.0004628102760761976, g_loss: 8.163339614868164\n","Epoch 1/2000, Step 433, d_loss: 0.0004915071767754853, g_loss: 8.06431770324707\n","Epoch 1/2000, Step 434, d_loss: 0.0005321552744135261, g_loss: 8.142387390136719\n","Epoch 1/2000, Step 435, d_loss: 0.00048161623999476433, g_loss: 8.152109146118164\n","Epoch 1/2000, Step 436, d_loss: 0.0004143903497606516, g_loss: 8.11821174621582\n","Epoch 1/2000, Step 437, d_loss: 0.00046782410936430097, g_loss: 8.109606742858887\n","Epoch 1/2000, Step 438, d_loss: 0.0004855894367210567, g_loss: 8.056041717529297\n","Epoch 1/2000, Step 439, d_loss: 0.0004850471450481564, g_loss: 8.04231071472168\n","Epoch 1/2000, Step 440, d_loss: 0.0004877114261034876, g_loss: 8.064295768737793\n","Epoch 1/2000, Step 441, d_loss: 0.0004940411308780313, g_loss: 8.057854652404785\n","Epoch 1/2000, Step 442, d_loss: 0.0005519315600395203, g_loss: 8.064399719238281\n","Epoch 1/2000, Step 443, d_loss: 0.0005619805306196213, g_loss: 8.054347038269043\n","Epoch 1/2000, Step 444, d_loss: 0.00044367872760631144, g_loss: 8.168993949890137\n","Epoch 1/2000, Step 445, d_loss: 0.0004446146194823086, g_loss: 8.13284969329834\n","Epoch 1/2000, Step 446, d_loss: 0.00048647128278389573, g_loss: 8.13860034942627\n","Epoch 1/2000, Step 447, d_loss: 0.0007791483076289296, g_loss: 8.094361305236816\n","Epoch 1/2000, Step 448, d_loss: 0.0005048395833000541, g_loss: 8.056753158569336\n","Epoch 1/2000, Step 449, d_loss: 0.00047658197581768036, g_loss: 8.081692695617676\n","Epoch 1/2000, Step 450, d_loss: 0.0005277566961012781, g_loss: 8.070685386657715\n","Epoch 1/2000, Step 451, d_loss: 0.0005432445323094726, g_loss: 7.946711540222168\n","Epoch 1/2000, Step 452, d_loss: 0.0004810332611668855, g_loss: 7.947817802429199\n","Epoch 1/2000, Step 453, d_loss: 0.0005115829408168793, g_loss: 7.9029083251953125\n","Epoch 1/2000, Step 454, d_loss: 0.0004918304039165378, g_loss: 7.867112159729004\n","Epoch 1/2000, Step 455, d_loss: 0.0005619037547148764, g_loss: 7.651525497436523\n","Epoch 1/2000, Step 456, d_loss: 0.0006412222282961011, g_loss: 7.84201192855835\n","Epoch 1/2000, Step 457, d_loss: 0.0006335270591080189, g_loss: 7.681536674499512\n","Epoch 1/2000, Step 458, d_loss: 0.0007541629020124674, g_loss: 7.525791168212891\n","Epoch 1/2000, Step 459, d_loss: 0.0006284050759859383, g_loss: 7.510629177093506\n","Epoch 1/2000, Step 460, d_loss: 0.0006840338464826345, g_loss: 7.4106316566467285\n","Epoch 1/2000, Step 461, d_loss: 0.0007034245645627379, g_loss: 7.360046863555908\n","Epoch 1/2000, Step 462, d_loss: 0.0008404178079217672, g_loss: 7.532976150512695\n","Epoch 1/2000, Step 463, d_loss: 0.0011636748677119613, g_loss: 7.380712509155273\n","Epoch 1/2000, Step 464, d_loss: 0.0009259034995920956, g_loss: 7.409745216369629\n","Epoch 1/2000, Step 465, d_loss: 0.0007607684237882495, g_loss: 7.221622467041016\n","Epoch 1/2000, Step 466, d_loss: 0.000953445618506521, g_loss: 7.4290771484375\n","Epoch 1/2000, Step 467, d_loss: 0.0009008793858811259, g_loss: 7.385887145996094\n","Epoch 1/2000, Step 468, d_loss: 0.0008754215086810291, g_loss: 7.034470558166504\n","Epoch 1/2000, Step 469, d_loss: 0.0010680628474801779, g_loss: 6.983792781829834\n","Epoch 1/2000, Step 470, d_loss: 0.001796502387151122, g_loss: 7.040806293487549\n","Epoch 1/2000, Step 471, d_loss: 0.0014658162835985422, g_loss: 6.855072498321533\n","Epoch 1/2000, Step 472, d_loss: 0.0009558481979183853, g_loss: 7.017642498016357\n","Epoch 1/2000, Step 473, d_loss: 0.0009949138620868325, g_loss: 7.088957786560059\n","Epoch 1/2000, Step 474, d_loss: 0.0008538231486454606, g_loss: 7.093307971954346\n","Epoch 1/2000, Step 475, d_loss: 0.0011798059567809105, g_loss: 7.382235050201416\n","Epoch 1/2000, Step 476, d_loss: 0.001035621389746666, g_loss: 7.055359363555908\n","Epoch 1/2000, Step 477, d_loss: 0.001197455800138414, g_loss: 6.787510871887207\n","Epoch 1/2000, Step 478, d_loss: 0.0016029245452955365, g_loss: 6.804115295410156\n","Epoch 1/2000, Step 479, d_loss: 0.0011035153875127435, g_loss: 6.943492412567139\n","Epoch 1/2000, Step 480, d_loss: 0.0013373356778174639, g_loss: 7.281216621398926\n","Epoch 1/2000, Step 481, d_loss: 0.0013289855560287833, g_loss: 6.724601745605469\n","Epoch 1/2000, Step 482, d_loss: 0.001238325727172196, g_loss: 6.917867183685303\n","Epoch 1/2000, Step 483, d_loss: 0.0022086440585553646, g_loss: 6.603927135467529\n","Epoch 1/2000, Step 484, d_loss: 0.002088212175294757, g_loss: 6.80047607421875\n","Epoch 1/2000, Step 485, d_loss: 0.0017702996265143156, g_loss: 6.730928421020508\n","Epoch 1/2000, Step 486, d_loss: 0.001831771107390523, g_loss: 6.419059753417969\n","Epoch 1/2000, Step 487, d_loss: 0.0019351918017491698, g_loss: 6.394718170166016\n","Epoch 1/2000, Step 488, d_loss: 0.002479619113728404, g_loss: 6.839200973510742\n","Epoch 1/2000, Step 489, d_loss: 0.0026975180953741074, g_loss: 6.400390148162842\n","Epoch 1/2000, Step 490, d_loss: 0.00275172246620059, g_loss: 6.282075881958008\n","Epoch 1/2000, Step 491, d_loss: 0.0034171254374086857, g_loss: 6.2136759757995605\n","Epoch 1/2000, Step 492, d_loss: 0.0026166338939219713, g_loss: 6.586742401123047\n","Epoch 1/2000, Step 493, d_loss: 0.0021803900599479675, g_loss: 6.576842308044434\n","Epoch 1/2000, Step 494, d_loss: 0.002302346518263221, g_loss: 6.84712553024292\n","Epoch 1/2000, Step 495, d_loss: 0.0017832429148256779, g_loss: 6.926889896392822\n","Epoch 1/2000, Step 496, d_loss: 0.0016727738548070192, g_loss: 7.116147994995117\n","Epoch 1/2000, Step 497, d_loss: 0.0015026729088276625, g_loss: 7.098808765411377\n","Epoch 1/2000, Step 498, d_loss: 0.0014186359476298094, g_loss: 7.23651647567749\n","Epoch 1/2000, Step 499, d_loss: 0.0016842230688780546, g_loss: 7.328958988189697\n","Epoch 1/2000, Step 500, d_loss: 0.0014232615940272808, g_loss: 7.26337194442749\n","Epoch 1/2000, Step 501, d_loss: 0.0011796778999269009, g_loss: 7.349521636962891\n","Epoch 1/2000, Step 502, d_loss: 0.0012848475016653538, g_loss: 7.062514781951904\n","Epoch 1/2000, Step 503, d_loss: 0.0010312246158719063, g_loss: 7.637446403503418\n","Epoch 1/2000, Step 504, d_loss: 0.002435622736811638, g_loss: 7.764204978942871\n","Epoch 1/2000, Step 505, d_loss: 0.0009857695549726486, g_loss: 7.805352687835693\n","Epoch 1/2000, Step 506, d_loss: 0.001052576000802219, g_loss: 7.8365960121154785\n","Epoch 1/2000, Step 507, d_loss: 0.001149762887507677, g_loss: 7.398535251617432\n","Epoch 1/2000, Step 508, d_loss: 0.0012058650609105825, g_loss: 7.6415181159973145\n","Epoch 1/2000, Step 509, d_loss: 0.0012366937007755041, g_loss: 7.528541088104248\n","Epoch 1/2000, Step 510, d_loss: 0.0014694815035909414, g_loss: 7.313382625579834\n","Epoch 1/2000, Step 511, d_loss: 0.0013595637865364552, g_loss: 7.041980743408203\n","Epoch 1/2000, Step 512, d_loss: 0.002026104601100087, g_loss: 7.1082282066345215\n","Epoch 1/2000, Step 513, d_loss: 0.002176417503505945, g_loss: 7.009465217590332\n","Epoch 1/2000, Step 514, d_loss: 0.0019823070615530014, g_loss: 7.032012462615967\n","Epoch 1/2000, Step 515, d_loss: 0.001679935841821134, g_loss: 7.251255035400391\n","Epoch 1/2000, Step 516, d_loss: 0.0015786895528435707, g_loss: 7.676451683044434\n","Epoch 1/2000, Step 517, d_loss: 0.0011973418295383453, g_loss: 7.9223313331604\n","Epoch 1/2000, Step 518, d_loss: 0.0011761137284338474, g_loss: 7.613258361816406\n","Epoch 1/2000, Step 519, d_loss: 0.0015597392339259386, g_loss: 7.4208455085754395\n","Epoch 1/2000, Step 520, d_loss: 0.0013703473377972841, g_loss: 8.381354331970215\n","Epoch 1/2000, Step 521, d_loss: 0.0009757985826581717, g_loss: 8.464083671569824\n","Epoch 1/2000, Step 522, d_loss: 0.0011709253303706646, g_loss: 8.43020248413086\n","Epoch 1/2000, Step 523, d_loss: 0.0019128923304378986, g_loss: 8.511003494262695\n","Epoch 1/2000, Step 524, d_loss: 0.0034721014089882374, g_loss: 8.76111888885498\n","Epoch 1/2000, Step 525, d_loss: 0.011492855846881866, g_loss: 6.942914009094238\n","Epoch 1/2000, Step 526, d_loss: 0.015461331233382225, g_loss: 7.769207000732422\n","Epoch 1/2000, Step 527, d_loss: 0.0866205245256424, g_loss: 1.4586541652679443\n","Epoch 1/2000, Step 528, d_loss: 0.045654017478227615, g_loss: 3.7765321731567383\n","Epoch 1/2000, Step 529, d_loss: 0.1285066306591034, g_loss: 2.9058825969696045\n","Epoch 1/2000, Step 530, d_loss: 0.07771587371826172, g_loss: 3.4736640453338623\n","Epoch 1/2000, Step 531, d_loss: 0.031951118260622025, g_loss: 1.0691229104995728\n","Epoch 1/2000, Step 532, d_loss: 0.18962493538856506, g_loss: 3.2228972911834717\n","Epoch 1/2000, Step 533, d_loss: 1.0186505317687988, g_loss: 1.0115444660186768\n","Epoch 1/2000, Step 534, d_loss: 6.855637550354004, g_loss: 2.4183132648468018\n","Epoch 1/2000, Step 535, d_loss: 1.7177660465240479, g_loss: 0.7985776662826538\n","Epoch 1/2000, Step 536, d_loss: 0.7685567140579224, g_loss: 2.23884654045105\n","Epoch 1/2000, Step 537, d_loss: 0.572565495967865, g_loss: 3.5332326889038086\n","Epoch 1/2000, Step 538, d_loss: 1.4624656438827515, g_loss: 3.47857666015625\n","Epoch 1/2000, Step 539, d_loss: 0.6367305517196655, g_loss: 2.204984426498413\n","Epoch 1/2000, Step 540, d_loss: 0.3888542950153351, g_loss: 1.281251072883606\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 2/2000, Step 1, d_loss: 0.8615337014198303, g_loss: 0.8261541128158569\n","Epoch 2/2000, Step 2, d_loss: 0.9037474393844604, g_loss: 1.5462254285812378\n","Epoch 2/2000, Step 3, d_loss: 0.6808109283447266, g_loss: 2.1169826984405518\n","Epoch 2/2000, Step 4, d_loss: 0.383059024810791, g_loss: 2.659634590148926\n","Epoch 2/2000, Step 5, d_loss: 0.26125407218933105, g_loss: 2.921893835067749\n","Epoch 2/2000, Step 6, d_loss: 0.2831141948699951, g_loss: 2.3845813274383545\n","Epoch 2/2000, Step 7, d_loss: 0.3946940302848816, g_loss: 1.7850708961486816\n","Epoch 2/2000, Step 8, d_loss: 0.19768141210079193, g_loss: 1.847028136253357\n","Epoch 2/2000, Step 9, d_loss: 0.5089949369430542, g_loss: 1.7548956871032715\n","Epoch 2/2000, Step 10, d_loss: 0.11606390029191971, g_loss: 3.063892364501953\n","Epoch 2/2000, Step 11, d_loss: 0.21292106807231903, g_loss: 2.867478847503662\n","Epoch 2/2000, Step 12, d_loss: 0.14826053380966187, g_loss: 4.453784942626953\n","Epoch 2/2000, Step 13, d_loss: 0.24061384797096252, g_loss: 3.7863857746124268\n","Epoch 2/2000, Step 14, d_loss: 0.6780517101287842, g_loss: 2.660566806793213\n","Epoch 2/2000, Step 15, d_loss: 0.34631913900375366, g_loss: 3.8406267166137695\n","Epoch 2/2000, Step 16, d_loss: 0.7062493562698364, g_loss: 3.301978826522827\n","Epoch 2/2000, Step 17, d_loss: 0.2718103229999542, g_loss: 2.8688621520996094\n","Epoch 2/2000, Step 18, d_loss: 0.4410321116447449, g_loss: 3.199113368988037\n","Epoch 2/2000, Step 19, d_loss: 0.14143043756484985, g_loss: 1.9278411865234375\n","Epoch 2/2000, Step 20, d_loss: 0.6514720320701599, g_loss: 2.6138038635253906\n","Epoch 2/2000, Step 21, d_loss: 0.17458796501159668, g_loss: 2.6864449977874756\n","Epoch 2/2000, Step 22, d_loss: 0.3039724826812744, g_loss: 3.0692880153656006\n","Epoch 2/2000, Step 23, d_loss: 0.4326164126396179, g_loss: 3.0240914821624756\n","Epoch 2/2000, Step 24, d_loss: 0.11870414018630981, g_loss: 2.895453453063965\n","Epoch 2/2000, Step 25, d_loss: 0.12066180258989334, g_loss: 3.888648271560669\n","Epoch 2/2000, Step 26, d_loss: 0.09668672829866409, g_loss: 3.436359405517578\n","Epoch 2/2000, Step 27, d_loss: 0.20000171661376953, g_loss: 3.1664438247680664\n","Epoch 2/2000, Step 28, d_loss: 0.25575533509254456, g_loss: 2.5559563636779785\n","Epoch 2/2000, Step 29, d_loss: 0.26240092515945435, g_loss: 2.3237318992614746\n","Epoch 2/2000, Step 30, d_loss: 0.06534621119499207, g_loss: 2.919987440109253\n","Epoch 2/2000, Step 31, d_loss: 0.20233502984046936, g_loss: 1.9250046014785767\n","Epoch 2/2000, Step 32, d_loss: 0.12970969080924988, g_loss: 4.006834030151367\n","Epoch 2/2000, Step 33, d_loss: 0.31563615798950195, g_loss: 3.4542410373687744\n","Epoch 2/2000, Step 34, d_loss: 0.3519904613494873, g_loss: 3.8396620750427246\n","Epoch 2/2000, Step 35, d_loss: 0.10841576755046844, g_loss: 4.6837358474731445\n","Epoch 2/2000, Step 36, d_loss: 0.11706522107124329, g_loss: 2.374833583831787\n","Epoch 2/2000, Step 37, d_loss: 0.13375626504421234, g_loss: 3.4378135204315186\n","Epoch 2/2000, Step 38, d_loss: 0.17876189947128296, g_loss: 2.6794562339782715\n","Epoch 2/2000, Step 39, d_loss: 0.06479950249195099, g_loss: 2.6722350120544434\n","Epoch 2/2000, Step 40, d_loss: 0.13264895975589752, g_loss: 2.411017417907715\n","Epoch 2/2000, Step 41, d_loss: 0.05484992265701294, g_loss: 3.2274181842803955\n","Epoch 2/2000, Step 42, d_loss: 0.11893898248672485, g_loss: 4.64948844909668\n","Epoch 2/2000, Step 43, d_loss: 0.14969275891780853, g_loss: 3.890399694442749\n","Epoch 2/2000, Step 44, d_loss: 0.02734268084168434, g_loss: 4.8287248611450195\n","Epoch 2/2000, Step 45, d_loss: 0.040438879281282425, g_loss: 3.4419825077056885\n","Epoch 2/2000, Step 46, d_loss: 0.04640601575374603, g_loss: 5.275044918060303\n","Epoch 2/2000, Step 47, d_loss: 0.169186532497406, g_loss: 4.794994831085205\n","Epoch 2/2000, Step 48, d_loss: 0.03575393930077553, g_loss: 4.279358386993408\n","Epoch 2/2000, Step 49, d_loss: 0.05826934054493904, g_loss: 4.506844997406006\n","Epoch 2/2000, Step 50, d_loss: 0.1302676647901535, g_loss: 5.323919773101807\n","Epoch 2/2000, Step 51, d_loss: 0.030507458373904228, g_loss: 4.792471885681152\n","Epoch 2/2000, Step 52, d_loss: 0.01772177405655384, g_loss: 4.228457927703857\n","Epoch 2/2000, Step 53, d_loss: 0.035380974411964417, g_loss: 4.8557233810424805\n","Epoch 2/2000, Step 54, d_loss: 0.03008943423628807, g_loss: 2.319408655166626\n","Epoch 2/2000, Step 55, d_loss: 0.024419276043772697, g_loss: 3.240232467651367\n","Epoch 2/2000, Step 56, d_loss: 0.011484341695904732, g_loss: 5.3968987464904785\n","Epoch 2/2000, Step 57, d_loss: 0.018925299867987633, g_loss: 4.520705699920654\n","Epoch 2/2000, Step 58, d_loss: 0.05733112245798111, g_loss: 5.498185634613037\n","Epoch 2/2000, Step 59, d_loss: 0.04005866497755051, g_loss: 3.676142454147339\n","Epoch 2/2000, Step 60, d_loss: 0.020086314529180527, g_loss: 0.96855628490448\n","Epoch 2/2000, Step 61, d_loss: 0.0167678389698267, g_loss: 4.22311544418335\n","Epoch 2/2000, Step 62, d_loss: 0.03800651431083679, g_loss: 5.007294654846191\n","Epoch 2/2000, Step 63, d_loss: 0.030236659571528435, g_loss: 5.574197769165039\n","Epoch 2/2000, Step 64, d_loss: 0.046112336218357086, g_loss: 5.143823146820068\n","Epoch 2/2000, Step 65, d_loss: 0.11839351058006287, g_loss: 6.013857364654541\n","Epoch 2/2000, Step 66, d_loss: 0.02398056723177433, g_loss: 6.932525157928467\n","Epoch 2/2000, Step 67, d_loss: 0.07226346433162689, g_loss: 7.5297698974609375\n","Epoch 2/2000, Step 68, d_loss: 0.021238334476947784, g_loss: 7.0988054275512695\n","Epoch 2/2000, Step 69, d_loss: 0.025674253702163696, g_loss: 7.540268421173096\n","Epoch 2/2000, Step 70, d_loss: 0.040490373969078064, g_loss: 6.027404308319092\n","Epoch 2/2000, Step 71, d_loss: 0.020944612100720406, g_loss: 3.3503472805023193\n","Epoch 2/2000, Step 72, d_loss: 0.022330911830067635, g_loss: 5.1836419105529785\n","Epoch 2/2000, Step 73, d_loss: 0.02391207590699196, g_loss: 7.059844970703125\n","Epoch 2/2000, Step 74, d_loss: 0.030893899500370026, g_loss: 5.006824493408203\n","Epoch 2/2000, Step 75, d_loss: 0.017708316445350647, g_loss: 5.970004558563232\n","Epoch 2/2000, Step 76, d_loss: 0.03402692452073097, g_loss: 5.470495223999023\n","Epoch 2/2000, Step 77, d_loss: 0.033559150993824005, g_loss: 3.829774856567383\n","Epoch 2/2000, Step 78, d_loss: 0.044139616191387177, g_loss: 5.1020331382751465\n","Epoch 2/2000, Step 79, d_loss: 0.14298902451992035, g_loss: 2.6532678604125977\n","Epoch 2/2000, Step 80, d_loss: 0.0396297350525856, g_loss: 5.767573356628418\n","Epoch 2/2000, Step 81, d_loss: 0.02329626865684986, g_loss: 5.284204959869385\n","Epoch 2/2000, Step 82, d_loss: 0.09517429023981094, g_loss: 4.296290397644043\n","Epoch 2/2000, Step 83, d_loss: 0.09883486479520798, g_loss: 6.508062839508057\n","Epoch 2/2000, Step 84, d_loss: 0.024290360510349274, g_loss: 5.714211940765381\n","Epoch 2/2000, Step 85, d_loss: 0.030987480655312538, g_loss: 6.8258748054504395\n","Epoch 2/2000, Step 86, d_loss: 0.05838916823267937, g_loss: 4.56737756729126\n","Epoch 2/2000, Step 87, d_loss: 0.16552644968032837, g_loss: 5.299077033996582\n","Epoch 2/2000, Step 88, d_loss: 0.04480960965156555, g_loss: 5.655780792236328\n","Epoch 2/2000, Step 89, d_loss: 0.03221338987350464, g_loss: 3.541698932647705\n","Epoch 2/2000, Step 90, d_loss: 0.03709480166435242, g_loss: 2.5726535320281982\n","Epoch 2/2000, Step 91, d_loss: 0.07860127836465836, g_loss: 3.4290151596069336\n","Epoch 2/2000, Step 92, d_loss: 0.03173462301492691, g_loss: 3.7592062950134277\n","Epoch 2/2000, Step 93, d_loss: 0.11766369640827179, g_loss: 4.137948989868164\n","Epoch 2/2000, Step 94, d_loss: 0.03107626363635063, g_loss: 3.63783860206604\n","Epoch 2/2000, Step 95, d_loss: 0.33481284976005554, g_loss: 3.8790812492370605\n","Epoch 2/2000, Step 96, d_loss: 0.11600440740585327, g_loss: 4.872851371765137\n","Epoch 2/2000, Step 97, d_loss: 0.13856898248195648, g_loss: 5.425991535186768\n","Epoch 2/2000, Step 98, d_loss: 0.10642264038324356, g_loss: 4.809179306030273\n","Epoch 2/2000, Step 99, d_loss: 0.6556856632232666, g_loss: 4.7168450355529785\n","Epoch 2/2000, Step 100, d_loss: 0.07151295989751816, g_loss: 3.6105692386627197\n","Epoch 2/2000, Step 101, d_loss: 0.059166066348552704, g_loss: 3.7157187461853027\n","Epoch 2/2000, Step 102, d_loss: 0.1681004762649536, g_loss: 2.1511354446411133\n","Epoch 2/2000, Step 103, d_loss: 0.057460539042949677, g_loss: 3.7645599842071533\n","Epoch 2/2000, Step 104, d_loss: 0.14035584032535553, g_loss: 3.6188697814941406\n","Epoch 2/2000, Step 105, d_loss: 0.05070514604449272, g_loss: 4.272319793701172\n","Epoch 2/2000, Step 106, d_loss: 0.05907943844795227, g_loss: 3.8786842823028564\n","Epoch 2/2000, Step 107, d_loss: 0.04896407946944237, g_loss: 4.509856700897217\n","Epoch 2/2000, Step 108, d_loss: 0.31019726395606995, g_loss: 4.553393363952637\n","Epoch 2/2000, Step 109, d_loss: 0.12924262881278992, g_loss: 1.132642149925232\n","Epoch 2/2000, Step 110, d_loss: 0.18572041392326355, g_loss: 4.24312162399292\n","Epoch 2/2000, Step 111, d_loss: 0.23377148807048798, g_loss: 3.1168272495269775\n","Epoch 2/2000, Step 112, d_loss: 0.7076513171195984, g_loss: 2.3897948265075684\n","Epoch 2/2000, Step 113, d_loss: 0.5485121607780457, g_loss: 3.6710097789764404\n","Epoch 2/2000, Step 114, d_loss: 0.09733472764492035, g_loss: 4.492815971374512\n","Epoch 2/2000, Step 115, d_loss: 0.0552239716053009, g_loss: 4.162509441375732\n","Epoch 2/2000, Step 116, d_loss: 0.4774712324142456, g_loss: 4.127000331878662\n","Epoch 2/2000, Step 117, d_loss: 0.5148410797119141, g_loss: 3.6180176734924316\n","Epoch 2/2000, Step 118, d_loss: 0.04072596877813339, g_loss: 3.2676315307617188\n","Epoch 2/2000, Step 119, d_loss: 0.062394753098487854, g_loss: 2.8754656314849854\n","Epoch 2/2000, Step 120, d_loss: 0.11389999091625214, g_loss: 1.328423023223877\n","Epoch 2/2000, Step 121, d_loss: 0.3265860378742218, g_loss: 1.6219806671142578\n","Epoch 2/2000, Step 122, d_loss: 0.07070890814065933, g_loss: 2.891063690185547\n","Epoch 2/2000, Step 123, d_loss: 0.11870264261960983, g_loss: 4.133229732513428\n","Epoch 2/2000, Step 124, d_loss: 0.03245433047413826, g_loss: 3.3879194259643555\n","Epoch 2/2000, Step 125, d_loss: 0.06406775861978531, g_loss: 5.848772048950195\n","Epoch 2/2000, Step 126, d_loss: 0.13356854021549225, g_loss: 5.976728916168213\n","Epoch 2/2000, Step 127, d_loss: 0.06903684139251709, g_loss: 2.319488525390625\n","Epoch 2/2000, Step 128, d_loss: 0.045516930520534515, g_loss: 3.7943484783172607\n","Epoch 2/2000, Step 129, d_loss: 0.10426265746355057, g_loss: 5.177230358123779\n","Epoch 2/2000, Step 130, d_loss: 0.07047128677368164, g_loss: 4.5153045654296875\n","Epoch 2/2000, Step 131, d_loss: 0.06703032553195953, g_loss: 4.554821014404297\n","Epoch 2/2000, Step 132, d_loss: 0.0463288240134716, g_loss: 4.851072311401367\n","Epoch 2/2000, Step 133, d_loss: 0.1259719580411911, g_loss: 4.135341167449951\n","Epoch 2/2000, Step 134, d_loss: 0.08617328852415085, g_loss: 4.674360275268555\n","Epoch 2/2000, Step 135, d_loss: 0.06243155896663666, g_loss: 4.59644889831543\n","Epoch 2/2000, Step 136, d_loss: 0.032893359661102295, g_loss: 4.642910957336426\n","Epoch 2/2000, Step 137, d_loss: 0.013649672269821167, g_loss: 5.383699893951416\n","Epoch 2/2000, Step 138, d_loss: 0.061826035380363464, g_loss: 5.180655002593994\n","Epoch 2/2000, Step 139, d_loss: 0.0832434892654419, g_loss: 6.17928409576416\n","Epoch 2/2000, Step 140, d_loss: 0.0678759515285492, g_loss: 4.136717796325684\n","Epoch 2/2000, Step 141, d_loss: 0.023989761248230934, g_loss: 4.787669658660889\n","Epoch 2/2000, Step 142, d_loss: 0.0313335619866848, g_loss: 5.642614364624023\n","Epoch 2/2000, Step 143, d_loss: 0.07556018978357315, g_loss: 3.8146841526031494\n","Epoch 2/2000, Step 144, d_loss: 0.019995609298348427, g_loss: 4.711787700653076\n","Epoch 2/2000, Step 145, d_loss: 0.07461608201265335, g_loss: 5.936095714569092\n","Epoch 2/2000, Step 146, d_loss: 0.02181318774819374, g_loss: 4.981759548187256\n","Epoch 2/2000, Step 147, d_loss: 0.030056018382310867, g_loss: 4.85875940322876\n","Epoch 2/2000, Step 148, d_loss: 0.023671675473451614, g_loss: 3.1114721298217773\n","Epoch 2/2000, Step 149, d_loss: 0.031015882268548012, g_loss: 4.9190497398376465\n","Epoch 2/2000, Step 150, d_loss: 0.021395843476057053, g_loss: 3.9883999824523926\n","Epoch 2/2000, Step 151, d_loss: 0.034591298550367355, g_loss: 3.5391461849212646\n","Epoch 2/2000, Step 152, d_loss: 0.014795660972595215, g_loss: 5.298189163208008\n","Epoch 2/2000, Step 153, d_loss: 0.06765450537204742, g_loss: 4.64475154876709\n","Epoch 2/2000, Step 154, d_loss: 0.2503354549407959, g_loss: 4.461132049560547\n","Epoch 2/2000, Step 155, d_loss: 0.010799305513501167, g_loss: 5.009984970092773\n","Epoch 2/2000, Step 156, d_loss: 0.02789713814854622, g_loss: 2.7421138286590576\n","Epoch 2/2000, Step 157, d_loss: 0.041888527572155, g_loss: 3.657679557800293\n","Epoch 2/2000, Step 158, d_loss: 1.5587635040283203, g_loss: 2.6694154739379883\n","Epoch 2/2000, Step 159, d_loss: 0.22400857508182526, g_loss: 2.0311126708984375\n","Epoch 2/2000, Step 160, d_loss: 0.05927145108580589, g_loss: 3.4341623783111572\n","Epoch 2/2000, Step 161, d_loss: 0.04963558912277222, g_loss: 4.948894500732422\n","Epoch 2/2000, Step 162, d_loss: 0.2773919701576233, g_loss: 5.252120494842529\n","Epoch 2/2000, Step 163, d_loss: 0.06285577267408371, g_loss: 5.598025321960449\n","Epoch 2/2000, Step 164, d_loss: 0.06951908767223358, g_loss: 5.2042765617370605\n","Epoch 2/2000, Step 165, d_loss: 0.7825074791908264, g_loss: 5.931820392608643\n","Epoch 2/2000, Step 166, d_loss: 0.042009081691503525, g_loss: 5.4544997215271\n","Epoch 2/2000, Step 167, d_loss: 0.029425177723169327, g_loss: 1.1854760646820068\n","Epoch 2/2000, Step 168, d_loss: 0.032819293439388275, g_loss: 1.0614384412765503\n","Epoch 2/2000, Step 169, d_loss: 0.09690604358911514, g_loss: 3.409578323364258\n","Epoch 2/2000, Step 170, d_loss: 0.03374326229095459, g_loss: 2.525031805038452\n","Epoch 2/2000, Step 171, d_loss: 0.2585211396217346, g_loss: 3.8751893043518066\n","Epoch 2/2000, Step 172, d_loss: 0.8079310059547424, g_loss: 5.369940280914307\n","Epoch 2/2000, Step 173, d_loss: 0.08976200968027115, g_loss: 8.250445365905762\n","Epoch 2/2000, Step 174, d_loss: 0.14887326955795288, g_loss: 8.099736213684082\n","Epoch 2/2000, Step 175, d_loss: 0.36055228114128113, g_loss: 7.599618911743164\n","Epoch 2/2000, Step 176, d_loss: 0.19474409520626068, g_loss: 7.672528266906738\n","Epoch 2/2000, Step 177, d_loss: 0.6404762864112854, g_loss: 6.008795738220215\n","Epoch 2/2000, Step 178, d_loss: 0.17597980797290802, g_loss: 5.718987464904785\n","Epoch 2/2000, Step 179, d_loss: 0.04503592848777771, g_loss: 4.425706386566162\n","Epoch 2/2000, Step 180, d_loss: 0.06966213881969452, g_loss: 3.3841865062713623\n","Epoch 2/2000, Step 181, d_loss: 0.020379045978188515, g_loss: 1.774379849433899\n","Epoch 2/2000, Step 182, d_loss: 0.9384851455688477, g_loss: 1.017980694770813\n","Epoch 2/2000, Step 183, d_loss: 0.8141245245933533, g_loss: 5.257347583770752\n","Epoch 2/2000, Step 184, d_loss: 0.16583110392093658, g_loss: 5.773642539978027\n","Epoch 2/2000, Step 185, d_loss: 1.7561266422271729, g_loss: 4.852425575256348\n","Epoch 2/2000, Step 186, d_loss: 1.6884883642196655, g_loss: 4.081326007843018\n","Epoch 2/2000, Step 187, d_loss: 0.6937921643257141, g_loss: 3.489610195159912\n","Epoch 2/2000, Step 188, d_loss: 0.33259350061416626, g_loss: 2.389143943786621\n","Epoch 2/2000, Step 189, d_loss: 0.8741064667701721, g_loss: 0.49659666419029236\n","Epoch 2/2000, Step 190, d_loss: 1.2526044845581055, g_loss: 1.474611759185791\n","Epoch 2/2000, Step 191, d_loss: 1.6171815395355225, g_loss: 1.4819234609603882\n","Epoch 2/2000, Step 192, d_loss: 0.44760942459106445, g_loss: 1.5478341579437256\n","Epoch 2/2000, Step 193, d_loss: 0.31352460384368896, g_loss: 2.5973353385925293\n","Epoch 2/2000, Step 194, d_loss: 0.067559152841568, g_loss: 2.8007426261901855\n","Epoch 2/2000, Step 195, d_loss: 0.08848156780004501, g_loss: 3.448315382003784\n","Epoch 2/2000, Step 196, d_loss: 0.08423241972923279, g_loss: 3.798377752304077\n","Epoch 2/2000, Step 197, d_loss: 0.21837954223155975, g_loss: 4.675991058349609\n","Epoch 2/2000, Step 198, d_loss: 0.3814738094806671, g_loss: 2.7530150413513184\n","Epoch 2/2000, Step 199, d_loss: 0.49376171827316284, g_loss: 3.1976816654205322\n","Epoch 2/2000, Step 200, d_loss: 0.2236018180847168, g_loss: 2.5363759994506836\n","Epoch 2/2000, Step 201, d_loss: 0.19528402388095856, g_loss: 2.7148125171661377\n","Epoch 2/2000, Step 202, d_loss: 0.06224353611469269, g_loss: 2.064211368560791\n","Epoch 2/2000, Step 203, d_loss: 0.06936303526163101, g_loss: 2.420987129211426\n","Epoch 2/2000, Step 204, d_loss: 0.1198928952217102, g_loss: 2.49340558052063\n","Epoch 2/2000, Step 205, d_loss: 0.44342347979545593, g_loss: 3.9117271900177\n","Epoch 2/2000, Step 206, d_loss: 0.15199610590934753, g_loss: 4.45416784286499\n","Epoch 2/2000, Step 207, d_loss: 0.431915819644928, g_loss: 3.2009403705596924\n","Epoch 2/2000, Step 208, d_loss: 0.4649032950401306, g_loss: 3.1247026920318604\n","Epoch 2/2000, Step 209, d_loss: 0.0828721821308136, g_loss: 2.719275951385498\n","Epoch 2/2000, Step 210, d_loss: 0.15820784866809845, g_loss: 3.0590386390686035\n","Epoch 2/2000, Step 211, d_loss: 0.23577067255973816, g_loss: 2.9264774322509766\n","Epoch 2/2000, Step 212, d_loss: 0.3831670880317688, g_loss: 2.509765863418579\n","Epoch 2/2000, Step 213, d_loss: 0.3545636534690857, g_loss: 1.892459750175476\n","Epoch 2/2000, Step 214, d_loss: 0.485226571559906, g_loss: 1.9815396070480347\n","Epoch 2/2000, Step 215, d_loss: 0.30544254183769226, g_loss: 4.450747966766357\n","Epoch 2/2000, Step 216, d_loss: 0.0839536041021347, g_loss: 2.8791189193725586\n","Epoch 2/2000, Step 217, d_loss: 0.049324966967105865, g_loss: 4.4527974128723145\n","Epoch 2/2000, Step 218, d_loss: 0.23087796568870544, g_loss: 3.949345350265503\n","Epoch 2/2000, Step 219, d_loss: 0.04720108211040497, g_loss: 3.166748523712158\n","Epoch 2/2000, Step 220, d_loss: 0.148945614695549, g_loss: 3.7795920372009277\n","Epoch 2/2000, Step 221, d_loss: 0.12270815670490265, g_loss: 1.338818907737732\n","Epoch 2/2000, Step 222, d_loss: 0.13039764761924744, g_loss: 2.3607685565948486\n","Epoch 2/2000, Step 223, d_loss: 0.2578828036785126, g_loss: 1.147996425628662\n","Epoch 2/2000, Step 224, d_loss: 0.04521096497774124, g_loss: 3.7895781993865967\n","Epoch 2/2000, Step 225, d_loss: 0.04225413501262665, g_loss: 1.128522276878357\n","Epoch 2/2000, Step 226, d_loss: 0.034134767949581146, g_loss: 3.3949503898620605\n","Epoch 2/2000, Step 227, d_loss: 0.04248369485139847, g_loss: 5.095995903015137\n","Epoch 2/2000, Step 228, d_loss: 0.07112256437540054, g_loss: 2.131343364715576\n","Epoch 2/2000, Step 229, d_loss: 0.023112479597330093, g_loss: 3.386241912841797\n","Epoch 2/2000, Step 230, d_loss: 0.15745431184768677, g_loss: 2.079214334487915\n","Epoch 2/2000, Step 231, d_loss: 0.21913540363311768, g_loss: 4.911373138427734\n","Epoch 2/2000, Step 232, d_loss: 0.01007283665239811, g_loss: 4.528235912322998\n","Epoch 2/2000, Step 233, d_loss: 0.06924298405647278, g_loss: 3.224865198135376\n","Epoch 2/2000, Step 234, d_loss: 0.3197695016860962, g_loss: 6.044521331787109\n","Epoch 2/2000, Step 235, d_loss: 0.03558892756700516, g_loss: 2.639540672302246\n","Epoch 2/2000, Step 236, d_loss: 0.07595200091600418, g_loss: 2.8602023124694824\n","Epoch 2/2000, Step 237, d_loss: 0.1812838613986969, g_loss: 5.026188850402832\n","Epoch 2/2000, Step 238, d_loss: 0.04531455039978027, g_loss: 4.6488494873046875\n","Epoch 2/2000, Step 239, d_loss: 0.05990699678659439, g_loss: 4.688510894775391\n","Epoch 2/2000, Step 240, d_loss: 0.17311175167560577, g_loss: 4.964653491973877\n","Epoch 2/2000, Step 241, d_loss: 0.06288747489452362, g_loss: 5.483138084411621\n","Epoch 2/2000, Step 242, d_loss: 0.02914539910852909, g_loss: 8.643638610839844\n","Epoch 2/2000, Step 243, d_loss: 0.11744478344917297, g_loss: 7.978084564208984\n","Epoch 2/2000, Step 244, d_loss: 0.09263437241315842, g_loss: 4.903295040130615\n","Epoch 2/2000, Step 245, d_loss: 0.017884811386466026, g_loss: 7.47542142868042\n","Epoch 2/2000, Step 246, d_loss: 0.05554484575986862, g_loss: 3.5429930686950684\n","Epoch 2/2000, Step 247, d_loss: 0.012020958587527275, g_loss: 2.7620434761047363\n","Epoch 2/2000, Step 248, d_loss: 0.020959822461009026, g_loss: 3.4591734409332275\n","Epoch 2/2000, Step 249, d_loss: 0.019638782367110252, g_loss: 3.5714452266693115\n","Epoch 2/2000, Step 250, d_loss: 0.018725227564573288, g_loss: 2.9604923725128174\n","Epoch 2/2000, Step 251, d_loss: 0.18936896324157715, g_loss: 3.0722978115081787\n","Epoch 2/2000, Step 252, d_loss: 0.024380262941122055, g_loss: 4.213548183441162\n","Epoch 2/2000, Step 253, d_loss: 0.14812102913856506, g_loss: 6.34964656829834\n","Epoch 2/2000, Step 254, d_loss: 0.014804918318986893, g_loss: 4.4546661376953125\n","Epoch 2/2000, Step 255, d_loss: 0.029681432992219925, g_loss: 6.2139081954956055\n","Epoch 2/2000, Step 256, d_loss: 0.023082565516233444, g_loss: 5.870017051696777\n","Epoch 2/2000, Step 257, d_loss: 0.05738458037376404, g_loss: 6.408149242401123\n","Epoch 2/2000, Step 258, d_loss: 0.010042451322078705, g_loss: 3.831693649291992\n","Epoch 2/2000, Step 259, d_loss: 0.03129378333687782, g_loss: 3.3214259147644043\n","Epoch 2/2000, Step 260, d_loss: 0.05821557715535164, g_loss: 6.452315330505371\n","Epoch 2/2000, Step 261, d_loss: 0.012634670361876488, g_loss: 6.064632892608643\n","Epoch 2/2000, Step 262, d_loss: 0.05727280303835869, g_loss: 2.658000946044922\n","Epoch 2/2000, Step 263, d_loss: 0.029517142102122307, g_loss: 5.230601787567139\n","Epoch 2/2000, Step 264, d_loss: 0.054452791810035706, g_loss: 3.8033149242401123\n","Epoch 2/2000, Step 265, d_loss: 0.03476783633232117, g_loss: 1.0918587446212769\n","Epoch 2/2000, Step 266, d_loss: 0.06573785096406937, g_loss: 4.090038776397705\n","Epoch 2/2000, Step 267, d_loss: 0.15190620720386505, g_loss: 2.8058815002441406\n","Epoch 2/2000, Step 268, d_loss: 0.20617268979549408, g_loss: 5.234248161315918\n","Epoch 2/2000, Step 269, d_loss: 0.017327511683106422, g_loss: 7.535778045654297\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 10\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 1\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 128\n","\n","# Size of feature maps in discriminator\n","ndf = 24\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='G:/My Drive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","                               transform=transforms.Compose([\n","                                   transforms.Grayscale(num_output_channels=1), # Add this line\n","                                   transforms.Resize(image_size),\n","                                   transforms.CenterCrop(image_size),\n","                                   transforms.ToTensor(),\n","                                   transforms.Normalize((0.5,), (0.5,))\n","                               ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # input is ``(nc) x 64 x 64``\n","            nn.Conv2d(1, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Conv2d(ndf , ndf * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(ndf *2 , ndf * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 4, 1, 3, 1, 0, bias=False),\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0018)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='G:/My Drive/Colab Notebooks/animals/checkpointWLL.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","\n","\n","            save_image(samples, f'G:/My Drive/Colab Notebooks/dandies/new_dandies/odludL_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((bs,), 1.0))\n","\n","\n","\n","\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images).view(-1).squeeze()\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointWLL.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(2000, dataloader, resume=False)"]},{"cell_type":"code","source":["pip install py"],"metadata":{"id":"d5PEzb2hfyfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7TZvDVQTtN6","executionInfo":{"status":"ok","timestamp":1690605787603,"user_tz":240,"elapsed":19,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"}},"outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1GN2L9UPYR6TCzDsqMBaHwwc6PkLlmsyg","authorship_tag":"ABX9TyMVFN46Btsk0Y+1kwx2wDuk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}