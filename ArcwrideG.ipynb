{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"10552912-916f-4568-ca9d-76bf7c38fef1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 48/2000, Step 3, d_loss: 0.3969104588031769, g_loss: 4.103240966796875\n","Epoch 48/2000, Step 4, d_loss: 0.4237571358680725, g_loss: 4.058638095855713\n","Epoch 48/2000, Step 5, d_loss: 0.4267755150794983, g_loss: 2.2584969997406006\n","Epoch 48/2000, Step 6, d_loss: 0.3863104581832886, g_loss: 2.637866258621216\n","Epoch 48/2000, Step 7, d_loss: 0.43827709555625916, g_loss: 4.036584377288818\n","Epoch 48/2000, Step 8, d_loss: 0.481548547744751, g_loss: 4.621013164520264\n","Epoch 48/2000, Step 9, d_loss: 0.3599863350391388, g_loss: 4.396162033081055\n","Epoch 48/2000, Step 10, d_loss: 0.36930081248283386, g_loss: 3.7496256828308105\n","Epoch 48/2000, Step 11, d_loss: 0.730631947517395, g_loss: 3.7632176876068115\n","Epoch 48/2000, Step 12, d_loss: 0.42021945118904114, g_loss: 2.5561985969543457\n","Epoch 48/2000, Step 13, d_loss: 0.46319302916526794, g_loss: 2.9737234115600586\n","Epoch 48/2000, Step 14, d_loss: 0.47272369265556335, g_loss: 1.8006054162979126\n","Epoch 48/2000, Step 15, d_loss: 0.705873966217041, g_loss: 3.231400489807129\n","Epoch 48/2000, Step 16, d_loss: 0.48355865478515625, g_loss: 3.1183242797851562\n","Epoch 48/2000, Step 17, d_loss: 0.39095500111579895, g_loss: 4.171873569488525\n","Epoch 48/2000, Step 18, d_loss: 0.5328169465065002, g_loss: 3.274991512298584\n","Epoch 48/2000, Step 19, d_loss: 0.5687314867973328, g_loss: 3.04097580909729\n","Epoch 48/2000, Step 20, d_loss: 0.443066269159317, g_loss: 3.2337825298309326\n","Epoch 48/2000, Step 21, d_loss: 0.4660071134567261, g_loss: 2.741398572921753\n","Epoch 48/2000, Step 22, d_loss: 0.4105335772037506, g_loss: 3.9551503658294678\n","Epoch 48/2000, Step 23, d_loss: 0.4694453477859497, g_loss: 3.540668249130249\n","Epoch 48/2000, Step 24, d_loss: 0.5093772411346436, g_loss: 3.8222272396087646\n","Epoch 48/2000, Step 25, d_loss: 0.42880958318710327, g_loss: 3.2096734046936035\n","Epoch 48/2000, Step 26, d_loss: 0.47747308015823364, g_loss: 4.454028129577637\n","Epoch 48/2000, Step 27, d_loss: 0.50972980260849, g_loss: 3.860455274581909\n","Epoch 48/2000, Step 28, d_loss: 0.4607203006744385, g_loss: 2.8209800720214844\n","Epoch 48/2000, Step 29, d_loss: 0.42014095187187195, g_loss: 2.852830171585083\n","Epoch 48/2000, Step 30, d_loss: 0.45102083683013916, g_loss: 2.930938959121704\n","Epoch 48/2000, Step 31, d_loss: 0.4043254554271698, g_loss: 1.835891604423523\n","Epoch 48/2000, Step 32, d_loss: 0.41476577520370483, g_loss: 2.5840187072753906\n","Epoch 48/2000, Step 33, d_loss: 0.7888830900192261, g_loss: 3.5139782428741455\n","Epoch 48/2000, Step 34, d_loss: 0.41562947630882263, g_loss: 4.054419994354248\n","Epoch 48/2000, Step 35, d_loss: 0.5861639380455017, g_loss: 3.854034423828125\n","Epoch 48/2000, Step 36, d_loss: 0.4241991341114044, g_loss: 3.606189250946045\n","Epoch 48/2000, Step 37, d_loss: 0.3982062339782715, g_loss: 3.352198600769043\n","Epoch 48/2000, Step 38, d_loss: 0.4146099388599396, g_loss: 3.6266777515411377\n","Epoch 48/2000, Step 39, d_loss: 0.41733407974243164, g_loss: 3.094089984893799\n","Epoch 48/2000, Step 40, d_loss: 0.4449479877948761, g_loss: 3.8444888591766357\n","Epoch 48/2000, Step 41, d_loss: 0.45096370577812195, g_loss: 4.83352518081665\n","Epoch 48/2000, Step 42, d_loss: 0.4357980787754059, g_loss: 2.2024521827697754\n","Epoch 48/2000, Step 43, d_loss: 0.3829532265663147, g_loss: 3.2493762969970703\n","Epoch 48/2000, Step 44, d_loss: 0.41044700145721436, g_loss: 2.976207971572876\n","Epoch 48/2000, Step 45, d_loss: 0.46769654750823975, g_loss: 4.539912223815918\n","Epoch 48/2000, Step 46, d_loss: 0.3598427176475525, g_loss: 3.614056348800659\n","Epoch 48/2000, Step 47, d_loss: 0.40366488695144653, g_loss: 3.4106006622314453\n","Epoch 48/2000, Step 48, d_loss: 0.5407648086547852, g_loss: 2.9008469581604004\n","Epoch 48/2000, Step 49, d_loss: 0.41741493344306946, g_loss: 2.6185367107391357\n","Epoch 48/2000, Step 50, d_loss: 0.37643375992774963, g_loss: 2.272198438644409\n","Epoch 48/2000, Step 51, d_loss: 0.36655551195144653, g_loss: 4.734400272369385\n","Epoch 48/2000, Step 52, d_loss: 0.4501970410346985, g_loss: 3.0552756786346436\n","Epoch 48/2000, Step 53, d_loss: 0.417339563369751, g_loss: 3.501737356185913\n","Epoch 48/2000, Step 54, d_loss: 0.42117229104042053, g_loss: 4.71856164932251\n","Epoch 48/2000, Step 55, d_loss: 0.4474440813064575, g_loss: 4.002163887023926\n","Epoch 48/2000, Step 56, d_loss: 0.43616920709609985, g_loss: 4.485321998596191\n","Epoch 48/2000, Step 57, d_loss: 0.49337953329086304, g_loss: 3.495694875717163\n","Epoch 48/2000, Step 58, d_loss: 0.4296410381793976, g_loss: 3.637273073196411\n","Epoch 48/2000, Step 59, d_loss: 0.38152918219566345, g_loss: 3.3194549083709717\n","Epoch 48/2000, Step 60, d_loss: 0.4276626706123352, g_loss: 2.7486612796783447\n","Epoch 48/2000, Step 61, d_loss: 0.38047122955322266, g_loss: 4.211162090301514\n","Epoch 48/2000, Step 62, d_loss: 0.4132445454597473, g_loss: 3.430839776992798\n","Epoch 48/2000, Step 63, d_loss: 0.3702898919582367, g_loss: 2.40018367767334\n","Epoch 48/2000, Step 64, d_loss: 0.48636189103126526, g_loss: 2.0354385375976562\n","Epoch 48/2000, Step 65, d_loss: 0.5115343332290649, g_loss: 4.210312843322754\n","Epoch 48/2000, Step 66, d_loss: 0.4033042788505554, g_loss: 4.223241806030273\n","Epoch 48/2000, Step 67, d_loss: 0.43226978182792664, g_loss: 4.73918342590332\n","Epoch 48/2000, Step 68, d_loss: 0.383304625749588, g_loss: 3.14551043510437\n","Epoch 48/2000, Step 69, d_loss: 0.4568571448326111, g_loss: 5.277475357055664\n","Epoch 48/2000, Step 70, d_loss: 0.44543105363845825, g_loss: 2.218524932861328\n","Epoch 48/2000, Step 71, d_loss: 0.5253002643585205, g_loss: 2.9567747116088867\n","Epoch 48/2000, Step 72, d_loss: 0.5151419639587402, g_loss: 2.483355760574341\n","Epoch 48/2000, Step 73, d_loss: 0.38607460260391235, g_loss: 3.5497052669525146\n","Epoch 48/2000, Step 74, d_loss: 0.5763489603996277, g_loss: 3.0935728549957275\n","Epoch 48/2000, Step 75, d_loss: 0.4052163362503052, g_loss: 3.3311400413513184\n","Epoch 48/2000, Step 76, d_loss: 0.4684242010116577, g_loss: 3.470900058746338\n","Epoch 48/2000, Step 77, d_loss: 0.3934782147407532, g_loss: 3.900092363357544\n","Epoch 48/2000, Step 78, d_loss: 0.6208585500717163, g_loss: 3.322918653488159\n","Epoch 48/2000, Step 79, d_loss: 0.4540705680847168, g_loss: 3.17292857170105\n","Epoch 48/2000, Step 80, d_loss: 0.5418912768363953, g_loss: 3.436347007751465\n","Epoch 48/2000, Step 81, d_loss: 0.4479098618030548, g_loss: 2.753821849822998\n","Epoch 48/2000, Step 82, d_loss: 0.50118488073349, g_loss: 2.463789939880371\n","Epoch 48/2000, Step 83, d_loss: 0.4879453480243683, g_loss: 2.325960397720337\n","Epoch 48/2000, Step 84, d_loss: 0.5118790864944458, g_loss: 3.3162736892700195\n","Epoch 48/2000, Step 85, d_loss: 0.5318192839622498, g_loss: 3.5602636337280273\n","Epoch 48/2000, Step 86, d_loss: 0.35780102014541626, g_loss: 3.0405900478363037\n","Epoch 48/2000, Step 87, d_loss: 0.46762919425964355, g_loss: 4.0053629875183105\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 49/2000, Step 1, d_loss: 0.5553540587425232, g_loss: 3.0857367515563965\n","Epoch 49/2000, Step 2, d_loss: 0.37526166439056396, g_loss: 3.6519813537597656\n","Epoch 49/2000, Step 3, d_loss: 0.39911267161369324, g_loss: 3.2833588123321533\n","Epoch 49/2000, Step 4, d_loss: 0.42140135169029236, g_loss: 2.1697194576263428\n","Epoch 49/2000, Step 5, d_loss: 0.5326370000839233, g_loss: 2.6192433834075928\n","Epoch 49/2000, Step 6, d_loss: 0.3873276114463806, g_loss: 4.159279823303223\n","Epoch 49/2000, Step 7, d_loss: 0.43390175700187683, g_loss: 2.4798197746276855\n","Epoch 49/2000, Step 8, d_loss: 0.4387027621269226, g_loss: 3.890550374984741\n","Epoch 49/2000, Step 9, d_loss: 0.40716299414634705, g_loss: 3.9473705291748047\n","Epoch 49/2000, Step 10, d_loss: 0.44032952189445496, g_loss: 2.7427523136138916\n","Epoch 49/2000, Step 11, d_loss: 0.42009440064430237, g_loss: 6.112605571746826\n","Epoch 49/2000, Step 12, d_loss: 0.5127522945404053, g_loss: 3.1983590126037598\n","Epoch 49/2000, Step 13, d_loss: 0.4183165729045868, g_loss: 4.696568965911865\n","Epoch 49/2000, Step 14, d_loss: 0.43584883213043213, g_loss: 1.7065390348434448\n","Epoch 49/2000, Step 15, d_loss: 0.392599880695343, g_loss: 1.8106352090835571\n","Epoch 49/2000, Step 16, d_loss: 0.6236698627471924, g_loss: 2.6986913681030273\n","Epoch 49/2000, Step 17, d_loss: 0.41373562812805176, g_loss: 3.156723737716675\n","Epoch 49/2000, Step 18, d_loss: 0.4256018102169037, g_loss: 4.157031059265137\n","Epoch 49/2000, Step 19, d_loss: 0.3575420677661896, g_loss: 4.977324962615967\n","Epoch 49/2000, Step 20, d_loss: 0.6758692264556885, g_loss: 4.144070148468018\n","Epoch 49/2000, Step 21, d_loss: 0.5920580625534058, g_loss: 3.1669793128967285\n","Epoch 49/2000, Step 22, d_loss: 0.3962152898311615, g_loss: 2.4364402294158936\n","Epoch 49/2000, Step 23, d_loss: 0.4207618534564972, g_loss: 1.5491045713424683\n","Epoch 49/2000, Step 24, d_loss: 0.44496461749076843, g_loss: 2.2133238315582275\n","Epoch 49/2000, Step 25, d_loss: 0.39702078700065613, g_loss: 1.8835879564285278\n","Epoch 49/2000, Step 26, d_loss: 0.5326656103134155, g_loss: 2.7431557178497314\n","Epoch 49/2000, Step 27, d_loss: 0.6716675162315369, g_loss: 2.642709493637085\n","Epoch 49/2000, Step 28, d_loss: 0.41589829325675964, g_loss: 2.9143152236938477\n","Epoch 49/2000, Step 29, d_loss: 0.47274112701416016, g_loss: 4.117354869842529\n","Epoch 49/2000, Step 30, d_loss: 0.49670639634132385, g_loss: 4.292476177215576\n","Epoch 49/2000, Step 31, d_loss: 0.47561028599739075, g_loss: 4.326406002044678\n","Epoch 49/2000, Step 32, d_loss: 0.3821031451225281, g_loss: 3.179917335510254\n","Epoch 49/2000, Step 33, d_loss: 0.4228304624557495, g_loss: 2.50040602684021\n","Epoch 49/2000, Step 34, d_loss: 0.3830358386039734, g_loss: 1.8814321756362915\n","Epoch 49/2000, Step 35, d_loss: 0.42393916845321655, g_loss: 1.988673448562622\n","Epoch 49/2000, Step 36, d_loss: 0.5961074829101562, g_loss: 2.5498480796813965\n","Epoch 49/2000, Step 37, d_loss: 0.5235018730163574, g_loss: 4.100652694702148\n","Epoch 49/2000, Step 38, d_loss: 0.46886691451072693, g_loss: 3.66833233833313\n","Epoch 49/2000, Step 39, d_loss: 0.44047167897224426, g_loss: 3.8001580238342285\n","Epoch 49/2000, Step 40, d_loss: 0.6625205278396606, g_loss: 3.1671340465545654\n","Epoch 49/2000, Step 41, d_loss: 0.4501134753227234, g_loss: 2.492211103439331\n","Epoch 49/2000, Step 42, d_loss: 0.43153461813926697, g_loss: 2.0672175884246826\n","Epoch 49/2000, Step 43, d_loss: 0.4712802767753601, g_loss: 2.6681346893310547\n","Epoch 49/2000, Step 44, d_loss: 0.47273337841033936, g_loss: 2.1177499294281006\n","Epoch 49/2000, Step 45, d_loss: 0.461138516664505, g_loss: 3.711911916732788\n","Epoch 49/2000, Step 46, d_loss: 0.4022214412689209, g_loss: 2.5418684482574463\n","Epoch 49/2000, Step 47, d_loss: 0.41743844747543335, g_loss: 4.8845038414001465\n","Epoch 49/2000, Step 48, d_loss: 0.41534268856048584, g_loss: 4.410698890686035\n","Epoch 49/2000, Step 49, d_loss: 0.5607403516769409, g_loss: 3.9653372764587402\n","Epoch 49/2000, Step 50, d_loss: 0.4177754819393158, g_loss: 4.232417583465576\n","Epoch 49/2000, Step 51, d_loss: 0.462445467710495, g_loss: 3.6367175579071045\n","Epoch 49/2000, Step 52, d_loss: 0.4751870036125183, g_loss: 3.877774477005005\n","Epoch 49/2000, Step 53, d_loss: 0.42975664138793945, g_loss: 2.5086710453033447\n","Epoch 49/2000, Step 54, d_loss: 0.4945339858531952, g_loss: 2.760619878768921\n","Epoch 49/2000, Step 55, d_loss: 0.4814980626106262, g_loss: 3.9233920574188232\n","Epoch 49/2000, Step 56, d_loss: 0.4785746932029724, g_loss: 3.8387134075164795\n","Epoch 49/2000, Step 57, d_loss: 0.500036895275116, g_loss: 3.418031930923462\n","Epoch 49/2000, Step 58, d_loss: 0.38406941294670105, g_loss: 2.7232213020324707\n","Epoch 49/2000, Step 59, d_loss: 0.4357215166091919, g_loss: 2.8143532276153564\n","Epoch 49/2000, Step 60, d_loss: 0.4694574475288391, g_loss: 3.291390895843506\n","Epoch 49/2000, Step 61, d_loss: 0.46749523282051086, g_loss: 4.394615650177002\n","Epoch 49/2000, Step 62, d_loss: 0.4483421742916107, g_loss: 2.5630104541778564\n","Epoch 49/2000, Step 63, d_loss: 0.4396199584007263, g_loss: 3.5055270195007324\n","Epoch 49/2000, Step 64, d_loss: 0.36738088726997375, g_loss: 3.4000260829925537\n","Epoch 49/2000, Step 65, d_loss: 0.39283427596092224, g_loss: 4.061389923095703\n","Epoch 49/2000, Step 66, d_loss: 0.3835951089859009, g_loss: 2.383190393447876\n","Epoch 49/2000, Step 67, d_loss: 0.4522803723812103, g_loss: 2.797943592071533\n","Epoch 49/2000, Step 68, d_loss: 0.3738470673561096, g_loss: 3.288273572921753\n","Epoch 49/2000, Step 69, d_loss: 0.4545798897743225, g_loss: 3.352128744125366\n","Epoch 49/2000, Step 70, d_loss: 0.37472468614578247, g_loss: 5.725973606109619\n","Epoch 49/2000, Step 71, d_loss: 0.5013068318367004, g_loss: 4.261375427246094\n","Epoch 49/2000, Step 72, d_loss: 0.37135791778564453, g_loss: 4.456530570983887\n","Epoch 49/2000, Step 73, d_loss: 0.38970106840133667, g_loss: 4.2198591232299805\n","Epoch 49/2000, Step 74, d_loss: 0.3925086557865143, g_loss: 2.2388973236083984\n","Epoch 49/2000, Step 75, d_loss: 0.4123994708061218, g_loss: 2.7561464309692383\n","Epoch 49/2000, Step 76, d_loss: 0.3787012994289398, g_loss: 3.7547249794006348\n","Epoch 49/2000, Step 77, d_loss: 0.39516082406044006, g_loss: 2.367114305496216\n","Epoch 49/2000, Step 78, d_loss: 0.49024608731269836, g_loss: 3.519214391708374\n","Epoch 49/2000, Step 79, d_loss: 0.4909398853778839, g_loss: 1.9829144477844238\n","Epoch 49/2000, Step 80, d_loss: 0.4148019850254059, g_loss: 3.2126593589782715\n","Epoch 49/2000, Step 81, d_loss: 0.4284514784812927, g_loss: 3.6383776664733887\n","Epoch 49/2000, Step 82, d_loss: 0.3787194490432739, g_loss: 3.5724971294403076\n","Epoch 49/2000, Step 83, d_loss: 0.41160157322883606, g_loss: 4.502043724060059\n","Epoch 49/2000, Step 84, d_loss: 0.350111186504364, g_loss: 6.408327579498291\n","Epoch 49/2000, Step 85, d_loss: 0.48250147700309753, g_loss: 4.69223690032959\n","Epoch 49/2000, Step 86, d_loss: 0.647916853427887, g_loss: 2.566636562347412\n","Epoch 49/2000, Step 87, d_loss: 0.44361183047294617, g_loss: 2.0466420650482178\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 50/2000, Step 1, d_loss: 0.43778449296951294, g_loss: 2.76192569732666\n","Epoch 50/2000, Step 2, d_loss: 0.4562041163444519, g_loss: 2.6386313438415527\n","Epoch 50/2000, Step 3, d_loss: 0.6575129628181458, g_loss: 2.6419684886932373\n","Epoch 50/2000, Step 4, d_loss: 0.41764119267463684, g_loss: 5.244015693664551\n","Epoch 50/2000, Step 5, d_loss: 0.4355280101299286, g_loss: 4.47609281539917\n","Epoch 50/2000, Step 6, d_loss: 0.3874320089817047, g_loss: 4.78337287902832\n","Epoch 50/2000, Step 7, d_loss: 0.4112837016582489, g_loss: 3.5320332050323486\n","Epoch 50/2000, Step 8, d_loss: 0.5120441913604736, g_loss: 3.923513650894165\n","Epoch 50/2000, Step 9, d_loss: 0.448106974363327, g_loss: 2.77170729637146\n","Epoch 50/2000, Step 10, d_loss: 0.4617190361022949, g_loss: 2.9111201763153076\n","Epoch 50/2000, Step 11, d_loss: 0.3984543979167938, g_loss: 2.151648998260498\n","Epoch 50/2000, Step 12, d_loss: 0.546065628528595, g_loss: 2.504185914993286\n","Epoch 50/2000, Step 13, d_loss: 0.4747874140739441, g_loss: 3.9503061771392822\n","Epoch 50/2000, Step 14, d_loss: 0.6436915397644043, g_loss: 3.443671464920044\n","Epoch 50/2000, Step 15, d_loss: 0.44203174114227295, g_loss: 4.390742301940918\n","Epoch 50/2000, Step 16, d_loss: 0.5775862336158752, g_loss: 3.9071311950683594\n","Epoch 50/2000, Step 17, d_loss: 0.5504290461540222, g_loss: 4.659588813781738\n","Epoch 50/2000, Step 18, d_loss: 0.5220692753791809, g_loss: 3.2653112411499023\n","Epoch 50/2000, Step 19, d_loss: 0.4946955740451813, g_loss: 2.603045701980591\n","Epoch 50/2000, Step 20, d_loss: 0.46788978576660156, g_loss: 1.6245633363723755\n","Epoch 50/2000, Step 21, d_loss: 0.4888506531715393, g_loss: 2.639801025390625\n","Epoch 50/2000, Step 22, d_loss: 0.45840197801589966, g_loss: 3.7739176750183105\n","Epoch 50/2000, Step 23, d_loss: 0.40467146039009094, g_loss: 4.724344730377197\n","Epoch 50/2000, Step 24, d_loss: 0.39673012495040894, g_loss: 3.8115744590759277\n","Epoch 50/2000, Step 25, d_loss: 0.541540801525116, g_loss: 3.660163164138794\n","Epoch 50/2000, Step 26, d_loss: 0.6158695220947266, g_loss: 2.270538568496704\n","Epoch 50/2000, Step 27, d_loss: 0.39593833684921265, g_loss: 2.1841399669647217\n","Epoch 50/2000, Step 28, d_loss: 0.5251649618148804, g_loss: 2.067020893096924\n","Epoch 50/2000, Step 29, d_loss: 0.4038952887058258, g_loss: 2.2198684215545654\n","Epoch 50/2000, Step 30, d_loss: 0.580244779586792, g_loss: 3.1582798957824707\n","Epoch 50/2000, Step 31, d_loss: 0.422948956489563, g_loss: 4.303161144256592\n","Epoch 50/2000, Step 32, d_loss: 0.4810260236263275, g_loss: 4.048386096954346\n","Epoch 50/2000, Step 33, d_loss: 0.5879412889480591, g_loss: 4.430458068847656\n","Epoch 50/2000, Step 34, d_loss: 0.4044288396835327, g_loss: 3.4844586849212646\n","Epoch 50/2000, Step 35, d_loss: 0.38945889472961426, g_loss: 2.9773459434509277\n","Epoch 50/2000, Step 36, d_loss: 0.49269965291023254, g_loss: 2.313556432723999\n","Epoch 50/2000, Step 37, d_loss: 0.4196520149707794, g_loss: 2.2584924697875977\n","Epoch 50/2000, Step 38, d_loss: 0.45881783962249756, g_loss: 2.8597490787506104\n","Epoch 50/2000, Step 39, d_loss: 0.49404141306877136, g_loss: 4.280243396759033\n","Epoch 50/2000, Step 40, d_loss: 0.4388938248157501, g_loss: 1.9085938930511475\n","Epoch 50/2000, Step 41, d_loss: 0.44766780734062195, g_loss: 3.3980579376220703\n","Epoch 50/2000, Step 42, d_loss: 0.42858657240867615, g_loss: 3.135181427001953\n","Epoch 50/2000, Step 43, d_loss: 0.42757707834243774, g_loss: 2.696207046508789\n","Epoch 50/2000, Step 44, d_loss: 0.38453516364097595, g_loss: 1.6728523969650269\n","Epoch 50/2000, Step 45, d_loss: 0.43586868047714233, g_loss: 2.653496503829956\n","Epoch 50/2000, Step 46, d_loss: 0.41176411509513855, g_loss: 2.4915547370910645\n","Epoch 50/2000, Step 47, d_loss: 0.39614805579185486, g_loss: 2.762296199798584\n","Epoch 50/2000, Step 48, d_loss: 0.5467768907546997, g_loss: 2.50807523727417\n","Epoch 50/2000, Step 49, d_loss: 0.41939929127693176, g_loss: 3.787484645843506\n","Epoch 50/2000, Step 50, d_loss: 0.5163451433181763, g_loss: 2.788973569869995\n","Epoch 50/2000, Step 51, d_loss: 0.4807407557964325, g_loss: 3.903024911880493\n","Epoch 50/2000, Step 52, d_loss: 0.4032719135284424, g_loss: 2.875610828399658\n","Epoch 50/2000, Step 53, d_loss: 0.42629075050354004, g_loss: 2.746342420578003\n","Epoch 50/2000, Step 54, d_loss: 0.6485114693641663, g_loss: 3.2659261226654053\n","Epoch 50/2000, Step 55, d_loss: 0.4860039949417114, g_loss: 4.487544536590576\n","Epoch 50/2000, Step 56, d_loss: 0.49244970083236694, g_loss: 2.751749038696289\n","Epoch 50/2000, Step 57, d_loss: 0.4545830488204956, g_loss: 4.031911849975586\n","Epoch 50/2000, Step 58, d_loss: 0.39284130930900574, g_loss: 4.033478260040283\n","Epoch 50/2000, Step 59, d_loss: 0.5812610387802124, g_loss: 3.2953402996063232\n","Epoch 50/2000, Step 60, d_loss: 0.4973256587982178, g_loss: 4.048170566558838\n","Epoch 50/2000, Step 61, d_loss: 0.4440729022026062, g_loss: 2.8025147914886475\n","Epoch 50/2000, Step 62, d_loss: 0.6497578024864197, g_loss: 3.848468780517578\n","Epoch 50/2000, Step 63, d_loss: 0.43218156695365906, g_loss: 2.5244767665863037\n","Epoch 50/2000, Step 64, d_loss: 0.451405793428421, g_loss: 3.0454463958740234\n","Epoch 50/2000, Step 65, d_loss: 0.46626952290534973, g_loss: 3.648115396499634\n","Epoch 50/2000, Step 66, d_loss: 0.42525240778923035, g_loss: 3.8663270473480225\n","Epoch 50/2000, Step 67, d_loss: 0.5827422738075256, g_loss: 2.898374319076538\n","Epoch 50/2000, Step 68, d_loss: 0.41133591532707214, g_loss: 4.580004692077637\n","Epoch 50/2000, Step 69, d_loss: 0.4716484546661377, g_loss: 3.7884061336517334\n","Epoch 50/2000, Step 70, d_loss: 0.37503567337989807, g_loss: 2.7123351097106934\n","Epoch 50/2000, Step 71, d_loss: 0.42499592900276184, g_loss: 2.388624668121338\n","Epoch 50/2000, Step 72, d_loss: 0.5434046983718872, g_loss: 2.8379969596862793\n","Epoch 50/2000, Step 73, d_loss: 0.4711698889732361, g_loss: 3.4399733543395996\n","Epoch 50/2000, Step 74, d_loss: 0.452496200799942, g_loss: 4.143128871917725\n","Epoch 50/2000, Step 75, d_loss: 0.47841835021972656, g_loss: 3.4152376651763916\n","Epoch 50/2000, Step 76, d_loss: 0.36674216389656067, g_loss: 3.3232967853546143\n","Epoch 50/2000, Step 77, d_loss: 0.39525434374809265, g_loss: 4.4691057205200195\n","Epoch 50/2000, Step 78, d_loss: 0.5175493359565735, g_loss: 3.6236732006073\n","Epoch 50/2000, Step 79, d_loss: 0.48243898153305054, g_loss: 3.135221481323242\n","Epoch 50/2000, Step 80, d_loss: 0.3699468672275543, g_loss: 2.456739664077759\n","Epoch 50/2000, Step 81, d_loss: 0.41068241000175476, g_loss: 2.324352502822876\n","Epoch 50/2000, Step 82, d_loss: 0.5630601644515991, g_loss: 2.596217155456543\n","Epoch 50/2000, Step 83, d_loss: 0.43279173970222473, g_loss: 2.8642637729644775\n","Epoch 50/2000, Step 84, d_loss: 0.4454854726791382, g_loss: 3.902763605117798\n","Epoch 50/2000, Step 85, d_loss: 0.37159818410873413, g_loss: 5.142403602600098\n","Epoch 50/2000, Step 86, d_loss: 0.39406633377075195, g_loss: 4.155490398406982\n","Epoch 50/2000, Step 87, d_loss: 0.5476046204566956, g_loss: 3.8407492637634277\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 51/2000, Step 1, d_loss: 0.4120999574661255, g_loss: 3.851499080657959\n","Epoch 51/2000, Step 2, d_loss: 0.3670984208583832, g_loss: 4.215062618255615\n","Epoch 51/2000, Step 3, d_loss: 0.386902391910553, g_loss: 2.7945892810821533\n","Epoch 51/2000, Step 4, d_loss: 0.47651079297065735, g_loss: 2.945629358291626\n","Epoch 51/2000, Step 5, d_loss: 0.36195942759513855, g_loss: 2.4775936603546143\n","Epoch 51/2000, Step 6, d_loss: 0.4650571346282959, g_loss: 2.783599615097046\n","Epoch 51/2000, Step 7, d_loss: 0.468685120344162, g_loss: 3.3062045574188232\n","Epoch 51/2000, Step 8, d_loss: 0.4564821422100067, g_loss: 2.890239953994751\n","Epoch 51/2000, Step 9, d_loss: 0.4584225118160248, g_loss: 2.5862538814544678\n","Epoch 51/2000, Step 10, d_loss: 0.3605503439903259, g_loss: 2.1851863861083984\n","Epoch 51/2000, Step 11, d_loss: 0.5392435193061829, g_loss: 2.4965498447418213\n","Epoch 51/2000, Step 12, d_loss: 0.48525530099868774, g_loss: 3.463710069656372\n","Epoch 51/2000, Step 13, d_loss: 0.41726455092430115, g_loss: 3.3214616775512695\n","Epoch 51/2000, Step 14, d_loss: 0.37505269050598145, g_loss: 3.8448169231414795\n","Epoch 51/2000, Step 15, d_loss: 0.4849371910095215, g_loss: 3.3222765922546387\n","Epoch 51/2000, Step 16, d_loss: 0.4170418679714203, g_loss: 3.4194552898406982\n","Epoch 51/2000, Step 17, d_loss: 0.43775704503059387, g_loss: 3.290463447570801\n","Epoch 51/2000, Step 18, d_loss: 0.5161281228065491, g_loss: 3.19111967086792\n","Epoch 51/2000, Step 19, d_loss: 0.5143111944198608, g_loss: 3.204054117202759\n","Epoch 51/2000, Step 20, d_loss: 0.4681384861469269, g_loss: 3.9580209255218506\n","Epoch 51/2000, Step 21, d_loss: 0.4056257903575897, g_loss: 3.632849931716919\n","Epoch 51/2000, Step 22, d_loss: 0.4988980293273926, g_loss: 2.78039813041687\n","Epoch 51/2000, Step 23, d_loss: 0.5761002898216248, g_loss: 4.077590465545654\n","Epoch 51/2000, Step 24, d_loss: 0.5558745265007019, g_loss: 3.9850244522094727\n","Epoch 51/2000, Step 25, d_loss: 0.4856579601764679, g_loss: 4.066528797149658\n","Epoch 51/2000, Step 26, d_loss: 0.45041945576667786, g_loss: 4.685460090637207\n","Epoch 51/2000, Step 27, d_loss: 0.5393117666244507, g_loss: 3.8477237224578857\n","Epoch 51/2000, Step 28, d_loss: 0.6317881941795349, g_loss: 3.556823253631592\n","Epoch 51/2000, Step 29, d_loss: 0.3911088705062866, g_loss: 3.977444648742676\n","Epoch 51/2000, Step 30, d_loss: 0.428581565618515, g_loss: 2.498504400253296\n","Epoch 51/2000, Step 31, d_loss: 0.5241982936859131, g_loss: 1.936060905456543\n","Epoch 51/2000, Step 32, d_loss: 0.4467547833919525, g_loss: 2.500558376312256\n","Epoch 51/2000, Step 33, d_loss: 0.45234215259552, g_loss: 2.992766857147217\n","Epoch 51/2000, Step 34, d_loss: 0.4505571126937866, g_loss: 2.9244203567504883\n","Epoch 51/2000, Step 35, d_loss: 0.36972275376319885, g_loss: 4.760154724121094\n","Epoch 51/2000, Step 36, d_loss: 0.4008200764656067, g_loss: 3.7413010597229004\n","Epoch 51/2000, Step 37, d_loss: 0.4167092442512512, g_loss: 4.0016679763793945\n","Epoch 51/2000, Step 38, d_loss: 0.4203929007053375, g_loss: 3.6691975593566895\n","Epoch 51/2000, Step 39, d_loss: 0.4462719261646271, g_loss: 2.9291462898254395\n","Epoch 51/2000, Step 40, d_loss: 0.3838132917881012, g_loss: 3.699573040008545\n","Epoch 51/2000, Step 41, d_loss: 0.3790261447429657, g_loss: 4.550192832946777\n","Epoch 51/2000, Step 42, d_loss: 0.4284360408782959, g_loss: 3.0031542778015137\n","Epoch 51/2000, Step 43, d_loss: 0.40378326177597046, g_loss: 2.9791321754455566\n","Epoch 51/2000, Step 44, d_loss: 0.3888845443725586, g_loss: 2.325552225112915\n","Epoch 51/2000, Step 45, d_loss: 0.4124314785003662, g_loss: 3.7564303874969482\n","Epoch 51/2000, Step 46, d_loss: 0.46649688482284546, g_loss: 3.100140333175659\n","Epoch 51/2000, Step 47, d_loss: 0.41518256068229675, g_loss: 3.0137288570404053\n","Epoch 51/2000, Step 48, d_loss: 0.4088504910469055, g_loss: 4.596055030822754\n","Epoch 51/2000, Step 49, d_loss: 0.3704560399055481, g_loss: 4.265050888061523\n","Epoch 51/2000, Step 50, d_loss: 0.462126761674881, g_loss: 3.98947811126709\n","Epoch 51/2000, Step 51, d_loss: 0.45127320289611816, g_loss: 4.148611068725586\n","Epoch 51/2000, Step 52, d_loss: 0.43483442068099976, g_loss: 3.844546318054199\n","Epoch 51/2000, Step 53, d_loss: 0.4748462438583374, g_loss: 2.2575528621673584\n","Epoch 51/2000, Step 54, d_loss: 0.4711379110813141, g_loss: 2.836684465408325\n","Epoch 51/2000, Step 55, d_loss: 0.41011399030685425, g_loss: 1.991681456565857\n","Epoch 51/2000, Step 56, d_loss: 0.45992588996887207, g_loss: 2.4031121730804443\n","Epoch 51/2000, Step 57, d_loss: 0.4788472056388855, g_loss: 3.762927293777466\n","Epoch 51/2000, Step 58, d_loss: 0.43453919887542725, g_loss: 3.857821464538574\n","Epoch 51/2000, Step 59, d_loss: 0.37406888604164124, g_loss: 2.743913412094116\n","Epoch 51/2000, Step 60, d_loss: 0.39962175488471985, g_loss: 2.982071876525879\n","Epoch 51/2000, Step 61, d_loss: 0.41939857602119446, g_loss: 3.2511162757873535\n","Epoch 51/2000, Step 62, d_loss: 0.37920689582824707, g_loss: 5.609100341796875\n","Epoch 51/2000, Step 63, d_loss: 0.4993080198764801, g_loss: 5.9627790451049805\n","Epoch 51/2000, Step 64, d_loss: 0.42614060640335083, g_loss: 4.330470561981201\n","Epoch 51/2000, Step 65, d_loss: 0.40629953145980835, g_loss: 3.2323925495147705\n","Epoch 51/2000, Step 66, d_loss: 0.5569748282432556, g_loss: 2.717439651489258\n","Epoch 51/2000, Step 67, d_loss: 0.449327290058136, g_loss: 2.4388539791107178\n","Epoch 51/2000, Step 68, d_loss: 0.5216565132141113, g_loss: 2.5238256454467773\n","Epoch 51/2000, Step 69, d_loss: 0.5514843463897705, g_loss: 3.1833152770996094\n","Epoch 51/2000, Step 70, d_loss: 0.581344485282898, g_loss: 2.181307315826416\n","Epoch 51/2000, Step 71, d_loss: 0.44326913356781006, g_loss: 3.2721164226531982\n","Epoch 51/2000, Step 72, d_loss: 0.40257471799850464, g_loss: 4.169018268585205\n","Epoch 51/2000, Step 73, d_loss: 0.6572319865226746, g_loss: 4.114602565765381\n","Epoch 51/2000, Step 74, d_loss: 0.4070507287979126, g_loss: 3.9479737281799316\n","Epoch 51/2000, Step 75, d_loss: 0.42241472005844116, g_loss: 3.496042490005493\n","Epoch 51/2000, Step 76, d_loss: 0.4065142571926117, g_loss: 3.094472646713257\n","Epoch 51/2000, Step 77, d_loss: 0.5410811901092529, g_loss: 3.952483892440796\n","Epoch 51/2000, Step 78, d_loss: 0.4197070300579071, g_loss: 1.8821898698806763\n","Epoch 51/2000, Step 79, d_loss: 0.4656347632408142, g_loss: 2.479624032974243\n","Epoch 51/2000, Step 80, d_loss: 0.4936937987804413, g_loss: 2.681448221206665\n","Epoch 51/2000, Step 81, d_loss: 0.4572606086730957, g_loss: 2.2795722484588623\n","Epoch 51/2000, Step 82, d_loss: 0.46788135170936584, g_loss: 2.1250159740448\n","Epoch 51/2000, Step 83, d_loss: 0.41159170866012573, g_loss: 3.4915125370025635\n","Epoch 51/2000, Step 84, d_loss: 0.4590372145175934, g_loss: 2.409639358520508\n","Epoch 51/2000, Step 85, d_loss: 0.4849276542663574, g_loss: 2.7683956623077393\n","Epoch 51/2000, Step 86, d_loss: 0.4544682204723358, g_loss: 3.879403591156006\n","Epoch 51/2000, Step 87, d_loss: 0.43478578329086304, g_loss: 1.8232605457305908\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 52/2000, Step 1, d_loss: 0.4638054072856903, g_loss: 1.7345045804977417\n","Epoch 52/2000, Step 2, d_loss: 0.4863596558570862, g_loss: 2.994974374771118\n","Epoch 52/2000, Step 3, d_loss: 0.48274028301239014, g_loss: 2.9907383918762207\n","Epoch 52/2000, Step 4, d_loss: 0.46436870098114014, g_loss: 3.4257912635803223\n","Epoch 52/2000, Step 5, d_loss: 0.4974256157875061, g_loss: 3.64163875579834\n","Epoch 52/2000, Step 6, d_loss: 0.5163277983665466, g_loss: 4.333678722381592\n","Epoch 52/2000, Step 7, d_loss: 0.4072846472263336, g_loss: 2.943610429763794\n","Epoch 52/2000, Step 8, d_loss: 0.4147742986679077, g_loss: 4.205297470092773\n","Epoch 52/2000, Step 9, d_loss: 0.4815465211868286, g_loss: 4.924494743347168\n","Epoch 52/2000, Step 10, d_loss: 0.5691924691200256, g_loss: 3.9269919395446777\n","Epoch 52/2000, Step 11, d_loss: 0.4284103214740753, g_loss: 3.740689516067505\n","Epoch 52/2000, Step 12, d_loss: 0.3749549090862274, g_loss: 3.378941774368286\n","Epoch 52/2000, Step 13, d_loss: 0.4272850453853607, g_loss: 2.4689996242523193\n","Epoch 52/2000, Step 14, d_loss: 0.49872082471847534, g_loss: 3.377302408218384\n","Epoch 52/2000, Step 15, d_loss: 0.44245967268943787, g_loss: 2.996746301651001\n","Epoch 52/2000, Step 16, d_loss: 0.43147012591362, g_loss: 2.977210760116577\n","Epoch 52/2000, Step 17, d_loss: 0.5053589940071106, g_loss: 3.7245798110961914\n","Epoch 52/2000, Step 18, d_loss: 0.47097694873809814, g_loss: 3.1678175926208496\n","Epoch 52/2000, Step 19, d_loss: 0.5354087352752686, g_loss: 3.914236307144165\n","Epoch 52/2000, Step 20, d_loss: 0.36520496010780334, g_loss: 3.498811960220337\n","Epoch 52/2000, Step 21, d_loss: 0.4493619203567505, g_loss: 3.157566785812378\n","Epoch 52/2000, Step 22, d_loss: 0.44801944494247437, g_loss: 3.3510823249816895\n","Epoch 52/2000, Step 23, d_loss: 0.5279866456985474, g_loss: 4.216437816619873\n","Epoch 52/2000, Step 24, d_loss: 0.49372825026512146, g_loss: 3.1773550510406494\n","Epoch 52/2000, Step 25, d_loss: 0.4013131856918335, g_loss: 4.141965866088867\n","Epoch 52/2000, Step 26, d_loss: 0.4284040331840515, g_loss: 3.5074827671051025\n","Epoch 52/2000, Step 27, d_loss: 0.4928940236568451, g_loss: 5.458495616912842\n","Epoch 52/2000, Step 28, d_loss: 0.44943809509277344, g_loss: 3.2401974201202393\n","Epoch 52/2000, Step 29, d_loss: 0.3822671175003052, g_loss: 3.0073482990264893\n","Epoch 52/2000, Step 30, d_loss: 0.38518765568733215, g_loss: 4.7492876052856445\n","Epoch 52/2000, Step 31, d_loss: 0.43453294038772583, g_loss: 3.496758460998535\n","Epoch 52/2000, Step 32, d_loss: 0.4709426760673523, g_loss: 2.565516710281372\n","Epoch 52/2000, Step 33, d_loss: 0.45296651124954224, g_loss: 3.3832879066467285\n","Epoch 52/2000, Step 34, d_loss: 0.3918454945087433, g_loss: 1.974151372909546\n","Epoch 52/2000, Step 35, d_loss: 0.4396222233772278, g_loss: 3.301286458969116\n","Epoch 52/2000, Step 36, d_loss: 0.4609079957008362, g_loss: 2.720871925354004\n","Epoch 52/2000, Step 37, d_loss: 0.4095587432384491, g_loss: 2.8680686950683594\n","Epoch 52/2000, Step 38, d_loss: 0.4059987962245941, g_loss: 4.083332061767578\n","Epoch 52/2000, Step 39, d_loss: 0.39999061822891235, g_loss: 4.629493713378906\n","Epoch 52/2000, Step 40, d_loss: 0.3563402593135834, g_loss: 3.7949721813201904\n","Epoch 52/2000, Step 41, d_loss: 0.47896677255630493, g_loss: 3.3348560333251953\n","Epoch 52/2000, Step 42, d_loss: 0.7629252076148987, g_loss: 2.8774256706237793\n","Epoch 52/2000, Step 43, d_loss: 0.45937395095825195, g_loss: 2.3524491786956787\n","Epoch 52/2000, Step 44, d_loss: 0.3938519060611725, g_loss: 3.622292995452881\n","Epoch 52/2000, Step 45, d_loss: 0.4799923598766327, g_loss: 2.447803258895874\n","Epoch 52/2000, Step 46, d_loss: 0.46759942173957825, g_loss: 2.5591208934783936\n","Epoch 52/2000, Step 47, d_loss: 0.4086598753929138, g_loss: 4.37313175201416\n","Epoch 52/2000, Step 48, d_loss: 0.4452914595603943, g_loss: 3.069134473800659\n","Epoch 52/2000, Step 49, d_loss: 0.39265453815460205, g_loss: 3.5957837104797363\n","Epoch 52/2000, Step 50, d_loss: 0.4807921350002289, g_loss: 4.230372428894043\n","Epoch 52/2000, Step 51, d_loss: 0.5525687336921692, g_loss: 4.685819149017334\n","Epoch 52/2000, Step 52, d_loss: 0.38884347677230835, g_loss: 5.3789472579956055\n","Epoch 52/2000, Step 53, d_loss: 0.43558260798454285, g_loss: 3.3100194931030273\n","Epoch 52/2000, Step 54, d_loss: 0.41591712832450867, g_loss: 3.54838228225708\n","Epoch 52/2000, Step 55, d_loss: 0.36186930537223816, g_loss: 2.6563498973846436\n","Epoch 52/2000, Step 56, d_loss: 0.43850964307785034, g_loss: 3.241220235824585\n","Epoch 52/2000, Step 57, d_loss: 0.4598587155342102, g_loss: 4.207054615020752\n","Epoch 52/2000, Step 58, d_loss: 0.48125705122947693, g_loss: 2.676982879638672\n","Epoch 52/2000, Step 59, d_loss: 0.4053979814052582, g_loss: 2.2671587467193604\n","Epoch 52/2000, Step 60, d_loss: 0.39508575201034546, g_loss: 2.740976333618164\n","Epoch 52/2000, Step 61, d_loss: 0.37202203273773193, g_loss: 4.360787391662598\n","Epoch 52/2000, Step 62, d_loss: 0.4956260025501251, g_loss: 3.6788618564605713\n","Epoch 52/2000, Step 63, d_loss: 0.3905790150165558, g_loss: 3.675680637359619\n","Epoch 52/2000, Step 64, d_loss: 0.5188109874725342, g_loss: 3.012685537338257\n","Epoch 52/2000, Step 65, d_loss: 0.4899015426635742, g_loss: 2.633960485458374\n","Epoch 52/2000, Step 66, d_loss: 0.4130131006240845, g_loss: 3.3908064365386963\n","Epoch 52/2000, Step 67, d_loss: 0.46704602241516113, g_loss: 3.3200631141662598\n","Epoch 52/2000, Step 68, d_loss: 0.4366424083709717, g_loss: 4.584759712219238\n","Epoch 52/2000, Step 69, d_loss: 0.42714741826057434, g_loss: 3.551701068878174\n","Epoch 52/2000, Step 70, d_loss: 0.36423414945602417, g_loss: 4.264560222625732\n","Epoch 52/2000, Step 71, d_loss: 0.40291857719421387, g_loss: 3.9184298515319824\n","Epoch 52/2000, Step 72, d_loss: 0.49390140175819397, g_loss: 3.7414028644561768\n","Epoch 52/2000, Step 73, d_loss: 0.4618705213069916, g_loss: 3.0307631492614746\n","Epoch 52/2000, Step 74, d_loss: 0.4332960247993469, g_loss: 2.847560167312622\n","Epoch 52/2000, Step 75, d_loss: 0.46517375111579895, g_loss: 3.1866812705993652\n","Epoch 52/2000, Step 76, d_loss: 0.5174167156219482, g_loss: 2.627636671066284\n","Epoch 52/2000, Step 77, d_loss: 0.41338464617729187, g_loss: 3.2091987133026123\n","Epoch 52/2000, Step 78, d_loss: 0.4611760973930359, g_loss: 3.901970624923706\n","Epoch 52/2000, Step 79, d_loss: 0.4052518904209137, g_loss: 3.400843858718872\n","Epoch 52/2000, Step 80, d_loss: 0.3854200839996338, g_loss: 4.866823196411133\n","Epoch 52/2000, Step 81, d_loss: 0.4626559615135193, g_loss: 5.022477626800537\n","Epoch 52/2000, Step 82, d_loss: 0.475590318441391, g_loss: 2.3943843841552734\n","Epoch 52/2000, Step 83, d_loss: 0.3621370196342468, g_loss: 1.9956209659576416\n","Epoch 52/2000, Step 84, d_loss: 0.4128801226615906, g_loss: 1.9726723432540894\n","Epoch 52/2000, Step 85, d_loss: 0.5182288885116577, g_loss: 2.6421964168548584\n","Epoch 52/2000, Step 86, d_loss: 0.4574200510978699, g_loss: 3.2013285160064697\n","Epoch 52/2000, Step 87, d_loss: 0.44742608070373535, g_loss: 3.170252799987793\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 53/2000, Step 1, d_loss: 0.37465450167655945, g_loss: 3.2580769062042236\n","Epoch 53/2000, Step 2, d_loss: 0.5257943868637085, g_loss: 3.0153121948242188\n","Epoch 53/2000, Step 3, d_loss: 0.4183309078216553, g_loss: 3.444427728652954\n","Epoch 53/2000, Step 4, d_loss: 0.4209364056587219, g_loss: 3.241245985031128\n","Epoch 53/2000, Step 5, d_loss: 0.44877538084983826, g_loss: 3.560467004776001\n","Epoch 53/2000, Step 6, d_loss: 0.4295465052127838, g_loss: 3.8973495960235596\n","Epoch 53/2000, Step 7, d_loss: 0.38122355937957764, g_loss: 4.051233768463135\n","Epoch 53/2000, Step 8, d_loss: 0.43650466203689575, g_loss: 3.841139078140259\n","Epoch 53/2000, Step 9, d_loss: 0.4318860173225403, g_loss: 3.1160237789154053\n","Epoch 53/2000, Step 10, d_loss: 0.48475098609924316, g_loss: 2.748891592025757\n","Epoch 53/2000, Step 11, d_loss: 0.429186075925827, g_loss: 2.047714948654175\n","Epoch 53/2000, Step 12, d_loss: 0.5170543789863586, g_loss: 3.408752202987671\n","Epoch 53/2000, Step 13, d_loss: 0.4001440405845642, g_loss: 2.7809433937072754\n","Epoch 53/2000, Step 14, d_loss: 0.44989991188049316, g_loss: 2.5457584857940674\n","Epoch 53/2000, Step 15, d_loss: 0.4974323809146881, g_loss: 3.0701348781585693\n","Epoch 53/2000, Step 16, d_loss: 0.42936405539512634, g_loss: 3.9563403129577637\n","Epoch 53/2000, Step 17, d_loss: 0.4131370186805725, g_loss: 3.507026195526123\n","Epoch 53/2000, Step 18, d_loss: 0.482107013463974, g_loss: 3.605807304382324\n","Epoch 53/2000, Step 19, d_loss: 0.37627261877059937, g_loss: 3.7336082458496094\n","Epoch 53/2000, Step 20, d_loss: 0.47930699586868286, g_loss: 3.813126564025879\n","Epoch 53/2000, Step 21, d_loss: 0.5431813597679138, g_loss: 3.9291701316833496\n","Epoch 53/2000, Step 22, d_loss: 0.38854748010635376, g_loss: 2.7061476707458496\n","Epoch 53/2000, Step 23, d_loss: 0.5442687273025513, g_loss: 2.6434717178344727\n","Epoch 53/2000, Step 24, d_loss: 0.48615649342536926, g_loss: 1.7150474786758423\n","Epoch 53/2000, Step 25, d_loss: 0.4902324080467224, g_loss: 3.101832151412964\n","Epoch 53/2000, Step 26, d_loss: 0.4092262089252472, g_loss: 2.9293465614318848\n","Epoch 53/2000, Step 27, d_loss: 0.39439719915390015, g_loss: 3.401757001876831\n","Epoch 53/2000, Step 28, d_loss: 0.38405683636665344, g_loss: 4.103118419647217\n","Epoch 53/2000, Step 29, d_loss: 0.45292508602142334, g_loss: 3.717027187347412\n","Epoch 53/2000, Step 30, d_loss: 0.4341573715209961, g_loss: 3.5600719451904297\n","Epoch 53/2000, Step 31, d_loss: 0.449600487947464, g_loss: 3.111602544784546\n","Epoch 53/2000, Step 32, d_loss: 0.3769322633743286, g_loss: 2.9240212440490723\n","Epoch 53/2000, Step 33, d_loss: 0.38170045614242554, g_loss: 2.8410487174987793\n","Epoch 53/2000, Step 34, d_loss: 0.47816649079322815, g_loss: 3.4772939682006836\n","Epoch 53/2000, Step 35, d_loss: 0.4121490716934204, g_loss: 2.8971140384674072\n","Epoch 53/2000, Step 36, d_loss: 0.44682013988494873, g_loss: 3.2947874069213867\n","Epoch 53/2000, Step 37, d_loss: 0.38556361198425293, g_loss: 2.5497756004333496\n","Epoch 53/2000, Step 38, d_loss: 0.4322911500930786, g_loss: 4.825423240661621\n","Epoch 53/2000, Step 39, d_loss: 0.4520552158355713, g_loss: 4.715907096862793\n","Epoch 53/2000, Step 40, d_loss: 0.39132919907569885, g_loss: 3.6408183574676514\n","Epoch 53/2000, Step 41, d_loss: 0.4155101180076599, g_loss: 2.7003026008605957\n","Epoch 53/2000, Step 42, d_loss: 0.4493465721607208, g_loss: 4.766472816467285\n","Epoch 53/2000, Step 43, d_loss: 0.38976776599884033, g_loss: 3.7170820236206055\n","Epoch 53/2000, Step 44, d_loss: 0.4606688320636749, g_loss: 4.148234844207764\n","Epoch 53/2000, Step 45, d_loss: 0.4073508083820343, g_loss: 4.001716613769531\n","Epoch 53/2000, Step 46, d_loss: 0.3894028067588806, g_loss: 3.9033594131469727\n","Epoch 53/2000, Step 47, d_loss: 0.4159059524536133, g_loss: 3.822455883026123\n","Epoch 53/2000, Step 48, d_loss: 0.4048616886138916, g_loss: 2.630910634994507\n","Epoch 53/2000, Step 49, d_loss: 0.42605265974998474, g_loss: 3.7167632579803467\n","Epoch 53/2000, Step 50, d_loss: 0.40495312213897705, g_loss: 4.560323238372803\n","Epoch 53/2000, Step 51, d_loss: 0.4173435568809509, g_loss: 3.4514310359954834\n","Epoch 53/2000, Step 52, d_loss: 0.3999026119709015, g_loss: 3.302665948867798\n","Epoch 53/2000, Step 53, d_loss: 0.4324927031993866, g_loss: 3.2312421798706055\n","Epoch 53/2000, Step 54, d_loss: 0.4081652760505676, g_loss: 3.913435459136963\n","Epoch 53/2000, Step 55, d_loss: 0.4360618591308594, g_loss: 2.853088140487671\n","Epoch 53/2000, Step 56, d_loss: 0.3519420623779297, g_loss: 4.459054470062256\n","Epoch 53/2000, Step 57, d_loss: 0.4473237693309784, g_loss: 4.048186779022217\n","Epoch 53/2000, Step 58, d_loss: 0.4135964512825012, g_loss: 3.4386539459228516\n","Epoch 53/2000, Step 59, d_loss: 0.3691914677619934, g_loss: 2.3689496517181396\n","Epoch 53/2000, Step 60, d_loss: 0.40434446930885315, g_loss: 3.2838973999023438\n","Epoch 53/2000, Step 61, d_loss: 0.37688466906547546, g_loss: 2.547459602355957\n","Epoch 53/2000, Step 62, d_loss: 0.4051024913787842, g_loss: 2.66896653175354\n","Epoch 53/2000, Step 63, d_loss: 0.37261608242988586, g_loss: 3.073754072189331\n","Epoch 53/2000, Step 64, d_loss: 0.5079848766326904, g_loss: 4.196206092834473\n","Epoch 53/2000, Step 65, d_loss: 0.42052197456359863, g_loss: 3.6721267700195312\n","Epoch 53/2000, Step 66, d_loss: 0.41170036792755127, g_loss: 3.8121328353881836\n","Epoch 53/2000, Step 67, d_loss: 0.3993132412433624, g_loss: 3.5935330390930176\n","Epoch 53/2000, Step 68, d_loss: 0.4541948437690735, g_loss: 4.752692699432373\n","Epoch 53/2000, Step 69, d_loss: 0.4690193831920624, g_loss: 2.597813606262207\n","Epoch 53/2000, Step 70, d_loss: 0.44601327180862427, g_loss: 2.7530910968780518\n","Epoch 53/2000, Step 71, d_loss: 0.4604794383049011, g_loss: 1.7456103563308716\n","Epoch 53/2000, Step 72, d_loss: 0.466473788022995, g_loss: 2.752913475036621\n","Epoch 53/2000, Step 73, d_loss: 0.4090103507041931, g_loss: 2.481269121170044\n","Epoch 53/2000, Step 74, d_loss: 0.4331929683685303, g_loss: 3.0594847202301025\n","Epoch 53/2000, Step 75, d_loss: 0.6204960942268372, g_loss: 3.8065345287323\n","Epoch 53/2000, Step 76, d_loss: 0.4012822210788727, g_loss: 3.520085096359253\n","Epoch 53/2000, Step 77, d_loss: 0.39332830905914307, g_loss: 4.116016387939453\n","Epoch 53/2000, Step 78, d_loss: 0.5053425431251526, g_loss: 3.065241813659668\n","Epoch 53/2000, Step 79, d_loss: 0.397582471370697, g_loss: 3.7860398292541504\n","Epoch 53/2000, Step 80, d_loss: 0.41219502687454224, g_loss: 3.542896270751953\n","Epoch 53/2000, Step 81, d_loss: 0.3714797794818878, g_loss: 3.3475286960601807\n","Epoch 53/2000, Step 82, d_loss: 0.38550540804862976, g_loss: 3.069580078125\n","Epoch 53/2000, Step 83, d_loss: 0.4200744330883026, g_loss: 3.420032262802124\n","Epoch 53/2000, Step 84, d_loss: 0.4174993336200714, g_loss: 2.597017288208008\n","Epoch 53/2000, Step 85, d_loss: 0.37688618898391724, g_loss: 3.79764986038208\n","Epoch 53/2000, Step 86, d_loss: 0.44587162137031555, g_loss: 4.113762855529785\n","Epoch 53/2000, Step 87, d_loss: 0.4154523015022278, g_loss: 3.9505014419555664\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 54/2000, Step 1, d_loss: 0.4115775227546692, g_loss: 2.3909060955047607\n","Epoch 54/2000, Step 2, d_loss: 0.4556044638156891, g_loss: 2.1131210327148438\n","Epoch 54/2000, Step 3, d_loss: 0.40314245223999023, g_loss: 3.426051139831543\n","Epoch 54/2000, Step 4, d_loss: 0.40117305517196655, g_loss: 3.537658452987671\n","Epoch 54/2000, Step 5, d_loss: 0.4866553246974945, g_loss: 5.033534526824951\n","Epoch 54/2000, Step 6, d_loss: 0.37881535291671753, g_loss: 3.5844640731811523\n","Epoch 54/2000, Step 7, d_loss: 0.38122931122779846, g_loss: 5.267148971557617\n","Epoch 54/2000, Step 8, d_loss: 0.4175868034362793, g_loss: 4.87051248550415\n","Epoch 54/2000, Step 9, d_loss: 1.0241074562072754, g_loss: 3.874330759048462\n","Epoch 54/2000, Step 10, d_loss: 0.3973102867603302, g_loss: 3.6414124965667725\n","Epoch 54/2000, Step 11, d_loss: 0.4186132252216339, g_loss: 3.5842602252960205\n","Epoch 54/2000, Step 12, d_loss: 0.4459720849990845, g_loss: 1.8062710762023926\n","Epoch 54/2000, Step 13, d_loss: 0.6179016828536987, g_loss: 1.968336582183838\n","Epoch 54/2000, Step 14, d_loss: 0.4070468246936798, g_loss: 4.818063735961914\n","Epoch 54/2000, Step 15, d_loss: 0.4271349310874939, g_loss: 3.2275550365448\n","Epoch 54/2000, Step 16, d_loss: 0.3872900605201721, g_loss: 4.374512195587158\n","Epoch 54/2000, Step 17, d_loss: 0.4395792782306671, g_loss: 4.3486456871032715\n","Epoch 54/2000, Step 18, d_loss: 0.41488543152809143, g_loss: 3.3656928539276123\n","Epoch 54/2000, Step 19, d_loss: 0.4410395622253418, g_loss: 4.274901866912842\n","Epoch 54/2000, Step 20, d_loss: 0.41726380586624146, g_loss: 4.17840576171875\n","Epoch 54/2000, Step 21, d_loss: 0.3593752682209015, g_loss: 3.3954291343688965\n","Epoch 54/2000, Step 22, d_loss: 0.36908119916915894, g_loss: 2.9233453273773193\n","Epoch 54/2000, Step 23, d_loss: 0.4353519082069397, g_loss: 3.365849018096924\n","Epoch 54/2000, Step 24, d_loss: 0.39569276571273804, g_loss: 2.7779736518859863\n","Epoch 54/2000, Step 25, d_loss: 0.4522525668144226, g_loss: 2.256697177886963\n","Epoch 54/2000, Step 26, d_loss: 0.3693319261074066, g_loss: 4.486967086791992\n","Epoch 54/2000, Step 27, d_loss: 0.3799411952495575, g_loss: 3.6631131172180176\n","Epoch 54/2000, Step 28, d_loss: 0.49411606788635254, g_loss: 3.6192052364349365\n","Epoch 54/2000, Step 29, d_loss: 0.38462376594543457, g_loss: 4.1847405433654785\n","Epoch 54/2000, Step 30, d_loss: 0.3733433187007904, g_loss: 3.7705657482147217\n","Epoch 54/2000, Step 31, d_loss: 0.3971292972564697, g_loss: 3.2073922157287598\n","Epoch 54/2000, Step 32, d_loss: 0.4081636965274811, g_loss: 3.923611879348755\n","Epoch 54/2000, Step 33, d_loss: 0.40208813548088074, g_loss: 3.049509286880493\n","Epoch 54/2000, Step 34, d_loss: 0.4118950366973877, g_loss: 3.9281723499298096\n","Epoch 54/2000, Step 35, d_loss: 0.3714866042137146, g_loss: 2.7078545093536377\n","Epoch 54/2000, Step 36, d_loss: 0.40309658646583557, g_loss: 3.4153716564178467\n","Epoch 54/2000, Step 37, d_loss: 0.4037115275859833, g_loss: 4.030468940734863\n","Epoch 54/2000, Step 38, d_loss: 0.4005599021911621, g_loss: 3.116633176803589\n","Epoch 54/2000, Step 39, d_loss: 0.44008708000183105, g_loss: 5.185618877410889\n","Epoch 54/2000, Step 40, d_loss: 0.44948261976242065, g_loss: 3.4192185401916504\n","Epoch 54/2000, Step 41, d_loss: 0.4126606285572052, g_loss: 4.833420276641846\n","Epoch 54/2000, Step 42, d_loss: 0.40326571464538574, g_loss: 5.331022262573242\n","Epoch 54/2000, Step 43, d_loss: 0.400592178106308, g_loss: 4.30200719833374\n","Epoch 54/2000, Step 44, d_loss: 0.4224589169025421, g_loss: 4.140332221984863\n","Epoch 54/2000, Step 45, d_loss: 0.3990331292152405, g_loss: 4.092492580413818\n","Epoch 54/2000, Step 46, d_loss: 0.44302406907081604, g_loss: 2.2998359203338623\n","Epoch 54/2000, Step 47, d_loss: 0.39283740520477295, g_loss: 2.9424760341644287\n","Epoch 54/2000, Step 48, d_loss: 0.44530194997787476, g_loss: 3.0592703819274902\n","Epoch 54/2000, Step 49, d_loss: 0.5403856635093689, g_loss: 3.549561023712158\n","Epoch 54/2000, Step 50, d_loss: 0.4088245630264282, g_loss: 2.9680733680725098\n","Epoch 54/2000, Step 51, d_loss: 0.4480670392513275, g_loss: 3.284291982650757\n","Epoch 54/2000, Step 52, d_loss: 0.39615046977996826, g_loss: 3.9085652828216553\n","Epoch 54/2000, Step 53, d_loss: 0.4808814227581024, g_loss: 3.4109537601470947\n","Epoch 54/2000, Step 54, d_loss: 0.3935718834400177, g_loss: 2.927412986755371\n","Epoch 54/2000, Step 55, d_loss: 0.5064561367034912, g_loss: 2.8013081550598145\n","Epoch 54/2000, Step 56, d_loss: 0.46283578872680664, g_loss: 3.095651626586914\n","Epoch 54/2000, Step 57, d_loss: 0.4477205276489258, g_loss: 2.2602033615112305\n","Epoch 54/2000, Step 58, d_loss: 0.4811813533306122, g_loss: 3.9788355827331543\n","Epoch 54/2000, Step 59, d_loss: 0.45125913619995117, g_loss: 2.4066989421844482\n","Epoch 54/2000, Step 60, d_loss: 0.42468446493148804, g_loss: 2.77616548538208\n","Epoch 54/2000, Step 61, d_loss: 0.4306553304195404, g_loss: 5.489235877990723\n","Epoch 54/2000, Step 62, d_loss: 0.3979552984237671, g_loss: 4.017394542694092\n","Epoch 54/2000, Step 63, d_loss: 0.43846654891967773, g_loss: 3.2372817993164062\n","Epoch 54/2000, Step 64, d_loss: 0.41769132018089294, g_loss: 3.3055260181427\n","Epoch 54/2000, Step 65, d_loss: 0.5293027758598328, g_loss: 2.610004425048828\n","Epoch 54/2000, Step 66, d_loss: 0.4145418405532837, g_loss: 3.3336243629455566\n","Epoch 54/2000, Step 67, d_loss: 0.5293235182762146, g_loss: 3.2230539321899414\n","Epoch 54/2000, Step 68, d_loss: 0.41961410641670227, g_loss: 4.036319732666016\n","Epoch 54/2000, Step 69, d_loss: 0.6582967042922974, g_loss: 3.2368643283843994\n","Epoch 54/2000, Step 70, d_loss: 0.42224350571632385, g_loss: 4.280447483062744\n","Epoch 54/2000, Step 71, d_loss: 0.4494856595993042, g_loss: 2.902758836746216\n","Epoch 54/2000, Step 72, d_loss: 0.48767560720443726, g_loss: 2.8111541271209717\n","Epoch 54/2000, Step 73, d_loss: 0.433240681886673, g_loss: 2.5564334392547607\n","Epoch 54/2000, Step 74, d_loss: 0.4627813696861267, g_loss: 2.1242542266845703\n","Epoch 54/2000, Step 75, d_loss: 0.4131315350532532, g_loss: 2.7510852813720703\n","Epoch 54/2000, Step 76, d_loss: 0.42344528436660767, g_loss: 3.8955941200256348\n","Epoch 54/2000, Step 77, d_loss: 0.3609858453273773, g_loss: 3.147031784057617\n","Epoch 54/2000, Step 78, d_loss: 0.4205883741378784, g_loss: 3.7319540977478027\n","Epoch 54/2000, Step 79, d_loss: 0.4440521001815796, g_loss: 4.0807271003723145\n","Epoch 54/2000, Step 80, d_loss: 0.3638891875743866, g_loss: 3.8846263885498047\n","Epoch 54/2000, Step 81, d_loss: 0.3986043930053711, g_loss: 4.284377098083496\n","Epoch 54/2000, Step 82, d_loss: 0.3697907030582428, g_loss: 3.081434965133667\n","Epoch 54/2000, Step 83, d_loss: 0.3713275194168091, g_loss: 3.134660243988037\n","Epoch 54/2000, Step 84, d_loss: 0.4650309383869171, g_loss: 3.124868154525757\n","Epoch 54/2000, Step 85, d_loss: 0.402037650346756, g_loss: 3.2614102363586426\n","Epoch 54/2000, Step 86, d_loss: 0.42648735642433167, g_loss: 2.705195903778076\n","Epoch 54/2000, Step 87, d_loss: 0.36875808238983154, g_loss: 4.284952163696289\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 55/2000, Step 1, d_loss: 0.41057807207107544, g_loss: 4.556807041168213\n","Epoch 55/2000, Step 2, d_loss: 0.4433053433895111, g_loss: 4.1909308433532715\n","Epoch 55/2000, Step 3, d_loss: 0.5244126915931702, g_loss: 3.0050437450408936\n","Epoch 55/2000, Step 4, d_loss: 0.38574886322021484, g_loss: 2.42006254196167\n","Epoch 55/2000, Step 5, d_loss: 0.4465171992778778, g_loss: 2.4219412803649902\n","Epoch 55/2000, Step 6, d_loss: 0.4957836866378784, g_loss: 2.826409101486206\n","Epoch 55/2000, Step 7, d_loss: 0.4163292944431305, g_loss: 3.9760687351226807\n","Epoch 55/2000, Step 8, d_loss: 0.41502314805984497, g_loss: 2.742741107940674\n","Epoch 55/2000, Step 9, d_loss: 0.366812527179718, g_loss: 3.99607253074646\n","Epoch 55/2000, Step 10, d_loss: 0.408074289560318, g_loss: 4.237720966339111\n","Epoch 55/2000, Step 11, d_loss: 0.4348122179508209, g_loss: 4.2992987632751465\n","Epoch 55/2000, Step 12, d_loss: 0.6746641397476196, g_loss: 3.6781139373779297\n","Epoch 55/2000, Step 13, d_loss: 0.39883822202682495, g_loss: 3.350755453109741\n","Epoch 55/2000, Step 14, d_loss: 0.4914213716983795, g_loss: 3.4944007396698\n","Epoch 55/2000, Step 15, d_loss: 0.3996899425983429, g_loss: 3.138561487197876\n","Epoch 55/2000, Step 16, d_loss: 0.41280898451805115, g_loss: 3.6290082931518555\n","Epoch 55/2000, Step 17, d_loss: 0.41126468777656555, g_loss: 4.082119941711426\n","Epoch 55/2000, Step 18, d_loss: 0.3813859820365906, g_loss: 3.9565958976745605\n","Epoch 55/2000, Step 19, d_loss: 0.37345582246780396, g_loss: 4.41132926940918\n","Epoch 55/2000, Step 20, d_loss: 0.5587735772132874, g_loss: 3.7277591228485107\n","Epoch 55/2000, Step 21, d_loss: 0.38670361042022705, g_loss: 3.945077419281006\n","Epoch 55/2000, Step 22, d_loss: 0.40951427817344666, g_loss: 2.544555187225342\n","Epoch 55/2000, Step 23, d_loss: 0.417683482170105, g_loss: 2.897249698638916\n","Epoch 55/2000, Step 24, d_loss: 0.4787411391735077, g_loss: 4.195425987243652\n","Epoch 55/2000, Step 25, d_loss: 0.4089643657207489, g_loss: 3.024932384490967\n","Epoch 55/2000, Step 26, d_loss: 0.4350430965423584, g_loss: 2.8397061824798584\n","Epoch 55/2000, Step 27, d_loss: 0.3796078562736511, g_loss: 3.3160459995269775\n","Epoch 55/2000, Step 28, d_loss: 0.473246693611145, g_loss: 2.757117509841919\n","Epoch 55/2000, Step 29, d_loss: 0.4797048568725586, g_loss: 3.0379385948181152\n","Epoch 55/2000, Step 30, d_loss: 0.611007809638977, g_loss: 4.248937129974365\n","Epoch 55/2000, Step 31, d_loss: 0.41526734828948975, g_loss: 4.503138065338135\n","Epoch 55/2000, Step 32, d_loss: 0.48647478222846985, g_loss: 3.2926554679870605\n","Epoch 55/2000, Step 33, d_loss: 0.444193035364151, g_loss: 3.451084852218628\n","Epoch 55/2000, Step 34, d_loss: 0.4080308675765991, g_loss: 2.8208343982696533\n","Epoch 55/2000, Step 35, d_loss: 0.4275045394897461, g_loss: 3.6972157955169678\n","Epoch 55/2000, Step 36, d_loss: 0.3749995529651642, g_loss: 3.7278504371643066\n","Epoch 55/2000, Step 37, d_loss: 0.4237910807132721, g_loss: 4.727672100067139\n","Epoch 55/2000, Step 38, d_loss: 0.3931434154510498, g_loss: 3.929844617843628\n","Epoch 55/2000, Step 39, d_loss: 0.40183761715888977, g_loss: 3.216766595840454\n","Epoch 55/2000, Step 40, d_loss: 0.4431966543197632, g_loss: 4.34377908706665\n","Epoch 55/2000, Step 41, d_loss: 0.425248920917511, g_loss: 4.901174545288086\n","Epoch 55/2000, Step 42, d_loss: 0.4047490060329437, g_loss: 4.250801086425781\n","Epoch 55/2000, Step 43, d_loss: 0.4012886583805084, g_loss: 2.9597408771514893\n","Epoch 55/2000, Step 44, d_loss: 0.4037148952484131, g_loss: 3.520573377609253\n","Epoch 55/2000, Step 45, d_loss: 0.42230045795440674, g_loss: 4.036071300506592\n","Epoch 55/2000, Step 46, d_loss: 0.4104989469051361, g_loss: 3.6250932216644287\n","Epoch 55/2000, Step 47, d_loss: 0.40245321393013, g_loss: 2.859121322631836\n","Epoch 55/2000, Step 48, d_loss: 0.4171794652938843, g_loss: 2.650388717651367\n","Epoch 55/2000, Step 49, d_loss: 0.37352654337882996, g_loss: 3.3394052982330322\n","Epoch 55/2000, Step 50, d_loss: 0.42733943462371826, g_loss: 3.6451587677001953\n","Epoch 55/2000, Step 51, d_loss: 0.38644930720329285, g_loss: 3.6451315879821777\n","Epoch 55/2000, Step 52, d_loss: 0.4121600091457367, g_loss: 5.263413906097412\n","Epoch 55/2000, Step 53, d_loss: 0.36116793751716614, g_loss: 3.7642154693603516\n","Epoch 55/2000, Step 54, d_loss: 0.3697931468486786, g_loss: 4.185116767883301\n","Epoch 55/2000, Step 55, d_loss: 0.3750903308391571, g_loss: 6.862307548522949\n","Epoch 55/2000, Step 56, d_loss: 0.3720881938934326, g_loss: 3.9525210857391357\n","Epoch 55/2000, Step 57, d_loss: 0.36621126532554626, g_loss: 3.784599781036377\n","Epoch 55/2000, Step 58, d_loss: 0.46888554096221924, g_loss: 4.085056304931641\n","Epoch 55/2000, Step 59, d_loss: 0.44756150245666504, g_loss: 3.8994438648223877\n","Epoch 55/2000, Step 60, d_loss: 0.425911009311676, g_loss: 2.5189151763916016\n","Epoch 55/2000, Step 61, d_loss: 0.41388389468193054, g_loss: 2.3576695919036865\n","Epoch 55/2000, Step 62, d_loss: 0.5058194994926453, g_loss: 2.141522169113159\n","Epoch 55/2000, Step 63, d_loss: 0.4335491359233856, g_loss: 2.678386926651001\n","Epoch 55/2000, Step 64, d_loss: 0.47123366594314575, g_loss: 2.788858413696289\n","Epoch 55/2000, Step 65, d_loss: 0.38773322105407715, g_loss: 3.6079230308532715\n","Epoch 55/2000, Step 66, d_loss: 0.4573311507701874, g_loss: 3.5328481197357178\n","Epoch 55/2000, Step 67, d_loss: 0.4689364731311798, g_loss: 4.39261531829834\n","Epoch 55/2000, Step 68, d_loss: 0.5956600308418274, g_loss: 3.4823379516601562\n","Epoch 55/2000, Step 69, d_loss: 0.4379281997680664, g_loss: 3.8007636070251465\n","Epoch 55/2000, Step 70, d_loss: 0.4448227286338806, g_loss: 3.6518197059631348\n","Epoch 55/2000, Step 71, d_loss: 0.548926830291748, g_loss: 3.364332914352417\n","Epoch 55/2000, Step 72, d_loss: 0.5032854676246643, g_loss: 3.633326292037964\n","Epoch 55/2000, Step 73, d_loss: 0.4515998959541321, g_loss: 4.416781902313232\n","Epoch 55/2000, Step 74, d_loss: 0.382646381855011, g_loss: 3.44336199760437\n","Epoch 55/2000, Step 75, d_loss: 0.46735498309135437, g_loss: 4.221097469329834\n","Epoch 55/2000, Step 76, d_loss: 0.5112143754959106, g_loss: 3.503941774368286\n","Epoch 55/2000, Step 77, d_loss: 0.41374921798706055, g_loss: 4.004030227661133\n","Epoch 55/2000, Step 78, d_loss: 0.4863821566104889, g_loss: 2.6258606910705566\n","Epoch 55/2000, Step 79, d_loss: 0.4701239764690399, g_loss: 2.819276809692383\n","Epoch 55/2000, Step 80, d_loss: 0.4934685528278351, g_loss: 2.7971413135528564\n","Epoch 55/2000, Step 81, d_loss: 0.42649322748184204, g_loss: 3.270777463912964\n","Epoch 55/2000, Step 82, d_loss: 0.4201323688030243, g_loss: 2.912419080734253\n","Epoch 55/2000, Step 83, d_loss: 0.5930066704750061, g_loss: 2.908069133758545\n","Epoch 55/2000, Step 84, d_loss: 0.4454033374786377, g_loss: 4.130184173583984\n","Epoch 55/2000, Step 85, d_loss: 0.38868802785873413, g_loss: 2.9284591674804688\n","Epoch 55/2000, Step 86, d_loss: 0.45790591835975647, g_loss: 3.0762741565704346\n","Epoch 55/2000, Step 87, d_loss: 0.5597574710845947, g_loss: 2.322129249572754\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 56/2000, Step 1, d_loss: 0.5436061024665833, g_loss: 4.153998851776123\n","Epoch 56/2000, Step 2, d_loss: 0.4101073443889618, g_loss: 3.644158363342285\n","Epoch 56/2000, Step 3, d_loss: 0.463630735874176, g_loss: 3.296187400817871\n","Epoch 56/2000, Step 4, d_loss: 0.4838991165161133, g_loss: 5.180103778839111\n","Epoch 56/2000, Step 5, d_loss: 0.42909643054008484, g_loss: 3.7907586097717285\n","Epoch 56/2000, Step 6, d_loss: 0.4188053011894226, g_loss: 4.192319869995117\n","Epoch 56/2000, Step 7, d_loss: 0.4517779052257538, g_loss: 4.8769330978393555\n","Epoch 56/2000, Step 8, d_loss: 0.47039294242858887, g_loss: 4.316658973693848\n","Epoch 56/2000, Step 9, d_loss: 0.546740710735321, g_loss: 3.4832427501678467\n","Epoch 56/2000, Step 10, d_loss: 0.3934955596923828, g_loss: 2.6638877391815186\n","Epoch 56/2000, Step 11, d_loss: 0.3968328535556793, g_loss: 2.593060255050659\n","Epoch 56/2000, Step 12, d_loss: 0.5174810886383057, g_loss: 3.110400676727295\n","Epoch 56/2000, Step 13, d_loss: 0.456470251083374, g_loss: 2.7077207565307617\n","Epoch 56/2000, Step 14, d_loss: 0.5498332381248474, g_loss: 3.4472851753234863\n","Epoch 56/2000, Step 15, d_loss: 0.5663594603538513, g_loss: 3.4928972721099854\n","Epoch 56/2000, Step 16, d_loss: 0.4400807023048401, g_loss: 3.2263193130493164\n","Epoch 56/2000, Step 17, d_loss: 0.4467450976371765, g_loss: 4.275022029876709\n","Epoch 56/2000, Step 18, d_loss: 0.5216190814971924, g_loss: 2.969327926635742\n","Epoch 56/2000, Step 19, d_loss: 0.5400380492210388, g_loss: 3.914433479309082\n","Epoch 56/2000, Step 20, d_loss: 0.4676419794559479, g_loss: 3.042393922805786\n","Epoch 56/2000, Step 21, d_loss: 0.40177175402641296, g_loss: 2.5295913219451904\n","Epoch 56/2000, Step 22, d_loss: 0.47175922989845276, g_loss: 2.710325241088867\n","Epoch 56/2000, Step 23, d_loss: 0.4460141956806183, g_loss: 3.285137414932251\n","Epoch 56/2000, Step 24, d_loss: 0.4560941457748413, g_loss: 3.340833902359009\n","Epoch 56/2000, Step 25, d_loss: 0.5928738117218018, g_loss: 2.5293986797332764\n","Epoch 56/2000, Step 26, d_loss: 0.5035253763198853, g_loss: 1.7762255668640137\n","Epoch 56/2000, Step 27, d_loss: 0.4599308669567108, g_loss: 1.835274338722229\n","Epoch 56/2000, Step 28, d_loss: 0.614346981048584, g_loss: 2.1621198654174805\n","Epoch 56/2000, Step 29, d_loss: 0.681760311126709, g_loss: 4.497351169586182\n","Epoch 56/2000, Step 30, d_loss: 0.384589821100235, g_loss: 3.0414979457855225\n","Epoch 56/2000, Step 31, d_loss: 0.4213343858718872, g_loss: 4.508774280548096\n","Epoch 56/2000, Step 32, d_loss: 0.48684412240982056, g_loss: 3.6207115650177\n","Epoch 56/2000, Step 33, d_loss: 0.5559579133987427, g_loss: 3.844088554382324\n","Epoch 56/2000, Step 34, d_loss: 0.42055007815361023, g_loss: 2.8192999362945557\n","Epoch 56/2000, Step 35, d_loss: 0.40823134779930115, g_loss: 2.9696638584136963\n","Epoch 56/2000, Step 36, d_loss: 0.4374735355377197, g_loss: 2.643535614013672\n","Epoch 56/2000, Step 37, d_loss: 0.5849720239639282, g_loss: 3.1728785037994385\n","Epoch 56/2000, Step 38, d_loss: 0.6184673309326172, g_loss: 3.215649366378784\n","Epoch 56/2000, Step 39, d_loss: 0.4145490229129791, g_loss: 4.226256370544434\n","Epoch 56/2000, Step 40, d_loss: 0.40660688281059265, g_loss: 4.108470916748047\n","Epoch 56/2000, Step 41, d_loss: 0.5259875059127808, g_loss: 4.475034713745117\n","Epoch 56/2000, Step 42, d_loss: 0.569861650466919, g_loss: 3.3984286785125732\n","Epoch 56/2000, Step 43, d_loss: 0.4630795121192932, g_loss: 2.158642053604126\n","Epoch 56/2000, Step 44, d_loss: 0.41101405024528503, g_loss: 2.6499407291412354\n","Epoch 56/2000, Step 45, d_loss: 0.4899766743183136, g_loss: 2.178022861480713\n","Epoch 56/2000, Step 46, d_loss: 0.46376681327819824, g_loss: 2.632869005203247\n","Epoch 56/2000, Step 47, d_loss: 0.47182878851890564, g_loss: 2.8992555141448975\n","Epoch 56/2000, Step 48, d_loss: 0.41183751821517944, g_loss: 4.333431243896484\n","Epoch 56/2000, Step 49, d_loss: 0.49760743975639343, g_loss: 3.9895825386047363\n","Epoch 56/2000, Step 50, d_loss: 0.3884720802307129, g_loss: 4.679389953613281\n","Epoch 56/2000, Step 51, d_loss: 0.5046613216400146, g_loss: 3.3753511905670166\n","Epoch 56/2000, Step 52, d_loss: 0.4091504216194153, g_loss: 3.4924135208129883\n","Epoch 56/2000, Step 53, d_loss: 0.5422439575195312, g_loss: 3.089599370956421\n","Epoch 56/2000, Step 54, d_loss: 0.5057228803634644, g_loss: 4.1046857833862305\n","Epoch 56/2000, Step 55, d_loss: 0.5132894515991211, g_loss: 4.785949230194092\n","Epoch 56/2000, Step 56, d_loss: 0.4717283248901367, g_loss: 4.368704319000244\n","Epoch 56/2000, Step 57, d_loss: 0.5487043857574463, g_loss: 4.733316421508789\n","Epoch 56/2000, Step 58, d_loss: 0.4478587806224823, g_loss: 2.1323256492614746\n","Epoch 56/2000, Step 59, d_loss: 0.39997100830078125, g_loss: 2.702662706375122\n","Epoch 56/2000, Step 60, d_loss: 0.49790966510772705, g_loss: 1.727698564529419\n","Epoch 56/2000, Step 61, d_loss: 0.4369509816169739, g_loss: 2.715265989303589\n","Epoch 56/2000, Step 62, d_loss: 0.4097369313240051, g_loss: 2.4403388500213623\n","Epoch 56/2000, Step 63, d_loss: 0.4412887394428253, g_loss: 3.7129180431365967\n","Epoch 56/2000, Step 64, d_loss: 0.3918789327144623, g_loss: 3.1672685146331787\n","Epoch 56/2000, Step 65, d_loss: 0.3833836615085602, g_loss: 4.491049289703369\n","Epoch 56/2000, Step 66, d_loss: 0.4379205107688904, g_loss: 5.05279016494751\n","Epoch 56/2000, Step 67, d_loss: 0.41390934586524963, g_loss: 3.033295154571533\n","Epoch 56/2000, Step 68, d_loss: 0.46727627515792847, g_loss: 3.966496706008911\n","Epoch 56/2000, Step 69, d_loss: 0.5108614563941956, g_loss: 2.390058994293213\n","Epoch 56/2000, Step 70, d_loss: 0.4212285578250885, g_loss: 2.3899192810058594\n","Epoch 56/2000, Step 71, d_loss: 0.4013386368751526, g_loss: 3.6323306560516357\n","Epoch 56/2000, Step 72, d_loss: 0.4437071681022644, g_loss: 3.7649266719818115\n","Epoch 56/2000, Step 73, d_loss: 0.4168129563331604, g_loss: 4.001908779144287\n","Epoch 56/2000, Step 74, d_loss: 0.44124698638916016, g_loss: 3.3547778129577637\n","Epoch 56/2000, Step 75, d_loss: 0.40740275382995605, g_loss: 3.4049110412597656\n","Epoch 56/2000, Step 76, d_loss: 0.3609239459037781, g_loss: 4.528655052185059\n","Epoch 56/2000, Step 77, d_loss: 0.37199950218200684, g_loss: 4.32551383972168\n","Epoch 56/2000, Step 78, d_loss: 0.717831552028656, g_loss: 4.101819038391113\n","Epoch 56/2000, Step 79, d_loss: 0.4263747036457062, g_loss: 4.184254169464111\n","Epoch 56/2000, Step 80, d_loss: 0.4627421498298645, g_loss: 4.336045265197754\n","Epoch 56/2000, Step 81, d_loss: 0.5088999271392822, g_loss: 2.336460590362549\n","Epoch 56/2000, Step 82, d_loss: 0.5305447578430176, g_loss: 2.55352783203125\n","Epoch 56/2000, Step 83, d_loss: 0.37673577666282654, g_loss: 2.7992103099823\n","Epoch 56/2000, Step 84, d_loss: 0.5060288906097412, g_loss: 2.4187049865722656\n","Epoch 56/2000, Step 85, d_loss: 0.6441355347633362, g_loss: 2.202641487121582\n","Epoch 56/2000, Step 86, d_loss: 0.5198113918304443, g_loss: 2.7749123573303223\n","Epoch 56/2000, Step 87, d_loss: 0.45737820863723755, g_loss: 2.844275712966919\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 57/2000, Step 1, d_loss: 0.4199804961681366, g_loss: 3.6874709129333496\n","Epoch 57/2000, Step 2, d_loss: 0.5227132439613342, g_loss: 4.623814105987549\n","Epoch 57/2000, Step 3, d_loss: 0.6104699969291687, g_loss: 3.952221393585205\n","Epoch 57/2000, Step 4, d_loss: 0.4458511769771576, g_loss: 5.8445844650268555\n","Epoch 57/2000, Step 5, d_loss: 0.42540034651756287, g_loss: 2.3832595348358154\n","Epoch 57/2000, Step 6, d_loss: 0.4714657962322235, g_loss: 3.9565179347991943\n","Epoch 57/2000, Step 7, d_loss: 0.4610699713230133, g_loss: 3.0490849018096924\n","Epoch 57/2000, Step 8, d_loss: 0.672834038734436, g_loss: 1.8787002563476562\n","Epoch 57/2000, Step 9, d_loss: 0.5473697185516357, g_loss: 1.6909762620925903\n","Epoch 57/2000, Step 10, d_loss: 0.44102928042411804, g_loss: 1.9768387079238892\n","Epoch 57/2000, Step 11, d_loss: 0.6634123921394348, g_loss: 4.03633451461792\n","Epoch 57/2000, Step 12, d_loss: 0.6003361344337463, g_loss: 3.1499569416046143\n","Epoch 57/2000, Step 13, d_loss: 0.45375287532806396, g_loss: 3.585139036178589\n","Epoch 57/2000, Step 14, d_loss: 0.5529352426528931, g_loss: 2.3483030796051025\n","Epoch 57/2000, Step 15, d_loss: 0.4696379601955414, g_loss: 3.634547472000122\n","Epoch 57/2000, Step 16, d_loss: 0.5513665080070496, g_loss: 3.0358779430389404\n","Epoch 57/2000, Step 17, d_loss: 0.49138927459716797, g_loss: 3.365152359008789\n","Epoch 57/2000, Step 18, d_loss: 0.45183539390563965, g_loss: 2.5211164951324463\n","Epoch 57/2000, Step 19, d_loss: 0.5317007303237915, g_loss: 3.077676296234131\n","Epoch 57/2000, Step 20, d_loss: 0.5178864598274231, g_loss: 2.945019006729126\n","Epoch 57/2000, Step 21, d_loss: 0.4526785612106323, g_loss: 4.423246383666992\n","Epoch 57/2000, Step 22, d_loss: 0.39988550543785095, g_loss: 4.0467400550842285\n","Epoch 57/2000, Step 23, d_loss: 0.43889960646629333, g_loss: 4.361417770385742\n","Epoch 57/2000, Step 24, d_loss: 0.6500639915466309, g_loss: 3.0662004947662354\n","Epoch 57/2000, Step 25, d_loss: 0.41553908586502075, g_loss: 2.4331765174865723\n","Epoch 57/2000, Step 26, d_loss: 0.46510225534439087, g_loss: 2.083922863006592\n","Epoch 57/2000, Step 27, d_loss: 0.5957261919975281, g_loss: 3.2909553050994873\n","Epoch 57/2000, Step 28, d_loss: 0.5158344507217407, g_loss: 2.763979434967041\n","Epoch 57/2000, Step 29, d_loss: 0.39265748858451843, g_loss: 1.6770035028457642\n","Epoch 57/2000, Step 30, d_loss: 0.4632892310619354, g_loss: 2.9414846897125244\n","Epoch 57/2000, Step 31, d_loss: 0.4268725514411926, g_loss: 4.159603595733643\n","Epoch 57/2000, Step 32, d_loss: 0.43008649349212646, g_loss: 3.624305248260498\n","Epoch 57/2000, Step 33, d_loss: 0.4547915458679199, g_loss: 3.6161797046661377\n","Epoch 57/2000, Step 34, d_loss: 0.4323303997516632, g_loss: 2.9955458641052246\n","Epoch 57/2000, Step 35, d_loss: 0.46074187755584717, g_loss: 4.390226364135742\n","Epoch 57/2000, Step 36, d_loss: 0.3916717767715454, g_loss: 3.9230284690856934\n","Epoch 57/2000, Step 37, d_loss: 0.6275300979614258, g_loss: 4.275876522064209\n","Epoch 57/2000, Step 38, d_loss: 0.4095000624656677, g_loss: 3.376105546951294\n","Epoch 57/2000, Step 39, d_loss: 0.40343040227890015, g_loss: 4.1195549964904785\n","Epoch 57/2000, Step 40, d_loss: 0.4026813805103302, g_loss: 2.9685075283050537\n","Epoch 57/2000, Step 41, d_loss: 0.3872941732406616, g_loss: 4.572942733764648\n","Epoch 57/2000, Step 42, d_loss: 0.4183976650238037, g_loss: 5.095845699310303\n","Epoch 57/2000, Step 43, d_loss: 0.38665786385536194, g_loss: 4.78718900680542\n","Epoch 57/2000, Step 44, d_loss: 0.424059122800827, g_loss: 4.433494567871094\n","Epoch 57/2000, Step 45, d_loss: 0.4033307135105133, g_loss: 3.2216336727142334\n","Epoch 57/2000, Step 46, d_loss: 0.3859780430793762, g_loss: 3.537501811981201\n","Epoch 57/2000, Step 47, d_loss: 0.38373228907585144, g_loss: 1.9813810586929321\n","Epoch 57/2000, Step 48, d_loss: 0.41857731342315674, g_loss: 3.0329816341400146\n","Epoch 57/2000, Step 49, d_loss: 0.38214150071144104, g_loss: 2.852475881576538\n","Epoch 57/2000, Step 50, d_loss: 0.5277310013771057, g_loss: 2.992701530456543\n","Epoch 57/2000, Step 51, d_loss: 0.3926103711128235, g_loss: 2.899203062057495\n","Epoch 57/2000, Step 52, d_loss: 0.4536445736885071, g_loss: 3.456357002258301\n","Epoch 57/2000, Step 53, d_loss: 0.44439634680747986, g_loss: 3.157045602798462\n","Epoch 57/2000, Step 54, d_loss: 0.4525361955165863, g_loss: 3.137451410293579\n","Epoch 57/2000, Step 55, d_loss: 0.4642464816570282, g_loss: 3.4118142127990723\n","Epoch 57/2000, Step 56, d_loss: 0.4176248610019684, g_loss: 3.3193764686584473\n","Epoch 57/2000, Step 57, d_loss: 0.3914834260940552, g_loss: 6.8473615646362305\n","Epoch 57/2000, Step 58, d_loss: 0.3784646689891815, g_loss: 4.169415473937988\n","Epoch 57/2000, Step 59, d_loss: 0.45618364214897156, g_loss: 4.948155403137207\n","Epoch 57/2000, Step 60, d_loss: 0.40316516160964966, g_loss: 5.674415111541748\n","Epoch 57/2000, Step 61, d_loss: 0.5090872049331665, g_loss: 4.917546272277832\n","Epoch 57/2000, Step 62, d_loss: 0.5139060616493225, g_loss: 1.683153510093689\n","Epoch 57/2000, Step 63, d_loss: 0.41583845019340515, g_loss: 2.6875481605529785\n","Epoch 57/2000, Step 64, d_loss: 0.4812597632408142, g_loss: 2.400831937789917\n","Epoch 57/2000, Step 65, d_loss: 0.5219627618789673, g_loss: 2.764127492904663\n","Epoch 57/2000, Step 66, d_loss: 0.4268493056297302, g_loss: 2.9128944873809814\n","Epoch 57/2000, Step 67, d_loss: 0.4671299457550049, g_loss: 3.079500913619995\n","Epoch 57/2000, Step 68, d_loss: 0.4041847884654999, g_loss: 4.337314605712891\n","Epoch 57/2000, Step 69, d_loss: 0.4726344048976898, g_loss: 3.3851306438446045\n","Epoch 57/2000, Step 70, d_loss: 0.47404003143310547, g_loss: 3.022557258605957\n","Epoch 57/2000, Step 71, d_loss: 0.3567492365837097, g_loss: 2.8159258365631104\n","Epoch 57/2000, Step 72, d_loss: 0.44006964564323425, g_loss: 2.2350213527679443\n","Epoch 57/2000, Step 73, d_loss: 0.4168981611728668, g_loss: 2.6883485317230225\n","Epoch 57/2000, Step 74, d_loss: 0.4320664703845978, g_loss: 2.8286187648773193\n","Epoch 57/2000, Step 75, d_loss: 0.4460061192512512, g_loss: 1.9508280754089355\n","Epoch 57/2000, Step 76, d_loss: 0.4026908278465271, g_loss: 5.532560348510742\n","Epoch 57/2000, Step 77, d_loss: 0.36901628971099854, g_loss: 3.361392021179199\n","Epoch 57/2000, Step 78, d_loss: 0.4518013596534729, g_loss: 2.896456241607666\n","Epoch 57/2000, Step 79, d_loss: 0.466400146484375, g_loss: 3.618833065032959\n","Epoch 57/2000, Step 80, d_loss: 0.38024404644966125, g_loss: 3.8420333862304688\n","Epoch 57/2000, Step 81, d_loss: 0.38173240423202515, g_loss: 3.2401676177978516\n","Epoch 57/2000, Step 82, d_loss: 0.46063360571861267, g_loss: 3.6865744590759277\n","Epoch 57/2000, Step 83, d_loss: 0.43897563219070435, g_loss: 4.653626441955566\n","Epoch 57/2000, Step 84, d_loss: 0.4140918552875519, g_loss: 2.5352468490600586\n","Epoch 57/2000, Step 85, d_loss: 0.3830878436565399, g_loss: 4.960509777069092\n","Epoch 57/2000, Step 86, d_loss: 0.40767067670822144, g_loss: 3.230288505554199\n","Epoch 57/2000, Step 87, d_loss: 0.5040766000747681, g_loss: 4.631840705871582\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 58/2000, Step 1, d_loss: 0.39964157342910767, g_loss: 3.781038761138916\n","Epoch 58/2000, Step 2, d_loss: 0.41457757353782654, g_loss: 4.208479881286621\n","Epoch 58/2000, Step 3, d_loss: 0.3770829737186432, g_loss: 3.5730032920837402\n","Epoch 58/2000, Step 4, d_loss: 0.40897923707962036, g_loss: 3.3862338066101074\n","Epoch 58/2000, Step 5, d_loss: 0.3781069815158844, g_loss: 4.260340213775635\n","Epoch 58/2000, Step 6, d_loss: 0.37551671266555786, g_loss: 3.6548707485198975\n","Epoch 58/2000, Step 7, d_loss: 0.4529924988746643, g_loss: 3.8860671520233154\n","Epoch 58/2000, Step 8, d_loss: 0.3973486125469208, g_loss: 3.6732282638549805\n","Epoch 58/2000, Step 9, d_loss: 0.5375401973724365, g_loss: 3.8014705181121826\n","Epoch 58/2000, Step 10, d_loss: 0.40208151936531067, g_loss: 3.6863481998443604\n","Epoch 58/2000, Step 11, d_loss: 0.3808944821357727, g_loss: 3.3360350131988525\n","Epoch 58/2000, Step 12, d_loss: 0.4579561948776245, g_loss: 2.8205201625823975\n","Epoch 58/2000, Step 13, d_loss: 0.4142972528934479, g_loss: 3.171614170074463\n","Epoch 58/2000, Step 14, d_loss: 0.36636313796043396, g_loss: 3.475222110748291\n","Epoch 58/2000, Step 15, d_loss: 0.3648955523967743, g_loss: 2.9298770427703857\n","Epoch 58/2000, Step 16, d_loss: 0.38100627064704895, g_loss: 4.027688026428223\n","Epoch 58/2000, Step 17, d_loss: 0.44938230514526367, g_loss: 3.3102848529815674\n","Epoch 58/2000, Step 18, d_loss: 0.3811407685279846, g_loss: 4.224111080169678\n","Epoch 58/2000, Step 19, d_loss: 0.35982486605644226, g_loss: 3.439718723297119\n","Epoch 58/2000, Step 20, d_loss: 0.5105859637260437, g_loss: 3.7660341262817383\n","Epoch 58/2000, Step 21, d_loss: 0.3682633340358734, g_loss: 3.7597999572753906\n","Epoch 58/2000, Step 22, d_loss: 0.3782542049884796, g_loss: 3.5137693881988525\n","Epoch 58/2000, Step 23, d_loss: 0.37022924423217773, g_loss: 3.124530792236328\n","Epoch 58/2000, Step 24, d_loss: 0.3665386438369751, g_loss: 2.9543182849884033\n","Epoch 58/2000, Step 25, d_loss: 0.45055946707725525, g_loss: 3.5146968364715576\n","Epoch 58/2000, Step 26, d_loss: 0.36645540595054626, g_loss: 5.093197822570801\n","Epoch 58/2000, Step 27, d_loss: 0.37373223900794983, g_loss: 3.7704694271087646\n","Epoch 58/2000, Step 28, d_loss: 0.3541361093521118, g_loss: 4.9991631507873535\n","Epoch 58/2000, Step 29, d_loss: 0.5144146680831909, g_loss: 4.448510646820068\n","Epoch 58/2000, Step 30, d_loss: 0.36276838183403015, g_loss: 5.179516792297363\n","Epoch 58/2000, Step 31, d_loss: 0.522686779499054, g_loss: 2.8155739307403564\n","Epoch 58/2000, Step 32, d_loss: 0.4063902795314789, g_loss: 1.8009876012802124\n","Epoch 58/2000, Step 33, d_loss: 0.4649243652820587, g_loss: 2.814448595046997\n","Epoch 58/2000, Step 34, d_loss: 0.5319209098815918, g_loss: 2.565629720687866\n","Epoch 58/2000, Step 35, d_loss: 0.47444450855255127, g_loss: 3.1119465827941895\n","Epoch 58/2000, Step 36, d_loss: 0.3903897702693939, g_loss: 4.052797317504883\n","Epoch 58/2000, Step 37, d_loss: 0.43555891513824463, g_loss: 4.168965816497803\n","Epoch 58/2000, Step 38, d_loss: 0.4158742129802704, g_loss: 5.155848979949951\n","Epoch 58/2000, Step 39, d_loss: 0.47720226645469666, g_loss: 4.339369773864746\n","Epoch 58/2000, Step 40, d_loss: 0.384536474943161, g_loss: 3.969435691833496\n","Epoch 58/2000, Step 41, d_loss: 0.41167575120925903, g_loss: 3.606365442276001\n","Epoch 58/2000, Step 42, d_loss: 0.40650397539138794, g_loss: 2.9721100330352783\n","Epoch 58/2000, Step 43, d_loss: 0.5449303388595581, g_loss: 3.000432252883911\n","Epoch 58/2000, Step 44, d_loss: 0.388450026512146, g_loss: 2.4206814765930176\n","Epoch 58/2000, Step 45, d_loss: 0.3907967507839203, g_loss: 3.801227569580078\n","Epoch 58/2000, Step 46, d_loss: 0.47148072719573975, g_loss: 2.9511301517486572\n","Epoch 58/2000, Step 47, d_loss: 0.43022778630256653, g_loss: 3.6394500732421875\n","Epoch 58/2000, Step 48, d_loss: 0.3990422785282135, g_loss: 3.9390087127685547\n","Epoch 58/2000, Step 49, d_loss: 0.41829466819763184, g_loss: 3.231726884841919\n","Epoch 58/2000, Step 50, d_loss: 0.399044394493103, g_loss: 2.108806848526001\n","Epoch 58/2000, Step 51, d_loss: 0.393085241317749, g_loss: 2.4563825130462646\n","Epoch 58/2000, Step 52, d_loss: 0.45484471321105957, g_loss: 3.1710784435272217\n","Epoch 58/2000, Step 53, d_loss: 0.4522104561328888, g_loss: 2.6310079097747803\n","Epoch 58/2000, Step 54, d_loss: 0.43558594584465027, g_loss: 3.6855952739715576\n","Epoch 58/2000, Step 55, d_loss: 0.4702196419239044, g_loss: 3.5988662242889404\n","Epoch 58/2000, Step 56, d_loss: 0.420792818069458, g_loss: 3.5220539569854736\n","Epoch 58/2000, Step 57, d_loss: 0.4271782636642456, g_loss: 3.3461835384368896\n","Epoch 58/2000, Step 58, d_loss: 0.447194367647171, g_loss: 2.4620532989501953\n","Epoch 58/2000, Step 59, d_loss: 0.4270194172859192, g_loss: 3.102426290512085\n","Epoch 58/2000, Step 60, d_loss: 0.4806407392024994, g_loss: 3.0322725772857666\n","Epoch 58/2000, Step 61, d_loss: 0.38799065351486206, g_loss: 4.223363876342773\n","Epoch 58/2000, Step 62, d_loss: 0.4606405794620514, g_loss: 2.382213830947876\n","Epoch 58/2000, Step 63, d_loss: 0.38340380787849426, g_loss: 3.60282826423645\n","Epoch 58/2000, Step 64, d_loss: 0.40976691246032715, g_loss: 4.609465599060059\n","Epoch 58/2000, Step 65, d_loss: 0.5152153968811035, g_loss: 3.6473376750946045\n","Epoch 58/2000, Step 66, d_loss: 0.6821984052658081, g_loss: 4.145383834838867\n","Epoch 58/2000, Step 67, d_loss: 0.3997829556465149, g_loss: 3.695434331893921\n","Epoch 58/2000, Step 68, d_loss: 0.5720458030700684, g_loss: 4.4724040031433105\n","Epoch 58/2000, Step 69, d_loss: 0.5082341432571411, g_loss: 2.805089235305786\n","Epoch 58/2000, Step 70, d_loss: 0.36416125297546387, g_loss: 2.7715091705322266\n","Epoch 58/2000, Step 71, d_loss: 0.4186996817588806, g_loss: 3.267692804336548\n","Epoch 58/2000, Step 72, d_loss: 0.4200158417224884, g_loss: 3.4681217670440674\n","Epoch 58/2000, Step 73, d_loss: 0.47259998321533203, g_loss: 4.1996235847473145\n","Epoch 58/2000, Step 74, d_loss: 0.3699972331523895, g_loss: 4.690952301025391\n","Epoch 58/2000, Step 75, d_loss: 0.36435577273368835, g_loss: 2.6934564113616943\n","Epoch 58/2000, Step 76, d_loss: 0.3875645101070404, g_loss: 4.149143695831299\n","Epoch 58/2000, Step 77, d_loss: 0.4408276379108429, g_loss: 3.924311399459839\n","Epoch 58/2000, Step 78, d_loss: 0.395423948764801, g_loss: 4.402195930480957\n","Epoch 58/2000, Step 79, d_loss: 0.4022503197193146, g_loss: 4.659477710723877\n","Epoch 58/2000, Step 80, d_loss: 0.38396209478378296, g_loss: 4.081243991851807\n","Epoch 58/2000, Step 81, d_loss: 0.4181142747402191, g_loss: 4.585787773132324\n","Epoch 58/2000, Step 82, d_loss: 0.47574493288993835, g_loss: 4.594826698303223\n","Epoch 58/2000, Step 83, d_loss: 0.48956018686294556, g_loss: 3.518364429473877\n","Epoch 58/2000, Step 84, d_loss: 0.4137694537639618, g_loss: 3.1122748851776123\n","Epoch 58/2000, Step 85, d_loss: 0.3862936198711395, g_loss: 2.1266777515411377\n","Epoch 58/2000, Step 86, d_loss: 0.4913232922554016, g_loss: 3.1353964805603027\n","Epoch 58/2000, Step 87, d_loss: 0.42347535490989685, g_loss: 3.0713891983032227\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 59/2000, Step 1, d_loss: 0.43036991357803345, g_loss: 3.535125970840454\n","Epoch 59/2000, Step 2, d_loss: 0.3771285116672516, g_loss: 3.3050501346588135\n","Epoch 59/2000, Step 3, d_loss: 0.3748161494731903, g_loss: 4.463115692138672\n","Epoch 59/2000, Step 4, d_loss: 0.37996265292167664, g_loss: 4.226933479309082\n","Epoch 59/2000, Step 5, d_loss: 0.4213225841522217, g_loss: 5.269016742706299\n","Epoch 59/2000, Step 6, d_loss: 0.38526973128318787, g_loss: 4.664127826690674\n","Epoch 59/2000, Step 7, d_loss: 0.4960631728172302, g_loss: 4.75715970993042\n","Epoch 59/2000, Step 8, d_loss: 0.5221796035766602, g_loss: 2.354541063308716\n","Epoch 59/2000, Step 9, d_loss: 0.379326194524765, g_loss: 3.604871988296509\n","Epoch 59/2000, Step 10, d_loss: 0.42690199613571167, g_loss: 3.125319004058838\n","Epoch 59/2000, Step 11, d_loss: 0.4340478479862213, g_loss: 3.4385390281677246\n","Epoch 59/2000, Step 12, d_loss: 0.45329123735427856, g_loss: 3.0892813205718994\n","Epoch 59/2000, Step 13, d_loss: 0.5412939190864563, g_loss: 3.4084794521331787\n","Epoch 59/2000, Step 14, d_loss: 0.4401560425758362, g_loss: 3.9460344314575195\n","Epoch 59/2000, Step 15, d_loss: 0.5544927716255188, g_loss: 2.706974744796753\n","Epoch 59/2000, Step 16, d_loss: 0.4303642213344574, g_loss: 4.3687357902526855\n","Epoch 59/2000, Step 17, d_loss: 0.4647262692451477, g_loss: 3.888552665710449\n","Epoch 59/2000, Step 18, d_loss: 0.5241649150848389, g_loss: 2.81070876121521\n","Epoch 59/2000, Step 19, d_loss: 0.4221833348274231, g_loss: 2.7746851444244385\n","Epoch 59/2000, Step 20, d_loss: 0.40213948488235474, g_loss: 3.5650036334991455\n","Epoch 59/2000, Step 21, d_loss: 0.41062960028648376, g_loss: 4.177170753479004\n","Epoch 59/2000, Step 22, d_loss: 0.5596598982810974, g_loss: 2.8569345474243164\n","Epoch 59/2000, Step 23, d_loss: 0.4440460205078125, g_loss: 2.9865574836730957\n","Epoch 59/2000, Step 24, d_loss: 0.4085158109664917, g_loss: 2.1585147380828857\n","Epoch 59/2000, Step 25, d_loss: 0.5102938413619995, g_loss: 2.618314027786255\n","Epoch 59/2000, Step 26, d_loss: 0.4630089998245239, g_loss: 3.557067394256592\n","Epoch 59/2000, Step 27, d_loss: 0.44995492696762085, g_loss: 4.298882007598877\n","Epoch 59/2000, Step 28, d_loss: 0.5172979235649109, g_loss: 2.989583969116211\n","Epoch 59/2000, Step 29, d_loss: 0.3903469741344452, g_loss: 3.164802312850952\n","Epoch 59/2000, Step 30, d_loss: 0.4074130654335022, g_loss: 2.3904812335968018\n","Epoch 59/2000, Step 31, d_loss: 0.4377271234989166, g_loss: 3.1285908222198486\n","Epoch 59/2000, Step 32, d_loss: 0.4097224473953247, g_loss: 4.136285781860352\n","Epoch 59/2000, Step 33, d_loss: 0.49798062443733215, g_loss: 4.245772838592529\n","Epoch 59/2000, Step 34, d_loss: 0.441195011138916, g_loss: 4.664302825927734\n","Epoch 59/2000, Step 35, d_loss: 0.426578551530838, g_loss: 4.407224655151367\n","Epoch 59/2000, Step 36, d_loss: 0.3858332633972168, g_loss: 4.761110305786133\n","Epoch 59/2000, Step 37, d_loss: 0.43737971782684326, g_loss: 2.9673919677734375\n","Epoch 59/2000, Step 38, d_loss: 0.3865130543708801, g_loss: 3.2797603607177734\n","Epoch 59/2000, Step 39, d_loss: 0.4443804621696472, g_loss: 4.360215187072754\n","Epoch 59/2000, Step 40, d_loss: 0.4325518310070038, g_loss: 4.418087005615234\n","Epoch 59/2000, Step 41, d_loss: 0.3504301905632019, g_loss: 3.546863555908203\n","Epoch 59/2000, Step 42, d_loss: 0.38547414541244507, g_loss: 3.840191602706909\n","Epoch 59/2000, Step 43, d_loss: 0.44372960925102234, g_loss: 2.801995038986206\n","Epoch 59/2000, Step 44, d_loss: 0.36643266677856445, g_loss: 2.054590940475464\n","Epoch 59/2000, Step 45, d_loss: 0.3673289716243744, g_loss: 2.596740245819092\n","Epoch 59/2000, Step 46, d_loss: 0.39363786578178406, g_loss: 3.6137285232543945\n","Epoch 59/2000, Step 47, d_loss: 0.4889547526836395, g_loss: 3.761479377746582\n","Epoch 59/2000, Step 48, d_loss: 0.4136972427368164, g_loss: 4.193358898162842\n","Epoch 59/2000, Step 49, d_loss: 0.37766188383102417, g_loss: 3.7459585666656494\n","Epoch 59/2000, Step 50, d_loss: 0.38240787386894226, g_loss: 3.2252960205078125\n","Epoch 59/2000, Step 51, d_loss: 0.41637444496154785, g_loss: 3.7940280437469482\n","Epoch 59/2000, Step 52, d_loss: 0.3897962272167206, g_loss: 4.226052284240723\n","Epoch 59/2000, Step 53, d_loss: 0.38817960023880005, g_loss: 3.6490650177001953\n","Epoch 59/2000, Step 54, d_loss: 0.3630533218383789, g_loss: 4.051435947418213\n","Epoch 59/2000, Step 55, d_loss: 0.39146173000335693, g_loss: 4.743094444274902\n","Epoch 59/2000, Step 56, d_loss: 0.3764934837818146, g_loss: 3.232573986053467\n","Epoch 59/2000, Step 57, d_loss: 0.384961873292923, g_loss: 3.6360421180725098\n","Epoch 59/2000, Step 58, d_loss: 0.3757896423339844, g_loss: 3.0925076007843018\n","Epoch 59/2000, Step 59, d_loss: 0.4201849400997162, g_loss: 3.6520917415618896\n","Epoch 59/2000, Step 60, d_loss: 0.4130256175994873, g_loss: 3.7261741161346436\n","Epoch 59/2000, Step 61, d_loss: 0.4285483658313751, g_loss: 3.774456739425659\n","Epoch 59/2000, Step 62, d_loss: 0.42925146222114563, g_loss: 3.532093048095703\n","Epoch 59/2000, Step 63, d_loss: 0.41318005323410034, g_loss: 3.8388986587524414\n","Epoch 59/2000, Step 64, d_loss: 0.39398711919784546, g_loss: 4.339057922363281\n","Epoch 59/2000, Step 65, d_loss: 0.3859149217605591, g_loss: 2.945502758026123\n","Epoch 59/2000, Step 66, d_loss: 0.3877403736114502, g_loss: 2.5796799659729004\n","Epoch 59/2000, Step 67, d_loss: 0.36887356638908386, g_loss: 2.882965564727783\n","Epoch 59/2000, Step 68, d_loss: 0.3954584300518036, g_loss: 3.252638101577759\n","Epoch 59/2000, Step 69, d_loss: 0.44409313797950745, g_loss: 3.0858280658721924\n","Epoch 59/2000, Step 70, d_loss: 0.41419750452041626, g_loss: 4.768858909606934\n","Epoch 59/2000, Step 71, d_loss: 0.3682699203491211, g_loss: 4.2042975425720215\n","Epoch 59/2000, Step 72, d_loss: 0.37760287523269653, g_loss: 4.517739772796631\n","Epoch 59/2000, Step 73, d_loss: 0.36400294303894043, g_loss: 4.659368991851807\n","Epoch 59/2000, Step 74, d_loss: 0.4283467233181, g_loss: 5.034153461456299\n","Epoch 59/2000, Step 75, d_loss: 0.37172409892082214, g_loss: 3.816452741622925\n","Epoch 59/2000, Step 76, d_loss: 0.45099401473999023, g_loss: 3.1791529655456543\n","Epoch 59/2000, Step 77, d_loss: 0.4221794009208679, g_loss: 2.654026985168457\n","Epoch 59/2000, Step 78, d_loss: 0.4981212615966797, g_loss: 2.9420700073242188\n","Epoch 59/2000, Step 79, d_loss: 0.4475247263908386, g_loss: 4.186499118804932\n","Epoch 59/2000, Step 80, d_loss: 0.3855831027030945, g_loss: 3.161409854888916\n","Epoch 59/2000, Step 81, d_loss: 0.38356420397758484, g_loss: 3.2130751609802246\n","Epoch 59/2000, Step 82, d_loss: 0.4243595004081726, g_loss: 4.012818336486816\n","Epoch 59/2000, Step 83, d_loss: 0.37990403175354004, g_loss: 4.763187885284424\n","Epoch 59/2000, Step 84, d_loss: 0.45984429121017456, g_loss: 3.8703818321228027\n","Epoch 59/2000, Step 85, d_loss: 0.403368204832077, g_loss: 4.001886367797852\n","Epoch 59/2000, Step 86, d_loss: 0.38078537583351135, g_loss: 3.662134885787964\n","Epoch 59/2000, Step 87, d_loss: 0.3603535294532776, g_loss: 3.2618632316589355\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 60/2000, Step 1, d_loss: 0.3826422393321991, g_loss: 2.8134963512420654\n","Epoch 60/2000, Step 2, d_loss: 0.43458327651023865, g_loss: 2.171957015991211\n","Epoch 60/2000, Step 3, d_loss: 0.4335067570209503, g_loss: 2.1067934036254883\n","Epoch 60/2000, Step 4, d_loss: 0.45149749517440796, g_loss: 2.4721107482910156\n","Epoch 60/2000, Step 5, d_loss: 0.4760109782218933, g_loss: 3.2591075897216797\n","Epoch 60/2000, Step 6, d_loss: 0.4112319052219391, g_loss: 3.7249081134796143\n","Epoch 60/2000, Step 7, d_loss: 0.42559775710105896, g_loss: 3.9890739917755127\n","Epoch 60/2000, Step 8, d_loss: 0.4659791588783264, g_loss: 4.076172351837158\n","Epoch 60/2000, Step 9, d_loss: 0.6594926118850708, g_loss: 3.0926284790039062\n","Epoch 60/2000, Step 10, d_loss: 0.508137047290802, g_loss: 4.825721263885498\n","Epoch 60/2000, Step 11, d_loss: 0.6710083484649658, g_loss: 2.4524996280670166\n","Epoch 60/2000, Step 12, d_loss: 0.5980758666992188, g_loss: 1.0025591850280762\n","Epoch 60/2000, Step 13, d_loss: 0.5184565782546997, g_loss: 2.6019229888916016\n","Epoch 60/2000, Step 14, d_loss: 0.6469700932502747, g_loss: 2.5600669384002686\n","Epoch 60/2000, Step 15, d_loss: 0.6205114722251892, g_loss: 3.574957847595215\n","Epoch 60/2000, Step 16, d_loss: 0.6843783259391785, g_loss: 3.221222400665283\n","Epoch 60/2000, Step 17, d_loss: 0.563429594039917, g_loss: 3.9377713203430176\n","Epoch 60/2000, Step 18, d_loss: 0.39286959171295166, g_loss: 4.939714431762695\n","Epoch 60/2000, Step 19, d_loss: 0.6179285645484924, g_loss: 4.603474140167236\n","Epoch 60/2000, Step 20, d_loss: 0.6849401593208313, g_loss: 5.236361503601074\n","Epoch 60/2000, Step 21, d_loss: 0.5146335959434509, g_loss: 3.7894129753112793\n","Epoch 60/2000, Step 22, d_loss: 0.3575121760368347, g_loss: 4.064116477966309\n","Epoch 60/2000, Step 23, d_loss: 0.4398272633552551, g_loss: 1.9502744674682617\n","Epoch 60/2000, Step 24, d_loss: 0.4438786804676056, g_loss: 3.5936520099639893\n","Epoch 60/2000, Step 25, d_loss: 0.575455904006958, g_loss: 3.1118276119232178\n","Epoch 60/2000, Step 26, d_loss: 0.4705020785331726, g_loss: 2.668414354324341\n","Epoch 60/2000, Step 27, d_loss: 0.4125825762748718, g_loss: 6.005832195281982\n","Epoch 60/2000, Step 28, d_loss: 0.518088698387146, g_loss: 3.766019582748413\n","Epoch 60/2000, Step 29, d_loss: 0.43176019191741943, g_loss: 3.237579822540283\n","Epoch 60/2000, Step 30, d_loss: 0.38924282789230347, g_loss: 1.6574815511703491\n","Epoch 60/2000, Step 31, d_loss: 0.5619388818740845, g_loss: 2.8082668781280518\n","Epoch 60/2000, Step 32, d_loss: 0.5746482014656067, g_loss: 2.3731448650360107\n","Epoch 60/2000, Step 33, d_loss: 0.4760711193084717, g_loss: 4.808132171630859\n","Epoch 60/2000, Step 34, d_loss: 0.4827520549297333, g_loss: 4.071132183074951\n","Epoch 60/2000, Step 35, d_loss: 0.4427018165588379, g_loss: 3.636824369430542\n","Epoch 60/2000, Step 36, d_loss: 0.4823538661003113, g_loss: 2.9238944053649902\n","Epoch 60/2000, Step 37, d_loss: 0.4455583095550537, g_loss: 3.8975253105163574\n","Epoch 60/2000, Step 38, d_loss: 0.5324612855911255, g_loss: 3.2916367053985596\n","Epoch 60/2000, Step 39, d_loss: 0.4114430844783783, g_loss: 3.377131223678589\n","Epoch 60/2000, Step 40, d_loss: 0.4211844801902771, g_loss: 2.959745168685913\n","Epoch 60/2000, Step 41, d_loss: 0.39577144384384155, g_loss: 3.561089038848877\n","Epoch 60/2000, Step 42, d_loss: 0.46188104152679443, g_loss: 3.6033239364624023\n","Epoch 60/2000, Step 43, d_loss: 0.38731247186660767, g_loss: 2.662510395050049\n","Epoch 60/2000, Step 44, d_loss: 0.41276445984840393, g_loss: 3.580549716949463\n","Epoch 60/2000, Step 45, d_loss: 0.4519953727722168, g_loss: 2.8093926906585693\n","Epoch 60/2000, Step 46, d_loss: 0.4607733190059662, g_loss: 2.265779733657837\n","Epoch 60/2000, Step 47, d_loss: 0.4473288059234619, g_loss: 3.785127878189087\n","Epoch 60/2000, Step 48, d_loss: 0.484360933303833, g_loss: 2.966249942779541\n","Epoch 60/2000, Step 49, d_loss: 0.4014626145362854, g_loss: 2.62064790725708\n","Epoch 60/2000, Step 50, d_loss: 0.5242400765419006, g_loss: 3.3531570434570312\n","Epoch 60/2000, Step 51, d_loss: 0.6754146218299866, g_loss: 5.079977035522461\n","Epoch 60/2000, Step 52, d_loss: 0.47224557399749756, g_loss: 2.8305089473724365\n","Epoch 60/2000, Step 53, d_loss: 0.6058593988418579, g_loss: 2.2267720699310303\n","Epoch 60/2000, Step 54, d_loss: 0.5172719955444336, g_loss: 2.881171941757202\n","Epoch 60/2000, Step 55, d_loss: 0.46129804849624634, g_loss: 2.7836856842041016\n","Epoch 60/2000, Step 56, d_loss: 0.4433321952819824, g_loss: 3.2511134147644043\n","Epoch 60/2000, Step 57, d_loss: 0.4471144676208496, g_loss: 4.139213562011719\n","Epoch 60/2000, Step 58, d_loss: 0.45999598503112793, g_loss: 4.31704568862915\n","Epoch 60/2000, Step 59, d_loss: 0.4232291281223297, g_loss: 4.319979190826416\n","Epoch 60/2000, Step 60, d_loss: 0.4339093565940857, g_loss: 4.314694404602051\n","Epoch 60/2000, Step 61, d_loss: 0.5187767148017883, g_loss: 3.7637298107147217\n","Epoch 60/2000, Step 62, d_loss: 0.3721655607223511, g_loss: 3.942659616470337\n","Epoch 60/2000, Step 63, d_loss: 0.42391663789749146, g_loss: 1.7098441123962402\n","Epoch 60/2000, Step 64, d_loss: 0.48029276728630066, g_loss: 1.933962106704712\n","Epoch 60/2000, Step 65, d_loss: 0.4746868908405304, g_loss: 3.31526780128479\n","Epoch 60/2000, Step 66, d_loss: 0.6316033601760864, g_loss: 4.168145656585693\n","Epoch 60/2000, Step 67, d_loss: 0.3694370687007904, g_loss: 4.796377182006836\n","Epoch 60/2000, Step 68, d_loss: 0.4039742350578308, g_loss: 5.502189636230469\n","Epoch 60/2000, Step 69, d_loss: 0.5198717713356018, g_loss: 5.562469482421875\n","Epoch 60/2000, Step 70, d_loss: 0.44487205147743225, g_loss: 4.820284843444824\n","Epoch 60/2000, Step 71, d_loss: 0.6468353271484375, g_loss: 3.840272903442383\n","Epoch 60/2000, Step 72, d_loss: 0.4196760952472687, g_loss: 2.752638339996338\n","Epoch 60/2000, Step 73, d_loss: 0.43659454584121704, g_loss: 1.9845380783081055\n","Epoch 60/2000, Step 74, d_loss: 0.45070403814315796, g_loss: 0.7459935545921326\n","Epoch 60/2000, Step 75, d_loss: 0.6324434280395508, g_loss: 2.1010148525238037\n","Epoch 60/2000, Step 76, d_loss: 0.7967271208763123, g_loss: 2.458632469177246\n","Epoch 60/2000, Step 77, d_loss: 0.49257761240005493, g_loss: 3.208678722381592\n","Epoch 60/2000, Step 78, d_loss: 0.4311584234237671, g_loss: 5.306636810302734\n","Epoch 60/2000, Step 79, d_loss: 0.6018121242523193, g_loss: 4.007631778717041\n","Epoch 60/2000, Step 80, d_loss: 0.5805991888046265, g_loss: 2.897348165512085\n","Epoch 60/2000, Step 81, d_loss: 0.422769695520401, g_loss: 5.7610673904418945\n","Epoch 60/2000, Step 82, d_loss: 0.45001688599586487, g_loss: 2.5749435424804688\n","Epoch 60/2000, Step 83, d_loss: 0.4100354015827179, g_loss: 2.9809112548828125\n","Epoch 60/2000, Step 84, d_loss: 0.5542557835578918, g_loss: 1.5856125354766846\n","Epoch 60/2000, Step 85, d_loss: 0.44198548793792725, g_loss: 2.4433257579803467\n","Epoch 60/2000, Step 86, d_loss: 0.4479862451553345, g_loss: 1.854951024055481\n","Epoch 60/2000, Step 87, d_loss: 0.3962075412273407, g_loss: 3.620938301086426\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 61/2000, Step 1, d_loss: 0.6522266864776611, g_loss: 3.528000593185425\n","Epoch 61/2000, Step 2, d_loss: 0.3840458393096924, g_loss: 4.863908767700195\n","Epoch 61/2000, Step 3, d_loss: 0.4925572872161865, g_loss: 3.0065765380859375\n","Epoch 61/2000, Step 4, d_loss: 0.5316444039344788, g_loss: 4.066042423248291\n","Epoch 61/2000, Step 5, d_loss: 0.5244740843772888, g_loss: 3.155796766281128\n","Epoch 61/2000, Step 6, d_loss: 0.41170474886894226, g_loss: 4.001952171325684\n","Epoch 61/2000, Step 7, d_loss: 0.4402390718460083, g_loss: 3.037693977355957\n","Epoch 61/2000, Step 8, d_loss: 0.5880132913589478, g_loss: 3.0641214847564697\n","Epoch 61/2000, Step 9, d_loss: 0.48636820912361145, g_loss: 3.181337356567383\n","Epoch 61/2000, Step 10, d_loss: 0.4515145421028137, g_loss: 4.0593461990356445\n","Epoch 61/2000, Step 11, d_loss: 0.430440217256546, g_loss: 4.834475517272949\n","Epoch 61/2000, Step 12, d_loss: 0.5735092163085938, g_loss: 3.383026123046875\n","Epoch 61/2000, Step 13, d_loss: 0.43441900610923767, g_loss: 3.5370066165924072\n","Epoch 61/2000, Step 14, d_loss: 0.5012057423591614, g_loss: 3.5131192207336426\n","Epoch 61/2000, Step 15, d_loss: 0.5491809248924255, g_loss: 3.5442044734954834\n","Epoch 61/2000, Step 16, d_loss: 0.5462473630905151, g_loss: 2.5304410457611084\n","Epoch 61/2000, Step 17, d_loss: 0.44738975167274475, g_loss: 3.620170831680298\n","Epoch 61/2000, Step 18, d_loss: 0.5358980894088745, g_loss: 3.7963950634002686\n","Epoch 61/2000, Step 19, d_loss: 0.4256584048271179, g_loss: 3.749340772628784\n","Epoch 61/2000, Step 20, d_loss: 0.3954419195652008, g_loss: 5.047756195068359\n","Epoch 61/2000, Step 21, d_loss: 0.4200400114059448, g_loss: 3.8778655529022217\n","Epoch 61/2000, Step 22, d_loss: 0.40543997287750244, g_loss: 3.6167778968811035\n","Epoch 61/2000, Step 23, d_loss: 0.3870583474636078, g_loss: 4.081717491149902\n","Epoch 61/2000, Step 24, d_loss: 0.3959013819694519, g_loss: 3.665736198425293\n","Epoch 61/2000, Step 25, d_loss: 0.42799514532089233, g_loss: 3.98537278175354\n","Epoch 61/2000, Step 26, d_loss: 0.43974271416664124, g_loss: 3.7938263416290283\n","Epoch 61/2000, Step 27, d_loss: 0.4574814736843109, g_loss: 2.7211759090423584\n","Epoch 61/2000, Step 28, d_loss: 0.4193844795227051, g_loss: 3.2857513427734375\n","Epoch 61/2000, Step 29, d_loss: 0.4017367660999298, g_loss: 2.0279154777526855\n","Epoch 61/2000, Step 30, d_loss: 0.5564946532249451, g_loss: 3.2960572242736816\n","Epoch 61/2000, Step 31, d_loss: 0.4169616401195526, g_loss: 4.052523612976074\n","Epoch 61/2000, Step 32, d_loss: 0.3768409490585327, g_loss: 4.479425430297852\n","Epoch 61/2000, Step 33, d_loss: 0.393547385931015, g_loss: 4.5123443603515625\n","Epoch 61/2000, Step 34, d_loss: 0.48962822556495667, g_loss: 3.4826958179473877\n","Epoch 61/2000, Step 35, d_loss: 0.38134947419166565, g_loss: 4.454627990722656\n","Epoch 61/2000, Step 36, d_loss: 0.4672519266605377, g_loss: 4.738293170928955\n","Epoch 61/2000, Step 37, d_loss: 0.38864150643348694, g_loss: 3.5620155334472656\n","Epoch 61/2000, Step 38, d_loss: 0.4206355810165405, g_loss: 2.5107364654541016\n","Epoch 61/2000, Step 39, d_loss: 0.4365735352039337, g_loss: 2.416351556777954\n","Epoch 61/2000, Step 40, d_loss: 0.46658024191856384, g_loss: 2.825606346130371\n","Epoch 61/2000, Step 41, d_loss: 0.5018917322158813, g_loss: 2.6927812099456787\n","Epoch 61/2000, Step 42, d_loss: 0.3716132640838623, g_loss: 4.08986759185791\n","Epoch 61/2000, Step 43, d_loss: 0.426207572221756, g_loss: 3.2477805614471436\n","Epoch 61/2000, Step 44, d_loss: 0.4061412811279297, g_loss: 4.2223286628723145\n","Epoch 61/2000, Step 45, d_loss: 0.40356650948524475, g_loss: 3.875605821609497\n","Epoch 61/2000, Step 46, d_loss: 0.5553162097930908, g_loss: 3.860455274581909\n","Epoch 61/2000, Step 47, d_loss: 0.37012749910354614, g_loss: 3.103588581085205\n","Epoch 61/2000, Step 48, d_loss: 0.4873415231704712, g_loss: 3.6205761432647705\n","Epoch 61/2000, Step 49, d_loss: 0.4234735369682312, g_loss: 3.6517109870910645\n","Epoch 61/2000, Step 50, d_loss: 0.47456496953964233, g_loss: 3.5619397163391113\n","Epoch 61/2000, Step 51, d_loss: 0.4082978069782257, g_loss: 3.725935697555542\n","Epoch 61/2000, Step 52, d_loss: 0.45076897740364075, g_loss: 4.348412990570068\n","Epoch 61/2000, Step 53, d_loss: 0.41866904497146606, g_loss: 3.5311167240142822\n","Epoch 61/2000, Step 54, d_loss: 0.39203783869743347, g_loss: 4.490617752075195\n","Epoch 61/2000, Step 55, d_loss: 0.40981218218803406, g_loss: 4.275574684143066\n","Epoch 61/2000, Step 56, d_loss: 0.48933061957359314, g_loss: 3.8986525535583496\n","Epoch 61/2000, Step 57, d_loss: 0.48916688561439514, g_loss: 3.742983102798462\n","Epoch 61/2000, Step 58, d_loss: 0.45883190631866455, g_loss: 3.738267660140991\n","Epoch 61/2000, Step 59, d_loss: 0.4334498643875122, g_loss: 3.026606798171997\n","Epoch 61/2000, Step 60, d_loss: 0.3895688056945801, g_loss: 1.7638972997665405\n","Epoch 61/2000, Step 61, d_loss: 0.43884333968162537, g_loss: 2.2167739868164062\n","Epoch 61/2000, Step 62, d_loss: 0.41806671023368835, g_loss: 2.6530909538269043\n","Epoch 61/2000, Step 63, d_loss: 0.4672502875328064, g_loss: 2.9749226570129395\n","Epoch 61/2000, Step 64, d_loss: 0.4524250626564026, g_loss: 3.652531147003174\n","Epoch 61/2000, Step 65, d_loss: 0.3821226954460144, g_loss: 3.2904679775238037\n","Epoch 61/2000, Step 66, d_loss: 0.3727274239063263, g_loss: 5.018626689910889\n","Epoch 61/2000, Step 67, d_loss: 0.6004213690757751, g_loss: 4.003965377807617\n","Epoch 61/2000, Step 68, d_loss: 0.5369008183479309, g_loss: 4.193366527557373\n","Epoch 61/2000, Step 69, d_loss: 0.5858785510063171, g_loss: 3.941277027130127\n","Epoch 61/2000, Step 70, d_loss: 0.3562219440937042, g_loss: 2.7066690921783447\n","Epoch 61/2000, Step 71, d_loss: 0.5180405974388123, g_loss: 3.2244410514831543\n","Epoch 61/2000, Step 72, d_loss: 0.5285995006561279, g_loss: 3.3354313373565674\n","Epoch 61/2000, Step 73, d_loss: 0.5561084747314453, g_loss: 3.4277701377868652\n","Epoch 61/2000, Step 74, d_loss: 0.4584253132343292, g_loss: 2.6394553184509277\n","Epoch 61/2000, Step 75, d_loss: 0.4820716679096222, g_loss: 2.6241841316223145\n","Epoch 61/2000, Step 76, d_loss: 0.3880566954612732, g_loss: 3.3722476959228516\n","Epoch 61/2000, Step 77, d_loss: 0.4529339075088501, g_loss: 4.323766231536865\n","Epoch 61/2000, Step 78, d_loss: 0.6204735040664673, g_loss: 4.3588690757751465\n","Epoch 61/2000, Step 79, d_loss: 0.4477122128009796, g_loss: 2.791774272918701\n","Epoch 61/2000, Step 80, d_loss: 0.6915659308433533, g_loss: 3.8636574745178223\n","Epoch 61/2000, Step 81, d_loss: 0.4999690353870392, g_loss: 3.3418149948120117\n","Epoch 61/2000, Step 82, d_loss: 0.48832422494888306, g_loss: 2.0037176609039307\n","Epoch 61/2000, Step 83, d_loss: 0.47674334049224854, g_loss: 1.6173889636993408\n","Epoch 61/2000, Step 84, d_loss: 0.4816064238548279, g_loss: 3.5187387466430664\n","Epoch 61/2000, Step 85, d_loss: 0.5637093782424927, g_loss: 2.5799529552459717\n","Epoch 61/2000, Step 86, d_loss: 0.5523281693458557, g_loss: 3.2543869018554688\n","Epoch 61/2000, Step 87, d_loss: 0.45542171597480774, g_loss: 3.1399052143096924\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 62/2000, Step 1, d_loss: 0.3629797697067261, g_loss: 4.255483627319336\n","Epoch 62/2000, Step 2, d_loss: 0.5552467703819275, g_loss: 4.71697998046875\n","Epoch 62/2000, Step 3, d_loss: 0.518195629119873, g_loss: 2.558382272720337\n","Epoch 62/2000, Step 4, d_loss: 0.41579318046569824, g_loss: 4.259466648101807\n","Epoch 62/2000, Step 5, d_loss: 0.43956732749938965, g_loss: 2.839939832687378\n","Epoch 62/2000, Step 6, d_loss: 0.43913525342941284, g_loss: 3.0278377532958984\n","Epoch 62/2000, Step 7, d_loss: 0.560496985912323, g_loss: 3.4436399936676025\n","Epoch 62/2000, Step 8, d_loss: 0.4291687607765198, g_loss: 3.577181816101074\n","Epoch 62/2000, Step 9, d_loss: 0.4374599754810333, g_loss: 4.235170841217041\n","Epoch 62/2000, Step 10, d_loss: 0.3698895573616028, g_loss: 3.7597298622131348\n","Epoch 62/2000, Step 11, d_loss: 0.40637123584747314, g_loss: 4.6858062744140625\n","Epoch 62/2000, Step 12, d_loss: 0.49904462695121765, g_loss: 3.7313294410705566\n","Epoch 62/2000, Step 13, d_loss: 0.4594191312789917, g_loss: 3.8400819301605225\n","Epoch 62/2000, Step 14, d_loss: 0.37944498658180237, g_loss: 3.2031190395355225\n","Epoch 62/2000, Step 15, d_loss: 0.40161898732185364, g_loss: 3.71295166015625\n","Epoch 62/2000, Step 16, d_loss: 0.48704326152801514, g_loss: 3.213178873062134\n","Epoch 62/2000, Step 17, d_loss: 0.5835228562355042, g_loss: 2.941667318344116\n","Epoch 62/2000, Step 18, d_loss: 0.46983057260513306, g_loss: 2.961054563522339\n","Epoch 62/2000, Step 19, d_loss: 0.42384257912635803, g_loss: 3.7407431602478027\n","Epoch 62/2000, Step 20, d_loss: 0.4019683301448822, g_loss: 3.2595624923706055\n","Epoch 62/2000, Step 21, d_loss: 0.5563336610794067, g_loss: 4.032358169555664\n","Epoch 62/2000, Step 22, d_loss: 0.48589369654655457, g_loss: 3.791858196258545\n","Epoch 62/2000, Step 23, d_loss: 0.4172919690608978, g_loss: 3.3320984840393066\n","Epoch 62/2000, Step 24, d_loss: 0.4137074053287506, g_loss: 3.9992284774780273\n","Epoch 62/2000, Step 25, d_loss: 0.37754958868026733, g_loss: 3.3261983394622803\n","Epoch 62/2000, Step 26, d_loss: 0.3676336407661438, g_loss: 3.7699925899505615\n","Epoch 62/2000, Step 27, d_loss: 0.41329681873321533, g_loss: 4.71453332901001\n","Epoch 62/2000, Step 28, d_loss: 0.4107227921485901, g_loss: 4.718596935272217\n","Epoch 62/2000, Step 29, d_loss: 0.36975014209747314, g_loss: 3.4887495040893555\n","Epoch 62/2000, Step 30, d_loss: 0.42016977071762085, g_loss: 4.2459917068481445\n","Epoch 62/2000, Step 31, d_loss: 0.46775850653648376, g_loss: 4.0361480712890625\n","Epoch 62/2000, Step 32, d_loss: 0.3821724057197571, g_loss: 4.15961217880249\n","Epoch 62/2000, Step 33, d_loss: 0.3702303171157837, g_loss: 1.892831802368164\n","Epoch 62/2000, Step 34, d_loss: 0.43965598940849304, g_loss: 3.8040354251861572\n","Epoch 62/2000, Step 35, d_loss: 0.4030313491821289, g_loss: 3.4700357913970947\n","Epoch 62/2000, Step 36, d_loss: 0.38204705715179443, g_loss: 3.217299222946167\n","Epoch 62/2000, Step 37, d_loss: 0.3733385503292084, g_loss: 3.7740225791931152\n","Epoch 62/2000, Step 38, d_loss: 0.3955758213996887, g_loss: 3.445773124694824\n","Epoch 62/2000, Step 39, d_loss: 0.38838523626327515, g_loss: 3.4552104473114014\n","Epoch 62/2000, Step 40, d_loss: 0.387116402387619, g_loss: 5.630589008331299\n","Epoch 62/2000, Step 41, d_loss: 0.4941282272338867, g_loss: 3.603773355484009\n","Epoch 62/2000, Step 42, d_loss: 0.3535767197608948, g_loss: 3.5318195819854736\n","Epoch 62/2000, Step 43, d_loss: 0.43380093574523926, g_loss: 3.2135698795318604\n","Epoch 62/2000, Step 44, d_loss: 0.3876523971557617, g_loss: 3.6820549964904785\n","Epoch 62/2000, Step 45, d_loss: 0.4311172366142273, g_loss: 3.0308732986450195\n","Epoch 62/2000, Step 46, d_loss: 0.35812294483184814, g_loss: 2.4003007411956787\n","Epoch 62/2000, Step 47, d_loss: 0.42345625162124634, g_loss: 2.873994827270508\n","Epoch 62/2000, Step 48, d_loss: 0.4389553368091583, g_loss: 3.990253448486328\n","Epoch 62/2000, Step 49, d_loss: 0.4286598563194275, g_loss: 3.8483734130859375\n","Epoch 62/2000, Step 50, d_loss: 0.3976410925388336, g_loss: 4.798137664794922\n","Epoch 62/2000, Step 51, d_loss: 0.5675176382064819, g_loss: 5.017886161804199\n","Epoch 62/2000, Step 52, d_loss: 0.379751056432724, g_loss: 3.556525230407715\n","Epoch 62/2000, Step 53, d_loss: 0.5475199818611145, g_loss: 2.7126970291137695\n","Epoch 62/2000, Step 54, d_loss: 0.4162825047969818, g_loss: 1.6772328615188599\n","Epoch 62/2000, Step 55, d_loss: 0.5057329535484314, g_loss: 3.1812801361083984\n","Epoch 62/2000, Step 56, d_loss: 0.464844286441803, g_loss: 2.8729798793792725\n","Epoch 62/2000, Step 57, d_loss: 0.4384561777114868, g_loss: 3.033121347427368\n","Epoch 62/2000, Step 58, d_loss: 0.4221680462360382, g_loss: 3.4209234714508057\n","Epoch 62/2000, Step 59, d_loss: 0.4647108316421509, g_loss: 5.054575443267822\n","Epoch 62/2000, Step 60, d_loss: 0.4446350634098053, g_loss: 4.180339813232422\n","Epoch 62/2000, Step 61, d_loss: 0.39379140734672546, g_loss: 4.186901569366455\n","Epoch 62/2000, Step 62, d_loss: 0.4125056862831116, g_loss: 3.081392288208008\n","Epoch 62/2000, Step 63, d_loss: 0.45411258935928345, g_loss: 3.3701255321502686\n","Epoch 62/2000, Step 64, d_loss: 0.4293223023414612, g_loss: 2.9867677688598633\n","Epoch 62/2000, Step 65, d_loss: 0.38172271847724915, g_loss: 1.9673428535461426\n","Epoch 62/2000, Step 66, d_loss: 0.41396641731262207, g_loss: 3.570993661880493\n","Epoch 62/2000, Step 67, d_loss: 0.42851388454437256, g_loss: 3.1619718074798584\n","Epoch 62/2000, Step 68, d_loss: 0.5323671698570251, g_loss: 3.373915910720825\n","Epoch 62/2000, Step 69, d_loss: 0.38993045687675476, g_loss: 3.365138292312622\n","Epoch 62/2000, Step 70, d_loss: 0.43381279706954956, g_loss: 3.3747408390045166\n","Epoch 62/2000, Step 71, d_loss: 0.4178301990032196, g_loss: 3.6852879524230957\n","Epoch 62/2000, Step 72, d_loss: 0.3728168308734894, g_loss: 3.8896827697753906\n","Epoch 62/2000, Step 73, d_loss: 0.4212384819984436, g_loss: 5.399396896362305\n","Epoch 62/2000, Step 74, d_loss: 0.3976549208164215, g_loss: 3.53426194190979\n","Epoch 62/2000, Step 75, d_loss: 0.37837710976600647, g_loss: 4.495477676391602\n","Epoch 62/2000, Step 76, d_loss: 0.40673089027404785, g_loss: 4.700669288635254\n","Epoch 62/2000, Step 77, d_loss: 0.3651045262813568, g_loss: 3.0136373043060303\n","Epoch 62/2000, Step 78, d_loss: 0.3801250457763672, g_loss: 3.4299228191375732\n","Epoch 62/2000, Step 79, d_loss: 0.39362505078315735, g_loss: 3.0621159076690674\n","Epoch 62/2000, Step 80, d_loss: 0.3656584620475769, g_loss: 4.253854274749756\n","Epoch 62/2000, Step 81, d_loss: 0.4395506978034973, g_loss: 4.284955978393555\n","Epoch 62/2000, Step 82, d_loss: 0.46936577558517456, g_loss: 3.721501588821411\n","Epoch 62/2000, Step 83, d_loss: 0.444396048784256, g_loss: 3.952589750289917\n","Epoch 62/2000, Step 84, d_loss: 0.4244758188724518, g_loss: 4.492547512054443\n","Epoch 62/2000, Step 85, d_loss: 0.44614091515541077, g_loss: 4.771695613861084\n","Epoch 62/2000, Step 86, d_loss: 0.41529929637908936, g_loss: 5.172767639160156\n","Epoch 62/2000, Step 87, d_loss: 0.4621506333351135, g_loss: 5.456785678863525\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 63/2000, Step 1, d_loss: 0.43378233909606934, g_loss: 3.9063560962677\n","Epoch 63/2000, Step 2, d_loss: 0.422000527381897, g_loss: 2.861661911010742\n","Epoch 63/2000, Step 3, d_loss: 0.350155770778656, g_loss: 3.3581624031066895\n","Epoch 63/2000, Step 4, d_loss: 0.37102010846138, g_loss: 3.2364330291748047\n","Epoch 63/2000, Step 5, d_loss: 0.6209878325462341, g_loss: 3.2372756004333496\n","Epoch 63/2000, Step 6, d_loss: 0.4301171600818634, g_loss: 2.6211111545562744\n","Epoch 63/2000, Step 7, d_loss: 0.48186182975769043, g_loss: 3.3540914058685303\n","Epoch 63/2000, Step 8, d_loss: 0.382481187582016, g_loss: 4.450448513031006\n","Epoch 63/2000, Step 9, d_loss: 0.4474676251411438, g_loss: 4.731462478637695\n","Epoch 63/2000, Step 10, d_loss: 0.3654502034187317, g_loss: 4.997276782989502\n","Epoch 63/2000, Step 11, d_loss: 0.5840056538581848, g_loss: 3.9685089588165283\n","Epoch 63/2000, Step 12, d_loss: 0.44685879349708557, g_loss: 4.093920707702637\n","Epoch 63/2000, Step 13, d_loss: 0.4127193093299866, g_loss: 2.44172739982605\n","Epoch 63/2000, Step 14, d_loss: 0.44687217473983765, g_loss: 3.0089643001556396\n","Epoch 63/2000, Step 15, d_loss: 0.4071023166179657, g_loss: 2.559471368789673\n","Epoch 63/2000, Step 16, d_loss: 0.49944689869880676, g_loss: 2.7628962993621826\n","Epoch 63/2000, Step 17, d_loss: 0.4293094575405121, g_loss: 2.5536701679229736\n","Epoch 63/2000, Step 18, d_loss: 0.5463957190513611, g_loss: 3.213003158569336\n","Epoch 63/2000, Step 19, d_loss: 0.45218732953071594, g_loss: 3.4029831886291504\n","Epoch 63/2000, Step 20, d_loss: 0.45162785053253174, g_loss: 3.867560625076294\n","Epoch 63/2000, Step 21, d_loss: 0.4737728238105774, g_loss: 4.417579650878906\n","Epoch 63/2000, Step 22, d_loss: 0.3826941251754761, g_loss: 3.8261606693267822\n","Epoch 63/2000, Step 23, d_loss: 0.43869057297706604, g_loss: 4.222993850708008\n","Epoch 63/2000, Step 24, d_loss: 0.406250536441803, g_loss: 3.574237823486328\n","Epoch 63/2000, Step 25, d_loss: 0.37908557057380676, g_loss: 3.419820547103882\n","Epoch 63/2000, Step 26, d_loss: 0.3752076029777527, g_loss: 3.378124475479126\n","Epoch 63/2000, Step 27, d_loss: 0.42267993092536926, g_loss: 3.382878541946411\n","Epoch 63/2000, Step 28, d_loss: 0.48884764313697815, g_loss: 3.3659636974334717\n","Epoch 63/2000, Step 29, d_loss: 0.36946406960487366, g_loss: 3.271116256713867\n","Epoch 63/2000, Step 30, d_loss: 0.6511867046356201, g_loss: 5.262887001037598\n","Epoch 63/2000, Step 31, d_loss: 0.4968782365322113, g_loss: 1.9013988971710205\n","Epoch 63/2000, Step 32, d_loss: 0.47443148493766785, g_loss: 2.595301389694214\n","Epoch 63/2000, Step 33, d_loss: 0.590634822845459, g_loss: 2.8278231620788574\n","Epoch 63/2000, Step 34, d_loss: 0.498771995306015, g_loss: 3.564448118209839\n","Epoch 63/2000, Step 35, d_loss: 0.40523847937583923, g_loss: 5.737905025482178\n","Epoch 63/2000, Step 36, d_loss: 0.3819442689418793, g_loss: 4.56833553314209\n","Epoch 63/2000, Step 37, d_loss: 0.8051074743270874, g_loss: 4.165223121643066\n","Epoch 63/2000, Step 38, d_loss: 0.4212060570716858, g_loss: 3.8588857650756836\n","Epoch 63/2000, Step 39, d_loss: 0.38740330934524536, g_loss: 2.589165449142456\n","Epoch 63/2000, Step 40, d_loss: 0.40162619948387146, g_loss: 3.1115148067474365\n","Epoch 63/2000, Step 41, d_loss: 0.4154757857322693, g_loss: 3.2186315059661865\n","Epoch 63/2000, Step 42, d_loss: 0.42584118247032166, g_loss: 4.312766075134277\n","Epoch 63/2000, Step 43, d_loss: 0.37392324209213257, g_loss: 3.53855299949646\n","Epoch 63/2000, Step 44, d_loss: 0.3967181444168091, g_loss: 5.93641471862793\n","Epoch 63/2000, Step 45, d_loss: 0.4244765043258667, g_loss: 4.267847537994385\n","Epoch 63/2000, Step 46, d_loss: 0.4618907868862152, g_loss: 5.098357677459717\n","Epoch 63/2000, Step 47, d_loss: 0.42201805114746094, g_loss: 2.090815782546997\n","Epoch 63/2000, Step 48, d_loss: 0.41755211353302, g_loss: 2.766674041748047\n","Epoch 63/2000, Step 49, d_loss: 0.4554590582847595, g_loss: 2.3016250133514404\n","Epoch 63/2000, Step 50, d_loss: 0.39364174008369446, g_loss: 2.283661365509033\n","Epoch 63/2000, Step 51, d_loss: 0.6753848791122437, g_loss: 4.4571943283081055\n","Epoch 63/2000, Step 52, d_loss: 0.44091254472732544, g_loss: 3.7947728633880615\n","Epoch 63/2000, Step 53, d_loss: 0.3876214623451233, g_loss: 5.200310230255127\n","Epoch 63/2000, Step 54, d_loss: 0.5864498615264893, g_loss: 4.510802745819092\n","Epoch 63/2000, Step 55, d_loss: 0.7203869819641113, g_loss: 3.054084300994873\n","Epoch 63/2000, Step 56, d_loss: 0.4434967637062073, g_loss: 1.6060317754745483\n","Epoch 63/2000, Step 57, d_loss: 0.53265380859375, g_loss: 2.7126734256744385\n","Epoch 63/2000, Step 58, d_loss: 0.5768149495124817, g_loss: 3.160362720489502\n","Epoch 63/2000, Step 59, d_loss: 0.5225744843482971, g_loss: 1.9887844324111938\n","Epoch 63/2000, Step 60, d_loss: 0.7715706825256348, g_loss: 3.2275853157043457\n","Epoch 63/2000, Step 61, d_loss: 0.4856262803077698, g_loss: 4.6130194664001465\n","Epoch 63/2000, Step 62, d_loss: 0.4530048966407776, g_loss: 6.21126127243042\n","Epoch 63/2000, Step 63, d_loss: 0.46132785081863403, g_loss: 3.7932400703430176\n","Epoch 63/2000, Step 64, d_loss: 0.4178538918495178, g_loss: 4.681615352630615\n","Epoch 63/2000, Step 65, d_loss: 0.41234713792800903, g_loss: 4.7379608154296875\n","Epoch 63/2000, Step 66, d_loss: 0.38668859004974365, g_loss: 3.650043249130249\n","Epoch 63/2000, Step 67, d_loss: 0.3805041015148163, g_loss: 3.621964931488037\n","Epoch 63/2000, Step 68, d_loss: 0.3870634436607361, g_loss: 3.683548927307129\n","Epoch 63/2000, Step 69, d_loss: 0.36780643463134766, g_loss: 3.339459180831909\n","Epoch 63/2000, Step 70, d_loss: 0.39245182275772095, g_loss: 4.855015277862549\n","Epoch 63/2000, Step 71, d_loss: 0.44191110134124756, g_loss: 3.0694644451141357\n","Epoch 63/2000, Step 72, d_loss: 0.3736487030982971, g_loss: 4.23801326751709\n","Epoch 63/2000, Step 73, d_loss: 0.41718387603759766, g_loss: 2.7624013423919678\n","Epoch 63/2000, Step 74, d_loss: 0.4118019640445709, g_loss: 2.670813798904419\n","Epoch 63/2000, Step 75, d_loss: 0.4059574007987976, g_loss: 3.6946990489959717\n","Epoch 63/2000, Step 76, d_loss: 0.405422180891037, g_loss: 2.9430148601531982\n","Epoch 63/2000, Step 77, d_loss: 0.43817687034606934, g_loss: 3.6316847801208496\n","Epoch 63/2000, Step 78, d_loss: 0.6165541410446167, g_loss: 2.5422701835632324\n","Epoch 63/2000, Step 79, d_loss: 0.4289874732494354, g_loss: 2.6539835929870605\n","Epoch 63/2000, Step 80, d_loss: 0.4121386706829071, g_loss: 4.488268852233887\n","Epoch 63/2000, Step 81, d_loss: 0.4365711212158203, g_loss: 3.420309543609619\n","Epoch 63/2000, Step 82, d_loss: 0.38415586948394775, g_loss: 5.386802673339844\n","Epoch 63/2000, Step 83, d_loss: 0.38440895080566406, g_loss: 4.45809268951416\n","Epoch 63/2000, Step 84, d_loss: 0.4985017478466034, g_loss: 4.705995082855225\n","Epoch 63/2000, Step 85, d_loss: 0.39562830328941345, g_loss: 4.581531524658203\n","Epoch 63/2000, Step 86, d_loss: 0.448883056640625, g_loss: 2.2648496627807617\n","Epoch 63/2000, Step 87, d_loss: 0.4416903555393219, g_loss: 2.618016481399536\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 64/2000, Step 1, d_loss: 0.6246017217636108, g_loss: 3.4632861614227295\n","Epoch 64/2000, Step 2, d_loss: 0.49905288219451904, g_loss: 3.958451747894287\n","Epoch 64/2000, Step 3, d_loss: 0.4510440528392792, g_loss: 4.628726959228516\n","Epoch 64/2000, Step 4, d_loss: 0.44474753737449646, g_loss: 4.681207656860352\n","Epoch 64/2000, Step 5, d_loss: 0.5121192932128906, g_loss: 4.053092002868652\n","Epoch 64/2000, Step 6, d_loss: 0.3826884329319, g_loss: 4.116875171661377\n","Epoch 64/2000, Step 7, d_loss: 0.6338405609130859, g_loss: 3.4183602333068848\n","Epoch 64/2000, Step 8, d_loss: 0.5591601729393005, g_loss: 2.7023682594299316\n","Epoch 64/2000, Step 9, d_loss: 0.4935823678970337, g_loss: 2.937587022781372\n","Epoch 64/2000, Step 10, d_loss: 0.5764333605766296, g_loss: 1.9779902696609497\n","Epoch 64/2000, Step 11, d_loss: 0.5687999725341797, g_loss: 2.44097638130188\n","Epoch 64/2000, Step 12, d_loss: 0.585002064704895, g_loss: 2.1410508155822754\n","Epoch 64/2000, Step 13, d_loss: 0.5219724774360657, g_loss: 1.654863715171814\n","Epoch 64/2000, Step 14, d_loss: 0.4356920123100281, g_loss: 1.7005102634429932\n","Epoch 64/2000, Step 15, d_loss: 0.5199494957923889, g_loss: 3.11197829246521\n","Epoch 64/2000, Step 16, d_loss: 0.4388701319694519, g_loss: 3.6402053833007812\n","Epoch 64/2000, Step 17, d_loss: 0.4722723960876465, g_loss: 4.485777854919434\n","Epoch 64/2000, Step 18, d_loss: 0.4255238175392151, g_loss: 5.228150367736816\n","Epoch 64/2000, Step 19, d_loss: 0.5265095829963684, g_loss: 4.846362113952637\n","Epoch 64/2000, Step 20, d_loss: 0.4080301523208618, g_loss: 3.3595221042633057\n","Epoch 64/2000, Step 21, d_loss: 0.41806042194366455, g_loss: 3.4270243644714355\n","Epoch 64/2000, Step 22, d_loss: 0.4121047258377075, g_loss: 4.8070220947265625\n","Epoch 64/2000, Step 23, d_loss: 0.4281978905200958, g_loss: 2.9248032569885254\n","Epoch 64/2000, Step 24, d_loss: 0.3692399561405182, g_loss: 2.695627212524414\n","Epoch 64/2000, Step 25, d_loss: 0.44741684198379517, g_loss: 3.529423952102661\n","Epoch 64/2000, Step 26, d_loss: 0.4127213656902313, g_loss: 3.646740674972534\n","Epoch 64/2000, Step 27, d_loss: 0.38704580068588257, g_loss: 3.9925477504730225\n","Epoch 64/2000, Step 28, d_loss: 0.4372248947620392, g_loss: 4.0704216957092285\n","Epoch 64/2000, Step 29, d_loss: 0.40791842341423035, g_loss: 3.9780170917510986\n","Epoch 64/2000, Step 30, d_loss: 0.4550439715385437, g_loss: 4.600718021392822\n","Epoch 64/2000, Step 31, d_loss: 0.44831937551498413, g_loss: 4.705283164978027\n","Epoch 64/2000, Step 32, d_loss: 0.4757170081138611, g_loss: 4.385550022125244\n","Epoch 64/2000, Step 33, d_loss: 0.4747040271759033, g_loss: 4.800215244293213\n","Epoch 64/2000, Step 34, d_loss: 0.44911080598831177, g_loss: 3.0929434299468994\n","Epoch 64/2000, Step 35, d_loss: 0.3931955099105835, g_loss: 3.7909953594207764\n","Epoch 64/2000, Step 36, d_loss: 0.39717262983322144, g_loss: 3.131669759750366\n","Epoch 64/2000, Step 37, d_loss: 0.3777439296245575, g_loss: 3.407947301864624\n","Epoch 64/2000, Step 38, d_loss: 0.37484702467918396, g_loss: 3.5978634357452393\n","Epoch 64/2000, Step 39, d_loss: 0.43887054920196533, g_loss: 4.295921802520752\n","Epoch 64/2000, Step 40, d_loss: 0.4270611107349396, g_loss: 3.072416305541992\n","Epoch 64/2000, Step 41, d_loss: 0.4236050844192505, g_loss: 3.890859603881836\n","Epoch 64/2000, Step 42, d_loss: 0.37788712978363037, g_loss: 4.7417168617248535\n","Epoch 64/2000, Step 43, d_loss: 0.41799578070640564, g_loss: 4.17469596862793\n","Epoch 64/2000, Step 44, d_loss: 0.3733851909637451, g_loss: 4.8149237632751465\n","Epoch 64/2000, Step 45, d_loss: 0.41305866837501526, g_loss: 4.31996488571167\n","Epoch 64/2000, Step 46, d_loss: 0.36833328008651733, g_loss: 3.579080581665039\n","Epoch 64/2000, Step 47, d_loss: 0.3757111728191376, g_loss: 2.6929774284362793\n","Epoch 64/2000, Step 48, d_loss: 0.3940742313861847, g_loss: 2.559624671936035\n","Epoch 64/2000, Step 49, d_loss: 0.418246328830719, g_loss: 2.033832550048828\n","Epoch 64/2000, Step 50, d_loss: 0.37205350399017334, g_loss: 3.0436832904815674\n","Epoch 64/2000, Step 51, d_loss: 0.40292730927467346, g_loss: 3.415168523788452\n","Epoch 64/2000, Step 52, d_loss: 0.39870068430900574, g_loss: 3.6875860691070557\n","Epoch 64/2000, Step 53, d_loss: 0.37374234199523926, g_loss: 3.234525203704834\n","Epoch 64/2000, Step 54, d_loss: 0.3963758647441864, g_loss: 2.9861228466033936\n","Epoch 64/2000, Step 55, d_loss: 0.3790247142314911, g_loss: 4.766068458557129\n","Epoch 64/2000, Step 56, d_loss: 0.35454827547073364, g_loss: 5.017982006072998\n","Epoch 64/2000, Step 57, d_loss: 0.4087230861186981, g_loss: 5.2488250732421875\n","Epoch 64/2000, Step 58, d_loss: 0.42823123931884766, g_loss: 4.201198101043701\n","Epoch 64/2000, Step 59, d_loss: 0.4335363805294037, g_loss: 3.474104166030884\n","Epoch 64/2000, Step 60, d_loss: 0.39381563663482666, g_loss: 4.032284736633301\n","Epoch 64/2000, Step 61, d_loss: 0.41606488823890686, g_loss: 4.73135232925415\n","Epoch 64/2000, Step 62, d_loss: 0.39513519406318665, g_loss: 2.874713182449341\n","Epoch 64/2000, Step 63, d_loss: 0.36743414402008057, g_loss: 2.9484188556671143\n","Epoch 64/2000, Step 64, d_loss: 0.4057660698890686, g_loss: 3.4433114528656006\n","Epoch 64/2000, Step 65, d_loss: 0.3837069571018219, g_loss: 2.8634376525878906\n","Epoch 64/2000, Step 66, d_loss: 0.4580672085285187, g_loss: 3.915957450866699\n","Epoch 64/2000, Step 67, d_loss: 0.40553802251815796, g_loss: 2.9611740112304688\n","Epoch 64/2000, Step 68, d_loss: 0.492728054523468, g_loss: 2.6585512161254883\n","Epoch 64/2000, Step 69, d_loss: 0.41353312134742737, g_loss: 3.4593160152435303\n","Epoch 64/2000, Step 70, d_loss: 0.3911595642566681, g_loss: 3.8059775829315186\n","Epoch 64/2000, Step 71, d_loss: 0.3810955584049225, g_loss: 3.612030267715454\n","Epoch 64/2000, Step 72, d_loss: 0.42430612444877625, g_loss: 3.1467909812927246\n","Epoch 64/2000, Step 73, d_loss: 0.38951539993286133, g_loss: 3.652663469314575\n","Epoch 64/2000, Step 74, d_loss: 0.39174938201904297, g_loss: 4.257204055786133\n","Epoch 64/2000, Step 75, d_loss: 0.3703102767467499, g_loss: 3.534351348876953\n","Epoch 64/2000, Step 76, d_loss: 0.4648037254810333, g_loss: 2.872603416442871\n","Epoch 64/2000, Step 77, d_loss: 0.37444064021110535, g_loss: 3.7909228801727295\n","Epoch 64/2000, Step 78, d_loss: 0.3895018696784973, g_loss: 3.634401321411133\n","Epoch 64/2000, Step 79, d_loss: 0.4032469093799591, g_loss: 3.3000845909118652\n","Epoch 64/2000, Step 80, d_loss: 0.42249786853790283, g_loss: 3.4988391399383545\n","Epoch 64/2000, Step 81, d_loss: 0.4776439964771271, g_loss: 4.432122230529785\n","Epoch 64/2000, Step 82, d_loss: 0.41308289766311646, g_loss: 4.262404918670654\n","Epoch 64/2000, Step 83, d_loss: 0.4150523841381073, g_loss: 4.747978687286377\n","Epoch 64/2000, Step 84, d_loss: 0.45938995480537415, g_loss: 2.394392490386963\n","Epoch 64/2000, Step 85, d_loss: 0.48444944620132446, g_loss: 4.12972354888916\n","Epoch 64/2000, Step 86, d_loss: 0.4358835816383362, g_loss: 2.9562437534332275\n","Epoch 64/2000, Step 87, d_loss: 0.4161950647830963, g_loss: 3.406083583831787\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 65/2000, Step 1, d_loss: 0.38613900542259216, g_loss: 3.5546934604644775\n","Epoch 65/2000, Step 2, d_loss: 0.4284270107746124, g_loss: 3.7627246379852295\n","Epoch 65/2000, Step 3, d_loss: 0.4093030095100403, g_loss: 4.726888656616211\n","Epoch 65/2000, Step 4, d_loss: 0.3713580071926117, g_loss: 3.1513214111328125\n","Epoch 65/2000, Step 5, d_loss: 0.367093563079834, g_loss: 4.369643688201904\n","Epoch 65/2000, Step 6, d_loss: 0.39077070355415344, g_loss: 4.3581461906433105\n","Epoch 65/2000, Step 7, d_loss: 0.42108961939811707, g_loss: 4.682361602783203\n","Epoch 65/2000, Step 8, d_loss: 0.36077606678009033, g_loss: 3.7020578384399414\n","Epoch 65/2000, Step 9, d_loss: 0.3773146867752075, g_loss: 4.905129432678223\n","Epoch 65/2000, Step 10, d_loss: 0.36195093393325806, g_loss: 2.7076985836029053\n","Epoch 65/2000, Step 11, d_loss: 0.3971279561519623, g_loss: 3.6333017349243164\n","Epoch 65/2000, Step 12, d_loss: 0.3701747953891754, g_loss: 3.3028478622436523\n","Epoch 65/2000, Step 13, d_loss: 0.3652958571910858, g_loss: 3.6095781326293945\n","Epoch 65/2000, Step 14, d_loss: 0.4069732129573822, g_loss: 4.510586261749268\n","Epoch 65/2000, Step 15, d_loss: 0.3612906336784363, g_loss: 3.7228195667266846\n","Epoch 65/2000, Step 16, d_loss: 0.5562763214111328, g_loss: 4.010247230529785\n","Epoch 65/2000, Step 17, d_loss: 0.40866753458976746, g_loss: 4.107647895812988\n","Epoch 65/2000, Step 18, d_loss: 0.4059809148311615, g_loss: 3.737755060195923\n","Epoch 65/2000, Step 19, d_loss: 0.7020969986915588, g_loss: 3.1912848949432373\n","Epoch 65/2000, Step 20, d_loss: 0.43377649784088135, g_loss: 2.760178804397583\n","Epoch 65/2000, Step 21, d_loss: 0.5413639545440674, g_loss: 4.336662769317627\n","Epoch 65/2000, Step 22, d_loss: 0.48170387744903564, g_loss: 3.7551989555358887\n","Epoch 65/2000, Step 23, d_loss: 0.4728343188762665, g_loss: 4.444851398468018\n","Epoch 65/2000, Step 24, d_loss: 0.49430039525032043, g_loss: 3.1077964305877686\n","Epoch 65/2000, Step 25, d_loss: 0.49933359026908875, g_loss: 3.595869779586792\n","Epoch 65/2000, Step 26, d_loss: 0.3954528868198395, g_loss: 3.3629703521728516\n","Epoch 65/2000, Step 27, d_loss: 0.4451315701007843, g_loss: 3.1435558795928955\n","Epoch 65/2000, Step 28, d_loss: 0.49217405915260315, g_loss: 3.229095697402954\n","Epoch 65/2000, Step 29, d_loss: 0.5443788766860962, g_loss: 2.6220061779022217\n","Epoch 65/2000, Step 30, d_loss: 0.4207943081855774, g_loss: 3.265500545501709\n","Epoch 65/2000, Step 31, d_loss: 0.5472978949546814, g_loss: 2.8825783729553223\n","Epoch 65/2000, Step 32, d_loss: 0.5072934627532959, g_loss: 3.3875491619110107\n","Epoch 65/2000, Step 33, d_loss: 0.474208265542984, g_loss: 4.4361252784729\n","Epoch 65/2000, Step 34, d_loss: 0.5395861268043518, g_loss: 4.395518779754639\n","Epoch 65/2000, Step 35, d_loss: 0.4558227062225342, g_loss: 4.478703022003174\n","Epoch 65/2000, Step 36, d_loss: 0.4847990572452545, g_loss: 2.79178786277771\n","Epoch 65/2000, Step 37, d_loss: 0.43634891510009766, g_loss: 2.598970890045166\n","Epoch 65/2000, Step 38, d_loss: 0.5395447015762329, g_loss: 2.6096103191375732\n","Epoch 65/2000, Step 39, d_loss: 0.5844477415084839, g_loss: 2.306262731552124\n","Epoch 65/2000, Step 40, d_loss: 0.5488239526748657, g_loss: 3.3334507942199707\n","Epoch 65/2000, Step 41, d_loss: 0.488812655210495, g_loss: 3.219531774520874\n","Epoch 65/2000, Step 42, d_loss: 0.4400734603404999, g_loss: 2.83429217338562\n","Epoch 65/2000, Step 43, d_loss: 0.44777533411979675, g_loss: 2.6494452953338623\n","Epoch 65/2000, Step 44, d_loss: 0.5030725002288818, g_loss: 3.8934717178344727\n","Epoch 65/2000, Step 45, d_loss: 0.4143686890602112, g_loss: 4.6634416580200195\n","Epoch 65/2000, Step 46, d_loss: 0.4762536287307739, g_loss: 4.46298885345459\n","Epoch 65/2000, Step 47, d_loss: 0.37303704023361206, g_loss: 3.878178834915161\n","Epoch 65/2000, Step 48, d_loss: 0.4804324209690094, g_loss: 4.151276588439941\n","Epoch 65/2000, Step 49, d_loss: 0.37647750973701477, g_loss: 2.8468515872955322\n","Epoch 65/2000, Step 50, d_loss: 0.464394211769104, g_loss: 4.014461517333984\n","Epoch 65/2000, Step 51, d_loss: 0.3879129886627197, g_loss: 3.644240140914917\n","Epoch 65/2000, Step 52, d_loss: 0.47952669858932495, g_loss: 3.8477461338043213\n","Epoch 65/2000, Step 53, d_loss: 0.447320818901062, g_loss: 4.889679908752441\n","Epoch 65/2000, Step 54, d_loss: 0.4146190583705902, g_loss: 3.8555452823638916\n","Epoch 65/2000, Step 55, d_loss: 0.48727312684059143, g_loss: 4.183751583099365\n","Epoch 65/2000, Step 56, d_loss: 0.4148753583431244, g_loss: 2.8752377033233643\n","Epoch 65/2000, Step 57, d_loss: 0.4359460771083832, g_loss: 2.592503786087036\n","Epoch 65/2000, Step 58, d_loss: 0.44547978043556213, g_loss: 2.458164930343628\n","Epoch 65/2000, Step 59, d_loss: 0.39779365062713623, g_loss: 2.859653949737549\n","Epoch 65/2000, Step 60, d_loss: 0.4181685149669647, g_loss: 3.598405599594116\n","Epoch 65/2000, Step 61, d_loss: 0.4275299310684204, g_loss: 2.8473503589630127\n","Epoch 65/2000, Step 62, d_loss: 0.5292460322380066, g_loss: 2.733591079711914\n","Epoch 65/2000, Step 63, d_loss: 0.4276295006275177, g_loss: 3.574899435043335\n","Epoch 65/2000, Step 64, d_loss: 0.39427661895751953, g_loss: 3.860280990600586\n","Epoch 65/2000, Step 65, d_loss: 0.4091361165046692, g_loss: 3.733959197998047\n","Epoch 65/2000, Step 66, d_loss: 0.5209518671035767, g_loss: 3.3972249031066895\n","Epoch 65/2000, Step 67, d_loss: 0.46269533038139343, g_loss: 5.078390598297119\n","Epoch 65/2000, Step 68, d_loss: 0.4282681345939636, g_loss: 4.4145188331604\n","Epoch 65/2000, Step 69, d_loss: 0.42825284600257874, g_loss: 3.218163251876831\n","Epoch 65/2000, Step 70, d_loss: 0.3720499277114868, g_loss: 3.618713140487671\n","Epoch 65/2000, Step 71, d_loss: 0.4883069396018982, g_loss: 4.210959434509277\n","Epoch 65/2000, Step 72, d_loss: 0.46965739130973816, g_loss: 3.2463488578796387\n","Epoch 65/2000, Step 73, d_loss: 0.40660974383354187, g_loss: 2.9223315715789795\n","Epoch 65/2000, Step 74, d_loss: 0.44354355335235596, g_loss: 4.467571258544922\n","Epoch 65/2000, Step 75, d_loss: 0.45872291922569275, g_loss: 5.301323413848877\n","Epoch 65/2000, Step 76, d_loss: 0.5040242075920105, g_loss: 3.6608798503875732\n","Epoch 65/2000, Step 77, d_loss: 0.3935163915157318, g_loss: 3.575967311859131\n","Epoch 65/2000, Step 78, d_loss: 0.37006500363349915, g_loss: 3.3152568340301514\n","Epoch 65/2000, Step 79, d_loss: 0.40193790197372437, g_loss: 3.6508941650390625\n","Epoch 65/2000, Step 80, d_loss: 0.3859550356864929, g_loss: 3.1766562461853027\n","Epoch 65/2000, Step 81, d_loss: 0.4178312420845032, g_loss: 2.827063798904419\n","Epoch 65/2000, Step 82, d_loss: 0.4937269687652588, g_loss: 5.117574214935303\n","Epoch 65/2000, Step 83, d_loss: 0.4524056017398834, g_loss: 2.536092519760132\n","Epoch 65/2000, Step 84, d_loss: 0.3942890763282776, g_loss: 2.2860732078552246\n","Epoch 65/2000, Step 85, d_loss: 0.3743334710597992, g_loss: 2.9679460525512695\n","Epoch 65/2000, Step 86, d_loss: 0.427979975938797, g_loss: 3.718630313873291\n","Epoch 65/2000, Step 87, d_loss: 0.38229086995124817, g_loss: 5.3537397384643555\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 66/2000, Step 1, d_loss: 0.4525054693222046, g_loss: 4.1201605796813965\n","Epoch 66/2000, Step 2, d_loss: 0.38143596053123474, g_loss: 4.205053806304932\n","Epoch 66/2000, Step 3, d_loss: 0.4091552495956421, g_loss: 3.921170473098755\n","Epoch 66/2000, Step 4, d_loss: 0.5762863159179688, g_loss: 3.1637086868286133\n","Epoch 66/2000, Step 5, d_loss: 0.4075574576854706, g_loss: 3.1580240726470947\n","Epoch 66/2000, Step 6, d_loss: 0.3836383819580078, g_loss: 2.7242298126220703\n","Epoch 66/2000, Step 7, d_loss: 0.4544456899166107, g_loss: 1.9125479459762573\n","Epoch 66/2000, Step 8, d_loss: 0.562126100063324, g_loss: 2.2868547439575195\n","Epoch 66/2000, Step 9, d_loss: 0.46710431575775146, g_loss: 2.1480965614318848\n","Epoch 66/2000, Step 10, d_loss: 0.43596795201301575, g_loss: 3.504818916320801\n","Epoch 66/2000, Step 11, d_loss: 0.45883291959762573, g_loss: 3.1008188724517822\n","Epoch 66/2000, Step 12, d_loss: 0.47675061225891113, g_loss: 4.599667549133301\n","Epoch 66/2000, Step 13, d_loss: 0.4337451756000519, g_loss: 3.7371058464050293\n","Epoch 66/2000, Step 14, d_loss: 0.5444203019142151, g_loss: 4.551461219787598\n","Epoch 66/2000, Step 15, d_loss: 0.369689404964447, g_loss: 4.451239585876465\n","Epoch 66/2000, Step 16, d_loss: 0.4000560939311981, g_loss: 3.893996477127075\n","Epoch 66/2000, Step 17, d_loss: 0.45322108268737793, g_loss: 4.306853294372559\n","Epoch 66/2000, Step 18, d_loss: 0.5351845622062683, g_loss: 4.2654290199279785\n","Epoch 66/2000, Step 19, d_loss: 0.5910151600837708, g_loss: 2.633465051651001\n","Epoch 66/2000, Step 20, d_loss: 0.4104986786842346, g_loss: 3.4189767837524414\n","Epoch 66/2000, Step 21, d_loss: 0.47659969329833984, g_loss: 3.698622703552246\n","Epoch 66/2000, Step 22, d_loss: 0.5885385274887085, g_loss: 2.352647542953491\n","Epoch 66/2000, Step 23, d_loss: 0.5690363049507141, g_loss: 3.2290825843811035\n","Epoch 66/2000, Step 24, d_loss: 0.9466137886047363, g_loss: 2.604421377182007\n","Epoch 66/2000, Step 25, d_loss: 0.48957303166389465, g_loss: 1.5157966613769531\n","Epoch 66/2000, Step 26, d_loss: 0.5269893407821655, g_loss: 3.1260440349578857\n","Epoch 66/2000, Step 27, d_loss: 0.7254960536956787, g_loss: 2.683767318725586\n","Epoch 66/2000, Step 28, d_loss: 1.0149471759796143, g_loss: 2.8225021362304688\n","Epoch 66/2000, Step 29, d_loss: 0.5523348450660706, g_loss: 2.56245756149292\n","Epoch 66/2000, Step 30, d_loss: 0.5346655249595642, g_loss: 2.688828468322754\n","Epoch 66/2000, Step 31, d_loss: 0.5066943168640137, g_loss: 2.4048924446105957\n","Epoch 66/2000, Step 32, d_loss: 0.5461533665657043, g_loss: 3.4935715198516846\n","Epoch 66/2000, Step 33, d_loss: 0.5624622106552124, g_loss: 4.2766032218933105\n","Epoch 66/2000, Step 34, d_loss: 0.5180723667144775, g_loss: 3.149855613708496\n","Epoch 66/2000, Step 35, d_loss: 0.5802431702613831, g_loss: 2.6554088592529297\n","Epoch 66/2000, Step 36, d_loss: 0.4443610906600952, g_loss: 3.7485084533691406\n","Epoch 66/2000, Step 37, d_loss: 0.43865278363227844, g_loss: 3.4754693508148193\n","Epoch 66/2000, Step 38, d_loss: 0.46247169375419617, g_loss: 3.4423630237579346\n","Epoch 66/2000, Step 39, d_loss: 0.6347718238830566, g_loss: 2.1906087398529053\n","Epoch 66/2000, Step 40, d_loss: 0.4374968111515045, g_loss: 4.0491251945495605\n","Epoch 66/2000, Step 41, d_loss: 0.37105226516723633, g_loss: 3.3428828716278076\n","Epoch 66/2000, Step 42, d_loss: 0.43310731649398804, g_loss: 3.9592080116271973\n","Epoch 66/2000, Step 43, d_loss: 0.5076606273651123, g_loss: 4.265225887298584\n","Epoch 66/2000, Step 44, d_loss: 0.44546112418174744, g_loss: 4.538768291473389\n","Epoch 66/2000, Step 45, d_loss: 0.429369181394577, g_loss: 3.8445839881896973\n","Epoch 66/2000, Step 46, d_loss: 0.49884018301963806, g_loss: 2.7275800704956055\n","Epoch 66/2000, Step 47, d_loss: 0.3815453350543976, g_loss: 3.2869765758514404\n","Epoch 66/2000, Step 48, d_loss: 0.42544880509376526, g_loss: 2.761569023132324\n","Epoch 66/2000, Step 49, d_loss: 0.41158753633499146, g_loss: 2.886225700378418\n","Epoch 66/2000, Step 50, d_loss: 0.4874308109283447, g_loss: 4.236045837402344\n","Epoch 66/2000, Step 51, d_loss: 0.41227370500564575, g_loss: 2.8702220916748047\n","Epoch 66/2000, Step 52, d_loss: 0.40827828645706177, g_loss: 3.4060943126678467\n","Epoch 66/2000, Step 53, d_loss: 0.42998871207237244, g_loss: 5.020208835601807\n","Epoch 66/2000, Step 54, d_loss: 0.3967475891113281, g_loss: 4.371067523956299\n","Epoch 66/2000, Step 55, d_loss: 0.46547213196754456, g_loss: 4.009866714477539\n","Epoch 66/2000, Step 56, d_loss: 0.44002285599708557, g_loss: 4.834349155426025\n","Epoch 66/2000, Step 57, d_loss: 0.4325615167617798, g_loss: 2.9379429817199707\n","Epoch 66/2000, Step 58, d_loss: 0.3877680003643036, g_loss: 2.581489324569702\n","Epoch 66/2000, Step 59, d_loss: 0.37063127756118774, g_loss: 2.7702481746673584\n","Epoch 66/2000, Step 60, d_loss: 0.45230305194854736, g_loss: 3.847970962524414\n","Epoch 66/2000, Step 61, d_loss: 0.4043728709220886, g_loss: 6.039270877838135\n","Epoch 66/2000, Step 62, d_loss: 0.368429571390152, g_loss: 3.0640780925750732\n","Epoch 66/2000, Step 63, d_loss: 0.5909444689750671, g_loss: 3.454514980316162\n","Epoch 66/2000, Step 64, d_loss: 0.36587071418762207, g_loss: 3.9103288650512695\n","Epoch 66/2000, Step 65, d_loss: 0.4582315683364868, g_loss: 3.1294820308685303\n","Epoch 66/2000, Step 66, d_loss: 0.3688565492630005, g_loss: 3.7580742835998535\n","Epoch 66/2000, Step 67, d_loss: 0.3882795572280884, g_loss: 3.3933558464050293\n","Epoch 66/2000, Step 68, d_loss: 0.35853391885757446, g_loss: 5.234724521636963\n","Epoch 66/2000, Step 69, d_loss: 0.3822171092033386, g_loss: 3.473557949066162\n","Epoch 66/2000, Step 70, d_loss: 0.3755831718444824, g_loss: 2.9901974201202393\n","Epoch 66/2000, Step 71, d_loss: 0.39009562134742737, g_loss: 3.762907028198242\n","Epoch 66/2000, Step 72, d_loss: 0.3722955882549286, g_loss: 3.600865125656128\n","Epoch 66/2000, Step 73, d_loss: 0.3706251382827759, g_loss: 4.256287097930908\n","Epoch 66/2000, Step 74, d_loss: 0.3769477605819702, g_loss: 4.327386856079102\n","Epoch 66/2000, Step 75, d_loss: 0.3833181858062744, g_loss: 3.086052894592285\n","Epoch 66/2000, Step 76, d_loss: 0.4142424762248993, g_loss: 3.209463119506836\n","Epoch 66/2000, Step 77, d_loss: 0.373961865901947, g_loss: 3.556518793106079\n","Epoch 66/2000, Step 78, d_loss: 0.46924740076065063, g_loss: 2.3173036575317383\n","Epoch 66/2000, Step 79, d_loss: 0.4056161046028137, g_loss: 3.1640658378601074\n","Epoch 66/2000, Step 80, d_loss: 0.41781163215637207, g_loss: 2.904665231704712\n","Epoch 66/2000, Step 81, d_loss: 0.40194952487945557, g_loss: 3.51047682762146\n","Epoch 66/2000, Step 82, d_loss: 0.4380249083042145, g_loss: 3.437976360321045\n","Epoch 66/2000, Step 83, d_loss: 0.5407421588897705, g_loss: 4.064632415771484\n","Epoch 66/2000, Step 84, d_loss: 0.4476138949394226, g_loss: 3.6738739013671875\n","Epoch 66/2000, Step 85, d_loss: 0.3951827585697174, g_loss: 3.9637291431427\n","Epoch 66/2000, Step 86, d_loss: 0.40921708941459656, g_loss: 4.342454433441162\n","Epoch 66/2000, Step 87, d_loss: 0.3832440972328186, g_loss: 3.4946300983428955\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 67/2000, Step 1, d_loss: 0.3857475519180298, g_loss: 2.3814871311187744\n","Epoch 67/2000, Step 2, d_loss: 0.37316200137138367, g_loss: 3.6812002658843994\n","Epoch 67/2000, Step 3, d_loss: 0.4634236991405487, g_loss: 4.021145820617676\n","Epoch 67/2000, Step 4, d_loss: 0.40336811542510986, g_loss: 2.2303202152252197\n","Epoch 67/2000, Step 5, d_loss: 0.49443867802619934, g_loss: 3.4875574111938477\n","Epoch 67/2000, Step 6, d_loss: 0.4318007826805115, g_loss: 3.917466878890991\n","Epoch 67/2000, Step 7, d_loss: 0.36193767189979553, g_loss: 5.2281622886657715\n","Epoch 67/2000, Step 8, d_loss: 0.3890340328216553, g_loss: 4.529897212982178\n","Epoch 67/2000, Step 9, d_loss: 0.7882499098777771, g_loss: 4.916191577911377\n","Epoch 67/2000, Step 10, d_loss: 0.3628172278404236, g_loss: 5.101881980895996\n","Epoch 67/2000, Step 11, d_loss: 0.42169898748397827, g_loss: 3.5203027725219727\n","Epoch 67/2000, Step 12, d_loss: 0.4362540543079376, g_loss: 2.1945509910583496\n","Epoch 67/2000, Step 13, d_loss: 0.41847312450408936, g_loss: 2.9203617572784424\n","Epoch 67/2000, Step 14, d_loss: 0.4185430109500885, g_loss: 3.912463903427124\n","Epoch 67/2000, Step 15, d_loss: 0.3647457957267761, g_loss: 2.9124252796173096\n","Epoch 67/2000, Step 16, d_loss: 0.4437064826488495, g_loss: 4.901583194732666\n","Epoch 67/2000, Step 17, d_loss: 0.46343177556991577, g_loss: 3.7318992614746094\n","Epoch 67/2000, Step 18, d_loss: 0.4045161306858063, g_loss: 2.6722865104675293\n","Epoch 67/2000, Step 19, d_loss: 0.3794975280761719, g_loss: 4.405783653259277\n","Epoch 67/2000, Step 20, d_loss: 0.371281236410141, g_loss: 3.160013437271118\n","Epoch 67/2000, Step 21, d_loss: 0.5494256019592285, g_loss: 2.477961778640747\n","Epoch 67/2000, Step 22, d_loss: 0.42165252566337585, g_loss: 2.4524030685424805\n","Epoch 67/2000, Step 23, d_loss: 0.467289000749588, g_loss: 2.7912209033966064\n","Epoch 67/2000, Step 24, d_loss: 0.4241190254688263, g_loss: 3.942640542984009\n","Epoch 67/2000, Step 25, d_loss: 0.43668946623802185, g_loss: 2.8585352897644043\n","Epoch 67/2000, Step 26, d_loss: 0.4644104242324829, g_loss: 2.9934208393096924\n","Epoch 67/2000, Step 27, d_loss: 0.4753662049770355, g_loss: 3.5943636894226074\n","Epoch 67/2000, Step 28, d_loss: 0.3948284387588501, g_loss: 3.7889139652252197\n","Epoch 67/2000, Step 29, d_loss: 0.3878880739212036, g_loss: 5.113773822784424\n","Epoch 67/2000, Step 30, d_loss: 0.374665766954422, g_loss: 5.37844705581665\n","Epoch 67/2000, Step 31, d_loss: 0.5337474346160889, g_loss: 4.992876052856445\n","Epoch 67/2000, Step 32, d_loss: 0.40275323390960693, g_loss: 3.757797956466675\n","Epoch 67/2000, Step 33, d_loss: 0.39675161242485046, g_loss: 3.6432580947875977\n","Epoch 67/2000, Step 34, d_loss: 0.3700791299343109, g_loss: 3.7549402713775635\n","Epoch 67/2000, Step 35, d_loss: 0.3926391303539276, g_loss: 3.4664933681488037\n","Epoch 67/2000, Step 36, d_loss: 0.4508036673069, g_loss: 2.5762522220611572\n","Epoch 67/2000, Step 37, d_loss: 0.4299100339412689, g_loss: 3.69272780418396\n","Epoch 67/2000, Step 38, d_loss: 0.39934858679771423, g_loss: 2.5361664295196533\n","Epoch 67/2000, Step 39, d_loss: 0.39518308639526367, g_loss: 5.212698936462402\n","Epoch 67/2000, Step 40, d_loss: 0.40810635685920715, g_loss: 3.788465738296509\n","Epoch 67/2000, Step 41, d_loss: 0.41589123010635376, g_loss: 4.213192939758301\n","Epoch 67/2000, Step 42, d_loss: 0.42088761925697327, g_loss: 4.7826666831970215\n","Epoch 67/2000, Step 43, d_loss: 0.3622376322746277, g_loss: 4.027839183807373\n","Epoch 67/2000, Step 44, d_loss: 0.6153951287269592, g_loss: 3.4145026206970215\n","Epoch 67/2000, Step 45, d_loss: 0.39474916458129883, g_loss: 3.3419644832611084\n","Epoch 67/2000, Step 46, d_loss: 0.4602946639060974, g_loss: 3.3834640979766846\n","Epoch 67/2000, Step 47, d_loss: 0.4144189953804016, g_loss: 3.650329113006592\n","Epoch 67/2000, Step 48, d_loss: 0.44831570982933044, g_loss: 3.3140523433685303\n","Epoch 67/2000, Step 49, d_loss: 0.4016302227973938, g_loss: 3.6995787620544434\n","Epoch 67/2000, Step 50, d_loss: 0.40894877910614014, g_loss: 4.229288578033447\n","Epoch 67/2000, Step 51, d_loss: 0.5905247926712036, g_loss: 3.2770984172821045\n","Epoch 67/2000, Step 52, d_loss: 0.38329073786735535, g_loss: 2.809222459793091\n","Epoch 67/2000, Step 53, d_loss: 0.4180256426334381, g_loss: 2.1217780113220215\n","Epoch 67/2000, Step 54, d_loss: 0.48643001914024353, g_loss: 5.0776047706604\n","Epoch 67/2000, Step 55, d_loss: 0.44056522846221924, g_loss: 3.7133285999298096\n","Epoch 67/2000, Step 56, d_loss: 0.3960825800895691, g_loss: 2.4111721515655518\n","Epoch 67/2000, Step 57, d_loss: 0.4221538007259369, g_loss: 3.94411563873291\n","Epoch 67/2000, Step 58, d_loss: 0.40675389766693115, g_loss: 5.141849517822266\n","Epoch 67/2000, Step 59, d_loss: 0.42830508947372437, g_loss: 4.357875823974609\n","Epoch 67/2000, Step 60, d_loss: 0.42785972356796265, g_loss: 5.2854461669921875\n","Epoch 67/2000, Step 61, d_loss: 0.47840702533721924, g_loss: 4.072338104248047\n","Epoch 67/2000, Step 62, d_loss: 0.37688103318214417, g_loss: 3.1917362213134766\n","Epoch 67/2000, Step 63, d_loss: 0.3568941354751587, g_loss: 3.7992773056030273\n","Epoch 67/2000, Step 64, d_loss: 0.38877949118614197, g_loss: 4.4608473777771\n","Epoch 67/2000, Step 65, d_loss: 0.4490325450897217, g_loss: 3.80059552192688\n","Epoch 67/2000, Step 66, d_loss: 0.422352135181427, g_loss: 3.5501766204833984\n","Epoch 67/2000, Step 67, d_loss: 0.38414400815963745, g_loss: 3.9245595932006836\n","Epoch 67/2000, Step 68, d_loss: 0.46916306018829346, g_loss: 3.7881906032562256\n","Epoch 67/2000, Step 69, d_loss: 0.38665759563446045, g_loss: 4.023354530334473\n","Epoch 67/2000, Step 70, d_loss: 0.41345492005348206, g_loss: 3.613368034362793\n","Epoch 67/2000, Step 71, d_loss: 0.38187912106513977, g_loss: 3.496121883392334\n","Epoch 67/2000, Step 72, d_loss: 0.3754871189594269, g_loss: 3.645491361618042\n","Epoch 67/2000, Step 73, d_loss: 0.4004185199737549, g_loss: 3.634931802749634\n","Epoch 67/2000, Step 74, d_loss: 0.4340088367462158, g_loss: 3.7791993618011475\n","Epoch 67/2000, Step 75, d_loss: 0.4492684304714203, g_loss: 3.130751132965088\n","Epoch 67/2000, Step 76, d_loss: 0.38348710536956787, g_loss: 3.662571907043457\n","Epoch 67/2000, Step 77, d_loss: 0.3684637248516083, g_loss: 3.2014000415802\n","Epoch 67/2000, Step 78, d_loss: 0.3994234800338745, g_loss: 3.007009267807007\n","Epoch 67/2000, Step 79, d_loss: 0.4222542345523834, g_loss: 3.422611951828003\n","Epoch 67/2000, Step 80, d_loss: 0.43063005805015564, g_loss: 3.228278160095215\n","Epoch 67/2000, Step 81, d_loss: 0.415261447429657, g_loss: 5.632577419281006\n","Epoch 67/2000, Step 82, d_loss: 0.3979511559009552, g_loss: 3.653151273727417\n","Epoch 67/2000, Step 83, d_loss: 0.40553486347198486, g_loss: 4.243322372436523\n","Epoch 67/2000, Step 84, d_loss: 0.5608848333358765, g_loss: 3.206496238708496\n","Epoch 67/2000, Step 85, d_loss: 0.3854949176311493, g_loss: 2.9901552200317383\n","Epoch 67/2000, Step 86, d_loss: 0.43275028467178345, g_loss: 2.515089988708496\n","Epoch 67/2000, Step 87, d_loss: 0.5553128123283386, g_loss: 2.8372583389282227\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 68/2000, Step 1, d_loss: 0.5102857947349548, g_loss: 2.84920072555542\n","Epoch 68/2000, Step 2, d_loss: 0.4386855661869049, g_loss: 3.340496301651001\n","Epoch 68/2000, Step 3, d_loss: 0.39860624074935913, g_loss: 4.13468599319458\n","Epoch 68/2000, Step 4, d_loss: 0.3723968267440796, g_loss: 4.047865867614746\n","Epoch 68/2000, Step 5, d_loss: 0.4222920536994934, g_loss: 4.959918022155762\n","Epoch 68/2000, Step 6, d_loss: 0.5581373572349548, g_loss: 4.795316219329834\n","Epoch 68/2000, Step 7, d_loss: 0.35097768902778625, g_loss: 4.333534240722656\n","Epoch 68/2000, Step 8, d_loss: 0.39240023493766785, g_loss: 3.053980827331543\n","Epoch 68/2000, Step 9, d_loss: 0.4807835817337036, g_loss: 2.5061378479003906\n","Epoch 68/2000, Step 10, d_loss: 0.41930311918258667, g_loss: 2.870927333831787\n","Epoch 68/2000, Step 11, d_loss: 0.39403825998306274, g_loss: 2.5188095569610596\n","Epoch 68/2000, Step 12, d_loss: 0.36668673157691956, g_loss: 2.935330867767334\n","Epoch 68/2000, Step 13, d_loss: 0.4348365068435669, g_loss: 3.0444750785827637\n","Epoch 68/2000, Step 14, d_loss: 0.37625396251678467, g_loss: 4.056987762451172\n","Epoch 68/2000, Step 15, d_loss: 0.5448483228683472, g_loss: 4.225574016571045\n","Epoch 68/2000, Step 16, d_loss: 0.48949873447418213, g_loss: 2.77211332321167\n","Epoch 68/2000, Step 17, d_loss: 0.42381471395492554, g_loss: 2.870311975479126\n","Epoch 68/2000, Step 18, d_loss: 0.4951055943965912, g_loss: 2.916071653366089\n","Epoch 68/2000, Step 19, d_loss: 0.5152043104171753, g_loss: 3.258776903152466\n","Epoch 68/2000, Step 20, d_loss: 0.4063676595687866, g_loss: 3.0598506927490234\n","Epoch 68/2000, Step 21, d_loss: 0.42675840854644775, g_loss: 3.3212192058563232\n","Epoch 68/2000, Step 22, d_loss: 0.38705572485923767, g_loss: 4.470489501953125\n","Epoch 68/2000, Step 23, d_loss: 0.4337306618690491, g_loss: 4.685139179229736\n","Epoch 68/2000, Step 24, d_loss: 0.49261343479156494, g_loss: 4.059628963470459\n","Epoch 68/2000, Step 25, d_loss: 0.3947695195674896, g_loss: 3.763387680053711\n","Epoch 68/2000, Step 26, d_loss: 0.3588675260543823, g_loss: 3.7967212200164795\n","Epoch 68/2000, Step 27, d_loss: 0.39025160670280457, g_loss: 4.30482816696167\n","Epoch 68/2000, Step 28, d_loss: 0.40562787652015686, g_loss: 3.153200149536133\n","Epoch 68/2000, Step 29, d_loss: 0.4062387943267822, g_loss: 2.9132635593414307\n","Epoch 68/2000, Step 30, d_loss: 0.3669460117816925, g_loss: 3.1531310081481934\n","Epoch 68/2000, Step 31, d_loss: 0.366012841463089, g_loss: 3.2573437690734863\n","Epoch 68/2000, Step 32, d_loss: 0.38875705003738403, g_loss: 3.774498462677002\n","Epoch 68/2000, Step 33, d_loss: 0.43451452255249023, g_loss: 4.576681137084961\n","Epoch 68/2000, Step 34, d_loss: 0.5198854207992554, g_loss: 5.220940589904785\n","Epoch 68/2000, Step 35, d_loss: 0.4572277367115021, g_loss: 4.66183614730835\n","Epoch 68/2000, Step 36, d_loss: 0.41319918632507324, g_loss: 3.0810070037841797\n","Epoch 68/2000, Step 37, d_loss: 0.4753042161464691, g_loss: 2.9892468452453613\n","Epoch 68/2000, Step 38, d_loss: 0.48585057258605957, g_loss: 2.713768720626831\n","Epoch 68/2000, Step 39, d_loss: 0.39178019762039185, g_loss: 2.7795045375823975\n","Epoch 68/2000, Step 40, d_loss: 0.4656111001968384, g_loss: 3.0762529373168945\n","Epoch 68/2000, Step 41, d_loss: 0.43628132343292236, g_loss: 2.715205669403076\n","Epoch 68/2000, Step 42, d_loss: 0.3990457057952881, g_loss: 3.5578551292419434\n","Epoch 68/2000, Step 43, d_loss: 0.5226977467536926, g_loss: 4.507248401641846\n","Epoch 68/2000, Step 44, d_loss: 0.4677214026451111, g_loss: 3.7543840408325195\n","Epoch 68/2000, Step 45, d_loss: 0.45154857635498047, g_loss: 4.255890369415283\n","Epoch 68/2000, Step 46, d_loss: 0.37110239267349243, g_loss: 3.7978944778442383\n","Epoch 68/2000, Step 47, d_loss: 0.43000105023384094, g_loss: 3.2947731018066406\n","Epoch 68/2000, Step 48, d_loss: 0.4296720325946808, g_loss: 2.5267627239227295\n","Epoch 68/2000, Step 49, d_loss: 0.39486563205718994, g_loss: 2.7760660648345947\n","Epoch 68/2000, Step 50, d_loss: 0.3903536796569824, g_loss: 3.0809898376464844\n","Epoch 68/2000, Step 51, d_loss: 0.376985102891922, g_loss: 3.3974101543426514\n","Epoch 68/2000, Step 52, d_loss: 0.41475698351860046, g_loss: 3.88749098777771\n","Epoch 68/2000, Step 53, d_loss: 0.42258232831954956, g_loss: 4.964747428894043\n","Epoch 68/2000, Step 54, d_loss: 0.42031729221343994, g_loss: 4.380823612213135\n","Epoch 68/2000, Step 55, d_loss: 0.4059235453605652, g_loss: 4.939165115356445\n","Epoch 68/2000, Step 56, d_loss: 0.5366359353065491, g_loss: 4.651760101318359\n","Epoch 68/2000, Step 57, d_loss: 0.41677218675613403, g_loss: 3.3334898948669434\n","Epoch 68/2000, Step 58, d_loss: 0.39057326316833496, g_loss: 3.6136293411254883\n","Epoch 68/2000, Step 59, d_loss: 0.47711074352264404, g_loss: 2.2624127864837646\n","Epoch 68/2000, Step 60, d_loss: 0.48992934823036194, g_loss: 2.087430000305176\n","Epoch 68/2000, Step 61, d_loss: 0.5332520604133606, g_loss: 2.398470878601074\n","Epoch 68/2000, Step 62, d_loss: 0.3785119354724884, g_loss: 3.5201070308685303\n","Epoch 68/2000, Step 63, d_loss: 0.41362565755844116, g_loss: 3.7807159423828125\n","Epoch 68/2000, Step 64, d_loss: 0.38876545429229736, g_loss: 2.8465092182159424\n","Epoch 68/2000, Step 65, d_loss: 0.4128771424293518, g_loss: 4.452516078948975\n","Epoch 68/2000, Step 66, d_loss: 0.45510154962539673, g_loss: 4.361081123352051\n","Epoch 68/2000, Step 67, d_loss: 0.38146859407424927, g_loss: 4.699872970581055\n","Epoch 68/2000, Step 68, d_loss: 0.4216686487197876, g_loss: 3.8676681518554688\n","Epoch 68/2000, Step 69, d_loss: 0.38536664843559265, g_loss: 3.859374523162842\n","Epoch 68/2000, Step 70, d_loss: 0.3846670091152191, g_loss: 5.003633975982666\n","Epoch 68/2000, Step 71, d_loss: 0.3811025023460388, g_loss: 4.559229373931885\n","Epoch 68/2000, Step 72, d_loss: 0.37698614597320557, g_loss: 4.5496826171875\n","Epoch 68/2000, Step 73, d_loss: 0.4206297993659973, g_loss: 3.5287556648254395\n","Epoch 68/2000, Step 74, d_loss: 0.36543354392051697, g_loss: 3.9779016971588135\n","Epoch 68/2000, Step 75, d_loss: 0.3805515468120575, g_loss: 4.2317891120910645\n","Epoch 68/2000, Step 76, d_loss: 0.3711340129375458, g_loss: 4.681360244750977\n","Epoch 68/2000, Step 77, d_loss: 0.3823827803134918, g_loss: 3.5930027961730957\n","Epoch 68/2000, Step 78, d_loss: 0.4538590908050537, g_loss: 4.004149913787842\n","Epoch 68/2000, Step 79, d_loss: 0.4211744964122772, g_loss: 4.046262741088867\n","Epoch 68/2000, Step 80, d_loss: 0.3960079848766327, g_loss: 3.2244913578033447\n","Epoch 68/2000, Step 81, d_loss: 0.3720148205757141, g_loss: 4.297364234924316\n","Epoch 68/2000, Step 82, d_loss: 0.43746697902679443, g_loss: 3.3927838802337646\n","Epoch 68/2000, Step 83, d_loss: 0.4067721366882324, g_loss: 4.494479179382324\n","Epoch 68/2000, Step 84, d_loss: 0.3615855872631073, g_loss: 3.9417688846588135\n","Epoch 68/2000, Step 85, d_loss: 0.42831870913505554, g_loss: 4.42894172668457\n","Epoch 68/2000, Step 86, d_loss: 0.38068056106567383, g_loss: 5.502228260040283\n","Epoch 68/2000, Step 87, d_loss: 0.3874707818031311, g_loss: 3.564121961593628\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 69/2000, Step 1, d_loss: 0.3739180862903595, g_loss: 4.169404029846191\n","Epoch 69/2000, Step 2, d_loss: 0.4346012473106384, g_loss: 3.5201737880706787\n","Epoch 69/2000, Step 3, d_loss: 0.39168229699134827, g_loss: 3.91338849067688\n","Epoch 69/2000, Step 4, d_loss: 0.37708786129951477, g_loss: 3.189319610595703\n","Epoch 69/2000, Step 5, d_loss: 0.3611833155155182, g_loss: 4.712369441986084\n","Epoch 69/2000, Step 6, d_loss: 0.38565924763679504, g_loss: 5.715250015258789\n","Epoch 69/2000, Step 7, d_loss: 0.36845648288726807, g_loss: 3.489307165145874\n","Epoch 69/2000, Step 8, d_loss: 0.39825737476348877, g_loss: 3.9006640911102295\n","Epoch 69/2000, Step 9, d_loss: 0.46989893913269043, g_loss: 4.47337007522583\n","Epoch 69/2000, Step 10, d_loss: 0.3745098114013672, g_loss: 4.085815906524658\n","Epoch 69/2000, Step 11, d_loss: 0.3998226821422577, g_loss: 4.553976058959961\n","Epoch 69/2000, Step 12, d_loss: 0.35780858993530273, g_loss: 3.515350103378296\n","Epoch 69/2000, Step 13, d_loss: 0.35404172539711, g_loss: 3.7104685306549072\n","Epoch 69/2000, Step 14, d_loss: 0.35448047518730164, g_loss: 3.024388551712036\n","Epoch 69/2000, Step 15, d_loss: 0.45106011629104614, g_loss: 3.4326305389404297\n","Epoch 69/2000, Step 16, d_loss: 0.41589367389678955, g_loss: 3.069047212600708\n","Epoch 69/2000, Step 17, d_loss: 0.4048352837562561, g_loss: 3.364697217941284\n","Epoch 69/2000, Step 18, d_loss: 0.39290690422058105, g_loss: 4.691313743591309\n","Epoch 69/2000, Step 19, d_loss: 0.3990066945552826, g_loss: 5.516463279724121\n","Epoch 69/2000, Step 20, d_loss: 0.34403878450393677, g_loss: 4.588336944580078\n","Epoch 69/2000, Step 21, d_loss: 0.41553398966789246, g_loss: 3.5345942974090576\n","Epoch 69/2000, Step 22, d_loss: 0.37218689918518066, g_loss: 2.821218967437744\n","Epoch 69/2000, Step 23, d_loss: 0.46289557218551636, g_loss: 2.750213861465454\n","Epoch 69/2000, Step 24, d_loss: 0.38632673025131226, g_loss: 3.2592484951019287\n","Epoch 69/2000, Step 25, d_loss: 0.4158719778060913, g_loss: 2.698436975479126\n","Epoch 69/2000, Step 26, d_loss: 0.42676079273223877, g_loss: 2.211167335510254\n","Epoch 69/2000, Step 27, d_loss: 0.4890178143978119, g_loss: 3.632688522338867\n","Epoch 69/2000, Step 28, d_loss: 0.42461517453193665, g_loss: 3.1781437397003174\n","Epoch 69/2000, Step 29, d_loss: 0.4012549817562103, g_loss: 4.8865742683410645\n","Epoch 69/2000, Step 30, d_loss: 0.37417298555374146, g_loss: 4.766727924346924\n","Epoch 69/2000, Step 31, d_loss: 0.3836345076560974, g_loss: 4.0434770584106445\n","Epoch 69/2000, Step 32, d_loss: 0.40236803889274597, g_loss: 3.9183905124664307\n","Epoch 69/2000, Step 33, d_loss: 0.3641411066055298, g_loss: 3.6396725177764893\n","Epoch 69/2000, Step 34, d_loss: 0.3747023046016693, g_loss: 4.453664779663086\n","Epoch 69/2000, Step 35, d_loss: 0.3711586892604828, g_loss: 3.358595609664917\n","Epoch 69/2000, Step 36, d_loss: 0.37552911043167114, g_loss: 5.718188762664795\n","Epoch 69/2000, Step 37, d_loss: 0.36851751804351807, g_loss: 3.5141592025756836\n","Epoch 69/2000, Step 38, d_loss: 0.34471163153648376, g_loss: 3.5578980445861816\n","Epoch 69/2000, Step 39, d_loss: 0.34687793254852295, g_loss: 3.2934017181396484\n","Epoch 69/2000, Step 40, d_loss: 0.3771841824054718, g_loss: 4.214388370513916\n","Epoch 69/2000, Step 41, d_loss: 0.39863350987434387, g_loss: 4.2979912757873535\n","Epoch 69/2000, Step 42, d_loss: 0.3692489564418793, g_loss: 4.833416938781738\n","Epoch 69/2000, Step 43, d_loss: 0.358973890542984, g_loss: 5.702289581298828\n","Epoch 69/2000, Step 44, d_loss: 0.39270779490470886, g_loss: 4.191915988922119\n","Epoch 69/2000, Step 45, d_loss: 0.37119466066360474, g_loss: 5.121906280517578\n","Epoch 69/2000, Step 46, d_loss: 0.3473667800426483, g_loss: 4.925821304321289\n","Epoch 69/2000, Step 47, d_loss: 0.42798173427581787, g_loss: 5.057063102722168\n","Epoch 69/2000, Step 48, d_loss: 0.3694647550582886, g_loss: 2.875584840774536\n","Epoch 69/2000, Step 49, d_loss: 0.42664003372192383, g_loss: 3.064383029937744\n","Epoch 69/2000, Step 50, d_loss: 0.38076677918434143, g_loss: 3.6153030395507812\n","Epoch 69/2000, Step 51, d_loss: 0.3964333236217499, g_loss: 4.156619071960449\n","Epoch 69/2000, Step 52, d_loss: 0.397527813911438, g_loss: 3.2655675411224365\n","Epoch 69/2000, Step 53, d_loss: 0.3757438361644745, g_loss: 3.4403340816497803\n","Epoch 69/2000, Step 54, d_loss: 0.350225567817688, g_loss: 4.461027145385742\n","Epoch 69/2000, Step 55, d_loss: 0.3715043365955353, g_loss: 3.501948118209839\n","Epoch 69/2000, Step 56, d_loss: 0.43014323711395264, g_loss: 4.283726215362549\n","Epoch 69/2000, Step 57, d_loss: 0.4264320135116577, g_loss: 3.823312282562256\n","Epoch 69/2000, Step 58, d_loss: 0.38870513439178467, g_loss: 3.543560743331909\n","Epoch 69/2000, Step 59, d_loss: 0.35341793298721313, g_loss: 2.7157185077667236\n","Epoch 69/2000, Step 60, d_loss: 0.40298399329185486, g_loss: 3.844940662384033\n","Epoch 69/2000, Step 61, d_loss: 0.39466285705566406, g_loss: 3.8771870136260986\n","Epoch 69/2000, Step 62, d_loss: 0.38517433404922485, g_loss: 3.8958988189697266\n","Epoch 69/2000, Step 63, d_loss: 0.38376885652542114, g_loss: 3.3060312271118164\n","Epoch 69/2000, Step 64, d_loss: 0.39349114894866943, g_loss: 3.6679587364196777\n","Epoch 69/2000, Step 65, d_loss: 0.3746318221092224, g_loss: 3.585733413696289\n","Epoch 69/2000, Step 66, d_loss: 0.4034225344657898, g_loss: 3.7921578884124756\n","Epoch 69/2000, Step 67, d_loss: 0.3539835810661316, g_loss: 4.205479621887207\n","Epoch 69/2000, Step 68, d_loss: 0.3923896551132202, g_loss: 4.675097942352295\n","Epoch 69/2000, Step 69, d_loss: 0.3990800380706787, g_loss: 4.124574184417725\n","Epoch 69/2000, Step 70, d_loss: 0.3759930431842804, g_loss: 3.1901960372924805\n","Epoch 69/2000, Step 71, d_loss: 0.35915786027908325, g_loss: 3.773026943206787\n","Epoch 69/2000, Step 72, d_loss: 0.3621935546398163, g_loss: 4.039527893066406\n","Epoch 69/2000, Step 73, d_loss: 0.37906694412231445, g_loss: 3.137572765350342\n","Epoch 69/2000, Step 74, d_loss: 0.4456070363521576, g_loss: 3.1923813819885254\n","Epoch 69/2000, Step 75, d_loss: 0.39326775074005127, g_loss: 3.0347139835357666\n","Epoch 69/2000, Step 76, d_loss: 0.4709221422672272, g_loss: 2.7581276893615723\n","Epoch 69/2000, Step 77, d_loss: 0.4167354106903076, g_loss: 3.435452699661255\n","Epoch 69/2000, Step 78, d_loss: 0.3614313304424286, g_loss: 3.4368324279785156\n","Epoch 69/2000, Step 79, d_loss: 0.47687944769859314, g_loss: 4.206343650817871\n","Epoch 69/2000, Step 80, d_loss: 0.44419538974761963, g_loss: 3.3522844314575195\n","Epoch 69/2000, Step 81, d_loss: 0.37148404121398926, g_loss: 3.708878517150879\n","Epoch 69/2000, Step 82, d_loss: 0.3470810651779175, g_loss: 2.7003402709960938\n","Epoch 69/2000, Step 83, d_loss: 0.4846875071525574, g_loss: 2.7561631202697754\n","Epoch 69/2000, Step 84, d_loss: 0.53275465965271, g_loss: 3.392988681793213\n","Epoch 69/2000, Step 85, d_loss: 0.46364694833755493, g_loss: 4.41403341293335\n","Epoch 69/2000, Step 86, d_loss: 0.42070263624191284, g_loss: 4.287793159484863\n","Epoch 69/2000, Step 87, d_loss: 0.3780284523963928, g_loss: 6.193222522735596\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 70/2000, Step 1, d_loss: 0.454233318567276, g_loss: 5.146849632263184\n","Epoch 70/2000, Step 2, d_loss: 0.47341156005859375, g_loss: 2.9678165912628174\n","Epoch 70/2000, Step 3, d_loss: 0.3951539993286133, g_loss: 3.5495712757110596\n","Epoch 70/2000, Step 4, d_loss: 0.4167660176753998, g_loss: 4.173155307769775\n","Epoch 70/2000, Step 5, d_loss: 0.40354371070861816, g_loss: 2.881157875061035\n","Epoch 70/2000, Step 6, d_loss: 0.7184616327285767, g_loss: 3.2721283435821533\n","Epoch 70/2000, Step 7, d_loss: 0.39119192957878113, g_loss: 2.7559397220611572\n","Epoch 70/2000, Step 8, d_loss: 0.4674413800239563, g_loss: 3.55033278465271\n","Epoch 70/2000, Step 9, d_loss: 0.4930316209793091, g_loss: 4.978699684143066\n","Epoch 70/2000, Step 10, d_loss: 0.3899058997631073, g_loss: 4.450979709625244\n","Epoch 70/2000, Step 11, d_loss: 0.41307517886161804, g_loss: 3.114492654800415\n","Epoch 70/2000, Step 12, d_loss: 0.39195698499679565, g_loss: 3.5855393409729004\n","Epoch 70/2000, Step 13, d_loss: 0.38922178745269775, g_loss: 3.621307611465454\n","Epoch 70/2000, Step 14, d_loss: 0.3890174329280853, g_loss: 4.312089443206787\n","Epoch 70/2000, Step 15, d_loss: 0.3913460075855255, g_loss: 2.5369961261749268\n","Epoch 70/2000, Step 16, d_loss: 0.3928968012332916, g_loss: 2.774700403213501\n","Epoch 70/2000, Step 17, d_loss: 0.3925684690475464, g_loss: 3.9823920726776123\n","Epoch 70/2000, Step 18, d_loss: 0.381511390209198, g_loss: 3.8607144355773926\n","Epoch 70/2000, Step 19, d_loss: 0.4236781895160675, g_loss: 4.051140308380127\n","Epoch 70/2000, Step 20, d_loss: 0.4358045160770416, g_loss: 4.623990535736084\n","Epoch 70/2000, Step 21, d_loss: 0.34550923109054565, g_loss: 4.5776753425598145\n","Epoch 70/2000, Step 22, d_loss: 0.37622717022895813, g_loss: 4.202330589294434\n","Epoch 70/2000, Step 23, d_loss: 0.3676737844944, g_loss: 2.641645908355713\n","Epoch 70/2000, Step 24, d_loss: 0.37554290890693665, g_loss: 3.3657901287078857\n","Epoch 70/2000, Step 25, d_loss: 0.37174373865127563, g_loss: 2.9460864067077637\n","Epoch 70/2000, Step 26, d_loss: 0.47748059034347534, g_loss: 3.661224126815796\n","Epoch 70/2000, Step 27, d_loss: 0.4235001504421234, g_loss: 4.904483318328857\n","Epoch 70/2000, Step 28, d_loss: 0.3842616677284241, g_loss: 3.8320236206054688\n","Epoch 70/2000, Step 29, d_loss: 0.37423232197761536, g_loss: 4.089766025543213\n","Epoch 70/2000, Step 30, d_loss: 0.39139559864997864, g_loss: 3.9632675647735596\n","Epoch 70/2000, Step 31, d_loss: 0.44375526905059814, g_loss: 4.197960376739502\n","Epoch 70/2000, Step 32, d_loss: 0.439831405878067, g_loss: 2.652092456817627\n","Epoch 70/2000, Step 33, d_loss: 0.41030648350715637, g_loss: 3.0023605823516846\n","Epoch 70/2000, Step 34, d_loss: 0.36944153904914856, g_loss: 2.968756675720215\n","Epoch 70/2000, Step 35, d_loss: 0.47156813740730286, g_loss: 2.7259750366210938\n","Epoch 70/2000, Step 36, d_loss: 0.4589814245700836, g_loss: 4.87687873840332\n","Epoch 70/2000, Step 37, d_loss: 0.4436566233634949, g_loss: 4.065642356872559\n","Epoch 70/2000, Step 38, d_loss: 0.379955530166626, g_loss: 3.7585606575012207\n","Epoch 70/2000, Step 39, d_loss: 0.33846116065979004, g_loss: 4.394253253936768\n","Epoch 70/2000, Step 40, d_loss: 0.44343501329421997, g_loss: 4.672204494476318\n","Epoch 70/2000, Step 41, d_loss: 0.40024223923683167, g_loss: 3.8095600605010986\n","Epoch 70/2000, Step 42, d_loss: 0.43035513162612915, g_loss: 3.9445037841796875\n","Epoch 70/2000, Step 43, d_loss: 0.4091653525829315, g_loss: 2.9717819690704346\n","Epoch 70/2000, Step 44, d_loss: 0.40710970759391785, g_loss: 3.108750820159912\n","Epoch 70/2000, Step 45, d_loss: 0.39638569951057434, g_loss: 3.5352160930633545\n","Epoch 70/2000, Step 46, d_loss: 0.45433762669563293, g_loss: 2.845762252807617\n","Epoch 70/2000, Step 47, d_loss: 0.4007560610771179, g_loss: 3.5119683742523193\n","Epoch 70/2000, Step 48, d_loss: 0.4628773331642151, g_loss: 2.7553179264068604\n","Epoch 70/2000, Step 49, d_loss: 0.3711911737918854, g_loss: 3.2976298332214355\n","Epoch 70/2000, Step 50, d_loss: 0.3686582148075104, g_loss: 5.728184223175049\n","Epoch 70/2000, Step 51, d_loss: 0.6402300000190735, g_loss: 4.599173545837402\n","Epoch 70/2000, Step 52, d_loss: 0.3921586275100708, g_loss: 4.617931842803955\n","Epoch 70/2000, Step 53, d_loss: 0.4220978021621704, g_loss: 3.1041955947875977\n","Epoch 70/2000, Step 54, d_loss: 0.4045265018939972, g_loss: 3.016812801361084\n","Epoch 70/2000, Step 55, d_loss: 0.4314994215965271, g_loss: 3.1103429794311523\n","Epoch 70/2000, Step 56, d_loss: 0.3996327519416809, g_loss: 3.697361946105957\n","Epoch 70/2000, Step 57, d_loss: 0.3751909136772156, g_loss: 3.643193483352661\n","Epoch 70/2000, Step 58, d_loss: 0.36436745524406433, g_loss: 4.475777626037598\n","Epoch 70/2000, Step 59, d_loss: 0.36012086272239685, g_loss: 4.359615802764893\n","Epoch 70/2000, Step 60, d_loss: 0.347799688577652, g_loss: 5.06329870223999\n","Epoch 70/2000, Step 61, d_loss: 0.42951473593711853, g_loss: 4.521854877471924\n","Epoch 70/2000, Step 62, d_loss: 0.3515947759151459, g_loss: 3.531569719314575\n","Epoch 70/2000, Step 63, d_loss: 0.371957391500473, g_loss: 5.912827491760254\n","Epoch 70/2000, Step 64, d_loss: 0.38493284583091736, g_loss: 3.207176923751831\n","Epoch 70/2000, Step 65, d_loss: 0.38896632194519043, g_loss: 3.9482460021972656\n","Epoch 70/2000, Step 66, d_loss: 0.380604088306427, g_loss: 4.2758588790893555\n","Epoch 70/2000, Step 67, d_loss: 0.34518659114837646, g_loss: 4.633916854858398\n","Epoch 70/2000, Step 68, d_loss: 0.38360437750816345, g_loss: 4.407137393951416\n","Epoch 70/2000, Step 69, d_loss: 0.3735344707965851, g_loss: 3.858436346054077\n","Epoch 70/2000, Step 70, d_loss: 0.36258187890052795, g_loss: 3.66317081451416\n","Epoch 70/2000, Step 71, d_loss: 0.5243543982505798, g_loss: 3.1398231983184814\n","Epoch 70/2000, Step 72, d_loss: 0.37101197242736816, g_loss: 3.9694035053253174\n","Epoch 70/2000, Step 73, d_loss: 0.37025922536849976, g_loss: 2.763566493988037\n","Epoch 70/2000, Step 74, d_loss: 0.5627439022064209, g_loss: 3.8653206825256348\n","Epoch 70/2000, Step 75, d_loss: 0.68196702003479, g_loss: 3.832871913909912\n","Epoch 70/2000, Step 76, d_loss: 0.3933534324169159, g_loss: 3.902109384536743\n","Epoch 70/2000, Step 77, d_loss: 0.4826566278934479, g_loss: 4.611378192901611\n","Epoch 70/2000, Step 78, d_loss: 0.5188621282577515, g_loss: 3.892146348953247\n","Epoch 70/2000, Step 79, d_loss: 0.4676518440246582, g_loss: 3.7412948608398438\n","Epoch 70/2000, Step 80, d_loss: 0.38064247369766235, g_loss: 4.053646564483643\n","Epoch 70/2000, Step 81, d_loss: 0.5281862020492554, g_loss: 3.877739667892456\n","Epoch 70/2000, Step 82, d_loss: 0.4676341414451599, g_loss: 3.8933041095733643\n","Epoch 70/2000, Step 83, d_loss: 0.47096696496009827, g_loss: 3.110578775405884\n","Epoch 70/2000, Step 84, d_loss: 0.4953094720840454, g_loss: 3.773949146270752\n","Epoch 70/2000, Step 85, d_loss: 0.4543054401874542, g_loss: 5.67655611038208\n","Epoch 70/2000, Step 86, d_loss: 0.4434397220611572, g_loss: 5.144767761230469\n","Epoch 70/2000, Step 87, d_loss: 0.6172373294830322, g_loss: 3.1330204010009766\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 71/2000, Step 1, d_loss: 0.3696413040161133, g_loss: 4.4798808097839355\n","Epoch 71/2000, Step 2, d_loss: 0.41986557841300964, g_loss: 3.7230708599090576\n","Epoch 71/2000, Step 3, d_loss: 0.46892407536506653, g_loss: 4.574185848236084\n","Epoch 71/2000, Step 4, d_loss: 0.41134393215179443, g_loss: 5.845388412475586\n","Epoch 71/2000, Step 5, d_loss: 0.3924306035041809, g_loss: 4.751559257507324\n","Epoch 71/2000, Step 6, d_loss: 0.3678149878978729, g_loss: 4.375662803649902\n","Epoch 71/2000, Step 7, d_loss: 0.37897807359695435, g_loss: 3.926318645477295\n","Epoch 71/2000, Step 8, d_loss: 0.4125911295413971, g_loss: 4.477508544921875\n","Epoch 71/2000, Step 9, d_loss: 0.381430447101593, g_loss: 4.410788536071777\n","Epoch 71/2000, Step 10, d_loss: 0.4356061816215515, g_loss: 4.437987804412842\n","Epoch 71/2000, Step 11, d_loss: 0.3602893352508545, g_loss: 3.417862892150879\n","Epoch 71/2000, Step 12, d_loss: 0.374186635017395, g_loss: 4.533949851989746\n","Epoch 71/2000, Step 13, d_loss: 0.40498149394989014, g_loss: 3.547694444656372\n","Epoch 71/2000, Step 14, d_loss: 0.37773817777633667, g_loss: 4.911139488220215\n","Epoch 71/2000, Step 15, d_loss: 0.4224088490009308, g_loss: 4.135725498199463\n","Epoch 71/2000, Step 16, d_loss: 0.3811192512512207, g_loss: 3.1314048767089844\n","Epoch 71/2000, Step 17, d_loss: 0.38363271951675415, g_loss: 5.315642356872559\n","Epoch 71/2000, Step 18, d_loss: 0.3583467900753021, g_loss: 4.399382591247559\n","Epoch 71/2000, Step 19, d_loss: 0.49012336134910583, g_loss: 5.290497303009033\n","Epoch 71/2000, Step 20, d_loss: 0.3576672673225403, g_loss: 3.3383781909942627\n","Epoch 71/2000, Step 21, d_loss: 0.38025689125061035, g_loss: 3.560777187347412\n","Epoch 71/2000, Step 22, d_loss: 0.4090660512447357, g_loss: 4.574543476104736\n","Epoch 71/2000, Step 23, d_loss: 0.40392765402793884, g_loss: 2.6355955600738525\n","Epoch 71/2000, Step 24, d_loss: 0.3680233955383301, g_loss: 4.25085973739624\n","Epoch 71/2000, Step 25, d_loss: 0.44627779722213745, g_loss: 3.329333782196045\n","Epoch 71/2000, Step 26, d_loss: 0.4010077714920044, g_loss: 3.207730293273926\n","Epoch 71/2000, Step 27, d_loss: 0.4740811288356781, g_loss: 3.958200693130493\n","Epoch 71/2000, Step 28, d_loss: 0.41280198097229004, g_loss: 3.892879009246826\n","Epoch 71/2000, Step 29, d_loss: 0.3781512975692749, g_loss: 4.7107696533203125\n","Epoch 71/2000, Step 30, d_loss: 0.36510205268859863, g_loss: 4.568881034851074\n","Epoch 71/2000, Step 31, d_loss: 0.45659464597702026, g_loss: 4.729254245758057\n","Epoch 71/2000, Step 32, d_loss: 0.42879050970077515, g_loss: 3.793384552001953\n","Epoch 71/2000, Step 33, d_loss: 0.3847310543060303, g_loss: 4.6317901611328125\n","Epoch 71/2000, Step 34, d_loss: 0.41408419609069824, g_loss: 3.7188379764556885\n","Epoch 71/2000, Step 35, d_loss: 0.3813561797142029, g_loss: 3.1794581413269043\n","Epoch 71/2000, Step 36, d_loss: 0.40532186627388, g_loss: 3.3649187088012695\n","Epoch 71/2000, Step 37, d_loss: 0.4046365022659302, g_loss: 3.3695590496063232\n","Epoch 71/2000, Step 38, d_loss: 0.3784639537334442, g_loss: 3.0023720264434814\n","Epoch 71/2000, Step 39, d_loss: 0.45738351345062256, g_loss: 4.174474716186523\n","Epoch 71/2000, Step 40, d_loss: 0.4817752242088318, g_loss: 4.070034980773926\n","Epoch 71/2000, Step 41, d_loss: 0.42376717925071716, g_loss: 4.404693603515625\n","Epoch 71/2000, Step 42, d_loss: 0.3987088203430176, g_loss: 4.222918510437012\n","Epoch 71/2000, Step 43, d_loss: 0.36459317803382874, g_loss: 4.118278980255127\n","Epoch 71/2000, Step 44, d_loss: 0.4068160951137543, g_loss: 4.7212042808532715\n","Epoch 71/2000, Step 45, d_loss: 0.39207929372787476, g_loss: 3.930074691772461\n","Epoch 71/2000, Step 46, d_loss: 0.3845072090625763, g_loss: 3.6668777465820312\n","Epoch 71/2000, Step 47, d_loss: 0.3763854503631592, g_loss: 3.877941846847534\n","Epoch 71/2000, Step 48, d_loss: 0.4127139449119568, g_loss: 3.7087244987487793\n","Epoch 71/2000, Step 49, d_loss: 0.4327980577945709, g_loss: 3.343259334564209\n","Epoch 71/2000, Step 50, d_loss: 0.3688141703605652, g_loss: 4.0696306228637695\n","Epoch 71/2000, Step 51, d_loss: 0.3719346225261688, g_loss: 5.170358657836914\n","Epoch 71/2000, Step 52, d_loss: 0.7049688100814819, g_loss: 4.400691032409668\n","Epoch 71/2000, Step 53, d_loss: 0.35751107335090637, g_loss: 3.4739513397216797\n","Epoch 71/2000, Step 54, d_loss: 0.37205395102500916, g_loss: 3.0791234970092773\n","Epoch 71/2000, Step 55, d_loss: 0.4042823016643524, g_loss: 3.907416820526123\n","Epoch 71/2000, Step 56, d_loss: 0.4032963216304779, g_loss: 3.4888792037963867\n","Epoch 71/2000, Step 57, d_loss: 0.43885353207588196, g_loss: 3.364405632019043\n","Epoch 71/2000, Step 58, d_loss: 0.3745991885662079, g_loss: 3.315295696258545\n","Epoch 71/2000, Step 59, d_loss: 0.3663047254085541, g_loss: 4.930401802062988\n","Epoch 71/2000, Step 60, d_loss: 0.4241590201854706, g_loss: 3.037935733795166\n","Epoch 71/2000, Step 61, d_loss: 0.3659130036830902, g_loss: 5.406332492828369\n","Epoch 71/2000, Step 62, d_loss: 0.3637309670448303, g_loss: 3.8482136726379395\n","Epoch 71/2000, Step 63, d_loss: 0.3689187169075012, g_loss: 3.5804498195648193\n","Epoch 71/2000, Step 64, d_loss: 0.4321761429309845, g_loss: 2.6292779445648193\n","Epoch 71/2000, Step 65, d_loss: 0.3730638921260834, g_loss: 3.8583333492279053\n","Epoch 71/2000, Step 66, d_loss: 0.35075169801712036, g_loss: 4.875508785247803\n","Epoch 71/2000, Step 67, d_loss: 0.38846829533576965, g_loss: 4.234644412994385\n","Epoch 71/2000, Step 68, d_loss: 0.3755578100681305, g_loss: 4.304941654205322\n","Epoch 71/2000, Step 69, d_loss: 0.5926423668861389, g_loss: 3.4051196575164795\n","Epoch 71/2000, Step 70, d_loss: 0.3588056266307831, g_loss: 4.031848907470703\n","Epoch 71/2000, Step 71, d_loss: 0.4498935341835022, g_loss: 4.182003974914551\n","Epoch 71/2000, Step 72, d_loss: 0.5451095700263977, g_loss: 4.020199298858643\n","Epoch 71/2000, Step 73, d_loss: 0.378771036863327, g_loss: 3.283182382583618\n","Epoch 71/2000, Step 74, d_loss: 0.41694796085357666, g_loss: 4.589088439941406\n","Epoch 71/2000, Step 75, d_loss: 0.4314103126525879, g_loss: 3.8927688598632812\n","Epoch 71/2000, Step 76, d_loss: 0.42488396167755127, g_loss: 4.038412094116211\n","Epoch 71/2000, Step 77, d_loss: 0.36491402983665466, g_loss: 3.9838650226593018\n","Epoch 71/2000, Step 78, d_loss: 0.3581622540950775, g_loss: 5.202982425689697\n","Epoch 71/2000, Step 79, d_loss: 0.38536641001701355, g_loss: 3.9748034477233887\n","Epoch 71/2000, Step 80, d_loss: 0.42233991622924805, g_loss: 3.4841020107269287\n","Epoch 71/2000, Step 81, d_loss: 0.5259501934051514, g_loss: 3.2858591079711914\n","Epoch 71/2000, Step 82, d_loss: 0.4234914779663086, g_loss: 5.227662086486816\n","Epoch 71/2000, Step 83, d_loss: 0.393638551235199, g_loss: 3.976081132888794\n","Epoch 71/2000, Step 84, d_loss: 0.40169671177864075, g_loss: 3.2946012020111084\n","Epoch 71/2000, Step 85, d_loss: 0.39060208201408386, g_loss: 3.0388307571411133\n","Epoch 71/2000, Step 86, d_loss: 0.3796357214450836, g_loss: 3.966930389404297\n","Epoch 71/2000, Step 87, d_loss: 0.4118516445159912, g_loss: 5.794070243835449\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 72/2000, Step 1, d_loss: 0.38518282771110535, g_loss: 5.290574073791504\n","Epoch 72/2000, Step 2, d_loss: 0.37589147686958313, g_loss: 4.71966028213501\n","Epoch 72/2000, Step 3, d_loss: 0.4020330607891083, g_loss: 6.007260322570801\n","Epoch 72/2000, Step 4, d_loss: 0.46550536155700684, g_loss: 6.232425689697266\n","Epoch 72/2000, Step 5, d_loss: 0.3521614372730255, g_loss: 3.560258388519287\n","Epoch 72/2000, Step 6, d_loss: 0.41455456614494324, g_loss: 3.815336227416992\n","Epoch 72/2000, Step 7, d_loss: 0.45533955097198486, g_loss: 3.729424476623535\n","Epoch 72/2000, Step 8, d_loss: 0.3868761360645294, g_loss: 2.7585222721099854\n","Epoch 72/2000, Step 9, d_loss: 0.38475388288497925, g_loss: 3.4036214351654053\n","Epoch 72/2000, Step 10, d_loss: 0.38274624943733215, g_loss: 3.2598876953125\n","Epoch 72/2000, Step 11, d_loss: 0.43167993426322937, g_loss: 3.2748122215270996\n","Epoch 72/2000, Step 12, d_loss: 0.40638819336891174, g_loss: 4.056183338165283\n","Epoch 72/2000, Step 13, d_loss: 0.40736329555511475, g_loss: 4.7004714012146\n","Epoch 72/2000, Step 14, d_loss: 0.46285271644592285, g_loss: 5.57479190826416\n","Epoch 72/2000, Step 15, d_loss: 0.4326784312725067, g_loss: 3.8509621620178223\n","Epoch 72/2000, Step 16, d_loss: 0.382996141910553, g_loss: 6.104280471801758\n","Epoch 72/2000, Step 17, d_loss: 0.3538147509098053, g_loss: 3.9305365085601807\n","Epoch 72/2000, Step 18, d_loss: 0.3601623475551605, g_loss: 4.0037522315979\n","Epoch 72/2000, Step 19, d_loss: 0.36695629358291626, g_loss: 4.32301139831543\n","Epoch 72/2000, Step 20, d_loss: 0.3635268807411194, g_loss: 4.813587188720703\n","Epoch 72/2000, Step 21, d_loss: 0.43987104296684265, g_loss: 3.1175243854522705\n","Epoch 72/2000, Step 22, d_loss: 0.40977203845977783, g_loss: 3.5754144191741943\n","Epoch 72/2000, Step 23, d_loss: 0.3632911145687103, g_loss: 3.4230189323425293\n","Epoch 72/2000, Step 24, d_loss: 0.4475434422492981, g_loss: 3.1205806732177734\n","Epoch 72/2000, Step 25, d_loss: 0.40536803007125854, g_loss: 3.4451711177825928\n","Epoch 72/2000, Step 26, d_loss: 0.37405630946159363, g_loss: 4.078171253204346\n","Epoch 72/2000, Step 27, d_loss: 0.37972357869148254, g_loss: 4.3313188552856445\n","Epoch 72/2000, Step 28, d_loss: 0.3697086572647095, g_loss: 3.734454870223999\n","Epoch 72/2000, Step 29, d_loss: 0.47003501653671265, g_loss: 4.257833957672119\n","Epoch 72/2000, Step 30, d_loss: 0.3688027262687683, g_loss: 3.26613187789917\n","Epoch 72/2000, Step 31, d_loss: 0.4164239168167114, g_loss: 3.3375048637390137\n","Epoch 72/2000, Step 32, d_loss: 0.4345652759075165, g_loss: 2.890519142150879\n","Epoch 72/2000, Step 33, d_loss: 0.47026869654655457, g_loss: 4.262871265411377\n","Epoch 72/2000, Step 34, d_loss: 0.35859888792037964, g_loss: 4.615902423858643\n","Epoch 72/2000, Step 35, d_loss: 0.37595584988594055, g_loss: 3.9383440017700195\n","Epoch 72/2000, Step 36, d_loss: 0.42493706941604614, g_loss: 4.145159721374512\n","Epoch 72/2000, Step 37, d_loss: 0.482219398021698, g_loss: 2.8187263011932373\n","Epoch 72/2000, Step 38, d_loss: 0.37480559945106506, g_loss: 3.006866931915283\n","Epoch 72/2000, Step 39, d_loss: 0.3593248128890991, g_loss: 2.5265114307403564\n","Epoch 72/2000, Step 40, d_loss: 0.42149806022644043, g_loss: 2.3391880989074707\n","Epoch 72/2000, Step 41, d_loss: 0.4600535035133362, g_loss: 3.8015761375427246\n","Epoch 72/2000, Step 42, d_loss: 0.3886440694332123, g_loss: 3.8592002391815186\n","Epoch 72/2000, Step 43, d_loss: 0.4031089246273041, g_loss: 3.0376288890838623\n","Epoch 72/2000, Step 44, d_loss: 0.3655541241168976, g_loss: 4.238452434539795\n","Epoch 72/2000, Step 45, d_loss: 0.382902592420578, g_loss: 4.397852897644043\n","Epoch 72/2000, Step 46, d_loss: 0.5544611215591431, g_loss: 5.738114833831787\n","Epoch 72/2000, Step 47, d_loss: 0.48076313734054565, g_loss: 2.93939208984375\n","Epoch 72/2000, Step 48, d_loss: 0.55538010597229, g_loss: 2.8503663539886475\n","Epoch 72/2000, Step 49, d_loss: 0.44304198026657104, g_loss: 1.8985507488250732\n","Epoch 72/2000, Step 50, d_loss: 0.4522796869277954, g_loss: 2.354799509048462\n","Epoch 72/2000, Step 51, d_loss: 0.41363903880119324, g_loss: 1.400957465171814\n","Epoch 72/2000, Step 52, d_loss: 0.41213735938072205, g_loss: 3.177492141723633\n","Epoch 72/2000, Step 53, d_loss: 0.4238314926624298, g_loss: 3.658895492553711\n","Epoch 72/2000, Step 54, d_loss: 0.5741307139396667, g_loss: 2.940875291824341\n","Epoch 72/2000, Step 55, d_loss: 0.38101235032081604, g_loss: 3.6866707801818848\n","Epoch 72/2000, Step 56, d_loss: 0.3967938721179962, g_loss: 3.6544525623321533\n","Epoch 72/2000, Step 57, d_loss: 0.47765299677848816, g_loss: 4.307520866394043\n","Epoch 72/2000, Step 58, d_loss: 0.4677225649356842, g_loss: 7.039492607116699\n","Epoch 72/2000, Step 59, d_loss: 0.35594606399536133, g_loss: 3.4687087535858154\n","Epoch 72/2000, Step 60, d_loss: 0.4227168560028076, g_loss: 3.847771644592285\n","Epoch 72/2000, Step 61, d_loss: 0.4412882328033447, g_loss: 3.229710340499878\n","Epoch 72/2000, Step 62, d_loss: 0.41810667514801025, g_loss: 2.864476442337036\n","Epoch 72/2000, Step 63, d_loss: 0.4344222843647003, g_loss: 3.657864809036255\n","Epoch 72/2000, Step 64, d_loss: 0.35190218687057495, g_loss: 6.065748691558838\n","Epoch 72/2000, Step 65, d_loss: 0.46062803268432617, g_loss: 4.641563892364502\n","Epoch 72/2000, Step 66, d_loss: 0.4063723087310791, g_loss: 3.7085883617401123\n","Epoch 72/2000, Step 67, d_loss: 0.3757058382034302, g_loss: 3.6503217220306396\n","Epoch 72/2000, Step 68, d_loss: 0.3812876343727112, g_loss: 3.700223922729492\n","Epoch 72/2000, Step 69, d_loss: 0.38832002878189087, g_loss: 4.106618404388428\n","Epoch 72/2000, Step 70, d_loss: 0.40323498845100403, g_loss: 3.3850831985473633\n","Epoch 72/2000, Step 71, d_loss: 0.34719181060791016, g_loss: 3.39186692237854\n","Epoch 72/2000, Step 72, d_loss: 0.37136110663414, g_loss: 4.503106117248535\n","Epoch 72/2000, Step 73, d_loss: 0.38247254490852356, g_loss: 4.054711818695068\n","Epoch 72/2000, Step 74, d_loss: 0.3550226092338562, g_loss: 4.356626510620117\n","Epoch 72/2000, Step 75, d_loss: 0.3771328032016754, g_loss: 3.905123472213745\n","Epoch 72/2000, Step 76, d_loss: 0.36668407917022705, g_loss: 3.331799268722534\n","Epoch 72/2000, Step 77, d_loss: 0.39258161187171936, g_loss: 4.590742588043213\n","Epoch 72/2000, Step 78, d_loss: 0.410109281539917, g_loss: 3.8209898471832275\n","Epoch 72/2000, Step 79, d_loss: 0.37048420310020447, g_loss: 3.247582197189331\n","Epoch 72/2000, Step 80, d_loss: 0.41567009687423706, g_loss: 3.376681327819824\n","Epoch 72/2000, Step 81, d_loss: 0.36574500799179077, g_loss: 4.9986653327941895\n","Epoch 72/2000, Step 82, d_loss: 0.39381706714630127, g_loss: 4.889781951904297\n","Epoch 72/2000, Step 83, d_loss: 0.38283848762512207, g_loss: 4.259372711181641\n","Epoch 72/2000, Step 84, d_loss: 0.36131080985069275, g_loss: 4.446240425109863\n","Epoch 72/2000, Step 85, d_loss: 0.35796165466308594, g_loss: 3.922733783721924\n","Epoch 72/2000, Step 86, d_loss: 0.39893466234207153, g_loss: 4.816559791564941\n","Epoch 72/2000, Step 87, d_loss: 0.3685915470123291, g_loss: 4.280548572540283\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 73/2000, Step 1, d_loss: 0.4456305503845215, g_loss: 4.102502822875977\n","Epoch 73/2000, Step 2, d_loss: 0.4006258249282837, g_loss: 2.9092886447906494\n","Epoch 73/2000, Step 3, d_loss: 0.46230679750442505, g_loss: 2.3484678268432617\n","Epoch 73/2000, Step 4, d_loss: 0.4862004220485687, g_loss: 2.652806282043457\n","Epoch 73/2000, Step 5, d_loss: 0.4654398560523987, g_loss: 2.51765775680542\n","Epoch 73/2000, Step 6, d_loss: 0.44847825169563293, g_loss: 3.361210823059082\n","Epoch 73/2000, Step 7, d_loss: 0.4829835593700409, g_loss: 5.519891738891602\n","Epoch 73/2000, Step 8, d_loss: 0.4349329173564911, g_loss: 4.7918853759765625\n","Epoch 73/2000, Step 9, d_loss: 0.5879736542701721, g_loss: 4.573342323303223\n","Epoch 73/2000, Step 10, d_loss: 0.41850754618644714, g_loss: 4.182031154632568\n","Epoch 73/2000, Step 11, d_loss: 0.3542284667491913, g_loss: 6.058074951171875\n","Epoch 73/2000, Step 12, d_loss: 0.4668474495410919, g_loss: 3.7671821117401123\n","Epoch 73/2000, Step 13, d_loss: 0.3848244547843933, g_loss: 3.118784189224243\n","Epoch 73/2000, Step 14, d_loss: 0.37116125226020813, g_loss: 3.6426761150360107\n","Epoch 73/2000, Step 15, d_loss: 0.4133770763874054, g_loss: 4.440717697143555\n","Epoch 73/2000, Step 16, d_loss: 0.41093745827674866, g_loss: 4.435110092163086\n","Epoch 73/2000, Step 17, d_loss: 0.4065326750278473, g_loss: 3.57487154006958\n","Epoch 73/2000, Step 18, d_loss: 0.49347686767578125, g_loss: 3.018897294998169\n","Epoch 73/2000, Step 19, d_loss: 0.4152334928512573, g_loss: 3.574186086654663\n","Epoch 73/2000, Step 20, d_loss: 0.40814870595932007, g_loss: 2.721503973007202\n","Epoch 73/2000, Step 21, d_loss: 0.39052072167396545, g_loss: 3.7977802753448486\n","Epoch 73/2000, Step 22, d_loss: 0.7241135835647583, g_loss: 3.3346757888793945\n","Epoch 73/2000, Step 23, d_loss: 0.3800453841686249, g_loss: 2.48390793800354\n","Epoch 73/2000, Step 24, d_loss: 0.45813098549842834, g_loss: 4.15629243850708\n","Epoch 73/2000, Step 25, d_loss: 0.4714473783969879, g_loss: 2.785921573638916\n","Epoch 73/2000, Step 26, d_loss: 0.4241873621940613, g_loss: 2.374772548675537\n","Epoch 73/2000, Step 27, d_loss: 0.3944295644760132, g_loss: 4.3854899406433105\n","Epoch 73/2000, Step 28, d_loss: 0.3465386927127838, g_loss: 4.548285484313965\n","Epoch 73/2000, Step 29, d_loss: 0.35939061641693115, g_loss: 4.2638983726501465\n","Epoch 73/2000, Step 30, d_loss: 0.4056873917579651, g_loss: 5.392846584320068\n","Epoch 73/2000, Step 31, d_loss: 0.36324459314346313, g_loss: 4.47964334487915\n","Epoch 73/2000, Step 32, d_loss: 0.6728772521018982, g_loss: 3.523343563079834\n","Epoch 73/2000, Step 33, d_loss: 0.3783225417137146, g_loss: 2.453789472579956\n","Epoch 73/2000, Step 34, d_loss: 0.43307170271873474, g_loss: 3.2848141193389893\n","Epoch 73/2000, Step 35, d_loss: 0.48087412118911743, g_loss: 2.6325273513793945\n","Epoch 73/2000, Step 36, d_loss: 0.4076172113418579, g_loss: 3.2949843406677246\n","Epoch 73/2000, Step 37, d_loss: 0.4244518280029297, g_loss: 3.2960188388824463\n","Epoch 73/2000, Step 38, d_loss: 0.471422404050827, g_loss: 3.09246826171875\n","Epoch 73/2000, Step 39, d_loss: 0.40597572922706604, g_loss: 4.879737854003906\n","Epoch 73/2000, Step 40, d_loss: 0.3823552131652832, g_loss: 3.6457183361053467\n","Epoch 73/2000, Step 41, d_loss: 0.4650983512401581, g_loss: 4.615278244018555\n","Epoch 73/2000, Step 42, d_loss: 0.4622756838798523, g_loss: 2.9581568241119385\n","Epoch 73/2000, Step 43, d_loss: 0.44084084033966064, g_loss: 2.4739151000976562\n","Epoch 73/2000, Step 44, d_loss: 0.450874924659729, g_loss: 3.132779359817505\n","Epoch 73/2000, Step 45, d_loss: 0.45475149154663086, g_loss: 3.4148011207580566\n","Epoch 73/2000, Step 46, d_loss: 0.4144018292427063, g_loss: 4.088571548461914\n","Epoch 73/2000, Step 47, d_loss: 0.3668130934238434, g_loss: 3.8033640384674072\n","Epoch 73/2000, Step 48, d_loss: 0.38413506746292114, g_loss: 4.058788776397705\n","Epoch 73/2000, Step 49, d_loss: 0.3749766945838928, g_loss: 5.375922679901123\n","Epoch 73/2000, Step 50, d_loss: 0.40817132592201233, g_loss: 4.607941150665283\n","Epoch 73/2000, Step 51, d_loss: 0.4593532383441925, g_loss: 4.5610785484313965\n","Epoch 73/2000, Step 52, d_loss: 0.4104892313480377, g_loss: 3.323513984680176\n","Epoch 73/2000, Step 53, d_loss: 0.3966335654258728, g_loss: 3.9756853580474854\n","Epoch 73/2000, Step 54, d_loss: 0.3965539336204529, g_loss: 5.23063850402832\n","Epoch 73/2000, Step 55, d_loss: 0.4198220372200012, g_loss: 4.716008186340332\n","Epoch 73/2000, Step 56, d_loss: 0.42584189772605896, g_loss: 3.917358160018921\n","Epoch 73/2000, Step 57, d_loss: 0.4070229232311249, g_loss: 3.390864849090576\n","Epoch 73/2000, Step 58, d_loss: 0.4535634517669678, g_loss: 3.6854891777038574\n","Epoch 73/2000, Step 59, d_loss: 0.3788411319255829, g_loss: 4.236789226531982\n","Epoch 73/2000, Step 60, d_loss: 0.6128073930740356, g_loss: 4.273946762084961\n","Epoch 73/2000, Step 61, d_loss: 0.37503907084465027, g_loss: 4.57930326461792\n","Epoch 73/2000, Step 62, d_loss: 0.3714238107204437, g_loss: 3.54998779296875\n","Epoch 73/2000, Step 63, d_loss: 0.38697630167007446, g_loss: 3.3960821628570557\n","Epoch 73/2000, Step 64, d_loss: 0.4055444598197937, g_loss: 4.958966255187988\n","Epoch 73/2000, Step 65, d_loss: 0.36405864357948303, g_loss: 4.35435152053833\n","Epoch 73/2000, Step 66, d_loss: 0.368326336145401, g_loss: 3.907229423522949\n","Epoch 73/2000, Step 67, d_loss: 0.3586311340332031, g_loss: 3.659212827682495\n","Epoch 73/2000, Step 68, d_loss: 0.3860470950603485, g_loss: 4.555298805236816\n","Epoch 73/2000, Step 69, d_loss: 0.4411785304546356, g_loss: 3.7281453609466553\n","Epoch 73/2000, Step 70, d_loss: 0.37818968296051025, g_loss: 2.6856393814086914\n","Epoch 73/2000, Step 71, d_loss: 0.3684042990207672, g_loss: 3.866246223449707\n","Epoch 73/2000, Step 72, d_loss: 0.39533936977386475, g_loss: 3.552680253982544\n","Epoch 73/2000, Step 73, d_loss: 0.40123963356018066, g_loss: 2.6061153411865234\n","Epoch 73/2000, Step 74, d_loss: 0.527036726474762, g_loss: 4.285529136657715\n","Epoch 73/2000, Step 75, d_loss: 0.37850069999694824, g_loss: 3.883883237838745\n","Epoch 73/2000, Step 76, d_loss: 0.40413933992385864, g_loss: 4.429668426513672\n","Epoch 73/2000, Step 77, d_loss: 0.3614937663078308, g_loss: 3.941922664642334\n","Epoch 73/2000, Step 78, d_loss: 0.38866594433784485, g_loss: 3.907296657562256\n","Epoch 73/2000, Step 79, d_loss: 0.3914848864078522, g_loss: 4.290195941925049\n","Epoch 73/2000, Step 80, d_loss: 0.36824607849121094, g_loss: 2.842308521270752\n","Epoch 73/2000, Step 81, d_loss: 0.3802720010280609, g_loss: 3.4894514083862305\n","Epoch 73/2000, Step 82, d_loss: 0.3856807351112366, g_loss: 2.982691526412964\n","Epoch 73/2000, Step 83, d_loss: 0.5157886147499084, g_loss: 3.9880526065826416\n","Epoch 73/2000, Step 84, d_loss: 0.3969753384590149, g_loss: 2.363762140274048\n","Epoch 73/2000, Step 85, d_loss: 0.4267043471336365, g_loss: 3.261162519454956\n","Epoch 73/2000, Step 86, d_loss: 0.45923617482185364, g_loss: 1.8985806703567505\n","Epoch 73/2000, Step 87, d_loss: 0.43981561064720154, g_loss: 3.093021869659424\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 74/2000, Step 1, d_loss: 0.4339893162250519, g_loss: 2.7394018173217773\n","Epoch 74/2000, Step 2, d_loss: 0.40682050585746765, g_loss: 3.8563053607940674\n","Epoch 74/2000, Step 3, d_loss: 0.37326154112815857, g_loss: 4.126104831695557\n","Epoch 74/2000, Step 4, d_loss: 0.38805392384529114, g_loss: 4.902808666229248\n","Epoch 74/2000, Step 5, d_loss: 0.37817618250846863, g_loss: 4.342464447021484\n","Epoch 74/2000, Step 6, d_loss: 0.37119460105895996, g_loss: 5.070032596588135\n","Epoch 74/2000, Step 7, d_loss: 0.42311882972717285, g_loss: 5.516850471496582\n","Epoch 74/2000, Step 8, d_loss: 0.38565266132354736, g_loss: 4.388913154602051\n","Epoch 74/2000, Step 9, d_loss: 0.38914763927459717, g_loss: 4.207215785980225\n","Epoch 74/2000, Step 10, d_loss: 0.41375866532325745, g_loss: 3.963078498840332\n","Epoch 74/2000, Step 11, d_loss: 0.3707740902900696, g_loss: 4.467426776885986\n","Epoch 74/2000, Step 12, d_loss: 0.4162396788597107, g_loss: 3.585686683654785\n","Epoch 74/2000, Step 13, d_loss: 0.36705777049064636, g_loss: 3.4148108959198\n","Epoch 74/2000, Step 14, d_loss: 0.37592580914497375, g_loss: 5.7236175537109375\n","Epoch 74/2000, Step 15, d_loss: 0.45098140835762024, g_loss: 3.529230833053589\n","Epoch 74/2000, Step 16, d_loss: 0.4680578112602234, g_loss: 3.5604395866394043\n","Epoch 74/2000, Step 17, d_loss: 0.3626912534236908, g_loss: 5.70503568649292\n","Epoch 74/2000, Step 18, d_loss: 0.5385771989822388, g_loss: 5.720496654510498\n","Epoch 74/2000, Step 19, d_loss: 0.3952220678329468, g_loss: 3.915863513946533\n","Epoch 74/2000, Step 20, d_loss: 0.38800913095474243, g_loss: 5.356922626495361\n","Epoch 74/2000, Step 21, d_loss: 0.5142530798912048, g_loss: 3.6906182765960693\n","Epoch 74/2000, Step 22, d_loss: 0.3973219394683838, g_loss: 4.080578327178955\n","Epoch 74/2000, Step 23, d_loss: 0.40406334400177, g_loss: 4.307613849639893\n","Epoch 74/2000, Step 24, d_loss: 0.5038580894470215, g_loss: 3.392793655395508\n","Epoch 74/2000, Step 25, d_loss: 0.41642600297927856, g_loss: 4.277466297149658\n","Epoch 74/2000, Step 26, d_loss: 0.40306156873703003, g_loss: 4.00287389755249\n","Epoch 74/2000, Step 27, d_loss: 0.40232354402542114, g_loss: 3.6463913917541504\n","Epoch 74/2000, Step 28, d_loss: 0.4360610842704773, g_loss: 4.529343128204346\n","Epoch 74/2000, Step 29, d_loss: 0.4080263376235962, g_loss: 4.846434593200684\n","Epoch 74/2000, Step 30, d_loss: 0.432407945394516, g_loss: 6.1115007400512695\n","Epoch 74/2000, Step 31, d_loss: 0.46868205070495605, g_loss: 3.7863590717315674\n","Epoch 74/2000, Step 32, d_loss: 0.36667007207870483, g_loss: 2.945096015930176\n","Epoch 74/2000, Step 33, d_loss: 0.4295458495616913, g_loss: 2.8106777667999268\n","Epoch 74/2000, Step 34, d_loss: 0.5049818754196167, g_loss: 2.6216304302215576\n","Epoch 74/2000, Step 35, d_loss: 0.45933544635772705, g_loss: 3.4885289669036865\n","Epoch 74/2000, Step 36, d_loss: 0.4159494638442993, g_loss: 3.5314035415649414\n","Epoch 74/2000, Step 37, d_loss: 0.43880710005760193, g_loss: 3.9697425365448\n","Epoch 74/2000, Step 38, d_loss: 0.4111427068710327, g_loss: 3.190479040145874\n","Epoch 74/2000, Step 39, d_loss: 0.41235148906707764, g_loss: 3.716609001159668\n","Epoch 74/2000, Step 40, d_loss: 0.3880463242530823, g_loss: 3.4833366870880127\n","Epoch 74/2000, Step 41, d_loss: 0.4247037470340729, g_loss: 3.560734272003174\n","Epoch 74/2000, Step 42, d_loss: 0.3754127621650696, g_loss: 4.405760288238525\n","Epoch 74/2000, Step 43, d_loss: 0.40406492352485657, g_loss: 4.1569132804870605\n","Epoch 74/2000, Step 44, d_loss: 0.39875733852386475, g_loss: 5.749050140380859\n","Epoch 74/2000, Step 45, d_loss: 0.5302577018737793, g_loss: 3.149770736694336\n","Epoch 74/2000, Step 46, d_loss: 0.45249518752098083, g_loss: 3.058608055114746\n","Epoch 74/2000, Step 47, d_loss: 0.5062511563301086, g_loss: 3.2839291095733643\n","Epoch 74/2000, Step 48, d_loss: 0.4745136499404907, g_loss: 4.4828009605407715\n","Epoch 74/2000, Step 49, d_loss: 0.4750658869743347, g_loss: 4.009063243865967\n","Epoch 74/2000, Step 50, d_loss: 0.3830730617046356, g_loss: 2.6013400554656982\n","Epoch 74/2000, Step 51, d_loss: 0.39253008365631104, g_loss: 2.972787857055664\n","Epoch 74/2000, Step 52, d_loss: 0.39717814326286316, g_loss: 4.930711269378662\n","Epoch 74/2000, Step 53, d_loss: 0.5640092492103577, g_loss: 3.8930184841156006\n","Epoch 74/2000, Step 54, d_loss: 0.4062480926513672, g_loss: 4.544588088989258\n","Epoch 74/2000, Step 55, d_loss: 0.4222356379032135, g_loss: 4.760685920715332\n","Epoch 74/2000, Step 56, d_loss: 0.6624327898025513, g_loss: 2.0837295055389404\n","Epoch 74/2000, Step 57, d_loss: 0.4015188217163086, g_loss: 2.359510660171509\n","Epoch 74/2000, Step 58, d_loss: 0.535284161567688, g_loss: 1.5893980264663696\n","Epoch 74/2000, Step 59, d_loss: 0.5517178177833557, g_loss: 3.5702459812164307\n","Epoch 74/2000, Step 60, d_loss: 0.4959527850151062, g_loss: 2.065215826034546\n","Epoch 74/2000, Step 61, d_loss: 0.6010524034500122, g_loss: 4.725789546966553\n","Epoch 74/2000, Step 62, d_loss: 0.42006513476371765, g_loss: 4.416250705718994\n","Epoch 74/2000, Step 63, d_loss: 0.38427039980888367, g_loss: 4.845808506011963\n","Epoch 74/2000, Step 64, d_loss: 0.5292527675628662, g_loss: 3.3090243339538574\n","Epoch 74/2000, Step 65, d_loss: 0.38407668471336365, g_loss: 2.5952301025390625\n","Epoch 74/2000, Step 66, d_loss: 0.4135339856147766, g_loss: 2.6504621505737305\n","Epoch 74/2000, Step 67, d_loss: 0.3791959881782532, g_loss: 2.505218267440796\n","Epoch 74/2000, Step 68, d_loss: 0.49847692251205444, g_loss: 3.7878127098083496\n","Epoch 74/2000, Step 69, d_loss: 0.43726128339767456, g_loss: 3.454152822494507\n","Epoch 74/2000, Step 70, d_loss: 0.38744252920150757, g_loss: 3.69986629486084\n","Epoch 74/2000, Step 71, d_loss: 0.46107423305511475, g_loss: 3.9284191131591797\n","Epoch 74/2000, Step 72, d_loss: 0.38895803689956665, g_loss: 4.3436455726623535\n","Epoch 74/2000, Step 73, d_loss: 0.3901573121547699, g_loss: 2.9953179359436035\n","Epoch 74/2000, Step 74, d_loss: 0.3897579610347748, g_loss: 5.488881587982178\n","Epoch 74/2000, Step 75, d_loss: 0.4220814108848572, g_loss: 3.726120710372925\n","Epoch 74/2000, Step 76, d_loss: 0.4375994801521301, g_loss: 3.4653139114379883\n","Epoch 74/2000, Step 77, d_loss: 0.45938077569007874, g_loss: 4.349193096160889\n","Epoch 74/2000, Step 78, d_loss: 0.4324938952922821, g_loss: 3.466156005859375\n","Epoch 74/2000, Step 79, d_loss: 0.432071715593338, g_loss: 3.656545877456665\n","Epoch 74/2000, Step 80, d_loss: 0.443753182888031, g_loss: 3.173853874206543\n","Epoch 74/2000, Step 81, d_loss: 0.4671655297279358, g_loss: 3.221667766571045\n","Epoch 74/2000, Step 82, d_loss: 0.3873538672924042, g_loss: 2.739654541015625\n","Epoch 74/2000, Step 83, d_loss: 0.5017786622047424, g_loss: 3.9840707778930664\n","Epoch 74/2000, Step 84, d_loss: 0.41102612018585205, g_loss: 3.7377207279205322\n","Epoch 74/2000, Step 85, d_loss: 0.45398563146591187, g_loss: 4.503518581390381\n","Epoch 74/2000, Step 86, d_loss: 0.4844038486480713, g_loss: 2.963460683822632\n","Epoch 74/2000, Step 87, d_loss: 0.5386559367179871, g_loss: 4.032528400421143\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 75/2000, Step 1, d_loss: 0.39887288212776184, g_loss: 4.457390308380127\n","Epoch 75/2000, Step 2, d_loss: 0.5216809511184692, g_loss: 2.1074397563934326\n","Epoch 75/2000, Step 3, d_loss: 0.46331149339675903, g_loss: 2.790055274963379\n","Epoch 75/2000, Step 4, d_loss: 0.4976244568824768, g_loss: 1.8417795896530151\n","Epoch 75/2000, Step 5, d_loss: 0.515410304069519, g_loss: 4.022650241851807\n","Epoch 75/2000, Step 6, d_loss: 0.46007364988327026, g_loss: 2.032625675201416\n","Epoch 75/2000, Step 7, d_loss: 0.4815017580986023, g_loss: 4.76708459854126\n","Epoch 75/2000, Step 8, d_loss: 0.46590596437454224, g_loss: 4.736085891723633\n","Epoch 75/2000, Step 9, d_loss: 0.6807224750518799, g_loss: 5.132400989532471\n","Epoch 75/2000, Step 10, d_loss: 0.4837585687637329, g_loss: 3.9464361667633057\n","Epoch 75/2000, Step 11, d_loss: 0.4280990958213806, g_loss: 4.15838623046875\n","Epoch 75/2000, Step 12, d_loss: 0.43785232305526733, g_loss: 4.2594380378723145\n","Epoch 75/2000, Step 13, d_loss: 0.4020973742008209, g_loss: 3.903783082962036\n","Epoch 75/2000, Step 14, d_loss: 0.419748455286026, g_loss: 4.878114223480225\n","Epoch 75/2000, Step 15, d_loss: 0.48854899406433105, g_loss: 3.874136209487915\n","Epoch 75/2000, Step 16, d_loss: 0.5346131324768066, g_loss: 3.5298566818237305\n","Epoch 75/2000, Step 17, d_loss: 0.4289080500602722, g_loss: 4.7406229972839355\n","Epoch 75/2000, Step 18, d_loss: 0.47375428676605225, g_loss: 4.219984531402588\n","Epoch 75/2000, Step 19, d_loss: 0.3998517394065857, g_loss: 3.7183966636657715\n","Epoch 75/2000, Step 20, d_loss: 0.4217037260532379, g_loss: 6.162547588348389\n","Epoch 75/2000, Step 21, d_loss: 0.45573726296424866, g_loss: 3.8515782356262207\n","Epoch 75/2000, Step 22, d_loss: 0.385026216506958, g_loss: 3.780390739440918\n","Epoch 75/2000, Step 23, d_loss: 0.4183238744735718, g_loss: 4.011516571044922\n","Epoch 75/2000, Step 24, d_loss: 0.4284643828868866, g_loss: 2.8741655349731445\n","Epoch 75/2000, Step 25, d_loss: 0.43175795674324036, g_loss: 2.8624842166900635\n","Epoch 75/2000, Step 26, d_loss: 0.3626122772693634, g_loss: 3.833888530731201\n","Epoch 75/2000, Step 27, d_loss: 0.4634169340133667, g_loss: 4.99259090423584\n","Epoch 75/2000, Step 28, d_loss: 0.41281482577323914, g_loss: 5.196101665496826\n","Epoch 75/2000, Step 29, d_loss: 0.4404926896095276, g_loss: 3.940448045730591\n","Epoch 75/2000, Step 30, d_loss: 0.5258846879005432, g_loss: 3.781169891357422\n","Epoch 75/2000, Step 31, d_loss: 0.4013321101665497, g_loss: 4.037474632263184\n","Epoch 75/2000, Step 32, d_loss: 0.35634368658065796, g_loss: 4.6338276863098145\n","Epoch 75/2000, Step 33, d_loss: 0.5156629085540771, g_loss: 4.751379013061523\n","Epoch 75/2000, Step 34, d_loss: 0.40150773525238037, g_loss: 3.5232298374176025\n","Epoch 75/2000, Step 35, d_loss: 0.447741836309433, g_loss: 3.49438214302063\n","Epoch 75/2000, Step 36, d_loss: 0.3757539689540863, g_loss: 3.828763723373413\n","Epoch 75/2000, Step 37, d_loss: 0.44666051864624023, g_loss: 3.7108049392700195\n","Epoch 75/2000, Step 38, d_loss: 0.4764763414859772, g_loss: 2.0225634574890137\n","Epoch 75/2000, Step 39, d_loss: 0.42954835295677185, g_loss: 4.249990940093994\n","Epoch 75/2000, Step 40, d_loss: 0.4130036532878876, g_loss: 4.8807244300842285\n","Epoch 75/2000, Step 41, d_loss: 0.4820060133934021, g_loss: 4.019911289215088\n","Epoch 75/2000, Step 42, d_loss: 0.3594057559967041, g_loss: 3.1706337928771973\n","Epoch 75/2000, Step 43, d_loss: 0.42765408754348755, g_loss: 4.215407848358154\n","Epoch 75/2000, Step 44, d_loss: 0.399700403213501, g_loss: 3.3269283771514893\n","Epoch 75/2000, Step 45, d_loss: 0.4242714047431946, g_loss: 4.403598785400391\n","Epoch 75/2000, Step 46, d_loss: 0.377713680267334, g_loss: 3.2702510356903076\n","Epoch 75/2000, Step 47, d_loss: 0.37543654441833496, g_loss: 3.592534065246582\n","Epoch 75/2000, Step 48, d_loss: 0.42031437158584595, g_loss: 4.77609395980835\n","Epoch 75/2000, Step 49, d_loss: 0.4675053358078003, g_loss: 2.95804762840271\n","Epoch 75/2000, Step 50, d_loss: 0.4180312752723694, g_loss: 4.439117431640625\n","Epoch 75/2000, Step 51, d_loss: 0.3968074917793274, g_loss: 3.8192291259765625\n","Epoch 75/2000, Step 52, d_loss: 0.3703458607196808, g_loss: 2.8914644718170166\n","Epoch 75/2000, Step 53, d_loss: 0.46170902252197266, g_loss: 2.8105902671813965\n","Epoch 75/2000, Step 54, d_loss: 0.48283687233924866, g_loss: 3.5866429805755615\n","Epoch 75/2000, Step 55, d_loss: 0.4140530824661255, g_loss: 3.739259958267212\n","Epoch 75/2000, Step 56, d_loss: 0.4183294177055359, g_loss: 4.148865222930908\n","Epoch 75/2000, Step 57, d_loss: 0.5442280173301697, g_loss: 4.640044689178467\n","Epoch 75/2000, Step 58, d_loss: 0.4465494751930237, g_loss: 3.776221513748169\n","Epoch 75/2000, Step 59, d_loss: 0.4320449233055115, g_loss: 3.9380931854248047\n","Epoch 75/2000, Step 60, d_loss: 0.3763473629951477, g_loss: 3.576035499572754\n","Epoch 75/2000, Step 61, d_loss: 0.4025912582874298, g_loss: 3.278571605682373\n","Epoch 75/2000, Step 62, d_loss: 0.39813029766082764, g_loss: 2.7801382541656494\n","Epoch 75/2000, Step 63, d_loss: 0.4925365746021271, g_loss: 3.639763593673706\n","Epoch 75/2000, Step 64, d_loss: 0.45519906282424927, g_loss: 2.7083370685577393\n","Epoch 75/2000, Step 65, d_loss: 0.36170026659965515, g_loss: 3.9779787063598633\n","Epoch 75/2000, Step 66, d_loss: 0.41405537724494934, g_loss: 3.953584671020508\n","Epoch 75/2000, Step 67, d_loss: 0.3786941468715668, g_loss: 4.128030300140381\n","Epoch 75/2000, Step 68, d_loss: 0.5331053137779236, g_loss: 3.9075636863708496\n","Epoch 75/2000, Step 69, d_loss: 0.41139164566993713, g_loss: 4.9863176345825195\n","Epoch 75/2000, Step 70, d_loss: 0.39766982197761536, g_loss: 4.971677780151367\n","Epoch 75/2000, Step 71, d_loss: 0.5201590657234192, g_loss: 2.1020736694335938\n","Epoch 75/2000, Step 72, d_loss: 0.5168737173080444, g_loss: 2.727106809616089\n","Epoch 75/2000, Step 73, d_loss: 0.4697113037109375, g_loss: 1.836916446685791\n","Epoch 75/2000, Step 74, d_loss: 0.352946937084198, g_loss: 4.8985371589660645\n","Epoch 75/2000, Step 75, d_loss: 0.46825096011161804, g_loss: 4.606121063232422\n","Epoch 75/2000, Step 76, d_loss: 0.39333608746528625, g_loss: 3.8774309158325195\n","Epoch 75/2000, Step 77, d_loss: 0.397258996963501, g_loss: 5.1423726081848145\n","Epoch 75/2000, Step 78, d_loss: 0.4104289710521698, g_loss: 4.526462554931641\n","Epoch 75/2000, Step 79, d_loss: 0.4016737639904022, g_loss: 4.485246658325195\n","Epoch 75/2000, Step 80, d_loss: 0.4371830224990845, g_loss: 3.4356822967529297\n","Epoch 75/2000, Step 81, d_loss: 0.4136343002319336, g_loss: 4.830848217010498\n","Epoch 75/2000, Step 82, d_loss: 0.4757002294063568, g_loss: 3.228705883026123\n","Epoch 75/2000, Step 83, d_loss: 0.5145384669303894, g_loss: 3.8859832286834717\n","Epoch 75/2000, Step 84, d_loss: 0.4819406270980835, g_loss: 3.038999080657959\n","Epoch 75/2000, Step 85, d_loss: 0.41627809405326843, g_loss: 3.8590056896209717\n","Epoch 75/2000, Step 86, d_loss: 0.4344313442707062, g_loss: 4.397924423217773\n","Epoch 75/2000, Step 87, d_loss: 0.48971229791641235, g_loss: 2.651259183883667\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 76/2000, Step 1, d_loss: 0.47370341420173645, g_loss: 4.713960647583008\n","Epoch 76/2000, Step 2, d_loss: 0.4156043231487274, g_loss: 4.403430461883545\n","Epoch 76/2000, Step 3, d_loss: 0.5088527202606201, g_loss: 3.2740182876586914\n","Epoch 76/2000, Step 4, d_loss: 0.45504891872406006, g_loss: 2.6354990005493164\n","Epoch 76/2000, Step 5, d_loss: 0.4204021096229553, g_loss: 2.7779557704925537\n","Epoch 76/2000, Step 6, d_loss: 0.4433092772960663, g_loss: 2.2650749683380127\n","Epoch 76/2000, Step 7, d_loss: 0.5898221731185913, g_loss: 2.9218528270721436\n","Epoch 76/2000, Step 8, d_loss: 0.409800261259079, g_loss: 3.7409756183624268\n","Epoch 76/2000, Step 9, d_loss: 0.39992135763168335, g_loss: 3.2671854496002197\n","Epoch 76/2000, Step 10, d_loss: 0.39393487572669983, g_loss: 5.26741886138916\n","Epoch 76/2000, Step 11, d_loss: 0.5156171321868896, g_loss: 4.387302875518799\n","Epoch 76/2000, Step 12, d_loss: 0.49434852600097656, g_loss: 4.55600643157959\n","Epoch 76/2000, Step 13, d_loss: 0.4117235243320465, g_loss: 3.297006368637085\n","Epoch 76/2000, Step 14, d_loss: 0.48827290534973145, g_loss: 5.317734718322754\n","Epoch 76/2000, Step 15, d_loss: 0.4447416365146637, g_loss: 3.569493293762207\n","Epoch 76/2000, Step 16, d_loss: 0.5113743543624878, g_loss: 2.985168695449829\n","Epoch 76/2000, Step 17, d_loss: 0.41055744886398315, g_loss: 3.436728000640869\n","Epoch 76/2000, Step 18, d_loss: 0.36192601919174194, g_loss: 3.8562047481536865\n","Epoch 76/2000, Step 19, d_loss: 0.5467228889465332, g_loss: 4.108436584472656\n","Epoch 76/2000, Step 20, d_loss: 0.3859511613845825, g_loss: 3.666435956954956\n","Epoch 76/2000, Step 21, d_loss: 0.7073779106140137, g_loss: 5.592989921569824\n","Epoch 76/2000, Step 22, d_loss: 0.5247287154197693, g_loss: 3.0788698196411133\n","Epoch 76/2000, Step 23, d_loss: 0.44432342052459717, g_loss: 1.953209400177002\n","Epoch 76/2000, Step 24, d_loss: 0.39281103014945984, g_loss: 2.4079794883728027\n","Epoch 76/2000, Step 25, d_loss: 0.41199958324432373, g_loss: 2.654067277908325\n","Epoch 76/2000, Step 26, d_loss: 0.452845960855484, g_loss: 1.8873826265335083\n","Epoch 76/2000, Step 27, d_loss: 0.437163382768631, g_loss: 3.571901559829712\n","Epoch 76/2000, Step 28, d_loss: 0.36753740906715393, g_loss: 3.68174147605896\n","Epoch 76/2000, Step 29, d_loss: 0.4418661296367645, g_loss: 5.041101932525635\n","Epoch 76/2000, Step 30, d_loss: 0.8686189651489258, g_loss: 2.378260612487793\n","Epoch 76/2000, Step 31, d_loss: 0.4654179513454437, g_loss: 3.6368043422698975\n","Epoch 76/2000, Step 32, d_loss: 0.5251381397247314, g_loss: 2.456592082977295\n","Epoch 76/2000, Step 33, d_loss: 0.5366908311843872, g_loss: 1.982456922531128\n","Epoch 76/2000, Step 34, d_loss: 0.58766108751297, g_loss: 4.229047775268555\n","Epoch 76/2000, Step 35, d_loss: 0.5001326203346252, g_loss: 3.603282928466797\n","Epoch 76/2000, Step 36, d_loss: 0.387040376663208, g_loss: 3.998858690261841\n","Epoch 76/2000, Step 37, d_loss: 0.5567417144775391, g_loss: 4.221303462982178\n","Epoch 76/2000, Step 38, d_loss: 0.45291391015052795, g_loss: 5.057530403137207\n","Epoch 76/2000, Step 39, d_loss: 0.47427478432655334, g_loss: 5.462305545806885\n","Epoch 76/2000, Step 40, d_loss: 0.40535929799079895, g_loss: 2.622648239135742\n","Epoch 76/2000, Step 41, d_loss: 0.43500664830207825, g_loss: 2.669015884399414\n","Epoch 76/2000, Step 42, d_loss: 0.5251436829566956, g_loss: 2.2982447147369385\n","Epoch 76/2000, Step 43, d_loss: 0.5273195505142212, g_loss: 2.818251371383667\n","Epoch 76/2000, Step 44, d_loss: 0.4393097758293152, g_loss: 3.3109309673309326\n","Epoch 76/2000, Step 45, d_loss: 0.4103600084781647, g_loss: 3.440412998199463\n","Epoch 76/2000, Step 46, d_loss: 0.39175719022750854, g_loss: 3.5798168182373047\n","Epoch 76/2000, Step 47, d_loss: 0.45432424545288086, g_loss: 4.012269973754883\n","Epoch 76/2000, Step 48, d_loss: 0.5834444761276245, g_loss: 3.851938486099243\n","Epoch 76/2000, Step 49, d_loss: 0.36643996834754944, g_loss: 4.326099395751953\n","Epoch 76/2000, Step 50, d_loss: 0.4790656268596649, g_loss: 4.21051549911499\n","Epoch 76/2000, Step 51, d_loss: 0.4583356976509094, g_loss: 3.480757236480713\n","Epoch 76/2000, Step 52, d_loss: 0.4853433668613434, g_loss: 3.4281816482543945\n","Epoch 76/2000, Step 53, d_loss: 0.44391483068466187, g_loss: 3.7855629920959473\n","Epoch 76/2000, Step 54, d_loss: 0.4399062991142273, g_loss: 2.5556371212005615\n","Epoch 76/2000, Step 55, d_loss: 0.44424134492874146, g_loss: 3.0107228755950928\n","Epoch 76/2000, Step 56, d_loss: 0.41995519399642944, g_loss: 3.284233808517456\n","Epoch 76/2000, Step 57, d_loss: 0.4907989799976349, g_loss: 3.3065075874328613\n","Epoch 76/2000, Step 58, d_loss: 0.38252341747283936, g_loss: 2.7629871368408203\n","Epoch 76/2000, Step 59, d_loss: 0.4597650468349457, g_loss: 3.8143835067749023\n","Epoch 76/2000, Step 60, d_loss: 0.4849444329738617, g_loss: 3.3970885276794434\n","Epoch 76/2000, Step 61, d_loss: 0.43030351400375366, g_loss: 4.022036552429199\n","Epoch 76/2000, Step 62, d_loss: 0.3655054271221161, g_loss: 5.6251935958862305\n","Epoch 76/2000, Step 63, d_loss: 0.38892897963523865, g_loss: 4.034080505371094\n","Epoch 76/2000, Step 64, d_loss: 0.407094806432724, g_loss: 3.4770379066467285\n","Epoch 76/2000, Step 65, d_loss: 0.39456668496131897, g_loss: 3.7151031494140625\n","Epoch 76/2000, Step 66, d_loss: 0.3921091854572296, g_loss: 3.8306326866149902\n","Epoch 76/2000, Step 67, d_loss: 0.40991419553756714, g_loss: 3.9260916709899902\n","Epoch 76/2000, Step 68, d_loss: 0.3654909133911133, g_loss: 3.504931688308716\n","Epoch 76/2000, Step 69, d_loss: 0.3681999444961548, g_loss: 3.8663418292999268\n","Epoch 76/2000, Step 70, d_loss: 0.388493150472641, g_loss: 3.5056867599487305\n","Epoch 76/2000, Step 71, d_loss: 0.3707042634487152, g_loss: 3.3095340728759766\n","Epoch 76/2000, Step 72, d_loss: 0.3961676061153412, g_loss: 4.04645299911499\n","Epoch 76/2000, Step 73, d_loss: 0.36889785528182983, g_loss: 3.2141544818878174\n","Epoch 76/2000, Step 74, d_loss: 0.3595231771469116, g_loss: 4.358730316162109\n","Epoch 76/2000, Step 75, d_loss: 0.3583931624889374, g_loss: 4.8263020515441895\n","Epoch 76/2000, Step 76, d_loss: 0.7820769548416138, g_loss: 4.215453624725342\n","Epoch 76/2000, Step 77, d_loss: 0.3894733190536499, g_loss: 3.296332836151123\n","Epoch 76/2000, Step 78, d_loss: 0.42770472168922424, g_loss: 2.2821152210235596\n","Epoch 76/2000, Step 79, d_loss: 0.5089216828346252, g_loss: 2.792285442352295\n","Epoch 76/2000, Step 80, d_loss: 0.44887775182724, g_loss: 3.1805386543273926\n","Epoch 76/2000, Step 81, d_loss: 0.4091300368309021, g_loss: 3.372708559036255\n","Epoch 76/2000, Step 82, d_loss: 0.4206615090370178, g_loss: 4.701442718505859\n","Epoch 76/2000, Step 83, d_loss: 0.46793434023857117, g_loss: 4.795156002044678\n","Epoch 76/2000, Step 84, d_loss: 0.4171699285507202, g_loss: 3.4058237075805664\n","Epoch 76/2000, Step 85, d_loss: 0.40495565533638, g_loss: 4.040400505065918\n","Epoch 76/2000, Step 86, d_loss: 0.4686734974384308, g_loss: 3.814663887023926\n","Epoch 76/2000, Step 87, d_loss: 0.38004592061042786, g_loss: 3.567207098007202\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 77/2000, Step 1, d_loss: 0.3750576376914978, g_loss: 3.0061726570129395\n","Epoch 77/2000, Step 2, d_loss: 0.3723710775375366, g_loss: 3.710441827774048\n","Epoch 77/2000, Step 3, d_loss: 0.44964757561683655, g_loss: 3.487304210662842\n","Epoch 77/2000, Step 4, d_loss: 0.39703911542892456, g_loss: 3.7181379795074463\n","Epoch 77/2000, Step 5, d_loss: 0.38474348187446594, g_loss: 3.8363168239593506\n","Epoch 77/2000, Step 6, d_loss: 0.37178468704223633, g_loss: 4.389632701873779\n","Epoch 77/2000, Step 7, d_loss: 0.5220052599906921, g_loss: 3.605539321899414\n","Epoch 77/2000, Step 8, d_loss: 0.3872278332710266, g_loss: 4.038980484008789\n","Epoch 77/2000, Step 9, d_loss: 0.4229986071586609, g_loss: 3.47956919670105\n","Epoch 77/2000, Step 10, d_loss: 0.4001001715660095, g_loss: 4.229098796844482\n","Epoch 77/2000, Step 11, d_loss: 0.4337293803691864, g_loss: 2.9361109733581543\n","Epoch 77/2000, Step 12, d_loss: 0.4363239109516144, g_loss: 3.0169806480407715\n","Epoch 77/2000, Step 13, d_loss: 0.3695842921733856, g_loss: 4.1597089767456055\n","Epoch 77/2000, Step 14, d_loss: 0.3939535617828369, g_loss: 4.285406589508057\n","Epoch 77/2000, Step 15, d_loss: 0.4457020163536072, g_loss: 2.8169658184051514\n","Epoch 77/2000, Step 16, d_loss: 0.40188220143318176, g_loss: 3.2543835639953613\n","Epoch 77/2000, Step 17, d_loss: 0.6204512715339661, g_loss: 3.0747931003570557\n","Epoch 77/2000, Step 18, d_loss: 0.400579035282135, g_loss: 3.591738700866699\n","Epoch 77/2000, Step 19, d_loss: 0.473766028881073, g_loss: 2.9745776653289795\n","Epoch 77/2000, Step 20, d_loss: 0.44304174184799194, g_loss: 3.979658603668213\n","Epoch 77/2000, Step 21, d_loss: 0.6431165933609009, g_loss: 2.8484537601470947\n","Epoch 77/2000, Step 22, d_loss: 0.3896194398403168, g_loss: 3.3673512935638428\n","Epoch 77/2000, Step 23, d_loss: 0.39010411500930786, g_loss: 3.637655258178711\n","Epoch 77/2000, Step 24, d_loss: 0.4364573359489441, g_loss: 3.6646180152893066\n","Epoch 77/2000, Step 25, d_loss: 0.45457887649536133, g_loss: 4.7667670249938965\n","Epoch 77/2000, Step 26, d_loss: 0.5519492626190186, g_loss: 3.201462745666504\n","Epoch 77/2000, Step 27, d_loss: 0.42729586362838745, g_loss: 2.6620357036590576\n","Epoch 77/2000, Step 28, d_loss: 0.38215774297714233, g_loss: 3.278381586074829\n","Epoch 77/2000, Step 29, d_loss: 0.3832641839981079, g_loss: 2.2948319911956787\n","Epoch 77/2000, Step 30, d_loss: 0.48308032751083374, g_loss: 3.793914556503296\n","Epoch 77/2000, Step 31, d_loss: 0.4396723806858063, g_loss: 3.6882760524749756\n","Epoch 77/2000, Step 32, d_loss: 0.5069045424461365, g_loss: 4.083157539367676\n","Epoch 77/2000, Step 33, d_loss: 0.37221333384513855, g_loss: 4.116937637329102\n","Epoch 77/2000, Step 34, d_loss: 0.4357749819755554, g_loss: 3.831773519515991\n","Epoch 77/2000, Step 35, d_loss: 0.4143013656139374, g_loss: 3.2742180824279785\n","Epoch 77/2000, Step 36, d_loss: 0.4331669807434082, g_loss: 3.2705307006835938\n","Epoch 77/2000, Step 37, d_loss: 0.3929695785045624, g_loss: 3.293334484100342\n","Epoch 77/2000, Step 38, d_loss: 0.36615943908691406, g_loss: 3.3322904109954834\n","Epoch 77/2000, Step 39, d_loss: 0.40729600191116333, g_loss: 3.4570322036743164\n","Epoch 77/2000, Step 40, d_loss: 0.4152749478816986, g_loss: 3.3173604011535645\n","Epoch 77/2000, Step 41, d_loss: 0.3619548976421356, g_loss: 2.986483573913574\n","Epoch 77/2000, Step 42, d_loss: 0.3914481997489929, g_loss: 3.8040833473205566\n","Epoch 77/2000, Step 43, d_loss: 0.3826621472835541, g_loss: 3.305382013320923\n","Epoch 77/2000, Step 44, d_loss: 0.3694661557674408, g_loss: 4.382590293884277\n","Epoch 77/2000, Step 45, d_loss: 0.5011255741119385, g_loss: 3.698896646499634\n","Epoch 77/2000, Step 46, d_loss: 0.45132577419281006, g_loss: 2.9555535316467285\n","Epoch 77/2000, Step 47, d_loss: 0.3929566740989685, g_loss: 2.7592086791992188\n","Epoch 77/2000, Step 48, d_loss: 0.4618888199329376, g_loss: 3.523867607116699\n","Epoch 77/2000, Step 49, d_loss: 0.39418256282806396, g_loss: 3.413419008255005\n","Epoch 77/2000, Step 50, d_loss: 0.5399875044822693, g_loss: 4.161253452301025\n","Epoch 77/2000, Step 51, d_loss: 0.4030866324901581, g_loss: 4.45382833480835\n","Epoch 77/2000, Step 52, d_loss: 0.39286428689956665, g_loss: 4.284836292266846\n","Epoch 77/2000, Step 53, d_loss: 0.5431599020957947, g_loss: 6.366779804229736\n","Epoch 77/2000, Step 54, d_loss: 0.5127798318862915, g_loss: 4.796995639801025\n","Epoch 77/2000, Step 55, d_loss: 0.47486788034439087, g_loss: 2.9138050079345703\n","Epoch 77/2000, Step 56, d_loss: 0.39412492513656616, g_loss: 3.9184699058532715\n","Epoch 77/2000, Step 57, d_loss: 0.4709955155849457, g_loss: 4.6027727127075195\n","Epoch 77/2000, Step 58, d_loss: 0.3818618059158325, g_loss: 3.757044553756714\n","Epoch 77/2000, Step 59, d_loss: 0.4659747779369354, g_loss: 2.242433786392212\n","Epoch 77/2000, Step 60, d_loss: 0.37670642137527466, g_loss: 4.052002429962158\n","Epoch 77/2000, Step 61, d_loss: 0.3843565583229065, g_loss: 3.616964340209961\n","Epoch 77/2000, Step 62, d_loss: 0.41715505719184875, g_loss: 3.2848825454711914\n","Epoch 77/2000, Step 63, d_loss: 0.37138184905052185, g_loss: 5.829003810882568\n","Epoch 77/2000, Step 64, d_loss: 0.4125766456127167, g_loss: 3.8118185997009277\n","Epoch 77/2000, Step 65, d_loss: 0.3768601417541504, g_loss: 4.494495391845703\n","Epoch 77/2000, Step 66, d_loss: 0.4585490822792053, g_loss: 4.226682186126709\n","Epoch 77/2000, Step 67, d_loss: 0.40542763471603394, g_loss: 3.6944684982299805\n","Epoch 77/2000, Step 68, d_loss: 0.4214268922805786, g_loss: 4.473639965057373\n","Epoch 77/2000, Step 69, d_loss: 0.38479146361351013, g_loss: 4.584572792053223\n","Epoch 77/2000, Step 70, d_loss: 0.4650306701660156, g_loss: 4.388333320617676\n","Epoch 77/2000, Step 71, d_loss: 0.40555939078330994, g_loss: 3.0338118076324463\n","Epoch 77/2000, Step 72, d_loss: 0.4291445314884186, g_loss: 4.570067405700684\n","Epoch 77/2000, Step 73, d_loss: 0.4589013159275055, g_loss: 3.3171980381011963\n","Epoch 77/2000, Step 74, d_loss: 0.4019196331501007, g_loss: 3.1630818843841553\n","Epoch 77/2000, Step 75, d_loss: 0.4144308567047119, g_loss: 3.445242166519165\n","Epoch 77/2000, Step 76, d_loss: 0.395144522190094, g_loss: 3.644137382507324\n","Epoch 77/2000, Step 77, d_loss: 0.4422374963760376, g_loss: 3.9185197353363037\n","Epoch 77/2000, Step 78, d_loss: 0.5121766328811646, g_loss: 3.650911569595337\n","Epoch 77/2000, Step 79, d_loss: 0.4698408246040344, g_loss: 3.6338658332824707\n","Epoch 77/2000, Step 80, d_loss: 0.427706778049469, g_loss: 3.189668893814087\n","Epoch 77/2000, Step 81, d_loss: 0.42962324619293213, g_loss: 2.6931862831115723\n","Epoch 77/2000, Step 82, d_loss: 0.48020264506340027, g_loss: 4.354142665863037\n","Epoch 77/2000, Step 83, d_loss: 0.48721545934677124, g_loss: 3.152527093887329\n","Epoch 77/2000, Step 84, d_loss: 0.46983134746551514, g_loss: 3.0628812313079834\n","Epoch 77/2000, Step 85, d_loss: 0.4123755395412445, g_loss: 4.098141670227051\n","Epoch 77/2000, Step 86, d_loss: 0.38948047161102295, g_loss: 4.689071178436279\n","Epoch 77/2000, Step 87, d_loss: 0.47249725461006165, g_loss: 3.4950273036956787\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 78/2000, Step 1, d_loss: 0.40667033195495605, g_loss: 4.334225177764893\n","Epoch 78/2000, Step 2, d_loss: 0.40143731236457825, g_loss: 3.977362871170044\n","Epoch 78/2000, Step 3, d_loss: 0.3888475000858307, g_loss: 3.5714616775512695\n","Epoch 78/2000, Step 4, d_loss: 0.42502138018608093, g_loss: 2.797900915145874\n","Epoch 78/2000, Step 5, d_loss: 0.43090569972991943, g_loss: 3.905133008956909\n","Epoch 78/2000, Step 6, d_loss: 0.5415749549865723, g_loss: 2.640845537185669\n","Epoch 78/2000, Step 7, d_loss: 0.3628401458263397, g_loss: 3.2474777698516846\n","Epoch 78/2000, Step 8, d_loss: 0.3738479018211365, g_loss: 3.630786418914795\n","Epoch 78/2000, Step 9, d_loss: 0.3526651859283447, g_loss: 4.41963529586792\n","Epoch 78/2000, Step 10, d_loss: 0.38266998529434204, g_loss: 4.449682235717773\n","Epoch 78/2000, Step 11, d_loss: 0.5295704007148743, g_loss: 3.036564826965332\n","Epoch 78/2000, Step 12, d_loss: 0.3730289340019226, g_loss: 4.205089092254639\n","Epoch 78/2000, Step 13, d_loss: 0.4332890808582306, g_loss: 4.351220607757568\n","Epoch 78/2000, Step 14, d_loss: 0.4165954887866974, g_loss: 4.8942790031433105\n","Epoch 78/2000, Step 15, d_loss: 0.5421669483184814, g_loss: 2.593294382095337\n","Epoch 78/2000, Step 16, d_loss: 0.40142393112182617, g_loss: 3.6061997413635254\n","Epoch 78/2000, Step 17, d_loss: 0.45824548602104187, g_loss: 2.84444522857666\n","Epoch 78/2000, Step 18, d_loss: 0.3899281919002533, g_loss: 2.8022851943969727\n","Epoch 78/2000, Step 19, d_loss: 0.3817985951900482, g_loss: 4.061220169067383\n","Epoch 78/2000, Step 20, d_loss: 0.3529578447341919, g_loss: 5.042325019836426\n","Epoch 78/2000, Step 21, d_loss: 0.3796621561050415, g_loss: 4.172106742858887\n","Epoch 78/2000, Step 22, d_loss: 0.5219261050224304, g_loss: 3.9293529987335205\n","Epoch 78/2000, Step 23, d_loss: 0.47821223735809326, g_loss: 4.240237236022949\n","Epoch 78/2000, Step 24, d_loss: 0.480172723531723, g_loss: 3.57559871673584\n","Epoch 78/2000, Step 25, d_loss: 0.40127032995224, g_loss: 1.799558162689209\n","Epoch 78/2000, Step 26, d_loss: 0.4746839106082916, g_loss: 2.371960401535034\n","Epoch 78/2000, Step 27, d_loss: 0.5080384612083435, g_loss: 1.778756856918335\n","Epoch 78/2000, Step 28, d_loss: 0.4863416254520416, g_loss: 2.1782846450805664\n","Epoch 78/2000, Step 29, d_loss: 0.5462489724159241, g_loss: 3.990506410598755\n","Epoch 78/2000, Step 30, d_loss: 0.3844546377658844, g_loss: 3.770179271697998\n","Epoch 78/2000, Step 31, d_loss: 0.3723790645599365, g_loss: 4.284271717071533\n","Epoch 78/2000, Step 32, d_loss: 0.4080789387226105, g_loss: 5.050901412963867\n","Epoch 78/2000, Step 33, d_loss: 0.43594446778297424, g_loss: 4.301400184631348\n","Epoch 78/2000, Step 34, d_loss: 0.388117253780365, g_loss: 4.995624542236328\n","Epoch 78/2000, Step 35, d_loss: 0.6537339687347412, g_loss: 4.9375152587890625\n","Epoch 78/2000, Step 36, d_loss: 0.38843750953674316, g_loss: 3.167442798614502\n","Epoch 78/2000, Step 37, d_loss: 0.482217401266098, g_loss: 1.8073488473892212\n","Epoch 78/2000, Step 38, d_loss: 0.5550169944763184, g_loss: 1.785380244255066\n","Epoch 78/2000, Step 39, d_loss: 0.46893683075904846, g_loss: 2.517000675201416\n","Epoch 78/2000, Step 40, d_loss: 0.4541732966899872, g_loss: 3.7152771949768066\n","Epoch 78/2000, Step 41, d_loss: 0.4503101706504822, g_loss: 3.538621187210083\n","Epoch 78/2000, Step 42, d_loss: 0.42671409249305725, g_loss: 4.26524543762207\n","Epoch 78/2000, Step 43, d_loss: 0.5521507263183594, g_loss: 4.339080810546875\n","Epoch 78/2000, Step 44, d_loss: 0.4302288591861725, g_loss: 5.519950866699219\n","Epoch 78/2000, Step 45, d_loss: 0.4981580972671509, g_loss: 4.141108989715576\n","Epoch 78/2000, Step 46, d_loss: 0.4814578890800476, g_loss: 2.909308910369873\n","Epoch 78/2000, Step 47, d_loss: 0.493334025144577, g_loss: 2.2057747840881348\n","Epoch 78/2000, Step 48, d_loss: 0.5109680891036987, g_loss: 3.9093642234802246\n","Epoch 78/2000, Step 49, d_loss: 0.5033361911773682, g_loss: 3.2190682888031006\n","Epoch 78/2000, Step 50, d_loss: 0.4173443019390106, g_loss: 3.6457223892211914\n","Epoch 78/2000, Step 51, d_loss: 0.4316200911998749, g_loss: 3.2160258293151855\n","Epoch 78/2000, Step 52, d_loss: 0.5161883234977722, g_loss: 3.400604009628296\n","Epoch 78/2000, Step 53, d_loss: 0.4567222595214844, g_loss: 3.000223398208618\n","Epoch 78/2000, Step 54, d_loss: 0.4183732271194458, g_loss: 3.9735007286071777\n","Epoch 78/2000, Step 55, d_loss: 0.5489754676818848, g_loss: 4.1611151695251465\n","Epoch 78/2000, Step 56, d_loss: 0.7921465039253235, g_loss: 2.938426971435547\n","Epoch 78/2000, Step 57, d_loss: 0.38337981700897217, g_loss: 2.8723292350769043\n","Epoch 78/2000, Step 58, d_loss: 0.5653239488601685, g_loss: 2.659553289413452\n","Epoch 78/2000, Step 59, d_loss: 0.7303656935691833, g_loss: 4.223504543304443\n","Epoch 78/2000, Step 60, d_loss: 0.7351253032684326, g_loss: 3.410505771636963\n","Epoch 78/2000, Step 61, d_loss: 0.6136903166770935, g_loss: 3.856041193008423\n","Epoch 78/2000, Step 62, d_loss: 0.41556206345558167, g_loss: 3.6038994789123535\n","Epoch 78/2000, Step 63, d_loss: 0.3905085623264313, g_loss: 4.947413921356201\n","Epoch 78/2000, Step 64, d_loss: 0.5404003858566284, g_loss: 4.040103435516357\n","Epoch 78/2000, Step 65, d_loss: 0.4925972521305084, g_loss: 3.946467399597168\n","Epoch 78/2000, Step 66, d_loss: 0.4924791753292084, g_loss: 3.915919542312622\n","Epoch 78/2000, Step 67, d_loss: 0.3882407248020172, g_loss: 3.519010066986084\n","Epoch 78/2000, Step 68, d_loss: 0.3666968047618866, g_loss: 3.3279829025268555\n","Epoch 78/2000, Step 69, d_loss: 0.42298176884651184, g_loss: 3.4359357357025146\n","Epoch 78/2000, Step 70, d_loss: 0.3876384496688843, g_loss: 2.3065948486328125\n","Epoch 78/2000, Step 71, d_loss: 0.4224424958229065, g_loss: 2.973304033279419\n","Epoch 78/2000, Step 72, d_loss: 0.45359987020492554, g_loss: 5.202810764312744\n","Epoch 78/2000, Step 73, d_loss: 0.39335495233535767, g_loss: 4.168254852294922\n","Epoch 78/2000, Step 74, d_loss: 0.39552754163742065, g_loss: 4.5827789306640625\n","Epoch 78/2000, Step 75, d_loss: 0.40149134397506714, g_loss: 4.500760555267334\n","Epoch 78/2000, Step 76, d_loss: 0.4210570454597473, g_loss: 5.7037577629089355\n","Epoch 78/2000, Step 77, d_loss: 0.5440705418586731, g_loss: 4.037810325622559\n","Epoch 78/2000, Step 78, d_loss: 0.44795653223991394, g_loss: 2.857905864715576\n","Epoch 78/2000, Step 79, d_loss: 0.47479912638664246, g_loss: 3.6959826946258545\n","Epoch 78/2000, Step 80, d_loss: 0.49581676721572876, g_loss: 2.428929090499878\n","Epoch 78/2000, Step 81, d_loss: 0.5252537131309509, g_loss: 3.050448417663574\n","Epoch 78/2000, Step 82, d_loss: 0.5325335264205933, g_loss: 3.4261324405670166\n","Epoch 78/2000, Step 83, d_loss: 0.4948115050792694, g_loss: 3.153468370437622\n","Epoch 78/2000, Step 84, d_loss: 0.4265948534011841, g_loss: 4.190776824951172\n","Epoch 78/2000, Step 85, d_loss: 0.40959689021110535, g_loss: 5.449004173278809\n","Epoch 78/2000, Step 86, d_loss: 0.39511778950691223, g_loss: 4.737096786499023\n","Epoch 78/2000, Step 87, d_loss: 0.47267282009124756, g_loss: 3.1671407222747803\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 79/2000, Step 1, d_loss: 0.4420023560523987, g_loss: 4.706906318664551\n","Epoch 79/2000, Step 2, d_loss: 0.42398884892463684, g_loss: 3.4291481971740723\n","Epoch 79/2000, Step 3, d_loss: 0.437652587890625, g_loss: 4.331996440887451\n","Epoch 79/2000, Step 4, d_loss: 0.4256160855293274, g_loss: 3.5575459003448486\n","Epoch 79/2000, Step 5, d_loss: 0.39110738039016724, g_loss: 4.5800251960754395\n","Epoch 79/2000, Step 6, d_loss: 0.3728139102458954, g_loss: 4.10051155090332\n","Epoch 79/2000, Step 7, d_loss: 0.3779987096786499, g_loss: 4.614840507507324\n","Epoch 79/2000, Step 8, d_loss: 0.36438095569610596, g_loss: 4.745120048522949\n","Epoch 79/2000, Step 9, d_loss: 0.3564595580101013, g_loss: 5.12718391418457\n","Epoch 79/2000, Step 10, d_loss: 0.394970178604126, g_loss: 3.8770785331726074\n","Epoch 79/2000, Step 11, d_loss: 0.3775950074195862, g_loss: 5.441251754760742\n","Epoch 79/2000, Step 12, d_loss: 0.4584552049636841, g_loss: 5.116000175476074\n","Epoch 79/2000, Step 13, d_loss: 0.3736383020877838, g_loss: 3.3898093700408936\n","Epoch 79/2000, Step 14, d_loss: 0.37714651226997375, g_loss: 3.9733831882476807\n","Epoch 79/2000, Step 15, d_loss: 0.4016514718532562, g_loss: 4.5475993156433105\n","Epoch 79/2000, Step 16, d_loss: 0.3710191547870636, g_loss: 4.363521575927734\n","Epoch 79/2000, Step 17, d_loss: 0.3837199807167053, g_loss: 3.65440034866333\n","Epoch 79/2000, Step 18, d_loss: 0.37106937170028687, g_loss: 4.429158687591553\n","Epoch 79/2000, Step 19, d_loss: 0.46342572569847107, g_loss: 4.411716461181641\n","Epoch 79/2000, Step 20, d_loss: 0.367747962474823, g_loss: 3.2960314750671387\n","Epoch 79/2000, Step 21, d_loss: 0.43077176809310913, g_loss: 3.568342447280884\n","Epoch 79/2000, Step 22, d_loss: 0.4117403030395508, g_loss: 3.0057034492492676\n","Epoch 79/2000, Step 23, d_loss: 0.48525241017341614, g_loss: 3.3204257488250732\n","Epoch 79/2000, Step 24, d_loss: 0.37479379773139954, g_loss: 6.060998439788818\n","Epoch 79/2000, Step 25, d_loss: 0.3639867603778839, g_loss: 4.386745452880859\n","Epoch 79/2000, Step 26, d_loss: 0.3758602738380432, g_loss: 4.607998847961426\n","Epoch 79/2000, Step 27, d_loss: 0.36302676796913147, g_loss: 4.503841876983643\n","Epoch 79/2000, Step 28, d_loss: 0.42172640562057495, g_loss: 5.566924095153809\n","Epoch 79/2000, Step 29, d_loss: 0.37842175364494324, g_loss: 3.6522107124328613\n","Epoch 79/2000, Step 30, d_loss: 0.3539890944957733, g_loss: 3.3696608543395996\n","Epoch 79/2000, Step 31, d_loss: 0.41441866755485535, g_loss: 2.3468942642211914\n","Epoch 79/2000, Step 32, d_loss: 0.4916403889656067, g_loss: 4.752278804779053\n","Epoch 79/2000, Step 33, d_loss: 0.38753101229667664, g_loss: 5.9837188720703125\n","Epoch 79/2000, Step 34, d_loss: 0.36415913701057434, g_loss: 4.38163423538208\n","Epoch 79/2000, Step 35, d_loss: 0.41493308544158936, g_loss: 4.38896369934082\n","Epoch 79/2000, Step 36, d_loss: 0.37830236554145813, g_loss: 3.9791557788848877\n","Epoch 79/2000, Step 37, d_loss: 0.40913400053977966, g_loss: 5.665045738220215\n","Epoch 79/2000, Step 38, d_loss: 0.42522841691970825, g_loss: 4.097900390625\n","Epoch 79/2000, Step 39, d_loss: 0.3881523013114929, g_loss: 3.928471088409424\n","Epoch 79/2000, Step 40, d_loss: 0.3941704034805298, g_loss: 2.7026023864746094\n","Epoch 79/2000, Step 41, d_loss: 0.38505563139915466, g_loss: 2.9933934211730957\n","Epoch 79/2000, Step 42, d_loss: 0.35203051567077637, g_loss: 4.364590644836426\n","Epoch 79/2000, Step 43, d_loss: 0.38933196663856506, g_loss: 2.248178720474243\n","Epoch 79/2000, Step 44, d_loss: 0.4270901679992676, g_loss: 2.9547269344329834\n","Epoch 79/2000, Step 45, d_loss: 0.4726182222366333, g_loss: 4.325037956237793\n","Epoch 79/2000, Step 46, d_loss: 0.40635937452316284, g_loss: 2.4452478885650635\n","Epoch 79/2000, Step 47, d_loss: 0.36868080496788025, g_loss: 5.261919975280762\n","Epoch 79/2000, Step 48, d_loss: 0.4237247109413147, g_loss: 3.9143147468566895\n","Epoch 79/2000, Step 49, d_loss: 0.36668455600738525, g_loss: 3.584033250808716\n","Epoch 79/2000, Step 50, d_loss: 0.35055962204933167, g_loss: 3.7824339866638184\n","Epoch 79/2000, Step 51, d_loss: 0.3722488284111023, g_loss: 5.321681022644043\n","Epoch 79/2000, Step 52, d_loss: 0.36350250244140625, g_loss: 3.292635202407837\n","Epoch 79/2000, Step 53, d_loss: 0.435301810503006, g_loss: 3.862061023712158\n","Epoch 79/2000, Step 54, d_loss: 0.39932748675346375, g_loss: 4.5684967041015625\n","Epoch 79/2000, Step 55, d_loss: 0.36642932891845703, g_loss: 3.299168825149536\n","Epoch 79/2000, Step 56, d_loss: 0.4103047847747803, g_loss: 2.3621418476104736\n","Epoch 79/2000, Step 57, d_loss: 0.41217365860939026, g_loss: 3.3945465087890625\n","Epoch 79/2000, Step 58, d_loss: 0.4958152174949646, g_loss: 2.4497570991516113\n","Epoch 79/2000, Step 59, d_loss: 0.3670545518398285, g_loss: 4.367977619171143\n","Epoch 79/2000, Step 60, d_loss: 0.3859817385673523, g_loss: 4.584767818450928\n","Epoch 79/2000, Step 61, d_loss: 0.41560885310173035, g_loss: 4.856027603149414\n","Epoch 79/2000, Step 62, d_loss: 0.4418811798095703, g_loss: 4.850668430328369\n","Epoch 79/2000, Step 63, d_loss: 0.3530982732772827, g_loss: 3.8278582096099854\n","Epoch 79/2000, Step 64, d_loss: 0.3864906132221222, g_loss: 4.200591087341309\n","Epoch 79/2000, Step 65, d_loss: 0.3875448405742645, g_loss: 3.82804536819458\n","Epoch 79/2000, Step 66, d_loss: 0.39908406138420105, g_loss: 3.100193500518799\n","Epoch 79/2000, Step 67, d_loss: 0.44752195477485657, g_loss: 3.0368571281433105\n","Epoch 79/2000, Step 68, d_loss: 0.3671117424964905, g_loss: 4.098501682281494\n","Epoch 79/2000, Step 69, d_loss: 0.35904625058174133, g_loss: 3.4397566318511963\n","Epoch 79/2000, Step 70, d_loss: 0.4542737603187561, g_loss: 4.466559410095215\n","Epoch 79/2000, Step 71, d_loss: 0.3996066153049469, g_loss: 4.416087627410889\n","Epoch 79/2000, Step 72, d_loss: 0.3875698149204254, g_loss: 3.1591668128967285\n","Epoch 79/2000, Step 73, d_loss: 0.4278203547000885, g_loss: 4.776608943939209\n","Epoch 79/2000, Step 74, d_loss: 0.3663265109062195, g_loss: 3.6212968826293945\n","Epoch 79/2000, Step 75, d_loss: 0.40277358889579773, g_loss: 4.005306720733643\n","Epoch 79/2000, Step 76, d_loss: 0.4375576972961426, g_loss: 4.378554344177246\n","Epoch 79/2000, Step 77, d_loss: 0.38515257835388184, g_loss: 3.3354651927948\n","Epoch 79/2000, Step 78, d_loss: 0.3816429376602173, g_loss: 3.7635016441345215\n","Epoch 79/2000, Step 79, d_loss: 0.37281209230422974, g_loss: 5.7803568840026855\n","Epoch 79/2000, Step 80, d_loss: 0.36583301424980164, g_loss: 3.599417209625244\n","Epoch 79/2000, Step 81, d_loss: 0.4095427393913269, g_loss: 6.182735443115234\n","Epoch 79/2000, Step 82, d_loss: 0.3743450939655304, g_loss: 5.122920513153076\n","Epoch 79/2000, Step 83, d_loss: 0.3590306341648102, g_loss: 3.768362283706665\n","Epoch 79/2000, Step 84, d_loss: 0.4865967035293579, g_loss: 4.9303436279296875\n","Epoch 79/2000, Step 85, d_loss: 0.43282678723335266, g_loss: 2.1949851512908936\n","Epoch 79/2000, Step 86, d_loss: 0.3782827854156494, g_loss: 3.0484211444854736\n","Epoch 79/2000, Step 87, d_loss: 0.40467703342437744, g_loss: 3.7791566848754883\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 80/2000, Step 1, d_loss: 0.4550200402736664, g_loss: 3.2691726684570312\n","Epoch 80/2000, Step 2, d_loss: 0.3969310522079468, g_loss: 3.6696977615356445\n","Epoch 80/2000, Step 3, d_loss: 0.36612141132354736, g_loss: 4.46659517288208\n","Epoch 80/2000, Step 4, d_loss: 0.35088521242141724, g_loss: 5.568016529083252\n","Epoch 80/2000, Step 5, d_loss: 0.3616156280040741, g_loss: 4.451939582824707\n","Epoch 80/2000, Step 6, d_loss: 0.35979846119880676, g_loss: 5.068007469177246\n","Epoch 80/2000, Step 7, d_loss: 0.47383078932762146, g_loss: 4.4484453201293945\n","Epoch 80/2000, Step 8, d_loss: 0.39894211292266846, g_loss: 3.8004262447357178\n","Epoch 80/2000, Step 9, d_loss: 0.3552364408969879, g_loss: 4.873898983001709\n","Epoch 80/2000, Step 10, d_loss: 0.3996155261993408, g_loss: 3.6750667095184326\n","Epoch 80/2000, Step 11, d_loss: 0.40933430194854736, g_loss: 3.6494991779327393\n","Epoch 80/2000, Step 12, d_loss: 0.37500667572021484, g_loss: 3.8218626976013184\n","Epoch 80/2000, Step 13, d_loss: 0.39959126710891724, g_loss: 2.6831295490264893\n","Epoch 80/2000, Step 14, d_loss: 0.4044676721096039, g_loss: 2.959526777267456\n","Epoch 80/2000, Step 15, d_loss: 0.4006209075450897, g_loss: 5.0602545738220215\n","Epoch 80/2000, Step 16, d_loss: 0.363430917263031, g_loss: 5.637293815612793\n","Epoch 80/2000, Step 17, d_loss: 0.5200592875480652, g_loss: 5.108854293823242\n","Epoch 80/2000, Step 18, d_loss: 0.4210192859172821, g_loss: 4.2374491691589355\n","Epoch 80/2000, Step 19, d_loss: 0.37844619154930115, g_loss: 3.966728448867798\n","Epoch 80/2000, Step 20, d_loss: 0.39426249265670776, g_loss: 3.8267109394073486\n","Epoch 80/2000, Step 21, d_loss: 0.4005860984325409, g_loss: 3.021407127380371\n","Epoch 80/2000, Step 22, d_loss: 0.4076994061470032, g_loss: 3.203124523162842\n","Epoch 80/2000, Step 23, d_loss: 0.3932913839817047, g_loss: 4.287106990814209\n","Epoch 80/2000, Step 24, d_loss: 0.35833823680877686, g_loss: 4.615564823150635\n","Epoch 80/2000, Step 25, d_loss: 0.3686717450618744, g_loss: 3.4040474891662598\n","Epoch 80/2000, Step 26, d_loss: 0.38698989152908325, g_loss: 4.605405807495117\n","Epoch 80/2000, Step 27, d_loss: 0.41774892807006836, g_loss: 4.018884181976318\n","Epoch 80/2000, Step 28, d_loss: 0.5337193012237549, g_loss: 4.160145282745361\n","Epoch 80/2000, Step 29, d_loss: 0.44724705815315247, g_loss: 3.7424545288085938\n","Epoch 80/2000, Step 30, d_loss: 0.4396075904369354, g_loss: 4.722041130065918\n","Epoch 80/2000, Step 31, d_loss: 0.3816923201084137, g_loss: 4.025022029876709\n","Epoch 80/2000, Step 32, d_loss: 0.42823830246925354, g_loss: 3.8232624530792236\n","Epoch 80/2000, Step 33, d_loss: 0.4057261347770691, g_loss: 3.9676270484924316\n","Epoch 80/2000, Step 34, d_loss: 0.3651137351989746, g_loss: 3.841827154159546\n","Epoch 80/2000, Step 35, d_loss: 0.42320212721824646, g_loss: 3.0759663581848145\n","Epoch 80/2000, Step 36, d_loss: 0.38717371225357056, g_loss: 3.76482892036438\n","Epoch 80/2000, Step 37, d_loss: 0.39069104194641113, g_loss: 5.4033589363098145\n","Epoch 80/2000, Step 38, d_loss: 0.37508079409599304, g_loss: 4.1724629402160645\n","Epoch 80/2000, Step 39, d_loss: 0.3778367340564728, g_loss: 4.8106369972229\n","Epoch 80/2000, Step 40, d_loss: 0.42382803559303284, g_loss: 5.058006286621094\n","Epoch 80/2000, Step 41, d_loss: 0.5794308185577393, g_loss: 4.039275646209717\n","Epoch 80/2000, Step 42, d_loss: 0.4047996997833252, g_loss: 3.415043354034424\n","Epoch 80/2000, Step 43, d_loss: 0.40483248233795166, g_loss: 3.430257558822632\n","Epoch 80/2000, Step 44, d_loss: 0.4088539481163025, g_loss: 3.3332173824310303\n","Epoch 80/2000, Step 45, d_loss: 0.719176709651947, g_loss: 4.414771556854248\n","Epoch 80/2000, Step 46, d_loss: 0.3899914622306824, g_loss: 4.42966890335083\n","Epoch 80/2000, Step 47, d_loss: 0.45057421922683716, g_loss: 3.8515377044677734\n","Epoch 80/2000, Step 48, d_loss: 0.39432841539382935, g_loss: 3.502094268798828\n","Epoch 80/2000, Step 49, d_loss: 0.4561862647533417, g_loss: 3.4214024543762207\n","Epoch 80/2000, Step 50, d_loss: 0.40998581051826477, g_loss: 4.387124061584473\n","Epoch 80/2000, Step 51, d_loss: 0.4161730706691742, g_loss: 3.9018194675445557\n","Epoch 80/2000, Step 52, d_loss: 0.3975926339626312, g_loss: 4.275158405303955\n","Epoch 80/2000, Step 53, d_loss: 0.39204132556915283, g_loss: 3.572228193283081\n","Epoch 80/2000, Step 54, d_loss: 0.4073862135410309, g_loss: 4.063652992248535\n","Epoch 80/2000, Step 55, d_loss: 0.40377283096313477, g_loss: 3.5384182929992676\n","Epoch 80/2000, Step 56, d_loss: 0.38856473565101624, g_loss: 3.585167407989502\n","Epoch 80/2000, Step 57, d_loss: 0.4195259213447571, g_loss: 3.847151517868042\n","Epoch 80/2000, Step 58, d_loss: 0.45110052824020386, g_loss: 3.6536316871643066\n","Epoch 80/2000, Step 59, d_loss: 0.4209960103034973, g_loss: 5.640476703643799\n","Epoch 80/2000, Step 60, d_loss: 0.39049971103668213, g_loss: 3.3610713481903076\n","Epoch 80/2000, Step 61, d_loss: 0.35951152443885803, g_loss: 3.2274394035339355\n","Epoch 80/2000, Step 62, d_loss: 0.3635440766811371, g_loss: 3.9296300411224365\n","Epoch 80/2000, Step 63, d_loss: 0.39679816365242004, g_loss: 3.243605613708496\n","Epoch 80/2000, Step 64, d_loss: 0.36748453974723816, g_loss: 3.0678653717041016\n","Epoch 80/2000, Step 65, d_loss: 0.39781051874160767, g_loss: 5.75888204574585\n","Epoch 80/2000, Step 66, d_loss: 0.4154511094093323, g_loss: 3.273411989212036\n","Epoch 80/2000, Step 67, d_loss: 0.39371970295906067, g_loss: 5.201816082000732\n","Epoch 80/2000, Step 68, d_loss: 0.44841575622558594, g_loss: 3.3442976474761963\n","Epoch 80/2000, Step 69, d_loss: 0.36418452858924866, g_loss: 3.3357160091400146\n","Epoch 80/2000, Step 70, d_loss: 0.4480046331882477, g_loss: 4.107738018035889\n","Epoch 80/2000, Step 71, d_loss: 0.4081776440143585, g_loss: 3.5166666507720947\n","Epoch 80/2000, Step 72, d_loss: 0.38868045806884766, g_loss: 3.8821005821228027\n","Epoch 80/2000, Step 73, d_loss: 0.373831182718277, g_loss: 3.445843458175659\n","Epoch 80/2000, Step 74, d_loss: 0.3479998707771301, g_loss: 3.876854419708252\n","Epoch 80/2000, Step 75, d_loss: 0.49558576941490173, g_loss: 4.0482378005981445\n","Epoch 80/2000, Step 76, d_loss: 0.38216108083724976, g_loss: 2.611300230026245\n","Epoch 80/2000, Step 77, d_loss: 0.4160059690475464, g_loss: 4.131755828857422\n","Epoch 80/2000, Step 78, d_loss: 0.3936230540275574, g_loss: 3.927020788192749\n","Epoch 80/2000, Step 79, d_loss: 0.37646985054016113, g_loss: 4.304905414581299\n","Epoch 80/2000, Step 80, d_loss: 0.3884931206703186, g_loss: 3.565842628479004\n","Epoch 80/2000, Step 81, d_loss: 0.36234337091445923, g_loss: 3.917677879333496\n","Epoch 80/2000, Step 82, d_loss: 0.3911655843257904, g_loss: 3.334885597229004\n","Epoch 80/2000, Step 83, d_loss: 0.41683581471443176, g_loss: 3.3891489505767822\n","Epoch 80/2000, Step 84, d_loss: 0.37322142720222473, g_loss: 2.9050285816192627\n","Epoch 80/2000, Step 85, d_loss: 0.44762253761291504, g_loss: 4.287160873413086\n","Epoch 80/2000, Step 86, d_loss: 0.49011728167533875, g_loss: 3.1723198890686035\n","Epoch 80/2000, Step 87, d_loss: 0.3407377600669861, g_loss: 3.4264206886291504\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 81/2000, Step 1, d_loss: 0.40353450179100037, g_loss: 4.0082478523254395\n","Epoch 81/2000, Step 2, d_loss: 0.39340075850486755, g_loss: 3.379448175430298\n","Epoch 81/2000, Step 3, d_loss: 0.4085468649864197, g_loss: 3.447200059890747\n","Epoch 81/2000, Step 4, d_loss: 0.3828010559082031, g_loss: 3.850940704345703\n","Epoch 81/2000, Step 5, d_loss: 0.3754656910896301, g_loss: 4.746605396270752\n","Epoch 81/2000, Step 6, d_loss: 0.36905500292778015, g_loss: 4.922861576080322\n","Epoch 81/2000, Step 7, d_loss: 0.35085004568099976, g_loss: 3.815842866897583\n","Epoch 81/2000, Step 8, d_loss: 0.3440788686275482, g_loss: 4.102848052978516\n","Epoch 81/2000, Step 9, d_loss: 0.36510926485061646, g_loss: 4.0768351554870605\n","Epoch 81/2000, Step 10, d_loss: 0.34737429022789, g_loss: 3.779766082763672\n","Epoch 81/2000, Step 11, d_loss: 0.3887057304382324, g_loss: 4.009335994720459\n","Epoch 81/2000, Step 12, d_loss: 0.4064725339412689, g_loss: 3.8013899326324463\n","Epoch 81/2000, Step 13, d_loss: 0.4087599217891693, g_loss: 3.5474584102630615\n","Epoch 81/2000, Step 14, d_loss: 0.3764517903327942, g_loss: 5.751460075378418\n","Epoch 81/2000, Step 15, d_loss: 0.3841733932495117, g_loss: 4.687356948852539\n","Epoch 81/2000, Step 16, d_loss: 0.35569676756858826, g_loss: 3.9526584148406982\n","Epoch 81/2000, Step 17, d_loss: 0.3566451668739319, g_loss: 5.1081132888793945\n","Epoch 81/2000, Step 18, d_loss: 0.39193400740623474, g_loss: 4.531558990478516\n","Epoch 81/2000, Step 19, d_loss: 0.36538246273994446, g_loss: 5.490965366363525\n","Epoch 81/2000, Step 20, d_loss: 0.39194241166114807, g_loss: 6.623828887939453\n","Epoch 81/2000, Step 21, d_loss: 0.3713025748729706, g_loss: 3.3896331787109375\n","Epoch 81/2000, Step 22, d_loss: 0.37125107645988464, g_loss: 4.912450790405273\n","Epoch 81/2000, Step 23, d_loss: 0.3818904757499695, g_loss: 4.640198230743408\n","Epoch 81/2000, Step 24, d_loss: 0.40221527218818665, g_loss: 4.219316482543945\n","Epoch 81/2000, Step 25, d_loss: 0.41060012578964233, g_loss: 3.0302557945251465\n","Epoch 81/2000, Step 26, d_loss: 0.37085241079330444, g_loss: 3.220815896987915\n","Epoch 81/2000, Step 27, d_loss: 0.37431374192237854, g_loss: 5.692348003387451\n","Epoch 81/2000, Step 28, d_loss: 0.3902431130409241, g_loss: 3.2376725673675537\n","Epoch 81/2000, Step 29, d_loss: 0.36061909794807434, g_loss: 4.778692245483398\n","Epoch 81/2000, Step 30, d_loss: 0.3658636212348938, g_loss: 3.45666241645813\n","Epoch 81/2000, Step 31, d_loss: 0.3778187036514282, g_loss: 3.801609992980957\n","Epoch 81/2000, Step 32, d_loss: 0.40617096424102783, g_loss: 3.2936530113220215\n","Epoch 81/2000, Step 33, d_loss: 0.3665340840816498, g_loss: 4.416689872741699\n","Epoch 81/2000, Step 34, d_loss: 0.39609384536743164, g_loss: 3.6023426055908203\n","Epoch 81/2000, Step 35, d_loss: 0.3641493022441864, g_loss: 3.3225739002227783\n","Epoch 81/2000, Step 36, d_loss: 0.6864069700241089, g_loss: 3.719456911087036\n","Epoch 81/2000, Step 37, d_loss: 0.34971582889556885, g_loss: 3.734269380569458\n","Epoch 81/2000, Step 38, d_loss: 0.387053519487381, g_loss: 3.789602756500244\n","Epoch 81/2000, Step 39, d_loss: 0.41017264127731323, g_loss: 2.981916904449463\n","Epoch 81/2000, Step 40, d_loss: 0.43310701847076416, g_loss: 3.815793752670288\n","Epoch 81/2000, Step 41, d_loss: 0.43939095735549927, g_loss: 4.297335624694824\n","Epoch 81/2000, Step 42, d_loss: 0.37624233961105347, g_loss: 3.6261396408081055\n","Epoch 81/2000, Step 43, d_loss: 0.3643723130226135, g_loss: 4.479604244232178\n","Epoch 81/2000, Step 44, d_loss: 0.4761703908443451, g_loss: 4.93314266204834\n","Epoch 81/2000, Step 45, d_loss: 0.4636443853378296, g_loss: 4.7567548751831055\n","Epoch 81/2000, Step 46, d_loss: 0.5152087807655334, g_loss: 3.4008519649505615\n","Epoch 81/2000, Step 47, d_loss: 0.38661056756973267, g_loss: 3.3915064334869385\n","Epoch 81/2000, Step 48, d_loss: 0.3915339410305023, g_loss: 2.761451005935669\n","Epoch 81/2000, Step 49, d_loss: 0.44158872961997986, g_loss: 2.5357556343078613\n","Epoch 81/2000, Step 50, d_loss: 0.4676123559474945, g_loss: 2.925518035888672\n","Epoch 81/2000, Step 51, d_loss: 0.49151766300201416, g_loss: 3.9128060340881348\n","Epoch 81/2000, Step 52, d_loss: 0.4577663242816925, g_loss: 3.478242874145508\n","Epoch 81/2000, Step 53, d_loss: 0.4140792787075043, g_loss: 3.9977500438690186\n","Epoch 81/2000, Step 54, d_loss: 0.45973894000053406, g_loss: 5.557307243347168\n","Epoch 81/2000, Step 55, d_loss: 0.46242350339889526, g_loss: 4.465768814086914\n","Epoch 81/2000, Step 56, d_loss: 0.4521733224391937, g_loss: 4.596596717834473\n","Epoch 81/2000, Step 57, d_loss: 0.4162113666534424, g_loss: 4.794594764709473\n","Epoch 81/2000, Step 58, d_loss: 0.48716503381729126, g_loss: 3.902719020843506\n","Epoch 81/2000, Step 59, d_loss: 0.40865015983581543, g_loss: 3.330263614654541\n","Epoch 81/2000, Step 60, d_loss: 0.4762156009674072, g_loss: 2.5716700553894043\n","Epoch 81/2000, Step 61, d_loss: 0.42104464769363403, g_loss: 2.9776439666748047\n","Epoch 81/2000, Step 62, d_loss: 0.4378732144832611, g_loss: 3.511131525039673\n","Epoch 81/2000, Step 63, d_loss: 0.44434112310409546, g_loss: 4.438064098358154\n","Epoch 81/2000, Step 64, d_loss: 0.4199087619781494, g_loss: 4.146031379699707\n","Epoch 81/2000, Step 65, d_loss: 0.424930602312088, g_loss: 2.886317253112793\n","Epoch 81/2000, Step 66, d_loss: 0.3984801173210144, g_loss: 3.3677399158477783\n","Epoch 81/2000, Step 67, d_loss: 0.39251449704170227, g_loss: 6.162126064300537\n","Epoch 81/2000, Step 68, d_loss: 0.4584943354129791, g_loss: 2.871587038040161\n","Epoch 81/2000, Step 69, d_loss: 0.3594757318496704, g_loss: 3.1571500301361084\n","Epoch 81/2000, Step 70, d_loss: 0.4019266963005066, g_loss: 3.207106113433838\n","Epoch 81/2000, Step 71, d_loss: 0.49043792486190796, g_loss: 3.4565823078155518\n","Epoch 81/2000, Step 72, d_loss: 0.4426041543483734, g_loss: 4.855464935302734\n","Epoch 81/2000, Step 73, d_loss: 0.4282718598842621, g_loss: 3.165931224822998\n","Epoch 81/2000, Step 74, d_loss: 0.38218092918395996, g_loss: 3.370084047317505\n","Epoch 81/2000, Step 75, d_loss: 0.34534525871276855, g_loss: 4.470933437347412\n","Epoch 81/2000, Step 76, d_loss: 0.47804054617881775, g_loss: 3.7896130084991455\n","Epoch 81/2000, Step 77, d_loss: 0.5242482423782349, g_loss: 3.688765525817871\n","Epoch 81/2000, Step 78, d_loss: 0.38320305943489075, g_loss: 3.2534821033477783\n","Epoch 81/2000, Step 79, d_loss: 0.41033971309661865, g_loss: 2.6750845909118652\n","Epoch 81/2000, Step 80, d_loss: 0.45247983932495117, g_loss: 3.6217527389526367\n","Epoch 81/2000, Step 81, d_loss: 0.4387715458869934, g_loss: 2.798344373703003\n","Epoch 81/2000, Step 82, d_loss: 0.4026908278465271, g_loss: 3.0975239276885986\n","Epoch 81/2000, Step 83, d_loss: 0.37717515230178833, g_loss: 3.2384369373321533\n","Epoch 81/2000, Step 84, d_loss: 0.42905184626579285, g_loss: 3.3294548988342285\n","Epoch 81/2000, Step 85, d_loss: 0.4694252014160156, g_loss: 6.180051803588867\n","Epoch 81/2000, Step 86, d_loss: 0.3872831463813782, g_loss: 3.7576372623443604\n","Epoch 81/2000, Step 87, d_loss: 0.41507798433303833, g_loss: 3.8062591552734375\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 82/2000, Step 1, d_loss: 0.39601626992225647, g_loss: 3.7226614952087402\n","Epoch 82/2000, Step 2, d_loss: 0.4435883164405823, g_loss: 3.3372902870178223\n","Epoch 82/2000, Step 3, d_loss: 0.45053181052207947, g_loss: 2.8733649253845215\n","Epoch 82/2000, Step 4, d_loss: 0.4756619334220886, g_loss: 2.232863187789917\n","Epoch 82/2000, Step 5, d_loss: 0.4275280237197876, g_loss: 2.911895751953125\n","Epoch 82/2000, Step 6, d_loss: 0.47858375310897827, g_loss: 2.8255341053009033\n","Epoch 82/2000, Step 7, d_loss: 0.4556228518486023, g_loss: 3.641505002975464\n","Epoch 82/2000, Step 8, d_loss: 0.42943212389945984, g_loss: 4.3554229736328125\n","Epoch 82/2000, Step 9, d_loss: 0.44606250524520874, g_loss: 3.6625096797943115\n","Epoch 82/2000, Step 10, d_loss: 0.4432588517665863, g_loss: 4.336877346038818\n","Epoch 82/2000, Step 11, d_loss: 0.40561801195144653, g_loss: 4.2171125411987305\n","Epoch 82/2000, Step 12, d_loss: 0.5379815101623535, g_loss: 4.11603307723999\n","Epoch 82/2000, Step 13, d_loss: 0.43438300490379333, g_loss: 2.9675188064575195\n","Epoch 82/2000, Step 14, d_loss: 0.39716851711273193, g_loss: 2.456495523452759\n","Epoch 82/2000, Step 15, d_loss: 0.5115985870361328, g_loss: 3.5263121128082275\n","Epoch 82/2000, Step 16, d_loss: 0.3990307152271271, g_loss: 3.892735719680786\n","Epoch 82/2000, Step 17, d_loss: 0.48757871985435486, g_loss: 3.2210750579833984\n","Epoch 82/2000, Step 18, d_loss: 0.38914021849632263, g_loss: 4.386646747589111\n","Epoch 82/2000, Step 19, d_loss: 0.3710545003414154, g_loss: 3.323110818862915\n","Epoch 82/2000, Step 20, d_loss: 0.4451594650745392, g_loss: 5.346248626708984\n","Epoch 82/2000, Step 21, d_loss: 0.4471183717250824, g_loss: 4.0885491371154785\n","Epoch 82/2000, Step 22, d_loss: 0.463733047246933, g_loss: 3.589886426925659\n","Epoch 82/2000, Step 23, d_loss: 0.42139363288879395, g_loss: 3.2442827224731445\n","Epoch 82/2000, Step 24, d_loss: 0.3917006552219391, g_loss: 4.972501754760742\n","Epoch 82/2000, Step 25, d_loss: 0.3646354079246521, g_loss: 4.078594207763672\n","Epoch 82/2000, Step 26, d_loss: 0.44656485319137573, g_loss: 4.060329914093018\n","Epoch 82/2000, Step 27, d_loss: 0.4253092110157013, g_loss: 3.3482413291931152\n","Epoch 82/2000, Step 28, d_loss: 0.38563910126686096, g_loss: 3.105246067047119\n","Epoch 82/2000, Step 29, d_loss: 0.3925331234931946, g_loss: 3.320384979248047\n","Epoch 82/2000, Step 30, d_loss: 0.4078659117221832, g_loss: 3.419889211654663\n","Epoch 82/2000, Step 31, d_loss: 0.43504786491394043, g_loss: 3.134674549102783\n","Epoch 82/2000, Step 32, d_loss: 0.47379279136657715, g_loss: 3.864922046661377\n","Epoch 82/2000, Step 33, d_loss: 0.4423918128013611, g_loss: 4.015347480773926\n","Epoch 82/2000, Step 34, d_loss: 0.3749314844608307, g_loss: 4.90111780166626\n","Epoch 82/2000, Step 35, d_loss: 0.42623260617256165, g_loss: 4.9107346534729\n","Epoch 82/2000, Step 36, d_loss: 0.4303961396217346, g_loss: 4.465066909790039\n","Epoch 82/2000, Step 37, d_loss: 0.3683330714702606, g_loss: 4.1104044914245605\n","Epoch 82/2000, Step 38, d_loss: 0.40467649698257446, g_loss: 3.9517459869384766\n","Epoch 82/2000, Step 39, d_loss: 0.382916122674942, g_loss: 4.819326877593994\n","Epoch 82/2000, Step 40, d_loss: 0.4087260663509369, g_loss: 4.273070812225342\n","Epoch 82/2000, Step 41, d_loss: 0.42787155508995056, g_loss: 3.5679686069488525\n","Epoch 82/2000, Step 42, d_loss: 0.4169619679450989, g_loss: 3.380131483078003\n","Epoch 82/2000, Step 43, d_loss: 0.3928958773612976, g_loss: 5.096573829650879\n","Epoch 82/2000, Step 44, d_loss: 0.38826027512550354, g_loss: 4.7275919914245605\n","Epoch 82/2000, Step 45, d_loss: 0.5850185751914978, g_loss: 3.641033411026001\n","Epoch 82/2000, Step 46, d_loss: 0.40122947096824646, g_loss: 4.8350677490234375\n","Epoch 82/2000, Step 47, d_loss: 0.4203852415084839, g_loss: 3.1755917072296143\n","Epoch 82/2000, Step 48, d_loss: 0.3996763825416565, g_loss: 4.502587795257568\n","Epoch 82/2000, Step 49, d_loss: 0.4514167308807373, g_loss: 2.9809205532073975\n","Epoch 82/2000, Step 50, d_loss: 0.4124743938446045, g_loss: 4.5638628005981445\n","Epoch 82/2000, Step 51, d_loss: 0.39419791102409363, g_loss: 3.878756284713745\n","Epoch 82/2000, Step 52, d_loss: 0.437101811170578, g_loss: 4.065793991088867\n","Epoch 82/2000, Step 53, d_loss: 0.39654141664505005, g_loss: 4.185545921325684\n","Epoch 82/2000, Step 54, d_loss: 0.43232640624046326, g_loss: 3.660339117050171\n","Epoch 82/2000, Step 55, d_loss: 0.4402061402797699, g_loss: 2.9848270416259766\n","Epoch 82/2000, Step 56, d_loss: 0.47303158044815063, g_loss: 3.9430348873138428\n","Epoch 82/2000, Step 57, d_loss: 0.41383683681488037, g_loss: 2.6611170768737793\n","Epoch 82/2000, Step 58, d_loss: 0.4008985459804535, g_loss: 3.662780523300171\n","Epoch 82/2000, Step 59, d_loss: 0.41220933198928833, g_loss: 3.156463384628296\n","Epoch 82/2000, Step 60, d_loss: 0.4888238310813904, g_loss: 2.825558662414551\n","Epoch 82/2000, Step 61, d_loss: 0.5029686689376831, g_loss: 2.8487136363983154\n","Epoch 82/2000, Step 62, d_loss: 0.45298001170158386, g_loss: 4.703065872192383\n","Epoch 82/2000, Step 63, d_loss: 0.35367798805236816, g_loss: 3.942796468734741\n","Epoch 82/2000, Step 64, d_loss: 0.41385069489479065, g_loss: 4.077511310577393\n","Epoch 82/2000, Step 65, d_loss: 0.40391892194747925, g_loss: 4.080723762512207\n","Epoch 82/2000, Step 66, d_loss: 0.40959712862968445, g_loss: 4.284941673278809\n","Epoch 82/2000, Step 67, d_loss: 0.419726699590683, g_loss: 4.168900489807129\n","Epoch 82/2000, Step 68, d_loss: 0.43251416087150574, g_loss: 4.3021368980407715\n","Epoch 82/2000, Step 69, d_loss: 0.3775724768638611, g_loss: 4.738199710845947\n","Epoch 82/2000, Step 70, d_loss: 0.4425046443939209, g_loss: 3.7677111625671387\n","Epoch 82/2000, Step 71, d_loss: 0.5949093103408813, g_loss: 3.077028274536133\n","Epoch 82/2000, Step 72, d_loss: 0.38400033116340637, g_loss: 2.702838182449341\n","Epoch 82/2000, Step 73, d_loss: 0.480813592672348, g_loss: 2.849463701248169\n","Epoch 82/2000, Step 74, d_loss: 0.5149449110031128, g_loss: 3.401757001876831\n","Epoch 82/2000, Step 75, d_loss: 0.47247910499572754, g_loss: 2.8767518997192383\n","Epoch 82/2000, Step 76, d_loss: 0.5681321024894714, g_loss: 3.4295027256011963\n","Epoch 82/2000, Step 77, d_loss: 0.411862313747406, g_loss: 4.058148384094238\n","Epoch 82/2000, Step 78, d_loss: 0.36484840512275696, g_loss: 4.645211219787598\n","Epoch 82/2000, Step 79, d_loss: 0.3996243476867676, g_loss: 4.343235015869141\n","Epoch 82/2000, Step 80, d_loss: 0.4613892734050751, g_loss: 3.885965585708618\n","Epoch 82/2000, Step 81, d_loss: 0.6689367890357971, g_loss: 4.169827938079834\n","Epoch 82/2000, Step 82, d_loss: 0.4359450340270996, g_loss: 3.054117441177368\n","Epoch 82/2000, Step 83, d_loss: 0.38450631499290466, g_loss: 4.271684646606445\n","Epoch 82/2000, Step 84, d_loss: 0.412978857755661, g_loss: 2.115281581878662\n","Epoch 82/2000, Step 85, d_loss: 0.6355308890342712, g_loss: 2.9545674324035645\n","Epoch 82/2000, Step 86, d_loss: 0.6669571399688721, g_loss: 3.90972638130188\n","Epoch 82/2000, Step 87, d_loss: 0.44328364729881287, g_loss: 3.806088447570801\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 83/2000, Step 1, d_loss: 0.5693037509918213, g_loss: 4.843902587890625\n","Epoch 83/2000, Step 2, d_loss: 0.3917313814163208, g_loss: 5.988770484924316\n","Epoch 83/2000, Step 3, d_loss: 0.44486382603645325, g_loss: 3.9852077960968018\n","Epoch 83/2000, Step 4, d_loss: 0.5136643052101135, g_loss: 4.658304214477539\n","Epoch 83/2000, Step 5, d_loss: 0.5439968109130859, g_loss: 3.140213966369629\n","Epoch 83/2000, Step 6, d_loss: 0.43087834119796753, g_loss: 3.153139352798462\n","Epoch 83/2000, Step 7, d_loss: 0.40027523040771484, g_loss: 2.6054797172546387\n","Epoch 83/2000, Step 8, d_loss: 0.4090922176837921, g_loss: 2.402639150619507\n","Epoch 83/2000, Step 9, d_loss: 0.5383791923522949, g_loss: 2.8097786903381348\n","Epoch 83/2000, Step 10, d_loss: 0.4977966845035553, g_loss: 3.923311233520508\n","Epoch 83/2000, Step 11, d_loss: 0.424567848443985, g_loss: 3.142425060272217\n","Epoch 83/2000, Step 12, d_loss: 0.48695826530456543, g_loss: 3.9885976314544678\n","Epoch 83/2000, Step 13, d_loss: 0.4236053228378296, g_loss: 5.580721378326416\n","Epoch 83/2000, Step 14, d_loss: 0.4488331377506256, g_loss: 3.6563925743103027\n","Epoch 83/2000, Step 15, d_loss: 0.5136550068855286, g_loss: 5.1711649894714355\n","Epoch 83/2000, Step 16, d_loss: 0.4982639253139496, g_loss: 4.157195091247559\n","Epoch 83/2000, Step 17, d_loss: 0.3913154602050781, g_loss: 3.5350193977355957\n","Epoch 83/2000, Step 18, d_loss: 0.41936686635017395, g_loss: 5.397129058837891\n","Epoch 83/2000, Step 19, d_loss: 0.450786828994751, g_loss: 2.2809393405914307\n","Epoch 83/2000, Step 20, d_loss: 0.5578831434249878, g_loss: 2.2770628929138184\n","Epoch 83/2000, Step 21, d_loss: 0.5219032168388367, g_loss: 2.9671316146850586\n","Epoch 83/2000, Step 22, d_loss: 0.4314846694469452, g_loss: 3.776743173599243\n","Epoch 83/2000, Step 23, d_loss: 0.3670139014720917, g_loss: 5.225551605224609\n","Epoch 83/2000, Step 24, d_loss: 0.42916014790534973, g_loss: 3.812551259994507\n","Epoch 83/2000, Step 25, d_loss: 0.3764221668243408, g_loss: 4.209553241729736\n","Epoch 83/2000, Step 26, d_loss: 0.6454381942749023, g_loss: 4.021989345550537\n","Epoch 83/2000, Step 27, d_loss: 0.4357836842536926, g_loss: 2.744293451309204\n","Epoch 83/2000, Step 28, d_loss: 0.38607457280158997, g_loss: 2.6266868114471436\n","Epoch 83/2000, Step 29, d_loss: 0.4876794219017029, g_loss: 2.229297637939453\n","Epoch 83/2000, Step 30, d_loss: 0.45724159479141235, g_loss: 2.3138434886932373\n","Epoch 83/2000, Step 31, d_loss: 0.4944121539592743, g_loss: 2.1740756034851074\n","Epoch 83/2000, Step 32, d_loss: 0.4093174338340759, g_loss: 3.9053149223327637\n","Epoch 83/2000, Step 33, d_loss: 0.4132048785686493, g_loss: 4.793773174285889\n","Epoch 83/2000, Step 34, d_loss: 0.37580448389053345, g_loss: 3.899885654449463\n","Epoch 83/2000, Step 35, d_loss: 0.44416335225105286, g_loss: 4.729140758514404\n","Epoch 83/2000, Step 36, d_loss: 0.37009134888648987, g_loss: 4.792783737182617\n","Epoch 83/2000, Step 37, d_loss: 0.5190044641494751, g_loss: 5.0214619636535645\n","Epoch 83/2000, Step 38, d_loss: 0.3886236846446991, g_loss: 4.347968578338623\n","Epoch 83/2000, Step 39, d_loss: 0.3713427782058716, g_loss: 4.595573425292969\n","Epoch 83/2000, Step 40, d_loss: 0.4839060604572296, g_loss: 2.696910858154297\n","Epoch 83/2000, Step 41, d_loss: 0.42015892267227173, g_loss: 5.482651233673096\n","Epoch 83/2000, Step 42, d_loss: 0.4603007137775421, g_loss: 2.384338855743408\n","Epoch 83/2000, Step 43, d_loss: 0.3987412452697754, g_loss: 4.265202522277832\n","Epoch 83/2000, Step 44, d_loss: 0.39835241436958313, g_loss: 4.021393775939941\n","Epoch 83/2000, Step 45, d_loss: 0.4589824378490448, g_loss: 4.14460563659668\n","Epoch 83/2000, Step 46, d_loss: 0.4153902232646942, g_loss: 2.789135217666626\n","Epoch 83/2000, Step 47, d_loss: 0.4368278384208679, g_loss: 3.727588176727295\n","Epoch 83/2000, Step 48, d_loss: 0.4509468972682953, g_loss: 3.766859292984009\n","Epoch 83/2000, Step 49, d_loss: 0.5694104433059692, g_loss: 4.0644755363464355\n","Epoch 83/2000, Step 50, d_loss: 0.4527207314968109, g_loss: 2.979328155517578\n","Epoch 83/2000, Step 51, d_loss: 0.4254000186920166, g_loss: 2.8345446586608887\n","Epoch 83/2000, Step 52, d_loss: 0.3981940448284149, g_loss: 3.799497127532959\n","Epoch 83/2000, Step 53, d_loss: 0.41304415464401245, g_loss: 4.611539363861084\n","Epoch 83/2000, Step 54, d_loss: 0.5330110788345337, g_loss: 4.889333724975586\n","Epoch 83/2000, Step 55, d_loss: 0.3645147681236267, g_loss: 6.536631107330322\n","Epoch 83/2000, Step 56, d_loss: 0.441020131111145, g_loss: 3.6762197017669678\n","Epoch 83/2000, Step 57, d_loss: 0.47520139813423157, g_loss: 3.271656036376953\n","Epoch 83/2000, Step 58, d_loss: 0.5226442217826843, g_loss: 3.007045269012451\n","Epoch 83/2000, Step 59, d_loss: 0.4056011438369751, g_loss: 3.3501410484313965\n","Epoch 83/2000, Step 60, d_loss: 0.44282475113868713, g_loss: 3.7692484855651855\n","Epoch 83/2000, Step 61, d_loss: 0.5677769780158997, g_loss: 2.4664154052734375\n","Epoch 83/2000, Step 62, d_loss: 0.5565469861030579, g_loss: 3.513456106185913\n","Epoch 83/2000, Step 63, d_loss: 0.4899386763572693, g_loss: 3.6086935997009277\n","Epoch 83/2000, Step 64, d_loss: 0.40755191445350647, g_loss: 3.884159803390503\n","Epoch 83/2000, Step 65, d_loss: 0.3944739103317261, g_loss: 4.181443214416504\n","Epoch 83/2000, Step 66, d_loss: 0.4802524447441101, g_loss: 4.674178600311279\n","Epoch 83/2000, Step 67, d_loss: 0.3957985043525696, g_loss: 5.122995376586914\n","Epoch 83/2000, Step 68, d_loss: 0.394055038690567, g_loss: 4.034889221191406\n","Epoch 83/2000, Step 69, d_loss: 0.5578019022941589, g_loss: 3.662872552871704\n","Epoch 83/2000, Step 70, d_loss: 0.42549851536750793, g_loss: 2.4785335063934326\n","Epoch 83/2000, Step 71, d_loss: 0.40304282307624817, g_loss: 2.926942825317383\n","Epoch 83/2000, Step 72, d_loss: 0.39040642976760864, g_loss: 2.1267690658569336\n","Epoch 83/2000, Step 73, d_loss: 0.49051403999328613, g_loss: 3.0490851402282715\n","Epoch 83/2000, Step 74, d_loss: 0.413092702627182, g_loss: 3.8835608959198\n","Epoch 83/2000, Step 75, d_loss: 0.36904042959213257, g_loss: 3.289914846420288\n","Epoch 83/2000, Step 76, d_loss: 0.3744836449623108, g_loss: 2.950673818588257\n","Epoch 83/2000, Step 77, d_loss: 0.3513791859149933, g_loss: 4.588814735412598\n","Epoch 83/2000, Step 78, d_loss: 0.41354385018348694, g_loss: 3.8340799808502197\n","Epoch 83/2000, Step 79, d_loss: 0.45371580123901367, g_loss: 4.360347747802734\n","Epoch 83/2000, Step 80, d_loss: 0.37953802943229675, g_loss: 3.283806324005127\n","Epoch 83/2000, Step 81, d_loss: 0.4074152112007141, g_loss: 3.8152334690093994\n","Epoch 83/2000, Step 82, d_loss: 0.36646437644958496, g_loss: 3.0088274478912354\n","Epoch 83/2000, Step 83, d_loss: 0.4713466167449951, g_loss: 3.190620183944702\n","Epoch 83/2000, Step 84, d_loss: 0.4194299280643463, g_loss: 3.3981847763061523\n","Epoch 83/2000, Step 85, d_loss: 0.40006741881370544, g_loss: 3.69146990776062\n","Epoch 83/2000, Step 86, d_loss: 0.37051713466644287, g_loss: 3.30257248878479\n","Epoch 83/2000, Step 87, d_loss: 0.3670225143432617, g_loss: 4.845374584197998\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 84/2000, Step 1, d_loss: 0.4064396917819977, g_loss: 3.3620824813842773\n","Epoch 84/2000, Step 2, d_loss: 0.36712345480918884, g_loss: 3.601771831512451\n","Epoch 84/2000, Step 3, d_loss: 0.3629276156425476, g_loss: 3.912400484085083\n","Epoch 84/2000, Step 4, d_loss: 0.39719662070274353, g_loss: 3.023164749145508\n","Epoch 84/2000, Step 5, d_loss: 0.36606714129447937, g_loss: 4.1059041023254395\n","Epoch 84/2000, Step 6, d_loss: 0.4506247043609619, g_loss: 3.2238776683807373\n","Epoch 84/2000, Step 7, d_loss: 0.37667953968048096, g_loss: 4.047408103942871\n","Epoch 84/2000, Step 8, d_loss: 0.3839436173439026, g_loss: 3.6212432384490967\n","Epoch 84/2000, Step 9, d_loss: 0.42588579654693604, g_loss: 2.8919191360473633\n","Epoch 84/2000, Step 10, d_loss: 0.3939729928970337, g_loss: 3.9793734550476074\n","Epoch 84/2000, Step 11, d_loss: 0.35747700929641724, g_loss: 4.138956546783447\n","Epoch 84/2000, Step 12, d_loss: 0.35936784744262695, g_loss: 3.6764771938323975\n","Epoch 84/2000, Step 13, d_loss: 0.38691285252571106, g_loss: 3.9459517002105713\n","Epoch 84/2000, Step 14, d_loss: 0.3693235218524933, g_loss: 4.247633934020996\n","Epoch 84/2000, Step 15, d_loss: 0.377852201461792, g_loss: 3.4575884342193604\n","Epoch 84/2000, Step 16, d_loss: 0.4214461147785187, g_loss: 5.019779682159424\n","Epoch 84/2000, Step 17, d_loss: 0.3633098006248474, g_loss: 3.696720600128174\n","Epoch 84/2000, Step 18, d_loss: 0.3754895031452179, g_loss: 3.5075490474700928\n","Epoch 84/2000, Step 19, d_loss: 0.3768226206302643, g_loss: 4.092662334442139\n","Epoch 84/2000, Step 20, d_loss: 0.3879924416542053, g_loss: 3.489079475402832\n","Epoch 84/2000, Step 21, d_loss: 0.3566606044769287, g_loss: 3.9426825046539307\n","Epoch 84/2000, Step 22, d_loss: 0.3921147882938385, g_loss: 3.811304807662964\n","Epoch 84/2000, Step 23, d_loss: 0.39365899562835693, g_loss: 5.378597259521484\n","Epoch 84/2000, Step 24, d_loss: 0.38180065155029297, g_loss: 3.756176471710205\n","Epoch 84/2000, Step 25, d_loss: 0.3968547582626343, g_loss: 4.25858736038208\n","Epoch 84/2000, Step 26, d_loss: 0.39795878529548645, g_loss: 4.291379928588867\n","Epoch 84/2000, Step 27, d_loss: 0.3881535530090332, g_loss: 4.088400363922119\n","Epoch 84/2000, Step 28, d_loss: 0.35188210010528564, g_loss: 4.736781120300293\n","Epoch 84/2000, Step 29, d_loss: 0.39668697118759155, g_loss: 4.656200408935547\n","Epoch 84/2000, Step 30, d_loss: 0.3737105429172516, g_loss: 4.389162063598633\n","Epoch 84/2000, Step 31, d_loss: 0.4150253236293793, g_loss: 3.6208367347717285\n","Epoch 84/2000, Step 32, d_loss: 0.3934199810028076, g_loss: 4.520501613616943\n","Epoch 84/2000, Step 33, d_loss: 0.3739803433418274, g_loss: 3.26737642288208\n","Epoch 84/2000, Step 34, d_loss: 0.3838074207305908, g_loss: 3.3874640464782715\n","Epoch 84/2000, Step 35, d_loss: 0.43457475304603577, g_loss: 2.3324179649353027\n","Epoch 84/2000, Step 36, d_loss: 0.40821269154548645, g_loss: 3.479689121246338\n","Epoch 84/2000, Step 37, d_loss: 0.4223712086677551, g_loss: 3.7349703311920166\n","Epoch 84/2000, Step 38, d_loss: 0.4143819510936737, g_loss: 3.103950262069702\n","Epoch 84/2000, Step 39, d_loss: 0.46556559205055237, g_loss: 4.409564971923828\n","Epoch 84/2000, Step 40, d_loss: 0.4094098210334778, g_loss: 5.523772239685059\n","Epoch 84/2000, Step 41, d_loss: 0.4925886392593384, g_loss: 4.329890727996826\n","Epoch 84/2000, Step 42, d_loss: 0.4642046391963959, g_loss: 4.192336082458496\n","Epoch 84/2000, Step 43, d_loss: 0.455539733171463, g_loss: 4.284892559051514\n","Epoch 84/2000, Step 44, d_loss: 0.4222850501537323, g_loss: 2.964672565460205\n","Epoch 84/2000, Step 45, d_loss: 0.4765739440917969, g_loss: 3.2489326000213623\n","Epoch 84/2000, Step 46, d_loss: 0.500934898853302, g_loss: 2.426546335220337\n","Epoch 84/2000, Step 47, d_loss: 0.4737488329410553, g_loss: 2.7952189445495605\n","Epoch 84/2000, Step 48, d_loss: 0.4005242586135864, g_loss: 2.906421184539795\n","Epoch 84/2000, Step 49, d_loss: 0.47761741280555725, g_loss: 3.1308059692382812\n","Epoch 84/2000, Step 50, d_loss: 0.3758007884025574, g_loss: 4.7478179931640625\n","Epoch 84/2000, Step 51, d_loss: 0.37751322984695435, g_loss: 6.349009990692139\n","Epoch 84/2000, Step 52, d_loss: 0.43920886516571045, g_loss: 5.039661407470703\n","Epoch 84/2000, Step 53, d_loss: 0.6100972890853882, g_loss: 4.348801136016846\n","Epoch 84/2000, Step 54, d_loss: 0.41677889227867126, g_loss: 4.588038921356201\n","Epoch 84/2000, Step 55, d_loss: 0.5450707674026489, g_loss: 2.9771623611450195\n","Epoch 84/2000, Step 56, d_loss: 0.4449783265590668, g_loss: 2.118928909301758\n","Epoch 84/2000, Step 57, d_loss: 0.46808069944381714, g_loss: 3.0173044204711914\n","Epoch 84/2000, Step 58, d_loss: 0.5180152654647827, g_loss: 3.105836868286133\n","Epoch 84/2000, Step 59, d_loss: 0.45310646295547485, g_loss: 5.678652286529541\n","Epoch 84/2000, Step 60, d_loss: 0.5344473123550415, g_loss: 4.641904354095459\n","Epoch 84/2000, Step 61, d_loss: 0.4132714867591858, g_loss: 3.0841121673583984\n","Epoch 84/2000, Step 62, d_loss: 0.4314603805541992, g_loss: 2.4111955165863037\n","Epoch 84/2000, Step 63, d_loss: 0.4506871998310089, g_loss: 3.0642201900482178\n","Epoch 84/2000, Step 64, d_loss: 0.3952634632587433, g_loss: 2.3928167819976807\n","Epoch 84/2000, Step 65, d_loss: 0.43237045407295227, g_loss: 3.673858880996704\n","Epoch 84/2000, Step 66, d_loss: 0.3995932340621948, g_loss: 4.555789470672607\n","Epoch 84/2000, Step 67, d_loss: 0.44032612442970276, g_loss: 4.750240325927734\n","Epoch 84/2000, Step 68, d_loss: 0.4187168478965759, g_loss: 3.8795080184936523\n","Epoch 84/2000, Step 69, d_loss: 0.4331901967525482, g_loss: 4.140129566192627\n","Epoch 84/2000, Step 70, d_loss: 0.38576802611351013, g_loss: 3.0402159690856934\n","Epoch 84/2000, Step 71, d_loss: 0.5115863084793091, g_loss: 2.0911216735839844\n","Epoch 84/2000, Step 72, d_loss: 0.4167884588241577, g_loss: 4.091267108917236\n","Epoch 84/2000, Step 73, d_loss: 0.40705037117004395, g_loss: 2.8649072647094727\n","Epoch 84/2000, Step 74, d_loss: 0.42056789994239807, g_loss: 3.2234065532684326\n","Epoch 84/2000, Step 75, d_loss: 0.39584076404571533, g_loss: 4.425398826599121\n","Epoch 84/2000, Step 76, d_loss: 0.42621204257011414, g_loss: 3.8899929523468018\n","Epoch 84/2000, Step 77, d_loss: 0.39106297492980957, g_loss: 3.78468918800354\n","Epoch 84/2000, Step 78, d_loss: 0.4404946267604828, g_loss: 3.8263471126556396\n","Epoch 84/2000, Step 79, d_loss: 0.38043826818466187, g_loss: 5.252948760986328\n","Epoch 84/2000, Step 80, d_loss: 0.4243271052837372, g_loss: 3.605339527130127\n","Epoch 84/2000, Step 81, d_loss: 0.46539679169654846, g_loss: 2.504023551940918\n","Epoch 84/2000, Step 82, d_loss: 0.5030593872070312, g_loss: 1.9408586025238037\n","Epoch 84/2000, Step 83, d_loss: 0.48544877767562866, g_loss: 3.511369466781616\n","Epoch 84/2000, Step 84, d_loss: 0.624695897102356, g_loss: 3.728386878967285\n","Epoch 84/2000, Step 85, d_loss: 0.4542093276977539, g_loss: 4.314398765563965\n","Epoch 84/2000, Step 86, d_loss: 0.4201452136039734, g_loss: 4.549891948699951\n","Epoch 84/2000, Step 87, d_loss: 0.49104321002960205, g_loss: 4.673521041870117\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 85/2000, Step 1, d_loss: 0.4447782635688782, g_loss: 4.894697666168213\n","Epoch 85/2000, Step 2, d_loss: 0.39056411385536194, g_loss: 2.8439135551452637\n","Epoch 85/2000, Step 3, d_loss: 0.4801250100135803, g_loss: 2.308861494064331\n","Epoch 85/2000, Step 4, d_loss: 0.48864153027534485, g_loss: 1.6649690866470337\n","Epoch 85/2000, Step 5, d_loss: 0.544949471950531, g_loss: 3.325623035430908\n","Epoch 85/2000, Step 6, d_loss: 0.45504435896873474, g_loss: 3.318765163421631\n","Epoch 85/2000, Step 7, d_loss: 0.40427738428115845, g_loss: 3.494349956512451\n","Epoch 85/2000, Step 8, d_loss: 0.4497630298137665, g_loss: 3.2439613342285156\n","Epoch 85/2000, Step 9, d_loss: 0.5305945873260498, g_loss: 3.7316627502441406\n","Epoch 85/2000, Step 10, d_loss: 0.36867377161979675, g_loss: 4.218904495239258\n","Epoch 85/2000, Step 11, d_loss: 0.40825027227401733, g_loss: 4.006277084350586\n","Epoch 85/2000, Step 12, d_loss: 0.37102437019348145, g_loss: 3.0403029918670654\n","Epoch 85/2000, Step 13, d_loss: 0.4030815362930298, g_loss: 3.937838077545166\n","Epoch 85/2000, Step 14, d_loss: 0.48956581950187683, g_loss: 4.447763919830322\n","Epoch 85/2000, Step 15, d_loss: 0.39212357997894287, g_loss: 3.489661455154419\n","Epoch 85/2000, Step 16, d_loss: 0.4161348044872284, g_loss: 3.6008291244506836\n","Epoch 85/2000, Step 17, d_loss: 0.49202433228492737, g_loss: 2.6326563358306885\n","Epoch 85/2000, Step 18, d_loss: 0.5069746971130371, g_loss: 3.014941453933716\n","Epoch 85/2000, Step 19, d_loss: 0.4589940309524536, g_loss: 4.564332485198975\n","Epoch 85/2000, Step 20, d_loss: 0.3887844681739807, g_loss: 3.5639688968658447\n","Epoch 85/2000, Step 21, d_loss: 0.3923802971839905, g_loss: 4.6073198318481445\n","Epoch 85/2000, Step 22, d_loss: 0.3832142651081085, g_loss: 5.170185565948486\n","Epoch 85/2000, Step 23, d_loss: 0.5293420553207397, g_loss: 3.793508768081665\n","Epoch 85/2000, Step 24, d_loss: 0.3625481426715851, g_loss: 4.4631667137146\n","Epoch 85/2000, Step 25, d_loss: 0.4274998903274536, g_loss: 3.2987403869628906\n","Epoch 85/2000, Step 26, d_loss: 0.4049811363220215, g_loss: 3.7504613399505615\n","Epoch 85/2000, Step 27, d_loss: 0.43449750542640686, g_loss: 3.502047538757324\n","Epoch 85/2000, Step 28, d_loss: 0.421988844871521, g_loss: 2.618333578109741\n","Epoch 85/2000, Step 29, d_loss: 0.41238653659820557, g_loss: 3.3898720741271973\n","Epoch 85/2000, Step 30, d_loss: 0.43103188276290894, g_loss: 3.922407865524292\n","Epoch 85/2000, Step 31, d_loss: 0.48310577869415283, g_loss: 3.5361273288726807\n","Epoch 85/2000, Step 32, d_loss: 0.399863064289093, g_loss: 4.895533084869385\n","Epoch 85/2000, Step 33, d_loss: 0.3694552481174469, g_loss: 4.1896162033081055\n","Epoch 85/2000, Step 34, d_loss: 0.3780370056629181, g_loss: 3.624382495880127\n","Epoch 85/2000, Step 35, d_loss: 0.4146079421043396, g_loss: 5.4864501953125\n","Epoch 85/2000, Step 36, d_loss: 0.41716453433036804, g_loss: 4.513408184051514\n","Epoch 85/2000, Step 37, d_loss: 0.39295876026153564, g_loss: 3.5458900928497314\n","Epoch 85/2000, Step 38, d_loss: 0.3596818745136261, g_loss: 2.999868869781494\n","Epoch 85/2000, Step 39, d_loss: 0.3995437026023865, g_loss: 3.7358551025390625\n","Epoch 85/2000, Step 40, d_loss: 0.4409022629261017, g_loss: 3.33508038520813\n","Epoch 85/2000, Step 41, d_loss: 0.38057178258895874, g_loss: 2.803229570388794\n","Epoch 85/2000, Step 42, d_loss: 0.4233250319957733, g_loss: 4.199341773986816\n","Epoch 85/2000, Step 43, d_loss: 0.4426952004432678, g_loss: 4.394046783447266\n","Epoch 85/2000, Step 44, d_loss: 0.4141673147678375, g_loss: 4.62885046005249\n","Epoch 85/2000, Step 45, d_loss: 0.4234786629676819, g_loss: 3.77285099029541\n","Epoch 85/2000, Step 46, d_loss: 0.472218781709671, g_loss: 4.189117908477783\n","Epoch 85/2000, Step 47, d_loss: 0.513611376285553, g_loss: 5.617607593536377\n","Epoch 85/2000, Step 48, d_loss: 0.3774057924747467, g_loss: 3.8044395446777344\n","Epoch 85/2000, Step 49, d_loss: 0.3748379349708557, g_loss: 2.69087815284729\n","Epoch 85/2000, Step 50, d_loss: 0.41681304574012756, g_loss: 2.819397211074829\n","Epoch 85/2000, Step 51, d_loss: 0.4162417948246002, g_loss: 3.303601026535034\n","Epoch 85/2000, Step 52, d_loss: 0.36342206597328186, g_loss: 3.6788330078125\n","Epoch 85/2000, Step 53, d_loss: 0.37986698746681213, g_loss: 5.506375789642334\n","Epoch 85/2000, Step 54, d_loss: 0.37998121976852417, g_loss: 4.0057477951049805\n","Epoch 85/2000, Step 55, d_loss: 0.3572721779346466, g_loss: 3.922525405883789\n","Epoch 85/2000, Step 56, d_loss: 0.3679954707622528, g_loss: 2.809359312057495\n","Epoch 85/2000, Step 57, d_loss: 0.4587797224521637, g_loss: 5.2847161293029785\n","Epoch 85/2000, Step 58, d_loss: 0.3831898272037506, g_loss: 4.829043865203857\n","Epoch 85/2000, Step 59, d_loss: 0.40688976645469666, g_loss: 3.1683809757232666\n","Epoch 85/2000, Step 60, d_loss: 0.39759013056755066, g_loss: 3.34765887260437\n","Epoch 85/2000, Step 61, d_loss: 0.373489648103714, g_loss: 4.059174060821533\n","Epoch 85/2000, Step 62, d_loss: 0.40983492136001587, g_loss: 3.7286643981933594\n","Epoch 85/2000, Step 63, d_loss: 0.407442182302475, g_loss: 3.7854623794555664\n","Epoch 85/2000, Step 64, d_loss: 0.3813961446285248, g_loss: 4.6621012687683105\n","Epoch 85/2000, Step 65, d_loss: 0.39290571212768555, g_loss: 3.51096510887146\n","Epoch 85/2000, Step 66, d_loss: 0.37810850143432617, g_loss: 3.759492874145508\n","Epoch 85/2000, Step 67, d_loss: 0.3753306269645691, g_loss: 4.599891185760498\n","Epoch 85/2000, Step 68, d_loss: 0.3639593720436096, g_loss: 4.512792110443115\n","Epoch 85/2000, Step 69, d_loss: 0.39226895570755005, g_loss: 3.3393959999084473\n","Epoch 85/2000, Step 70, d_loss: 0.3754754066467285, g_loss: 3.4474329948425293\n","Epoch 85/2000, Step 71, d_loss: 0.3876047432422638, g_loss: 2.7417521476745605\n","Epoch 85/2000, Step 72, d_loss: 0.390910804271698, g_loss: 3.4008688926696777\n","Epoch 85/2000, Step 73, d_loss: 0.3630823493003845, g_loss: 4.103209495544434\n","Epoch 85/2000, Step 74, d_loss: 0.34487852454185486, g_loss: 2.974782705307007\n","Epoch 85/2000, Step 75, d_loss: 0.4117741286754608, g_loss: 4.357468128204346\n","Epoch 85/2000, Step 76, d_loss: 0.4567190408706665, g_loss: 4.113153457641602\n","Epoch 85/2000, Step 77, d_loss: 0.3599240183830261, g_loss: 3.1137521266937256\n","Epoch 85/2000, Step 78, d_loss: 0.36136019229888916, g_loss: 3.9337337017059326\n","Epoch 85/2000, Step 79, d_loss: 0.4111882448196411, g_loss: 3.7413530349731445\n","Epoch 85/2000, Step 80, d_loss: 0.3711441159248352, g_loss: 4.451157569885254\n","Epoch 85/2000, Step 81, d_loss: 0.3774528503417969, g_loss: 4.606462001800537\n","Epoch 85/2000, Step 82, d_loss: 0.36954575777053833, g_loss: 4.4749345779418945\n","Epoch 85/2000, Step 83, d_loss: 0.36930084228515625, g_loss: 4.876313209533691\n","Epoch 85/2000, Step 84, d_loss: 0.37957295775413513, g_loss: 3.6845643520355225\n","Epoch 85/2000, Step 85, d_loss: 0.4552183747291565, g_loss: 5.051203727722168\n","Epoch 85/2000, Step 86, d_loss: 0.383883535861969, g_loss: 4.027861595153809\n","Epoch 85/2000, Step 87, d_loss: 0.3475036323070526, g_loss: 3.7428669929504395\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 86/2000, Step 1, d_loss: 0.3783765733242035, g_loss: 3.007596254348755\n","Epoch 86/2000, Step 2, d_loss: 0.4045516550540924, g_loss: 3.5401451587677\n","Epoch 86/2000, Step 3, d_loss: 0.3566085994243622, g_loss: 4.497162342071533\n","Epoch 86/2000, Step 4, d_loss: 0.3564852476119995, g_loss: 3.76328444480896\n","Epoch 86/2000, Step 5, d_loss: 0.3545653522014618, g_loss: 3.7766165733337402\n","Epoch 86/2000, Step 6, d_loss: 0.3854198157787323, g_loss: 3.308070182800293\n","Epoch 86/2000, Step 7, d_loss: 0.37937524914741516, g_loss: 4.6845808029174805\n","Epoch 86/2000, Step 8, d_loss: 0.3817821145057678, g_loss: 5.671792984008789\n","Epoch 86/2000, Step 9, d_loss: 0.3744686543941498, g_loss: 4.462430953979492\n","Epoch 86/2000, Step 10, d_loss: 0.3629586696624756, g_loss: 3.63844895362854\n","Epoch 86/2000, Step 11, d_loss: 0.37133440375328064, g_loss: 4.238105773925781\n","Epoch 86/2000, Step 12, d_loss: 0.38496512174606323, g_loss: 2.6010029315948486\n","Epoch 86/2000, Step 13, d_loss: 0.38204875588417053, g_loss: 4.845548152923584\n","Epoch 86/2000, Step 14, d_loss: 0.3843255639076233, g_loss: 2.0066702365875244\n","Epoch 86/2000, Step 15, d_loss: 0.44550201296806335, g_loss: 4.210759162902832\n","Epoch 86/2000, Step 16, d_loss: 0.3703121840953827, g_loss: 3.921231508255005\n","Epoch 86/2000, Step 17, d_loss: 0.3769124746322632, g_loss: 3.816643238067627\n","Epoch 86/2000, Step 18, d_loss: 0.3572646975517273, g_loss: 3.7419490814208984\n","Epoch 86/2000, Step 19, d_loss: 0.4330858588218689, g_loss: 5.192502498626709\n","Epoch 86/2000, Step 20, d_loss: 0.4556601643562317, g_loss: 3.8190524578094482\n","Epoch 86/2000, Step 21, d_loss: 0.4011532962322235, g_loss: 4.763974666595459\n","Epoch 86/2000, Step 22, d_loss: 0.3734397888183594, g_loss: 4.006871223449707\n","Epoch 86/2000, Step 23, d_loss: 0.36269012093544006, g_loss: 3.7830350399017334\n","Epoch 86/2000, Step 24, d_loss: 0.37774956226348877, g_loss: 5.548834323883057\n","Epoch 86/2000, Step 25, d_loss: 0.3650279939174652, g_loss: 2.20346999168396\n","Epoch 86/2000, Step 26, d_loss: 0.3758944869041443, g_loss: 3.1610829830169678\n","Epoch 86/2000, Step 27, d_loss: 0.3749169111251831, g_loss: 3.3780734539031982\n","Epoch 86/2000, Step 28, d_loss: 0.3482397496700287, g_loss: 3.409817934036255\n","Epoch 86/2000, Step 29, d_loss: 0.3886398375034332, g_loss: 3.980579137802124\n","Epoch 86/2000, Step 30, d_loss: 0.3593096137046814, g_loss: 3.4006543159484863\n","Epoch 86/2000, Step 31, d_loss: 0.3631324768066406, g_loss: 3.4084720611572266\n","Epoch 86/2000, Step 32, d_loss: 0.3709001839160919, g_loss: 3.2632482051849365\n","Epoch 86/2000, Step 33, d_loss: 0.3763170540332794, g_loss: 3.2009170055389404\n","Epoch 86/2000, Step 34, d_loss: 0.3559185266494751, g_loss: 2.546534299850464\n","Epoch 86/2000, Step 35, d_loss: 0.3767027258872986, g_loss: 3.8855631351470947\n","Epoch 86/2000, Step 36, d_loss: 0.3666848838329315, g_loss: 4.101325035095215\n","Epoch 86/2000, Step 37, d_loss: 0.3459213972091675, g_loss: 3.701570510864258\n","Epoch 86/2000, Step 38, d_loss: 0.3532208800315857, g_loss: 4.800144195556641\n","Epoch 86/2000, Step 39, d_loss: 0.4246446490287781, g_loss: 3.6582581996917725\n","Epoch 86/2000, Step 40, d_loss: 0.3584592342376709, g_loss: 4.058653354644775\n","Epoch 86/2000, Step 41, d_loss: 0.36997678875923157, g_loss: 3.378633499145508\n","Epoch 86/2000, Step 42, d_loss: 0.3791745603084564, g_loss: 3.388841152191162\n","Epoch 86/2000, Step 43, d_loss: 0.3800618052482605, g_loss: 4.356060028076172\n","Epoch 86/2000, Step 44, d_loss: 0.4079580307006836, g_loss: 3.5206737518310547\n","Epoch 86/2000, Step 45, d_loss: 0.3994017243385315, g_loss: 4.4334492683410645\n","Epoch 86/2000, Step 46, d_loss: 0.3748827874660492, g_loss: 4.584049224853516\n","Epoch 86/2000, Step 47, d_loss: 0.43738988041877747, g_loss: 4.135653018951416\n","Epoch 86/2000, Step 48, d_loss: 0.3749123811721802, g_loss: 3.3092570304870605\n","Epoch 86/2000, Step 49, d_loss: 0.369932621717453, g_loss: 4.123699188232422\n","Epoch 86/2000, Step 50, d_loss: 0.36771804094314575, g_loss: 3.126206874847412\n","Epoch 86/2000, Step 51, d_loss: 0.4146427810192108, g_loss: 3.9079091548919678\n","Epoch 86/2000, Step 52, d_loss: 0.3687398135662079, g_loss: 4.304162979125977\n","Epoch 86/2000, Step 53, d_loss: 0.3677760064601898, g_loss: 3.4062938690185547\n","Epoch 86/2000, Step 54, d_loss: 0.4156867265701294, g_loss: 2.701098918914795\n","Epoch 86/2000, Step 55, d_loss: 0.34961646795272827, g_loss: 4.745455741882324\n","Epoch 86/2000, Step 56, d_loss: 0.36391109228134155, g_loss: 4.725839614868164\n","Epoch 86/2000, Step 57, d_loss: 0.42102938890457153, g_loss: 4.651933670043945\n","Epoch 86/2000, Step 58, d_loss: 0.3642137944698334, g_loss: 5.96154260635376\n","Epoch 86/2000, Step 59, d_loss: 0.3710328936576843, g_loss: 4.156304836273193\n","Epoch 86/2000, Step 60, d_loss: 0.45962125062942505, g_loss: 3.6620097160339355\n","Epoch 86/2000, Step 61, d_loss: 0.4587690234184265, g_loss: 4.209704399108887\n","Epoch 86/2000, Step 62, d_loss: 0.37427401542663574, g_loss: 2.5083110332489014\n","Epoch 86/2000, Step 63, d_loss: 0.4083298146724701, g_loss: 3.365616798400879\n","Epoch 86/2000, Step 64, d_loss: 0.43392741680145264, g_loss: 3.958747148513794\n","Epoch 86/2000, Step 65, d_loss: 0.3680785000324249, g_loss: 4.402565002441406\n","Epoch 86/2000, Step 66, d_loss: 0.38894665241241455, g_loss: 4.146842956542969\n","Epoch 86/2000, Step 67, d_loss: 0.36850211024284363, g_loss: 4.2983880043029785\n","Epoch 86/2000, Step 68, d_loss: 0.36941057443618774, g_loss: 5.253840446472168\n","Epoch 86/2000, Step 69, d_loss: 0.3903181552886963, g_loss: 5.264561653137207\n","Epoch 86/2000, Step 70, d_loss: 0.3737914562225342, g_loss: 4.521501541137695\n","Epoch 86/2000, Step 71, d_loss: 0.3942892849445343, g_loss: 4.371970176696777\n","Epoch 86/2000, Step 72, d_loss: 0.3756678104400635, g_loss: 4.624380111694336\n","Epoch 86/2000, Step 73, d_loss: 0.36048877239227295, g_loss: 5.422266960144043\n","Epoch 86/2000, Step 74, d_loss: 0.39072585105895996, g_loss: 4.304144382476807\n","Epoch 86/2000, Step 75, d_loss: 0.37873968482017517, g_loss: 4.302091598510742\n","Epoch 86/2000, Step 76, d_loss: 0.40270912647247314, g_loss: 4.825326919555664\n","Epoch 86/2000, Step 77, d_loss: 0.36696264147758484, g_loss: 3.890010118484497\n","Epoch 86/2000, Step 78, d_loss: 0.3870503604412079, g_loss: 3.9630846977233887\n","Epoch 86/2000, Step 79, d_loss: 0.4120371341705322, g_loss: 4.1806416511535645\n","Epoch 86/2000, Step 80, d_loss: 0.36236441135406494, g_loss: 3.522472381591797\n","Epoch 86/2000, Step 81, d_loss: 0.39787349104881287, g_loss: 3.9831459522247314\n","Epoch 86/2000, Step 82, d_loss: 0.43963104486465454, g_loss: 2.7734014987945557\n","Epoch 86/2000, Step 83, d_loss: 0.35634681582450867, g_loss: 2.465921640396118\n","Epoch 86/2000, Step 84, d_loss: 0.4652318060398102, g_loss: 4.316618919372559\n","Epoch 86/2000, Step 85, d_loss: 0.38073644042015076, g_loss: 2.834641695022583\n","Epoch 86/2000, Step 86, d_loss: 0.38986948132514954, g_loss: 5.466028690338135\n","Epoch 86/2000, Step 87, d_loss: 0.35367435216903687, g_loss: 3.969071865081787\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 87/2000, Step 1, d_loss: 0.41481882333755493, g_loss: 4.639646530151367\n","Epoch 87/2000, Step 2, d_loss: 0.39642518758773804, g_loss: 4.250755310058594\n","Epoch 87/2000, Step 3, d_loss: 0.35752904415130615, g_loss: 3.3915021419525146\n","Epoch 87/2000, Step 4, d_loss: 0.36755695939064026, g_loss: 3.5049235820770264\n","Epoch 87/2000, Step 5, d_loss: 0.39059725403785706, g_loss: 2.718050479888916\n","Epoch 87/2000, Step 6, d_loss: 0.35236549377441406, g_loss: 3.8606679439544678\n","Epoch 87/2000, Step 7, d_loss: 0.39319664239883423, g_loss: 4.667172908782959\n","Epoch 87/2000, Step 8, d_loss: 0.4366665184497833, g_loss: 3.4993743896484375\n","Epoch 87/2000, Step 9, d_loss: 0.38710758090019226, g_loss: 4.05505895614624\n","Epoch 87/2000, Step 10, d_loss: 0.38542258739471436, g_loss: 3.4777939319610596\n","Epoch 87/2000, Step 11, d_loss: 0.45742568373680115, g_loss: 4.743997097015381\n","Epoch 87/2000, Step 12, d_loss: 0.36790481209754944, g_loss: 5.0643630027771\n","Epoch 87/2000, Step 13, d_loss: 0.3661265969276428, g_loss: 3.895869731903076\n","Epoch 87/2000, Step 14, d_loss: 0.39771541953086853, g_loss: 3.532191514968872\n","Epoch 87/2000, Step 15, d_loss: 0.3583245575428009, g_loss: 3.1558587551116943\n","Epoch 87/2000, Step 16, d_loss: 0.3685404658317566, g_loss: 3.9593183994293213\n","Epoch 87/2000, Step 17, d_loss: 0.4091358482837677, g_loss: 3.798428773880005\n","Epoch 87/2000, Step 18, d_loss: 0.39642399549484253, g_loss: 3.586195230484009\n","Epoch 87/2000, Step 19, d_loss: 0.3820748031139374, g_loss: 5.197778701782227\n","Epoch 87/2000, Step 20, d_loss: 0.37990128993988037, g_loss: 3.491594076156616\n","Epoch 87/2000, Step 21, d_loss: 0.37846019864082336, g_loss: 3.461153030395508\n","Epoch 87/2000, Step 22, d_loss: 0.4606946110725403, g_loss: 3.788344383239746\n","Epoch 87/2000, Step 23, d_loss: 0.4011266529560089, g_loss: 3.6641621589660645\n","Epoch 87/2000, Step 24, d_loss: 0.3549516201019287, g_loss: 3.3813562393188477\n","Epoch 87/2000, Step 25, d_loss: 0.40381720662117004, g_loss: 3.712977409362793\n","Epoch 87/2000, Step 26, d_loss: 0.502652108669281, g_loss: 3.0124502182006836\n","Epoch 87/2000, Step 27, d_loss: 0.38892173767089844, g_loss: 3.655799150466919\n","Epoch 87/2000, Step 28, d_loss: 0.41001731157302856, g_loss: 3.831408977508545\n","Epoch 87/2000, Step 29, d_loss: 0.35135698318481445, g_loss: 4.159687042236328\n","Epoch 87/2000, Step 30, d_loss: 0.47544562816619873, g_loss: 4.66789436340332\n","Epoch 87/2000, Step 31, d_loss: 0.3648885488510132, g_loss: 5.791634559631348\n","Epoch 87/2000, Step 32, d_loss: 0.3473575711250305, g_loss: 5.379685401916504\n","Epoch 87/2000, Step 33, d_loss: 0.33889058232307434, g_loss: 4.6498122215271\n","Epoch 87/2000, Step 34, d_loss: 0.41616421937942505, g_loss: 4.632719039916992\n","Epoch 87/2000, Step 35, d_loss: 0.40326911211013794, g_loss: 3.813567638397217\n","Epoch 87/2000, Step 36, d_loss: 0.38732802867889404, g_loss: 2.908252239227295\n","Epoch 87/2000, Step 37, d_loss: 0.39833682775497437, g_loss: 2.949309825897217\n","Epoch 87/2000, Step 38, d_loss: 0.4436532258987427, g_loss: 2.24977970123291\n","Epoch 87/2000, Step 39, d_loss: 0.5660482048988342, g_loss: 2.80302357673645\n","Epoch 87/2000, Step 40, d_loss: 0.3919789791107178, g_loss: 2.8423967361450195\n","Epoch 87/2000, Step 41, d_loss: 0.4475780725479126, g_loss: 3.7583768367767334\n","Epoch 87/2000, Step 42, d_loss: 0.34629687666893005, g_loss: 5.077887535095215\n","Epoch 87/2000, Step 43, d_loss: 0.3951135575771332, g_loss: 4.61879301071167\n","Epoch 87/2000, Step 44, d_loss: 0.6821736693382263, g_loss: 4.53299617767334\n","Epoch 87/2000, Step 45, d_loss: 0.34338754415512085, g_loss: 4.93526029586792\n","Epoch 87/2000, Step 46, d_loss: 0.360336571931839, g_loss: 4.538412570953369\n","Epoch 87/2000, Step 47, d_loss: 0.3590528666973114, g_loss: 4.355759620666504\n","Epoch 87/2000, Step 48, d_loss: 0.3734360635280609, g_loss: 3.3866324424743652\n","Epoch 87/2000, Step 49, d_loss: 0.3764567971229553, g_loss: 3.2321102619171143\n","Epoch 87/2000, Step 50, d_loss: 0.40288761258125305, g_loss: 3.0345232486724854\n","Epoch 87/2000, Step 51, d_loss: 0.3825317323207855, g_loss: 2.9306044578552246\n","Epoch 87/2000, Step 52, d_loss: 0.3991544544696808, g_loss: 2.8171393871307373\n","Epoch 87/2000, Step 53, d_loss: 0.4167706072330475, g_loss: 3.5035793781280518\n","Epoch 87/2000, Step 54, d_loss: 0.40143805742263794, g_loss: 4.526019096374512\n","Epoch 87/2000, Step 55, d_loss: 0.4540873169898987, g_loss: 3.0287270545959473\n","Epoch 87/2000, Step 56, d_loss: 0.37421897053718567, g_loss: 3.288430690765381\n","Epoch 87/2000, Step 57, d_loss: 0.4025042951107025, g_loss: 2.6432416439056396\n","Epoch 87/2000, Step 58, d_loss: 0.47631505131721497, g_loss: 4.940375328063965\n","Epoch 87/2000, Step 59, d_loss: 0.46449360251426697, g_loss: 3.328131675720215\n","Epoch 87/2000, Step 60, d_loss: 0.4136940836906433, g_loss: 4.815373420715332\n","Epoch 87/2000, Step 61, d_loss: 0.35544905066490173, g_loss: 3.4378764629364014\n","Epoch 87/2000, Step 62, d_loss: 0.3572152256965637, g_loss: 4.357691287994385\n","Epoch 87/2000, Step 63, d_loss: 0.5863831639289856, g_loss: 5.402979850769043\n","Epoch 87/2000, Step 64, d_loss: 0.4078141748905182, g_loss: 3.3638174533843994\n","Epoch 87/2000, Step 65, d_loss: 0.41769421100616455, g_loss: 3.2368600368499756\n","Epoch 87/2000, Step 66, d_loss: 0.4048856794834137, g_loss: 3.2541911602020264\n","Epoch 87/2000, Step 67, d_loss: 0.49031922221183777, g_loss: 2.53920316696167\n","Epoch 87/2000, Step 68, d_loss: 0.44035136699676514, g_loss: 3.3880996704101562\n","Epoch 87/2000, Step 69, d_loss: 0.4652479887008667, g_loss: 3.0616614818573\n","Epoch 87/2000, Step 70, d_loss: 0.3763490617275238, g_loss: 4.450108051300049\n","Epoch 87/2000, Step 71, d_loss: 0.3582335412502289, g_loss: 4.840993404388428\n","Epoch 87/2000, Step 72, d_loss: 0.375434935092926, g_loss: 4.286018371582031\n","Epoch 87/2000, Step 73, d_loss: 0.6589356064796448, g_loss: 4.388980388641357\n","Epoch 87/2000, Step 74, d_loss: 0.3530074656009674, g_loss: 3.4191341400146484\n","Epoch 87/2000, Step 75, d_loss: 0.3876357674598694, g_loss: 4.595821857452393\n","Epoch 87/2000, Step 76, d_loss: 0.5159357786178589, g_loss: 2.634533166885376\n","Epoch 87/2000, Step 77, d_loss: 0.40517139434814453, g_loss: 2.9102206230163574\n","Epoch 87/2000, Step 78, d_loss: 0.5067780017852783, g_loss: 3.050823926925659\n","Epoch 87/2000, Step 79, d_loss: 0.5311527252197266, g_loss: 3.5943400859832764\n","Epoch 87/2000, Step 80, d_loss: 0.43859750032424927, g_loss: 4.450209617614746\n","Epoch 87/2000, Step 81, d_loss: 0.3997954726219177, g_loss: 4.966507434844971\n","Epoch 87/2000, Step 82, d_loss: 0.4651658535003662, g_loss: 5.671227931976318\n","Epoch 87/2000, Step 83, d_loss: 0.549839973449707, g_loss: 3.477273941040039\n","Epoch 87/2000, Step 84, d_loss: 0.41028299927711487, g_loss: 2.2114346027374268\n","Epoch 87/2000, Step 85, d_loss: 0.38328656554222107, g_loss: 3.3141872882843018\n","Epoch 87/2000, Step 86, d_loss: 0.446118026971817, g_loss: 3.468514919281006\n","Epoch 87/2000, Step 87, d_loss: 0.3825297951698303, g_loss: 2.717729330062866\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 88/2000, Step 1, d_loss: 0.4177735447883606, g_loss: 3.458941698074341\n","Epoch 88/2000, Step 2, d_loss: 0.38791728019714355, g_loss: 3.573882579803467\n","Epoch 88/2000, Step 3, d_loss: 0.3899472653865814, g_loss: 5.41025972366333\n","Epoch 88/2000, Step 4, d_loss: 0.4053276479244232, g_loss: 4.485823631286621\n","Epoch 88/2000, Step 5, d_loss: 0.5603197813034058, g_loss: 3.9344568252563477\n","Epoch 88/2000, Step 6, d_loss: 0.41693979501724243, g_loss: 3.1928131580352783\n","Epoch 88/2000, Step 7, d_loss: 0.41346898674964905, g_loss: 2.028470277786255\n","Epoch 88/2000, Step 8, d_loss: 0.4771779179573059, g_loss: 3.5263209342956543\n","Epoch 88/2000, Step 9, d_loss: 0.5815322399139404, g_loss: 2.4461934566497803\n","Epoch 88/2000, Step 10, d_loss: 0.394022136926651, g_loss: 4.05166482925415\n","Epoch 88/2000, Step 11, d_loss: 0.5115618109703064, g_loss: 5.969242095947266\n","Epoch 88/2000, Step 12, d_loss: 0.47051334381103516, g_loss: 5.049911975860596\n","Epoch 88/2000, Step 13, d_loss: 0.602703332901001, g_loss: 4.825998306274414\n","Epoch 88/2000, Step 14, d_loss: 0.5549708604812622, g_loss: 2.7523131370544434\n","Epoch 88/2000, Step 15, d_loss: 0.3728885352611542, g_loss: 2.872612714767456\n","Epoch 88/2000, Step 16, d_loss: 0.535051703453064, g_loss: 2.447836399078369\n","Epoch 88/2000, Step 17, d_loss: 0.5417230129241943, g_loss: 1.7725870609283447\n","Epoch 88/2000, Step 18, d_loss: 0.542361319065094, g_loss: 2.419961452484131\n","Epoch 88/2000, Step 19, d_loss: 0.5413427948951721, g_loss: 3.358285427093506\n","Epoch 88/2000, Step 20, d_loss: 0.3949368894100189, g_loss: 3.5881500244140625\n","Epoch 88/2000, Step 21, d_loss: 0.4686432480812073, g_loss: 4.128296375274658\n","Epoch 88/2000, Step 22, d_loss: 0.520389974117279, g_loss: 5.203279495239258\n","Epoch 88/2000, Step 23, d_loss: 0.3807017505168915, g_loss: 4.803617477416992\n","Epoch 88/2000, Step 24, d_loss: 0.4444107413291931, g_loss: 4.7447733879089355\n","Epoch 88/2000, Step 25, d_loss: 0.3973802626132965, g_loss: 3.2668344974517822\n","Epoch 88/2000, Step 26, d_loss: 0.4115293323993683, g_loss: 4.097796440124512\n","Epoch 88/2000, Step 27, d_loss: 0.43433821201324463, g_loss: 3.26492977142334\n","Epoch 88/2000, Step 28, d_loss: 0.42220431566238403, g_loss: 3.5425920486450195\n","Epoch 88/2000, Step 29, d_loss: 0.4312381148338318, g_loss: 3.319551944732666\n","Epoch 88/2000, Step 30, d_loss: 0.4303417205810547, g_loss: 5.645584583282471\n","Epoch 88/2000, Step 31, d_loss: 0.4647925794124603, g_loss: 4.1880927085876465\n","Epoch 88/2000, Step 32, d_loss: 0.45167604088783264, g_loss: 3.7269463539123535\n","Epoch 88/2000, Step 33, d_loss: 0.3601790964603424, g_loss: 5.123315334320068\n","Epoch 88/2000, Step 34, d_loss: 0.39363038539886475, g_loss: 3.58097767829895\n","Epoch 88/2000, Step 35, d_loss: 0.40562596917152405, g_loss: 3.9934325218200684\n","Epoch 88/2000, Step 36, d_loss: 0.4167194366455078, g_loss: 4.086115837097168\n","Epoch 88/2000, Step 37, d_loss: 0.37310320138931274, g_loss: 3.2278549671173096\n","Epoch 88/2000, Step 38, d_loss: 0.42466264963150024, g_loss: 4.280284404754639\n","Epoch 88/2000, Step 39, d_loss: 0.3781331777572632, g_loss: 4.671809673309326\n","Epoch 88/2000, Step 40, d_loss: 0.3676655888557434, g_loss: 4.347667694091797\n","Epoch 88/2000, Step 41, d_loss: 0.38654252886772156, g_loss: 6.6845526695251465\n","Epoch 88/2000, Step 42, d_loss: 0.3941183090209961, g_loss: 4.0644731521606445\n","Epoch 88/2000, Step 43, d_loss: 0.5094259977340698, g_loss: 4.07342004776001\n","Epoch 88/2000, Step 44, d_loss: 0.3911150097846985, g_loss: 4.325714588165283\n","Epoch 88/2000, Step 45, d_loss: 0.3658073842525482, g_loss: 3.416710138320923\n","Epoch 88/2000, Step 46, d_loss: 0.5281903743743896, g_loss: 3.9454751014709473\n","Epoch 88/2000, Step 47, d_loss: 0.4540744423866272, g_loss: 2.1288394927978516\n","Epoch 88/2000, Step 48, d_loss: 0.46622925996780396, g_loss: 4.827175617218018\n","Epoch 88/2000, Step 49, d_loss: 0.38741177320480347, g_loss: 3.77809739112854\n","Epoch 88/2000, Step 50, d_loss: 0.372101366519928, g_loss: 4.818717956542969\n","Epoch 88/2000, Step 51, d_loss: 0.3717905282974243, g_loss: 4.521656036376953\n","Epoch 88/2000, Step 52, d_loss: 0.5165720582008362, g_loss: 4.280972003936768\n","Epoch 88/2000, Step 53, d_loss: 0.3660718500614166, g_loss: 3.699855089187622\n","Epoch 88/2000, Step 54, d_loss: 0.401923805475235, g_loss: 3.909562110900879\n","Epoch 88/2000, Step 55, d_loss: 0.39547115564346313, g_loss: 5.208364963531494\n","Epoch 88/2000, Step 56, d_loss: 0.35142242908477783, g_loss: 3.6240344047546387\n","Epoch 88/2000, Step 57, d_loss: 0.37614232301712036, g_loss: 4.245859146118164\n","Epoch 88/2000, Step 58, d_loss: 0.3998602628707886, g_loss: 3.34893798828125\n","Epoch 88/2000, Step 59, d_loss: 0.37381836771965027, g_loss: 4.0374956130981445\n","Epoch 88/2000, Step 60, d_loss: 0.3785072863101959, g_loss: 4.45403528213501\n","Epoch 88/2000, Step 61, d_loss: 0.41244128346443176, g_loss: 6.893824100494385\n","Epoch 88/2000, Step 62, d_loss: 0.3597347140312195, g_loss: 4.12016487121582\n","Epoch 88/2000, Step 63, d_loss: 0.3934584856033325, g_loss: 4.83565616607666\n","Epoch 88/2000, Step 64, d_loss: 0.4015316963195801, g_loss: 4.4672980308532715\n","Epoch 88/2000, Step 65, d_loss: 0.390326589345932, g_loss: 4.1212897300720215\n","Epoch 88/2000, Step 66, d_loss: 0.4481167495250702, g_loss: 3.8620591163635254\n","Epoch 88/2000, Step 67, d_loss: 0.37133342027664185, g_loss: 3.4794349670410156\n","Epoch 88/2000, Step 68, d_loss: 0.3697117567062378, g_loss: 2.980316162109375\n","Epoch 88/2000, Step 69, d_loss: 0.3830883502960205, g_loss: 4.689438343048096\n","Epoch 88/2000, Step 70, d_loss: 0.37851589918136597, g_loss: 5.635317325592041\n","Epoch 88/2000, Step 71, d_loss: 0.4000282287597656, g_loss: 4.207939147949219\n","Epoch 88/2000, Step 72, d_loss: 0.49597644805908203, g_loss: 4.114528179168701\n","Epoch 88/2000, Step 73, d_loss: 0.3438315987586975, g_loss: 4.299010753631592\n","Epoch 88/2000, Step 74, d_loss: 0.37840062379837036, g_loss: 4.499881267547607\n","Epoch 88/2000, Step 75, d_loss: 0.3887473940849304, g_loss: 3.9891865253448486\n","Epoch 88/2000, Step 76, d_loss: 0.41710779070854187, g_loss: 4.803594589233398\n","Epoch 88/2000, Step 77, d_loss: 0.39726242423057556, g_loss: 3.0691046714782715\n","Epoch 88/2000, Step 78, d_loss: 0.3528892695903778, g_loss: 4.3451080322265625\n","Epoch 88/2000, Step 79, d_loss: 0.3726680278778076, g_loss: 4.502985954284668\n","Epoch 88/2000, Step 80, d_loss: 0.3738873600959778, g_loss: 4.777459144592285\n","Epoch 88/2000, Step 81, d_loss: 0.3598242998123169, g_loss: 4.369628429412842\n","Epoch 88/2000, Step 82, d_loss: 0.37063297629356384, g_loss: 3.4203343391418457\n","Epoch 88/2000, Step 83, d_loss: 0.38102978467941284, g_loss: 3.2147574424743652\n","Epoch 88/2000, Step 84, d_loss: 0.4032611548900604, g_loss: 3.3810791969299316\n","Epoch 88/2000, Step 85, d_loss: 0.3765697777271271, g_loss: 3.728877305984497\n","Epoch 88/2000, Step 86, d_loss: 0.40613484382629395, g_loss: 3.53446888923645\n","Epoch 88/2000, Step 87, d_loss: 0.45344385504722595, g_loss: 4.475954055786133\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 89/2000, Step 1, d_loss: 0.42808645963668823, g_loss: 3.0514004230499268\n","Epoch 89/2000, Step 2, d_loss: 0.4646707773208618, g_loss: 3.564953565597534\n","Epoch 89/2000, Step 3, d_loss: 0.48656484484672546, g_loss: 5.387363433837891\n","Epoch 89/2000, Step 4, d_loss: 0.39331257343292236, g_loss: 3.6688051223754883\n","Epoch 89/2000, Step 5, d_loss: 0.4027017056941986, g_loss: 3.168804407119751\n","Epoch 89/2000, Step 6, d_loss: 0.48237696290016174, g_loss: 3.793954610824585\n","Epoch 89/2000, Step 7, d_loss: 0.3923155963420868, g_loss: 4.56747579574585\n","Epoch 89/2000, Step 8, d_loss: 0.4075077772140503, g_loss: 4.174258708953857\n","Epoch 89/2000, Step 9, d_loss: 0.38466694951057434, g_loss: 3.2872161865234375\n","Epoch 89/2000, Step 10, d_loss: 0.43483373522758484, g_loss: 4.443535804748535\n","Epoch 89/2000, Step 11, d_loss: 0.44111478328704834, g_loss: 5.307993412017822\n","Epoch 89/2000, Step 12, d_loss: 0.37631455063819885, g_loss: 4.695782661437988\n","Epoch 89/2000, Step 13, d_loss: 0.41087910532951355, g_loss: 5.085656642913818\n","Epoch 89/2000, Step 14, d_loss: 0.38474178314208984, g_loss: 3.1090967655181885\n","Epoch 89/2000, Step 15, d_loss: 0.40699243545532227, g_loss: 3.6851189136505127\n","Epoch 89/2000, Step 16, d_loss: 0.4173819124698639, g_loss: 3.7079591751098633\n","Epoch 89/2000, Step 17, d_loss: 0.3825494945049286, g_loss: 4.208012580871582\n","Epoch 89/2000, Step 18, d_loss: 0.35718244314193726, g_loss: 3.706739664077759\n","Epoch 89/2000, Step 19, d_loss: 0.3665649890899658, g_loss: 4.423460483551025\n","Epoch 89/2000, Step 20, d_loss: 0.48274457454681396, g_loss: 4.16396951675415\n","Epoch 89/2000, Step 21, d_loss: 0.3618124723434448, g_loss: 4.28977632522583\n","Epoch 89/2000, Step 22, d_loss: 0.3651532828807831, g_loss: 3.7283778190612793\n","Epoch 89/2000, Step 23, d_loss: 0.40178224444389343, g_loss: 3.563999652862549\n","Epoch 89/2000, Step 24, d_loss: 0.3502758741378784, g_loss: 3.154433012008667\n","Epoch 89/2000, Step 25, d_loss: 0.4016898572444916, g_loss: 2.797522783279419\n","Epoch 89/2000, Step 26, d_loss: 0.4023098349571228, g_loss: 3.705265760421753\n","Epoch 89/2000, Step 27, d_loss: 0.4100838005542755, g_loss: 3.4929990768432617\n","Epoch 89/2000, Step 28, d_loss: 0.6361063718795776, g_loss: 3.2130956649780273\n","Epoch 89/2000, Step 29, d_loss: 0.3838953375816345, g_loss: 3.6763880252838135\n","Epoch 89/2000, Step 30, d_loss: 0.39271923899650574, g_loss: 3.9666624069213867\n","Epoch 89/2000, Step 31, d_loss: 0.38058316707611084, g_loss: 3.936729669570923\n","Epoch 89/2000, Step 32, d_loss: 0.5431458950042725, g_loss: 4.902033805847168\n","Epoch 89/2000, Step 33, d_loss: 0.4084292948246002, g_loss: 4.806574821472168\n","Epoch 89/2000, Step 34, d_loss: 0.3961809575557709, g_loss: 3.8809850215911865\n","Epoch 89/2000, Step 35, d_loss: 0.39090046286582947, g_loss: 5.4832282066345215\n","Epoch 89/2000, Step 36, d_loss: 0.5820039510726929, g_loss: 3.4203715324401855\n","Epoch 89/2000, Step 37, d_loss: 0.3838043510913849, g_loss: 3.1321771144866943\n","Epoch 89/2000, Step 38, d_loss: 0.4829026162624359, g_loss: 3.1438119411468506\n","Epoch 89/2000, Step 39, d_loss: 0.4565601646900177, g_loss: 4.790811538696289\n","Epoch 89/2000, Step 40, d_loss: 0.6010740399360657, g_loss: 2.8336195945739746\n","Epoch 89/2000, Step 41, d_loss: 0.5539281368255615, g_loss: 4.494811534881592\n","Epoch 89/2000, Step 42, d_loss: 0.3605107069015503, g_loss: 3.6467185020446777\n","Epoch 89/2000, Step 43, d_loss: 0.4511042535305023, g_loss: 3.9957048892974854\n","Epoch 89/2000, Step 44, d_loss: 0.4970128834247589, g_loss: 3.8773036003112793\n","Epoch 89/2000, Step 45, d_loss: 0.448104590177536, g_loss: 3.6824874877929688\n","Epoch 89/2000, Step 46, d_loss: 0.4031519293785095, g_loss: 3.0128908157348633\n","Epoch 89/2000, Step 47, d_loss: 0.37990090250968933, g_loss: 3.9842495918273926\n","Epoch 89/2000, Step 48, d_loss: 0.4123687148094177, g_loss: 2.6279072761535645\n","Epoch 89/2000, Step 49, d_loss: 0.48443663120269775, g_loss: 3.3808414936065674\n","Epoch 89/2000, Step 50, d_loss: 0.4905411899089813, g_loss: 5.99749755859375\n","Epoch 89/2000, Step 51, d_loss: 0.5242543816566467, g_loss: 4.2816691398620605\n","Epoch 89/2000, Step 52, d_loss: 0.4233400821685791, g_loss: 4.5978779792785645\n","Epoch 89/2000, Step 53, d_loss: 0.3595855236053467, g_loss: 3.6735458374023438\n","Epoch 89/2000, Step 54, d_loss: 0.40399977564811707, g_loss: 4.831109046936035\n","Epoch 89/2000, Step 55, d_loss: 0.5957905054092407, g_loss: 3.490178108215332\n","Epoch 89/2000, Step 56, d_loss: 0.4033752977848053, g_loss: 3.2170140743255615\n","Epoch 89/2000, Step 57, d_loss: 0.4031781554222107, g_loss: 3.320918321609497\n","Epoch 89/2000, Step 58, d_loss: 0.38836604356765747, g_loss: 3.2543585300445557\n","Epoch 89/2000, Step 59, d_loss: 0.49682071805000305, g_loss: 3.414346933364868\n","Epoch 89/2000, Step 60, d_loss: 0.4397740960121155, g_loss: 2.6894664764404297\n","Epoch 89/2000, Step 61, d_loss: 0.5077826976776123, g_loss: 4.3608880043029785\n","Epoch 89/2000, Step 62, d_loss: 0.37230172753334045, g_loss: 5.138280391693115\n","Epoch 89/2000, Step 63, d_loss: 0.3813697397708893, g_loss: 4.968112468719482\n","Epoch 89/2000, Step 64, d_loss: 0.5243822932243347, g_loss: 5.486270427703857\n","Epoch 89/2000, Step 65, d_loss: 0.3614841401576996, g_loss: 3.8977441787719727\n","Epoch 89/2000, Step 66, d_loss: 0.36429697275161743, g_loss: 3.50570011138916\n","Epoch 89/2000, Step 67, d_loss: 0.3810827434062958, g_loss: 4.187499523162842\n","Epoch 89/2000, Step 68, d_loss: 0.3627958595752716, g_loss: 3.599374771118164\n","Epoch 89/2000, Step 69, d_loss: 0.46947306394577026, g_loss: 3.135850191116333\n","Epoch 89/2000, Step 70, d_loss: 0.3950980305671692, g_loss: 3.1163957118988037\n","Epoch 89/2000, Step 71, d_loss: 0.44028037786483765, g_loss: 2.9127562046051025\n","Epoch 89/2000, Step 72, d_loss: 0.46897098422050476, g_loss: 4.048171043395996\n","Epoch 89/2000, Step 73, d_loss: 0.4468649625778198, g_loss: 4.348294258117676\n","Epoch 89/2000, Step 74, d_loss: 0.3836992084980011, g_loss: 4.08740758895874\n","Epoch 89/2000, Step 75, d_loss: 0.39038413763046265, g_loss: 4.704610347747803\n","Epoch 89/2000, Step 76, d_loss: 0.44686460494995117, g_loss: 4.044076919555664\n","Epoch 89/2000, Step 77, d_loss: 0.3833197355270386, g_loss: 3.535200357437134\n","Epoch 89/2000, Step 78, d_loss: 0.3777419924736023, g_loss: 3.246825695037842\n","Epoch 89/2000, Step 79, d_loss: 0.4126202464103699, g_loss: 3.730592727661133\n","Epoch 89/2000, Step 80, d_loss: 0.3856576383113861, g_loss: 5.50395393371582\n","Epoch 89/2000, Step 81, d_loss: 0.3605276346206665, g_loss: 4.186124324798584\n","Epoch 89/2000, Step 82, d_loss: 0.4277042746543884, g_loss: 3.766430616378784\n","Epoch 89/2000, Step 83, d_loss: 0.35421034693717957, g_loss: 3.2443296909332275\n","Epoch 89/2000, Step 84, d_loss: 0.6107374429702759, g_loss: 4.016417026519775\n","Epoch 89/2000, Step 85, d_loss: 0.3911879360675812, g_loss: 4.028387546539307\n","Epoch 89/2000, Step 86, d_loss: 0.3763934075832367, g_loss: 2.953749656677246\n","Epoch 89/2000, Step 87, d_loss: 0.45959097146987915, g_loss: 4.173965930938721\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 90/2000, Step 1, d_loss: 0.4386935234069824, g_loss: 3.9670443534851074\n","Epoch 90/2000, Step 2, d_loss: 0.4235289394855499, g_loss: 2.703803062438965\n","Epoch 90/2000, Step 3, d_loss: 0.4263084828853607, g_loss: 4.262080192565918\n","Epoch 90/2000, Step 4, d_loss: 0.39009228348731995, g_loss: 4.394747734069824\n","Epoch 90/2000, Step 5, d_loss: 0.3754655122756958, g_loss: 4.33569860458374\n","Epoch 90/2000, Step 6, d_loss: 0.47691279649734497, g_loss: 4.444217681884766\n","Epoch 90/2000, Step 7, d_loss: 0.43438777327537537, g_loss: 5.590633392333984\n","Epoch 90/2000, Step 8, d_loss: 0.4500327408313751, g_loss: 3.582019805908203\n","Epoch 90/2000, Step 9, d_loss: 0.39258038997650146, g_loss: 2.8887133598327637\n","Epoch 90/2000, Step 10, d_loss: 0.39419645071029663, g_loss: 3.8336329460144043\n","Epoch 90/2000, Step 11, d_loss: 0.39486071467399597, g_loss: 3.526177406311035\n","Epoch 90/2000, Step 12, d_loss: 0.3780241906642914, g_loss: 3.683584213256836\n","Epoch 90/2000, Step 13, d_loss: 0.4304002523422241, g_loss: 4.330300331115723\n","Epoch 90/2000, Step 14, d_loss: 0.42428523302078247, g_loss: 5.309412479400635\n","Epoch 90/2000, Step 15, d_loss: 0.39762651920318604, g_loss: 5.7902512550354\n","Epoch 90/2000, Step 16, d_loss: 0.3836921751499176, g_loss: 4.2547383308410645\n","Epoch 90/2000, Step 17, d_loss: 0.5317066311836243, g_loss: 5.663974285125732\n","Epoch 90/2000, Step 18, d_loss: 0.37801405787467957, g_loss: 4.315571308135986\n","Epoch 90/2000, Step 19, d_loss: 0.39214375615119934, g_loss: 4.790003299713135\n","Epoch 90/2000, Step 20, d_loss: 0.4016135036945343, g_loss: 4.3820672035217285\n","Epoch 90/2000, Step 21, d_loss: 0.37442678213119507, g_loss: 2.87882399559021\n","Epoch 90/2000, Step 22, d_loss: 0.4334391951560974, g_loss: 3.205721855163574\n","Epoch 90/2000, Step 23, d_loss: 0.3662542700767517, g_loss: 3.471903085708618\n","Epoch 90/2000, Step 24, d_loss: 0.39143091440200806, g_loss: 3.688333511352539\n","Epoch 90/2000, Step 25, d_loss: 0.35330086946487427, g_loss: 4.360487461090088\n","Epoch 90/2000, Step 26, d_loss: 0.41614896059036255, g_loss: 4.250672817230225\n","Epoch 90/2000, Step 27, d_loss: 0.3671768009662628, g_loss: 3.7250430583953857\n","Epoch 90/2000, Step 28, d_loss: 0.37939757108688354, g_loss: 3.781008005142212\n","Epoch 90/2000, Step 29, d_loss: 0.3862430453300476, g_loss: 3.4044415950775146\n","Epoch 90/2000, Step 30, d_loss: 0.3463538587093353, g_loss: 4.201308250427246\n","Epoch 90/2000, Step 31, d_loss: 0.44029155373573303, g_loss: 4.13472843170166\n","Epoch 90/2000, Step 32, d_loss: 0.3832801878452301, g_loss: 4.727280139923096\n","Epoch 90/2000, Step 33, d_loss: 0.39213186502456665, g_loss: 3.3239667415618896\n","Epoch 90/2000, Step 34, d_loss: 0.3801349103450775, g_loss: 4.432529926300049\n","Epoch 90/2000, Step 35, d_loss: 0.3785909116268158, g_loss: 3.6587765216827393\n","Epoch 90/2000, Step 36, d_loss: 0.37258291244506836, g_loss: 4.524725914001465\n","Epoch 90/2000, Step 37, d_loss: 0.38139718770980835, g_loss: 4.5855488777160645\n","Epoch 90/2000, Step 38, d_loss: 0.3559797406196594, g_loss: 4.518318176269531\n","Epoch 90/2000, Step 39, d_loss: 0.36868932843208313, g_loss: 4.072356224060059\n","Epoch 90/2000, Step 40, d_loss: 0.46550413966178894, g_loss: 3.790600061416626\n","Epoch 90/2000, Step 41, d_loss: 0.3969489634037018, g_loss: 3.418895721435547\n","Epoch 90/2000, Step 42, d_loss: 0.39956313371658325, g_loss: 2.8491082191467285\n","Epoch 90/2000, Step 43, d_loss: 0.43109753727912903, g_loss: 2.7100400924682617\n","Epoch 90/2000, Step 44, d_loss: 0.45979708433151245, g_loss: 3.3809549808502197\n","Epoch 90/2000, Step 45, d_loss: 0.468626469373703, g_loss: 3.558032512664795\n","Epoch 90/2000, Step 46, d_loss: 0.4059365689754486, g_loss: 3.3909666538238525\n","Epoch 90/2000, Step 47, d_loss: 0.45769813656806946, g_loss: 4.060670852661133\n","Epoch 90/2000, Step 48, d_loss: 0.4142812490463257, g_loss: 4.080779552459717\n","Epoch 90/2000, Step 49, d_loss: 0.4298652112483978, g_loss: 3.7061960697174072\n","Epoch 90/2000, Step 50, d_loss: 0.6515036225318909, g_loss: 4.159564971923828\n","Epoch 90/2000, Step 51, d_loss: 0.3578296899795532, g_loss: 4.437222480773926\n","Epoch 90/2000, Step 52, d_loss: 0.43088048696517944, g_loss: 3.7206881046295166\n","Epoch 90/2000, Step 53, d_loss: 0.45398807525634766, g_loss: 3.9481139183044434\n","Epoch 90/2000, Step 54, d_loss: 0.42603111267089844, g_loss: 4.2500834465026855\n","Epoch 90/2000, Step 55, d_loss: 0.4815784692764282, g_loss: 3.462293863296509\n","Epoch 90/2000, Step 56, d_loss: 0.44949281215667725, g_loss: 3.624772310256958\n","Epoch 90/2000, Step 57, d_loss: 0.38458332419395447, g_loss: 3.9887759685516357\n","Epoch 90/2000, Step 58, d_loss: 0.4211462140083313, g_loss: 5.039106845855713\n","Epoch 90/2000, Step 59, d_loss: 0.39838874340057373, g_loss: 3.985218048095703\n","Epoch 90/2000, Step 60, d_loss: 0.38399389386177063, g_loss: 5.052822113037109\n","Epoch 90/2000, Step 61, d_loss: 0.3566969335079193, g_loss: 4.691603660583496\n","Epoch 90/2000, Step 62, d_loss: 0.3740025460720062, g_loss: 3.8196239471435547\n","Epoch 90/2000, Step 63, d_loss: 0.38105788826942444, g_loss: 4.224452495574951\n","Epoch 90/2000, Step 64, d_loss: 0.3829396069049835, g_loss: 3.0389294624328613\n","Epoch 90/2000, Step 65, d_loss: 0.39657363295555115, g_loss: 3.63167667388916\n","Epoch 90/2000, Step 66, d_loss: 0.43117043375968933, g_loss: 3.781757116317749\n","Epoch 90/2000, Step 67, d_loss: 0.38930758833885193, g_loss: 4.993786334991455\n","Epoch 90/2000, Step 68, d_loss: 0.3676477074623108, g_loss: 4.458703994750977\n","Epoch 90/2000, Step 69, d_loss: 0.42653897404670715, g_loss: 3.723822832107544\n","Epoch 90/2000, Step 70, d_loss: 0.39405423402786255, g_loss: 3.594097137451172\n","Epoch 90/2000, Step 71, d_loss: 0.39303362369537354, g_loss: 4.431686878204346\n","Epoch 90/2000, Step 72, d_loss: 0.3897368907928467, g_loss: 3.595729112625122\n","Epoch 90/2000, Step 73, d_loss: 0.3992576003074646, g_loss: 4.561738014221191\n","Epoch 90/2000, Step 74, d_loss: 0.42993488907814026, g_loss: 3.5380935668945312\n","Epoch 90/2000, Step 75, d_loss: 0.35690489411354065, g_loss: 4.2896342277526855\n","Epoch 90/2000, Step 76, d_loss: 0.3740476071834564, g_loss: 4.1940436363220215\n","Epoch 90/2000, Step 77, d_loss: 0.37098050117492676, g_loss: 3.7153592109680176\n","Epoch 90/2000, Step 78, d_loss: 0.4212820827960968, g_loss: 4.065513610839844\n","Epoch 90/2000, Step 79, d_loss: 0.4767981171607971, g_loss: 4.659136772155762\n","Epoch 90/2000, Step 80, d_loss: 0.3802585303783417, g_loss: 4.002626895904541\n","Epoch 90/2000, Step 81, d_loss: 0.39406511187553406, g_loss: 4.024453163146973\n","Epoch 90/2000, Step 82, d_loss: 0.3504560887813568, g_loss: 4.079471588134766\n","Epoch 90/2000, Step 83, d_loss: 0.4177232086658478, g_loss: 4.228732585906982\n","Epoch 90/2000, Step 84, d_loss: 0.39065784215927124, g_loss: 3.601405143737793\n","Epoch 90/2000, Step 85, d_loss: 0.38768425583839417, g_loss: 2.7454304695129395\n","Epoch 90/2000, Step 86, d_loss: 0.34872138500213623, g_loss: 3.8581762313842773\n","Epoch 90/2000, Step 87, d_loss: 0.36866968870162964, g_loss: 3.976768732070923\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 91/2000, Step 1, d_loss: 0.3660709857940674, g_loss: 6.370246887207031\n","Epoch 91/2000, Step 2, d_loss: 0.36074358224868774, g_loss: 4.865079402923584\n","Epoch 91/2000, Step 3, d_loss: 0.3666074872016907, g_loss: 3.5089762210845947\n","Epoch 91/2000, Step 4, d_loss: 0.3663465678691864, g_loss: 4.50753116607666\n","Epoch 91/2000, Step 5, d_loss: 0.3678407073020935, g_loss: 5.171081066131592\n","Epoch 91/2000, Step 6, d_loss: 0.3552365005016327, g_loss: 4.694562911987305\n","Epoch 91/2000, Step 7, d_loss: 0.4412381947040558, g_loss: 4.062286853790283\n","Epoch 91/2000, Step 8, d_loss: 0.4152158200740814, g_loss: 4.908921718597412\n","Epoch 91/2000, Step 9, d_loss: 0.39144381880760193, g_loss: 3.375692844390869\n","Epoch 91/2000, Step 10, d_loss: 0.43149334192276, g_loss: 3.4854860305786133\n","Epoch 91/2000, Step 11, d_loss: 0.4100083112716675, g_loss: 3.5935850143432617\n","Epoch 91/2000, Step 12, d_loss: 0.4258604645729065, g_loss: 3.4459033012390137\n","Epoch 91/2000, Step 13, d_loss: 0.3609170913696289, g_loss: 2.999546766281128\n","Epoch 91/2000, Step 14, d_loss: 0.3959401547908783, g_loss: 3.6345109939575195\n","Epoch 91/2000, Step 15, d_loss: 0.36516162753105164, g_loss: 4.263171672821045\n","Epoch 91/2000, Step 16, d_loss: 0.399664044380188, g_loss: 5.039213180541992\n","Epoch 91/2000, Step 17, d_loss: 0.40218716859817505, g_loss: 5.411041736602783\n","Epoch 91/2000, Step 18, d_loss: 0.373185396194458, g_loss: 3.88303542137146\n","Epoch 91/2000, Step 19, d_loss: 0.38093575835227966, g_loss: 4.514466285705566\n","Epoch 91/2000, Step 20, d_loss: 0.3789883852005005, g_loss: 4.119816780090332\n","Epoch 91/2000, Step 21, d_loss: 0.4045761227607727, g_loss: 5.100491523742676\n","Epoch 91/2000, Step 22, d_loss: 0.35619112849235535, g_loss: 4.179316520690918\n","Epoch 91/2000, Step 23, d_loss: 0.36760738492012024, g_loss: 3.707810878753662\n","Epoch 91/2000, Step 24, d_loss: 0.3628031015396118, g_loss: 3.9720308780670166\n","Epoch 91/2000, Step 25, d_loss: 0.4255445599555969, g_loss: 4.210073471069336\n","Epoch 91/2000, Step 26, d_loss: 0.4012869894504547, g_loss: 4.528642654418945\n","Epoch 91/2000, Step 27, d_loss: 0.39173805713653564, g_loss: 4.1671671867370605\n","Epoch 91/2000, Step 28, d_loss: 0.4298984110355377, g_loss: 4.1341233253479\n","Epoch 91/2000, Step 29, d_loss: 0.4143197238445282, g_loss: 5.570587158203125\n","Epoch 91/2000, Step 30, d_loss: 0.38037729263305664, g_loss: 3.9328997135162354\n","Epoch 91/2000, Step 31, d_loss: 0.38545656204223633, g_loss: 3.1224160194396973\n","Epoch 91/2000, Step 32, d_loss: 0.3800409138202667, g_loss: 3.843158483505249\n","Epoch 91/2000, Step 33, d_loss: 0.38222193717956543, g_loss: 3.530503034591675\n","Epoch 91/2000, Step 34, d_loss: 0.38145118951797485, g_loss: 3.8592007160186768\n","Epoch 91/2000, Step 35, d_loss: 0.37253013253211975, g_loss: 5.686727523803711\n","Epoch 91/2000, Step 36, d_loss: 0.35838577151298523, g_loss: 5.1181488037109375\n","Epoch 91/2000, Step 37, d_loss: 0.7369050979614258, g_loss: 3.2944860458374023\n","Epoch 91/2000, Step 38, d_loss: 0.3657567501068115, g_loss: 3.9972610473632812\n","Epoch 91/2000, Step 39, d_loss: 0.42004698514938354, g_loss: 2.7965354919433594\n","Epoch 91/2000, Step 40, d_loss: 0.4516203999519348, g_loss: 2.6589794158935547\n","Epoch 91/2000, Step 41, d_loss: 0.5774098634719849, g_loss: 3.102299213409424\n","Epoch 91/2000, Step 42, d_loss: 0.4807538390159607, g_loss: 4.364956378936768\n","Epoch 91/2000, Step 43, d_loss: 0.4059584140777588, g_loss: 5.190489292144775\n","Epoch 91/2000, Step 44, d_loss: 0.3971312940120697, g_loss: 4.050650119781494\n","Epoch 91/2000, Step 45, d_loss: 0.5103892087936401, g_loss: 3.8910045623779297\n","Epoch 91/2000, Step 46, d_loss: 0.41967177391052246, g_loss: 3.771564245223999\n","Epoch 91/2000, Step 47, d_loss: 0.4078333377838135, g_loss: 3.5588245391845703\n","Epoch 91/2000, Step 48, d_loss: 0.4410526752471924, g_loss: 1.9720402956008911\n","Epoch 91/2000, Step 49, d_loss: 0.40880265831947327, g_loss: 3.142582416534424\n","Epoch 91/2000, Step 50, d_loss: 0.43974804878234863, g_loss: 3.3437957763671875\n","Epoch 91/2000, Step 51, d_loss: 0.4201653301715851, g_loss: 5.0467448234558105\n","Epoch 91/2000, Step 52, d_loss: 0.3740079998970032, g_loss: 3.958160638809204\n","Epoch 91/2000, Step 53, d_loss: 0.36093956232070923, g_loss: 3.1041078567504883\n","Epoch 91/2000, Step 54, d_loss: 0.41381406784057617, g_loss: 3.960937738418579\n","Epoch 91/2000, Step 55, d_loss: 0.4535472095012665, g_loss: 4.337229251861572\n","Epoch 91/2000, Step 56, d_loss: 0.3981722891330719, g_loss: 2.9955174922943115\n","Epoch 91/2000, Step 57, d_loss: 0.41647377610206604, g_loss: 3.8926072120666504\n","Epoch 91/2000, Step 58, d_loss: 0.46348556876182556, g_loss: 2.5046491622924805\n","Epoch 91/2000, Step 59, d_loss: 0.4323633909225464, g_loss: 3.1846859455108643\n","Epoch 91/2000, Step 60, d_loss: 0.5013611316680908, g_loss: 2.7920355796813965\n","Epoch 91/2000, Step 61, d_loss: 0.4754381775856018, g_loss: 2.5276081562042236\n","Epoch 91/2000, Step 62, d_loss: 0.4049358665943146, g_loss: 4.7341461181640625\n","Epoch 91/2000, Step 63, d_loss: 0.44704562425613403, g_loss: 5.0559611320495605\n","Epoch 91/2000, Step 64, d_loss: 0.5140920281410217, g_loss: 3.744757652282715\n","Epoch 91/2000, Step 65, d_loss: 0.44800788164138794, g_loss: 3.560398817062378\n","Epoch 91/2000, Step 66, d_loss: 0.41388139128685, g_loss: 2.2843031883239746\n","Epoch 91/2000, Step 67, d_loss: 0.4631690979003906, g_loss: 2.979147434234619\n","Epoch 91/2000, Step 68, d_loss: 0.47082626819610596, g_loss: 3.4707229137420654\n","Epoch 91/2000, Step 69, d_loss: 0.47383949160575867, g_loss: 4.318453788757324\n","Epoch 91/2000, Step 70, d_loss: 0.42614981532096863, g_loss: 3.8467202186584473\n","Epoch 91/2000, Step 71, d_loss: 0.4188796281814575, g_loss: 4.647835731506348\n","Epoch 91/2000, Step 72, d_loss: 0.40847039222717285, g_loss: 4.154726505279541\n","Epoch 91/2000, Step 73, d_loss: 0.4101644456386566, g_loss: 4.376842498779297\n","Epoch 91/2000, Step 74, d_loss: 0.4755644202232361, g_loss: 4.903208255767822\n","Epoch 91/2000, Step 75, d_loss: 0.37660902738571167, g_loss: 4.0387749671936035\n","Epoch 91/2000, Step 76, d_loss: 0.3890869915485382, g_loss: 2.98177170753479\n","Epoch 91/2000, Step 77, d_loss: 0.5176517367362976, g_loss: 3.6113381385803223\n","Epoch 91/2000, Step 78, d_loss: 0.4153582453727722, g_loss: 2.5738799571990967\n","Epoch 91/2000, Step 79, d_loss: 0.4157079756259918, g_loss: 3.8849291801452637\n","Epoch 91/2000, Step 80, d_loss: 0.41335126757621765, g_loss: 4.64382791519165\n","Epoch 91/2000, Step 81, d_loss: 0.37502846121788025, g_loss: 4.686516761779785\n","Epoch 91/2000, Step 82, d_loss: 0.45501476526260376, g_loss: 5.040599346160889\n","Epoch 91/2000, Step 83, d_loss: 0.5390529036521912, g_loss: 6.796474933624268\n","Epoch 91/2000, Step 84, d_loss: 0.3885270953178406, g_loss: 3.829653263092041\n","Epoch 91/2000, Step 85, d_loss: 0.4264591336250305, g_loss: 3.0123770236968994\n","Epoch 91/2000, Step 86, d_loss: 0.39505717158317566, g_loss: 2.938533306121826\n","Epoch 91/2000, Step 87, d_loss: 0.41898614168167114, g_loss: 3.279937982559204\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 92/2000, Step 1, d_loss: 0.4354800879955292, g_loss: 3.046011209487915\n","Epoch 92/2000, Step 2, d_loss: 0.40931007266044617, g_loss: 3.9021246433258057\n","Epoch 92/2000, Step 3, d_loss: 0.4032401740550995, g_loss: 3.499070405960083\n","Epoch 92/2000, Step 4, d_loss: 0.38191863894462585, g_loss: 4.009018421173096\n","Epoch 92/2000, Step 5, d_loss: 0.38140445947647095, g_loss: 3.9718666076660156\n","Epoch 92/2000, Step 6, d_loss: 0.3935123682022095, g_loss: 4.854561805725098\n","Epoch 92/2000, Step 7, d_loss: 0.3832554221153259, g_loss: 4.431609630584717\n","Epoch 92/2000, Step 8, d_loss: 0.35631823539733887, g_loss: 4.2950849533081055\n","Epoch 92/2000, Step 9, d_loss: 0.3684613108634949, g_loss: 5.087197303771973\n","Epoch 92/2000, Step 10, d_loss: 0.42395278811454773, g_loss: 4.7358078956604\n","Epoch 92/2000, Step 11, d_loss: 0.45613282918930054, g_loss: 4.891637325286865\n","Epoch 92/2000, Step 12, d_loss: 0.3782159686088562, g_loss: 2.3252902030944824\n","Epoch 92/2000, Step 13, d_loss: 0.38908010721206665, g_loss: 3.649810791015625\n","Epoch 92/2000, Step 14, d_loss: 0.43942520022392273, g_loss: 3.166317939758301\n","Epoch 92/2000, Step 15, d_loss: 0.4058004319667816, g_loss: 4.159999370574951\n","Epoch 92/2000, Step 16, d_loss: 0.5125244855880737, g_loss: 4.485301494598389\n","Epoch 92/2000, Step 17, d_loss: 0.45319607853889465, g_loss: 4.192625045776367\n","Epoch 92/2000, Step 18, d_loss: 0.3923230767250061, g_loss: 3.5287399291992188\n","Epoch 92/2000, Step 19, d_loss: 0.39437827467918396, g_loss: 4.295840263366699\n","Epoch 92/2000, Step 20, d_loss: 0.3798329830169678, g_loss: 5.117417335510254\n","Epoch 92/2000, Step 21, d_loss: 0.38192567229270935, g_loss: 6.332364559173584\n","Epoch 92/2000, Step 22, d_loss: 0.38292279839515686, g_loss: 4.80391788482666\n","Epoch 92/2000, Step 23, d_loss: 0.42967554926872253, g_loss: 4.086217880249023\n","Epoch 92/2000, Step 24, d_loss: 0.36962172389030457, g_loss: 3.904493808746338\n","Epoch 92/2000, Step 25, d_loss: 0.3703957498073578, g_loss: 3.85081148147583\n","Epoch 92/2000, Step 26, d_loss: 0.4115976095199585, g_loss: 2.828388214111328\n","Epoch 92/2000, Step 27, d_loss: 0.37640678882598877, g_loss: 2.3218955993652344\n","Epoch 92/2000, Step 28, d_loss: 0.4600929915904999, g_loss: 4.611052513122559\n","Epoch 92/2000, Step 29, d_loss: 0.4138109087944031, g_loss: 4.289382457733154\n","Epoch 92/2000, Step 30, d_loss: 0.36974963545799255, g_loss: 3.2626891136169434\n","Epoch 92/2000, Step 31, d_loss: 0.3661068379878998, g_loss: 4.53117036819458\n","Epoch 92/2000, Step 32, d_loss: 0.37182679772377014, g_loss: 5.969354152679443\n","Epoch 92/2000, Step 33, d_loss: 0.5571224093437195, g_loss: 4.127396106719971\n","Epoch 92/2000, Step 34, d_loss: 0.4310621917247772, g_loss: 4.1654181480407715\n","Epoch 92/2000, Step 35, d_loss: 0.4070499539375305, g_loss: 4.052066802978516\n","Epoch 92/2000, Step 36, d_loss: 0.4307858943939209, g_loss: 4.482213020324707\n","Epoch 92/2000, Step 37, d_loss: 0.5084766149520874, g_loss: 3.381049633026123\n","Epoch 92/2000, Step 38, d_loss: 0.44187378883361816, g_loss: 4.817755222320557\n","Epoch 92/2000, Step 39, d_loss: 0.4262741804122925, g_loss: 2.8253731727600098\n","Epoch 92/2000, Step 40, d_loss: 0.3923538327217102, g_loss: 3.945966958999634\n","Epoch 92/2000, Step 41, d_loss: 0.4062347710132599, g_loss: 4.8876051902771\n","Epoch 92/2000, Step 42, d_loss: 0.42023134231567383, g_loss: 4.458974361419678\n","Epoch 92/2000, Step 43, d_loss: 0.7896220088005066, g_loss: 3.9375994205474854\n","Epoch 92/2000, Step 44, d_loss: 0.384419709444046, g_loss: 4.270886421203613\n","Epoch 92/2000, Step 45, d_loss: 0.4328189492225647, g_loss: 2.5773701667785645\n","Epoch 92/2000, Step 46, d_loss: 0.5389164686203003, g_loss: 3.475525140762329\n","Epoch 92/2000, Step 47, d_loss: 0.5568252801895142, g_loss: 2.613447904586792\n","Epoch 92/2000, Step 48, d_loss: 0.5239955186843872, g_loss: 3.1210200786590576\n","Epoch 92/2000, Step 49, d_loss: 0.42895835638046265, g_loss: 4.131638050079346\n","Epoch 92/2000, Step 50, d_loss: 0.39128509163856506, g_loss: 3.752697467803955\n","Epoch 92/2000, Step 51, d_loss: 0.39179566502571106, g_loss: 4.447737693786621\n","Epoch 92/2000, Step 52, d_loss: 0.5862786173820496, g_loss: 3.781790018081665\n","Epoch 92/2000, Step 53, d_loss: 0.41804102063179016, g_loss: 4.0476155281066895\n","Epoch 92/2000, Step 54, d_loss: 0.3728950023651123, g_loss: 4.718207836151123\n","Epoch 92/2000, Step 55, d_loss: 0.375455766916275, g_loss: 4.703734397888184\n","Epoch 92/2000, Step 56, d_loss: 0.394876092672348, g_loss: 4.551208019256592\n","Epoch 92/2000, Step 57, d_loss: 0.4213564097881317, g_loss: 4.164180755615234\n","Epoch 92/2000, Step 58, d_loss: 0.3968699276447296, g_loss: 3.652418613433838\n","Epoch 92/2000, Step 59, d_loss: 0.3609658479690552, g_loss: 3.2727558612823486\n","Epoch 92/2000, Step 60, d_loss: 0.46887966990470886, g_loss: 3.7285571098327637\n","Epoch 92/2000, Step 61, d_loss: 0.39910247921943665, g_loss: 6.137185096740723\n","Epoch 92/2000, Step 62, d_loss: 0.38597190380096436, g_loss: 4.335358142852783\n","Epoch 92/2000, Step 63, d_loss: 0.40254807472229004, g_loss: 6.04054594039917\n","Epoch 92/2000, Step 64, d_loss: 0.4241357743740082, g_loss: 4.454900741577148\n","Epoch 92/2000, Step 65, d_loss: 0.5144076347351074, g_loss: 4.1485161781311035\n","Epoch 92/2000, Step 66, d_loss: 0.37030908465385437, g_loss: 3.95145583152771\n","Epoch 92/2000, Step 67, d_loss: 0.5515556335449219, g_loss: 3.2154245376586914\n","Epoch 92/2000, Step 68, d_loss: 0.5590114593505859, g_loss: 2.6870803833007812\n","Epoch 92/2000, Step 69, d_loss: 0.42948055267333984, g_loss: 2.856118679046631\n","Epoch 92/2000, Step 70, d_loss: 0.45642322301864624, g_loss: 3.6098101139068604\n","Epoch 92/2000, Step 71, d_loss: 0.47410404682159424, g_loss: 3.6531853675842285\n","Epoch 92/2000, Step 72, d_loss: 0.5038798451423645, g_loss: 3.6579432487487793\n","Epoch 92/2000, Step 73, d_loss: 0.41552501916885376, g_loss: 2.751246452331543\n","Epoch 92/2000, Step 74, d_loss: 0.6614218354225159, g_loss: 4.183979511260986\n","Epoch 92/2000, Step 75, d_loss: 0.5214706063270569, g_loss: 3.377922296524048\n","Epoch 92/2000, Step 76, d_loss: 0.39306145906448364, g_loss: 4.020210266113281\n","Epoch 92/2000, Step 77, d_loss: 0.6122562885284424, g_loss: 3.721255302429199\n","Epoch 92/2000, Step 78, d_loss: 0.6313527822494507, g_loss: 3.5286967754364014\n","Epoch 92/2000, Step 79, d_loss: 0.49999314546585083, g_loss: 2.8913533687591553\n","Epoch 92/2000, Step 80, d_loss: 0.5400451421737671, g_loss: 2.2345523834228516\n","Epoch 92/2000, Step 81, d_loss: 0.5164638757705688, g_loss: 4.366247177124023\n","Epoch 92/2000, Step 82, d_loss: 0.5721942782402039, g_loss: 3.523001194000244\n","Epoch 92/2000, Step 83, d_loss: 0.5103856325149536, g_loss: 4.174638748168945\n","Epoch 92/2000, Step 84, d_loss: 0.42660459876060486, g_loss: 3.382310628890991\n","Epoch 92/2000, Step 85, d_loss: 0.3972037434577942, g_loss: 3.7391726970672607\n","Epoch 92/2000, Step 86, d_loss: 0.39687004685401917, g_loss: 4.262552261352539\n","Epoch 92/2000, Step 87, d_loss: 0.44159242510795593, g_loss: 4.259364604949951\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 93/2000, Step 1, d_loss: 0.39575374126434326, g_loss: 3.982362985610962\n","Epoch 93/2000, Step 2, d_loss: 0.36564117670059204, g_loss: 4.854499340057373\n","Epoch 93/2000, Step 3, d_loss: 0.43999630212783813, g_loss: 3.356290817260742\n","Epoch 93/2000, Step 4, d_loss: 0.4777716100215912, g_loss: 3.643798589706421\n","Epoch 93/2000, Step 5, d_loss: 0.4136198163032532, g_loss: 4.404214382171631\n","Epoch 93/2000, Step 6, d_loss: 0.3657401204109192, g_loss: 3.9142251014709473\n","Epoch 93/2000, Step 7, d_loss: 0.38381996750831604, g_loss: 4.408000946044922\n","Epoch 93/2000, Step 8, d_loss: 0.4189388155937195, g_loss: 3.5093648433685303\n","Epoch 93/2000, Step 9, d_loss: 0.3882240951061249, g_loss: 4.663227558135986\n","Epoch 93/2000, Step 10, d_loss: 0.4370361864566803, g_loss: 4.685251235961914\n","Epoch 93/2000, Step 11, d_loss: 0.36245691776275635, g_loss: 4.307392597198486\n","Epoch 93/2000, Step 12, d_loss: 0.3807641565799713, g_loss: 4.606514930725098\n","Epoch 93/2000, Step 13, d_loss: 0.3589063584804535, g_loss: 5.1499481201171875\n","Epoch 93/2000, Step 14, d_loss: 0.683099627494812, g_loss: 3.9248383045196533\n","Epoch 93/2000, Step 15, d_loss: 0.3697167634963989, g_loss: 3.2242848873138428\n","Epoch 93/2000, Step 16, d_loss: 0.4581589102745056, g_loss: 3.7092390060424805\n","Epoch 93/2000, Step 17, d_loss: 0.4628744125366211, g_loss: 3.928623914718628\n","Epoch 93/2000, Step 18, d_loss: 0.3981696367263794, g_loss: 3.4205126762390137\n","Epoch 93/2000, Step 19, d_loss: 0.4479497969150543, g_loss: 2.8873050212860107\n","Epoch 93/2000, Step 20, d_loss: 0.48123809695243835, g_loss: 3.9029393196105957\n","Epoch 93/2000, Step 21, d_loss: 0.37645092606544495, g_loss: 5.654440402984619\n","Epoch 93/2000, Step 22, d_loss: 0.6272782683372498, g_loss: 5.990278720855713\n","Epoch 93/2000, Step 23, d_loss: 0.3974069058895111, g_loss: 3.772242546081543\n","Epoch 93/2000, Step 24, d_loss: 0.3881264925003052, g_loss: 4.3234357833862305\n","Epoch 93/2000, Step 25, d_loss: 0.39387381076812744, g_loss: 3.327597141265869\n","Epoch 93/2000, Step 26, d_loss: 0.40706267952919006, g_loss: 3.50569486618042\n","Epoch 93/2000, Step 27, d_loss: 0.3926859200000763, g_loss: 3.1089937686920166\n","Epoch 93/2000, Step 28, d_loss: 0.3874776363372803, g_loss: 5.538313388824463\n","Epoch 93/2000, Step 29, d_loss: 0.5237789750099182, g_loss: 4.046740531921387\n","Epoch 93/2000, Step 30, d_loss: 0.4054355323314667, g_loss: 2.8971736431121826\n","Epoch 93/2000, Step 31, d_loss: 0.4293868839740753, g_loss: 2.459468364715576\n","Epoch 93/2000, Step 32, d_loss: 0.3682648539543152, g_loss: 4.050349235534668\n","Epoch 93/2000, Step 33, d_loss: 0.3733125627040863, g_loss: 3.7312493324279785\n","Epoch 93/2000, Step 34, d_loss: 0.399936318397522, g_loss: 2.9638803005218506\n","Epoch 93/2000, Step 35, d_loss: 0.4350779354572296, g_loss: 3.757847547531128\n","Epoch 93/2000, Step 36, d_loss: 0.445386677980423, g_loss: 3.81280255317688\n","Epoch 93/2000, Step 37, d_loss: 0.36368095874786377, g_loss: 3.107057571411133\n","Epoch 93/2000, Step 38, d_loss: 0.3864831328392029, g_loss: 4.1219353675842285\n","Epoch 93/2000, Step 39, d_loss: 0.5415061116218567, g_loss: 2.646800994873047\n","Epoch 93/2000, Step 40, d_loss: 0.3870384991168976, g_loss: 2.977543354034424\n","Epoch 93/2000, Step 41, d_loss: 0.43233177065849304, g_loss: 3.51716947555542\n","Epoch 93/2000, Step 42, d_loss: 0.4195536673069, g_loss: 3.884514093399048\n","Epoch 93/2000, Step 43, d_loss: 0.4273312985897064, g_loss: 3.8406622409820557\n","Epoch 93/2000, Step 44, d_loss: 0.4364963471889496, g_loss: 3.6290547847747803\n","Epoch 93/2000, Step 45, d_loss: 0.3716665506362915, g_loss: 4.458201885223389\n","Epoch 93/2000, Step 46, d_loss: 0.3755793869495392, g_loss: 4.543998718261719\n","Epoch 93/2000, Step 47, d_loss: 0.5815302729606628, g_loss: 4.5531325340271\n","Epoch 93/2000, Step 48, d_loss: 0.35899409651756287, g_loss: 5.638768196105957\n","Epoch 93/2000, Step 49, d_loss: 0.39750272035598755, g_loss: 3.678448438644409\n","Epoch 93/2000, Step 50, d_loss: 0.4393351078033447, g_loss: 3.7612898349761963\n","Epoch 93/2000, Step 51, d_loss: 0.3939048945903778, g_loss: 2.6407856941223145\n","Epoch 93/2000, Step 52, d_loss: 0.4286785125732422, g_loss: 3.2192578315734863\n","Epoch 93/2000, Step 53, d_loss: 0.4621915817260742, g_loss: 2.3471038341522217\n","Epoch 93/2000, Step 54, d_loss: 0.38375720381736755, g_loss: 2.946638584136963\n","Epoch 93/2000, Step 55, d_loss: 0.387719988822937, g_loss: 3.291447162628174\n","Epoch 93/2000, Step 56, d_loss: 0.42716196179389954, g_loss: 4.017556667327881\n","Epoch 93/2000, Step 57, d_loss: 0.5062152743339539, g_loss: 3.5986392498016357\n","Epoch 93/2000, Step 58, d_loss: 0.37270620465278625, g_loss: 3.3926901817321777\n","Epoch 93/2000, Step 59, d_loss: 0.4721045196056366, g_loss: 3.510274648666382\n","Epoch 93/2000, Step 60, d_loss: 0.41343578696250916, g_loss: 4.4238996505737305\n","Epoch 93/2000, Step 61, d_loss: 0.3651184141635895, g_loss: 3.2345504760742188\n","Epoch 93/2000, Step 62, d_loss: 0.37905964255332947, g_loss: 4.7101216316223145\n","Epoch 93/2000, Step 63, d_loss: 0.40704357624053955, g_loss: 3.9980056285858154\n","Epoch 93/2000, Step 64, d_loss: 0.3923823833465576, g_loss: 4.671708106994629\n","Epoch 93/2000, Step 65, d_loss: 0.3527192175388336, g_loss: 2.9886178970336914\n","Epoch 93/2000, Step 66, d_loss: 0.3949306607246399, g_loss: 4.562595367431641\n","Epoch 93/2000, Step 67, d_loss: 0.37048447132110596, g_loss: 4.893226623535156\n","Epoch 93/2000, Step 68, d_loss: 0.389590322971344, g_loss: 4.380134582519531\n","Epoch 93/2000, Step 69, d_loss: 0.41635894775390625, g_loss: 3.3274779319763184\n","Epoch 93/2000, Step 70, d_loss: 0.369957834482193, g_loss: 4.256652355194092\n","Epoch 93/2000, Step 71, d_loss: 0.4009277820587158, g_loss: 3.832717180252075\n","Epoch 93/2000, Step 72, d_loss: 0.37063607573509216, g_loss: 4.536728382110596\n","Epoch 93/2000, Step 73, d_loss: 0.42199787497520447, g_loss: 2.211568832397461\n","Epoch 93/2000, Step 74, d_loss: 0.40259087085723877, g_loss: 3.7141590118408203\n","Epoch 93/2000, Step 75, d_loss: 0.3790307939052582, g_loss: 3.664093255996704\n","Epoch 93/2000, Step 76, d_loss: 0.42446833848953247, g_loss: 3.58870792388916\n","Epoch 93/2000, Step 77, d_loss: 0.3980650007724762, g_loss: 5.016414165496826\n","Epoch 93/2000, Step 78, d_loss: 0.3622642457485199, g_loss: 5.5851616859436035\n","Epoch 93/2000, Step 79, d_loss: 0.5434997081756592, g_loss: 3.626781702041626\n","Epoch 93/2000, Step 80, d_loss: 0.3750618100166321, g_loss: 3.2913143634796143\n","Epoch 93/2000, Step 81, d_loss: 0.3634227514266968, g_loss: 2.9977962970733643\n","Epoch 93/2000, Step 82, d_loss: 0.3949872553348541, g_loss: 2.340991258621216\n","Epoch 93/2000, Step 83, d_loss: 0.47604864835739136, g_loss: 2.9692864418029785\n","Epoch 93/2000, Step 84, d_loss: 0.4458595812320709, g_loss: 4.395339488983154\n","Epoch 93/2000, Step 85, d_loss: 0.3915528953075409, g_loss: 3.4612808227539062\n","Epoch 93/2000, Step 86, d_loss: 0.3449152708053589, g_loss: 4.755990982055664\n","Epoch 93/2000, Step 87, d_loss: 0.3743078112602234, g_loss: 4.716926574707031\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 94/2000, Step 1, d_loss: 0.4486255645751953, g_loss: 4.392704010009766\n","Epoch 94/2000, Step 2, d_loss: 0.5405316352844238, g_loss: 3.347494602203369\n","Epoch 94/2000, Step 3, d_loss: 0.40026789903640747, g_loss: 2.739982843399048\n","Epoch 94/2000, Step 4, d_loss: 0.5691941380500793, g_loss: 3.080488443374634\n","Epoch 94/2000, Step 5, d_loss: 0.50588059425354, g_loss: 2.6520938873291016\n","Epoch 94/2000, Step 6, d_loss: 0.4627951681613922, g_loss: 2.876823902130127\n","Epoch 94/2000, Step 7, d_loss: 0.4077032804489136, g_loss: 4.137450218200684\n","Epoch 94/2000, Step 8, d_loss: 0.3877679109573364, g_loss: 3.9781618118286133\n","Epoch 94/2000, Step 9, d_loss: 0.40977609157562256, g_loss: 5.142573833465576\n","Epoch 94/2000, Step 10, d_loss: 0.43684688210487366, g_loss: 5.407698154449463\n","Epoch 94/2000, Step 11, d_loss: 0.5718554258346558, g_loss: 3.083317279815674\n","Epoch 94/2000, Step 12, d_loss: 0.3719845116138458, g_loss: 4.640388011932373\n","Epoch 94/2000, Step 13, d_loss: 0.38179880380630493, g_loss: 4.576528072357178\n","Epoch 94/2000, Step 14, d_loss: 0.4811602234840393, g_loss: 3.5636723041534424\n","Epoch 94/2000, Step 15, d_loss: 0.4904351234436035, g_loss: 5.494924068450928\n","Epoch 94/2000, Step 16, d_loss: 0.43806028366088867, g_loss: 4.386996746063232\n","Epoch 94/2000, Step 17, d_loss: 0.4603927731513977, g_loss: 4.33482027053833\n","Epoch 94/2000, Step 18, d_loss: 0.4279949367046356, g_loss: 3.704787492752075\n","Epoch 94/2000, Step 19, d_loss: 0.38981544971466064, g_loss: 4.321535110473633\n","Epoch 94/2000, Step 20, d_loss: 0.6489406824111938, g_loss: 4.514277458190918\n","Epoch 94/2000, Step 21, d_loss: 0.4584638476371765, g_loss: 2.2616872787475586\n","Epoch 94/2000, Step 22, d_loss: 0.500969409942627, g_loss: 3.063267707824707\n","Epoch 94/2000, Step 23, d_loss: 0.5422579050064087, g_loss: 2.0397164821624756\n","Epoch 94/2000, Step 24, d_loss: 0.4740869104862213, g_loss: 3.7508442401885986\n","Epoch 94/2000, Step 25, d_loss: 0.448382169008255, g_loss: 4.66615104675293\n","Epoch 94/2000, Step 26, d_loss: 0.39949867129325867, g_loss: 6.330193996429443\n","Epoch 94/2000, Step 27, d_loss: 0.43150848150253296, g_loss: 4.115772724151611\n","Epoch 94/2000, Step 28, d_loss: 0.4400591254234314, g_loss: 5.1934027671813965\n","Epoch 94/2000, Step 29, d_loss: 0.4031471312046051, g_loss: 4.093706130981445\n","Epoch 94/2000, Step 30, d_loss: 0.40508729219436646, g_loss: 3.038271427154541\n","Epoch 94/2000, Step 31, d_loss: 0.3602805733680725, g_loss: 4.360859394073486\n","Epoch 94/2000, Step 32, d_loss: 0.40922239422798157, g_loss: 2.688148021697998\n","Epoch 94/2000, Step 33, d_loss: 0.5034632086753845, g_loss: 5.627015590667725\n","Epoch 94/2000, Step 34, d_loss: 0.3777239918708801, g_loss: 3.0202383995056152\n","Epoch 94/2000, Step 35, d_loss: 0.4070742130279541, g_loss: 4.527881145477295\n","Epoch 94/2000, Step 36, d_loss: 0.3535427749156952, g_loss: 3.741865634918213\n","Epoch 94/2000, Step 37, d_loss: 0.4857819378376007, g_loss: 4.364047527313232\n","Epoch 94/2000, Step 38, d_loss: 0.3624584376811981, g_loss: 5.65740442276001\n","Epoch 94/2000, Step 39, d_loss: 0.38425394892692566, g_loss: 5.171882629394531\n","Epoch 94/2000, Step 40, d_loss: 0.43567928671836853, g_loss: 4.671933174133301\n","Epoch 94/2000, Step 41, d_loss: 0.38861146569252014, g_loss: 2.7841451168060303\n","Epoch 94/2000, Step 42, d_loss: 0.4233287572860718, g_loss: 3.0984065532684326\n","Epoch 94/2000, Step 43, d_loss: 0.4595784842967987, g_loss: 3.8213846683502197\n","Epoch 94/2000, Step 44, d_loss: 0.47102653980255127, g_loss: 4.139494895935059\n","Epoch 94/2000, Step 45, d_loss: 0.42935511469841003, g_loss: 4.858928203582764\n","Epoch 94/2000, Step 46, d_loss: 0.3676755726337433, g_loss: 4.175852298736572\n","Epoch 94/2000, Step 47, d_loss: 0.46684297919273376, g_loss: 4.061381816864014\n","Epoch 94/2000, Step 48, d_loss: 0.42000100016593933, g_loss: 4.646674633026123\n","Epoch 94/2000, Step 49, d_loss: 0.4077122211456299, g_loss: 3.8092308044433594\n","Epoch 94/2000, Step 50, d_loss: 0.3861934244632721, g_loss: 4.9537458419799805\n","Epoch 94/2000, Step 51, d_loss: 0.3787888288497925, g_loss: 3.4635281562805176\n","Epoch 94/2000, Step 52, d_loss: 0.5073944926261902, g_loss: 4.405731201171875\n","Epoch 94/2000, Step 53, d_loss: 0.3681786358356476, g_loss: 3.2647273540496826\n","Epoch 94/2000, Step 54, d_loss: 0.4197113513946533, g_loss: 3.726024866104126\n","Epoch 94/2000, Step 55, d_loss: 0.44661399722099304, g_loss: 4.2089762687683105\n","Epoch 94/2000, Step 56, d_loss: 0.38415205478668213, g_loss: 3.0276153087615967\n","Epoch 94/2000, Step 57, d_loss: 0.42657291889190674, g_loss: 3.9018211364746094\n","Epoch 94/2000, Step 58, d_loss: 0.40363091230392456, g_loss: 3.880070686340332\n","Epoch 94/2000, Step 59, d_loss: 0.43749532103538513, g_loss: 3.6909308433532715\n","Epoch 94/2000, Step 60, d_loss: 0.7328639030456543, g_loss: 2.7538650035858154\n","Epoch 94/2000, Step 61, d_loss: 0.38716116547584534, g_loss: 2.6959540843963623\n","Epoch 94/2000, Step 62, d_loss: 0.44319286942481995, g_loss: 2.9687843322753906\n","Epoch 94/2000, Step 63, d_loss: 0.680016279220581, g_loss: 3.815382242202759\n","Epoch 94/2000, Step 64, d_loss: 0.5443751811981201, g_loss: 3.837658405303955\n","Epoch 94/2000, Step 65, d_loss: 0.44052696228027344, g_loss: 4.405745506286621\n","Epoch 94/2000, Step 66, d_loss: 0.389918714761734, g_loss: 4.967651844024658\n","Epoch 94/2000, Step 67, d_loss: 0.4181404411792755, g_loss: 3.9895899295806885\n","Epoch 94/2000, Step 68, d_loss: 0.49582499265670776, g_loss: 3.6849608421325684\n","Epoch 94/2000, Step 69, d_loss: 0.4747954308986664, g_loss: 3.00850248336792\n","Epoch 94/2000, Step 70, d_loss: 0.4885901212692261, g_loss: 4.664821147918701\n","Epoch 94/2000, Step 71, d_loss: 0.41353508830070496, g_loss: 4.070714950561523\n","Epoch 94/2000, Step 72, d_loss: 0.4366028308868408, g_loss: 3.828958034515381\n","Epoch 94/2000, Step 73, d_loss: 0.3709595501422882, g_loss: 4.52720308303833\n","Epoch 94/2000, Step 74, d_loss: 0.5534668564796448, g_loss: 4.934067249298096\n","Epoch 94/2000, Step 75, d_loss: 0.3946055769920349, g_loss: 5.852040767669678\n","Epoch 94/2000, Step 76, d_loss: 0.4355488717556, g_loss: 4.113102912902832\n","Epoch 94/2000, Step 77, d_loss: 0.43103328347206116, g_loss: 4.607761383056641\n","Epoch 94/2000, Step 78, d_loss: 0.3795168399810791, g_loss: 4.8626251220703125\n","Epoch 94/2000, Step 79, d_loss: 0.3771916329860687, g_loss: 3.9994494915008545\n","Epoch 94/2000, Step 80, d_loss: 0.3680100739002228, g_loss: 3.705463409423828\n","Epoch 94/2000, Step 81, d_loss: 0.3806372284889221, g_loss: 4.959783554077148\n","Epoch 94/2000, Step 82, d_loss: 0.3834204077720642, g_loss: 4.507193565368652\n","Epoch 94/2000, Step 83, d_loss: 0.37882789969444275, g_loss: 4.745275497436523\n","Epoch 94/2000, Step 84, d_loss: 0.3964185416698456, g_loss: 4.633490562438965\n","Epoch 94/2000, Step 85, d_loss: 0.3984770178794861, g_loss: 4.579826831817627\n","Epoch 94/2000, Step 86, d_loss: 0.3761768639087677, g_loss: 4.20800256729126\n","Epoch 94/2000, Step 87, d_loss: 0.34978941082954407, g_loss: 3.5727319717407227\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 95/2000, Step 1, d_loss: 0.3698059916496277, g_loss: 3.8680989742279053\n","Epoch 95/2000, Step 2, d_loss: 0.3940596580505371, g_loss: 3.7096657752990723\n","Epoch 95/2000, Step 3, d_loss: 0.3891254961490631, g_loss: 4.34817361831665\n","Epoch 95/2000, Step 4, d_loss: 0.4020717442035675, g_loss: 4.59580135345459\n","Epoch 95/2000, Step 5, d_loss: 0.39763936400413513, g_loss: 4.6134161949157715\n","Epoch 95/2000, Step 6, d_loss: 0.41017159819602966, g_loss: 5.574525356292725\n","Epoch 95/2000, Step 7, d_loss: 0.38049694895744324, g_loss: 3.9710288047790527\n","Epoch 95/2000, Step 8, d_loss: 0.39120426774024963, g_loss: 4.185586452484131\n","Epoch 95/2000, Step 9, d_loss: 0.389845073223114, g_loss: 3.9647412300109863\n","Epoch 95/2000, Step 10, d_loss: 0.38756513595581055, g_loss: 5.838376998901367\n","Epoch 95/2000, Step 11, d_loss: 0.41477373242378235, g_loss: 4.878592014312744\n","Epoch 95/2000, Step 12, d_loss: 0.36276566982269287, g_loss: 3.6940643787384033\n","Epoch 95/2000, Step 13, d_loss: 0.38054925203323364, g_loss: 3.8519113063812256\n","Epoch 95/2000, Step 14, d_loss: 0.37691399455070496, g_loss: 2.61881947517395\n","Epoch 95/2000, Step 15, d_loss: 0.38438907265663147, g_loss: 4.433961868286133\n","Epoch 95/2000, Step 16, d_loss: 0.4134793281555176, g_loss: 4.782389163970947\n","Epoch 95/2000, Step 17, d_loss: 0.3722642660140991, g_loss: 4.157848358154297\n","Epoch 95/2000, Step 18, d_loss: 0.40872398018836975, g_loss: 4.852593898773193\n","Epoch 95/2000, Step 19, d_loss: 0.42499181628227234, g_loss: 4.12249231338501\n","Epoch 95/2000, Step 20, d_loss: 0.3549061417579651, g_loss: 5.87784481048584\n","Epoch 95/2000, Step 21, d_loss: 0.3650002181529999, g_loss: 4.680999279022217\n","Epoch 95/2000, Step 22, d_loss: 0.48230981826782227, g_loss: 5.071926116943359\n","Epoch 95/2000, Step 23, d_loss: 0.35951322317123413, g_loss: 4.8128981590271\n","Epoch 95/2000, Step 24, d_loss: 0.3592248857021332, g_loss: 4.155025005340576\n","Epoch 95/2000, Step 25, d_loss: 0.4001230001449585, g_loss: 4.418015003204346\n","Epoch 95/2000, Step 26, d_loss: 0.3914155066013336, g_loss: 3.4192404747009277\n","Epoch 95/2000, Step 27, d_loss: 0.3932839334011078, g_loss: 4.074293613433838\n","Epoch 95/2000, Step 28, d_loss: 0.36430102586746216, g_loss: 3.517927408218384\n","Epoch 95/2000, Step 29, d_loss: 0.36348119378089905, g_loss: 4.415568828582764\n","Epoch 95/2000, Step 30, d_loss: 0.36416035890579224, g_loss: 4.743208885192871\n","Epoch 95/2000, Step 31, d_loss: 0.3622128665447235, g_loss: 4.451776504516602\n","Epoch 95/2000, Step 32, d_loss: 0.347075492143631, g_loss: 5.1615891456604\n","Epoch 95/2000, Step 33, d_loss: 0.35732153058052063, g_loss: 5.209975719451904\n","Epoch 95/2000, Step 34, d_loss: 0.36532527208328247, g_loss: 4.792761325836182\n","Epoch 95/2000, Step 35, d_loss: 0.3557426929473877, g_loss: 4.062756538391113\n","Epoch 95/2000, Step 36, d_loss: 0.36507225036621094, g_loss: 4.851571083068848\n","Epoch 95/2000, Step 37, d_loss: 0.374735563993454, g_loss: 5.174460411071777\n","Epoch 95/2000, Step 38, d_loss: 0.40035566687583923, g_loss: 3.5606844425201416\n","Epoch 95/2000, Step 39, d_loss: 0.3805150091648102, g_loss: 3.9188528060913086\n","Epoch 95/2000, Step 40, d_loss: 0.37338942289352417, g_loss: 5.005960941314697\n","Epoch 95/2000, Step 41, d_loss: 0.3766084909439087, g_loss: 4.964600086212158\n","Epoch 95/2000, Step 42, d_loss: 0.39468520879745483, g_loss: 3.910552978515625\n","Epoch 95/2000, Step 43, d_loss: 0.3497455418109894, g_loss: 3.6777091026306152\n","Epoch 95/2000, Step 44, d_loss: 0.3388114869594574, g_loss: 4.537441730499268\n","Epoch 95/2000, Step 45, d_loss: 0.3821347951889038, g_loss: 4.197273254394531\n","Epoch 95/2000, Step 46, d_loss: 0.3556309640407562, g_loss: 4.070310115814209\n","Epoch 95/2000, Step 47, d_loss: 0.36745089292526245, g_loss: 6.134748458862305\n","Epoch 95/2000, Step 48, d_loss: 0.4311789870262146, g_loss: 4.232475757598877\n","Epoch 95/2000, Step 49, d_loss: 0.3964504301548004, g_loss: 3.0215566158294678\n","Epoch 95/2000, Step 50, d_loss: 0.3712584674358368, g_loss: 2.9534270763397217\n","Epoch 95/2000, Step 51, d_loss: 0.37273699045181274, g_loss: 4.163841247558594\n","Epoch 95/2000, Step 52, d_loss: 0.39517104625701904, g_loss: 3.056013584136963\n","Epoch 95/2000, Step 53, d_loss: 0.43738746643066406, g_loss: 4.498863697052002\n","Epoch 95/2000, Step 54, d_loss: 0.3859845995903015, g_loss: 3.579719066619873\n","Epoch 95/2000, Step 55, d_loss: 0.36991575360298157, g_loss: 4.617796897888184\n","Epoch 95/2000, Step 56, d_loss: 0.3759459853172302, g_loss: 3.977410078048706\n","Epoch 95/2000, Step 57, d_loss: 0.62798672914505, g_loss: 4.117649078369141\n","Epoch 95/2000, Step 58, d_loss: 0.44903764128685, g_loss: 4.644167423248291\n","Epoch 95/2000, Step 59, d_loss: 0.4838760495185852, g_loss: 3.1085307598114014\n","Epoch 95/2000, Step 60, d_loss: 0.42693230509757996, g_loss: 2.6171610355377197\n","Epoch 95/2000, Step 61, d_loss: 0.5353593826293945, g_loss: 2.834857940673828\n","Epoch 95/2000, Step 62, d_loss: 0.5046851634979248, g_loss: 3.216604709625244\n","Epoch 95/2000, Step 63, d_loss: 0.43999943137168884, g_loss: 2.5272529125213623\n","Epoch 95/2000, Step 64, d_loss: 0.424550861120224, g_loss: 3.005645275115967\n","Epoch 95/2000, Step 65, d_loss: 0.4198554754257202, g_loss: 5.732959270477295\n","Epoch 95/2000, Step 66, d_loss: 0.56923907995224, g_loss: 3.5822465419769287\n","Epoch 95/2000, Step 67, d_loss: 0.3631040155887604, g_loss: 4.478137016296387\n","Epoch 95/2000, Step 68, d_loss: 0.4574582278728485, g_loss: 4.27987003326416\n","Epoch 95/2000, Step 69, d_loss: 0.797080934047699, g_loss: 4.953161239624023\n","Epoch 95/2000, Step 70, d_loss: 0.4390075206756592, g_loss: 2.628037452697754\n","Epoch 95/2000, Step 71, d_loss: 0.40021541714668274, g_loss: 4.751498699188232\n","Epoch 95/2000, Step 72, d_loss: 0.5378870964050293, g_loss: 3.1620755195617676\n","Epoch 95/2000, Step 73, d_loss: 0.5382874011993408, g_loss: 2.8008105754852295\n","Epoch 95/2000, Step 74, d_loss: 0.47371092438697815, g_loss: 3.64843487739563\n","Epoch 95/2000, Step 75, d_loss: 0.5687508583068848, g_loss: 4.2716779708862305\n","Epoch 95/2000, Step 76, d_loss: 0.42027702927589417, g_loss: 3.95854115486145\n","Epoch 95/2000, Step 77, d_loss: 0.40170520544052124, g_loss: 3.4803709983825684\n","Epoch 95/2000, Step 78, d_loss: 0.6374573111534119, g_loss: 4.334641933441162\n","Epoch 95/2000, Step 79, d_loss: 0.43654632568359375, g_loss: 3.5629539489746094\n","Epoch 95/2000, Step 80, d_loss: 0.43912357091903687, g_loss: 3.1323163509368896\n","Epoch 95/2000, Step 81, d_loss: 0.43632179498672485, g_loss: 2.783994197845459\n","Epoch 95/2000, Step 82, d_loss: 0.4321281611919403, g_loss: 3.2479121685028076\n","Epoch 95/2000, Step 83, d_loss: 0.4341685473918915, g_loss: 4.400387287139893\n","Epoch 95/2000, Step 84, d_loss: 0.5265748500823975, g_loss: 2.6643996238708496\n","Epoch 95/2000, Step 85, d_loss: 0.41892290115356445, g_loss: 3.1587812900543213\n","Epoch 95/2000, Step 86, d_loss: 0.3842673599720001, g_loss: 3.6800293922424316\n","Epoch 95/2000, Step 87, d_loss: 0.3862215578556061, g_loss: 3.4996719360351562\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 96/2000, Step 1, d_loss: 0.4188201129436493, g_loss: 3.719428539276123\n","Epoch 96/2000, Step 2, d_loss: 0.42326638102531433, g_loss: 4.309820652008057\n","Epoch 96/2000, Step 3, d_loss: 0.534721314907074, g_loss: 4.655928134918213\n","Epoch 96/2000, Step 4, d_loss: 0.4292563796043396, g_loss: 4.400081634521484\n","Epoch 96/2000, Step 5, d_loss: 0.4009188115596771, g_loss: 4.4187517166137695\n","Epoch 96/2000, Step 6, d_loss: 0.49445971846580505, g_loss: 3.8231351375579834\n","Epoch 96/2000, Step 7, d_loss: 0.44939160346984863, g_loss: 6.2025556564331055\n","Epoch 96/2000, Step 8, d_loss: 0.46563372015953064, g_loss: 7.4594645500183105\n","Epoch 96/2000, Step 9, d_loss: 0.4221324622631073, g_loss: 4.330515384674072\n","Epoch 96/2000, Step 10, d_loss: 0.3773188591003418, g_loss: 3.68888521194458\n","Epoch 96/2000, Step 11, d_loss: 0.40938007831573486, g_loss: 3.7868356704711914\n","Epoch 96/2000, Step 12, d_loss: 0.39724522829055786, g_loss: 4.303177356719971\n","Epoch 96/2000, Step 13, d_loss: 0.4401252865791321, g_loss: 3.9182050228118896\n","Epoch 96/2000, Step 14, d_loss: 0.4404109716415405, g_loss: 6.2910685539245605\n","Epoch 96/2000, Step 15, d_loss: 0.6005114316940308, g_loss: 3.6104273796081543\n","Epoch 96/2000, Step 16, d_loss: 0.3889150321483612, g_loss: 3.8070995807647705\n","Epoch 96/2000, Step 17, d_loss: 0.4069859981536865, g_loss: 2.6583433151245117\n","Epoch 96/2000, Step 18, d_loss: 0.45541834831237793, g_loss: 3.4847543239593506\n","Epoch 96/2000, Step 19, d_loss: 0.44368648529052734, g_loss: 4.755390644073486\n","Epoch 96/2000, Step 20, d_loss: 0.5244925618171692, g_loss: 4.25610876083374\n","Epoch 96/2000, Step 21, d_loss: 0.4092715084552765, g_loss: 4.722529411315918\n","Epoch 96/2000, Step 22, d_loss: 0.40596914291381836, g_loss: 3.053187847137451\n","Epoch 96/2000, Step 23, d_loss: 0.40256738662719727, g_loss: 4.905405521392822\n","Epoch 96/2000, Step 24, d_loss: 0.4436197280883789, g_loss: 4.2243123054504395\n","Epoch 96/2000, Step 25, d_loss: 0.4119776487350464, g_loss: 4.659247875213623\n","Epoch 96/2000, Step 26, d_loss: 0.49814334511756897, g_loss: 3.512352466583252\n","Epoch 96/2000, Step 27, d_loss: 0.4147242605686188, g_loss: 5.100180149078369\n","Epoch 96/2000, Step 28, d_loss: 0.416952908039093, g_loss: 4.203975677490234\n","Epoch 96/2000, Step 29, d_loss: 0.42704537510871887, g_loss: 4.444091320037842\n","Epoch 96/2000, Step 30, d_loss: 0.4068911075592041, g_loss: 3.4921200275421143\n","Epoch 96/2000, Step 31, d_loss: 0.4400504231452942, g_loss: 3.5610158443450928\n","Epoch 96/2000, Step 32, d_loss: 0.38493454456329346, g_loss: 3.2832722663879395\n","Epoch 96/2000, Step 33, d_loss: 0.4049887955188751, g_loss: 4.233345985412598\n","Epoch 96/2000, Step 34, d_loss: 0.4045151472091675, g_loss: 4.221331596374512\n","Epoch 96/2000, Step 35, d_loss: 0.3885901868343353, g_loss: 5.55760383605957\n","Epoch 96/2000, Step 36, d_loss: 0.4053865969181061, g_loss: 4.628119468688965\n","Epoch 96/2000, Step 37, d_loss: 0.47320273518562317, g_loss: 4.83541202545166\n","Epoch 96/2000, Step 38, d_loss: 0.38771942257881165, g_loss: 3.462568521499634\n","Epoch 96/2000, Step 39, d_loss: 0.3698396682739258, g_loss: 3.2955222129821777\n","Epoch 96/2000, Step 40, d_loss: 0.5213816165924072, g_loss: 2.223252534866333\n","Epoch 96/2000, Step 41, d_loss: 0.4587010145187378, g_loss: 3.189770460128784\n","Epoch 96/2000, Step 42, d_loss: 0.37749892473220825, g_loss: 3.865532159805298\n","Epoch 96/2000, Step 43, d_loss: 0.4661232829093933, g_loss: 2.816800832748413\n","Epoch 96/2000, Step 44, d_loss: 0.3767198622226715, g_loss: 4.775665283203125\n","Epoch 96/2000, Step 45, d_loss: 0.4249383509159088, g_loss: 4.5234503746032715\n","Epoch 96/2000, Step 46, d_loss: 0.5954731702804565, g_loss: 3.8199994564056396\n","Epoch 96/2000, Step 47, d_loss: 0.3786531090736389, g_loss: 5.300379753112793\n","Epoch 96/2000, Step 48, d_loss: 0.38672634959220886, g_loss: 3.6121065616607666\n","Epoch 96/2000, Step 49, d_loss: 0.40044087171554565, g_loss: 3.1202192306518555\n","Epoch 96/2000, Step 50, d_loss: 0.39079591631889343, g_loss: 4.517221927642822\n","Epoch 96/2000, Step 51, d_loss: 0.442377507686615, g_loss: 5.179109573364258\n","Epoch 96/2000, Step 52, d_loss: 0.35605761408805847, g_loss: 4.572665214538574\n","Epoch 96/2000, Step 53, d_loss: 0.36791467666625977, g_loss: 4.127002239227295\n","Epoch 96/2000, Step 54, d_loss: 0.3787674307823181, g_loss: 3.3010547161102295\n","Epoch 96/2000, Step 55, d_loss: 0.4029982089996338, g_loss: 2.8044397830963135\n","Epoch 96/2000, Step 56, d_loss: 0.39860913157463074, g_loss: 4.027785778045654\n","Epoch 96/2000, Step 57, d_loss: 0.4271659851074219, g_loss: 3.5901336669921875\n","Epoch 96/2000, Step 58, d_loss: 0.43084239959716797, g_loss: 4.268013954162598\n","Epoch 96/2000, Step 59, d_loss: 0.37150976061820984, g_loss: 3.8409879207611084\n","Epoch 96/2000, Step 60, d_loss: 0.36939552426338196, g_loss: 4.223494529724121\n","Epoch 96/2000, Step 61, d_loss: 0.36725953221321106, g_loss: 5.090565204620361\n","Epoch 96/2000, Step 62, d_loss: 0.4755329489707947, g_loss: 5.14867639541626\n","Epoch 96/2000, Step 63, d_loss: 0.36835548281669617, g_loss: 3.9827866554260254\n","Epoch 96/2000, Step 64, d_loss: 0.4730919897556305, g_loss: 2.912299871444702\n","Epoch 96/2000, Step 65, d_loss: 0.37860235571861267, g_loss: 3.378690004348755\n","Epoch 96/2000, Step 66, d_loss: 0.4169597923755646, g_loss: 3.832723617553711\n","Epoch 96/2000, Step 67, d_loss: 0.3659009039402008, g_loss: 3.0370800495147705\n","Epoch 96/2000, Step 68, d_loss: 0.47845983505249023, g_loss: 3.431730270385742\n","Epoch 96/2000, Step 69, d_loss: 0.36893513798713684, g_loss: 3.7520852088928223\n","Epoch 96/2000, Step 70, d_loss: 0.3805866241455078, g_loss: 4.738431930541992\n","Epoch 96/2000, Step 71, d_loss: 0.40893423557281494, g_loss: 4.501242160797119\n","Epoch 96/2000, Step 72, d_loss: 0.3421512246131897, g_loss: 3.645894765853882\n","Epoch 96/2000, Step 73, d_loss: 0.39857590198516846, g_loss: 3.735776424407959\n","Epoch 96/2000, Step 74, d_loss: 0.37033766508102417, g_loss: 3.386291980743408\n","Epoch 96/2000, Step 75, d_loss: 0.3839765787124634, g_loss: 4.196268081665039\n","Epoch 96/2000, Step 76, d_loss: 0.3605480194091797, g_loss: 4.450500965118408\n","Epoch 96/2000, Step 77, d_loss: 0.4392346739768982, g_loss: 3.931119918823242\n","Epoch 96/2000, Step 78, d_loss: 0.37679341435432434, g_loss: 3.3772170543670654\n","Epoch 96/2000, Step 79, d_loss: 0.4671594798564911, g_loss: 3.71699857711792\n","Epoch 96/2000, Step 80, d_loss: 0.37459802627563477, g_loss: 3.1644582748413086\n","Epoch 96/2000, Step 81, d_loss: 0.3814886510372162, g_loss: 3.445221424102783\n","Epoch 96/2000, Step 82, d_loss: 0.4230242967605591, g_loss: 3.7115390300750732\n","Epoch 96/2000, Step 83, d_loss: 0.4715411067008972, g_loss: 3.7044458389282227\n","Epoch 96/2000, Step 84, d_loss: 0.44273269176483154, g_loss: 4.475529193878174\n","Epoch 96/2000, Step 85, d_loss: 0.4423529803752899, g_loss: 4.300511360168457\n","Epoch 96/2000, Step 86, d_loss: 0.3723454177379608, g_loss: 5.190504550933838\n","Epoch 96/2000, Step 87, d_loss: 0.3641396164894104, g_loss: 6.524188041687012\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 97/2000, Step 1, d_loss: 0.4648665487766266, g_loss: 3.6960880756378174\n","Epoch 97/2000, Step 2, d_loss: 0.3680218458175659, g_loss: 4.540000915527344\n","Epoch 97/2000, Step 3, d_loss: 0.3717525601387024, g_loss: 2.9491918087005615\n","Epoch 97/2000, Step 4, d_loss: 0.36120033264160156, g_loss: 3.130849838256836\n","Epoch 97/2000, Step 5, d_loss: 0.42941945791244507, g_loss: 3.611386299133301\n","Epoch 97/2000, Step 6, d_loss: 0.4113325774669647, g_loss: 2.518054962158203\n","Epoch 97/2000, Step 7, d_loss: 0.3970325291156769, g_loss: 1.9337875843048096\n","Epoch 97/2000, Step 8, d_loss: 0.48116379976272583, g_loss: 3.5722172260284424\n","Epoch 97/2000, Step 9, d_loss: 0.4694706201553345, g_loss: 2.268376350402832\n","Epoch 97/2000, Step 10, d_loss: 0.5350733995437622, g_loss: 3.683223009109497\n","Epoch 97/2000, Step 11, d_loss: 0.4594786763191223, g_loss: 3.033651113510132\n","Epoch 97/2000, Step 12, d_loss: 0.4118727445602417, g_loss: 3.9337124824523926\n","Epoch 97/2000, Step 13, d_loss: 0.4504866302013397, g_loss: 2.941044330596924\n","Epoch 97/2000, Step 14, d_loss: 0.4132304787635803, g_loss: 3.960179567337036\n","Epoch 97/2000, Step 15, d_loss: 0.5125744938850403, g_loss: 4.670343399047852\n","Epoch 97/2000, Step 16, d_loss: 0.3504275381565094, g_loss: 4.713925361633301\n","Epoch 97/2000, Step 17, d_loss: 0.39461132884025574, g_loss: 2.5257325172424316\n","Epoch 97/2000, Step 18, d_loss: 0.45815804600715637, g_loss: 4.26983642578125\n","Epoch 97/2000, Step 19, d_loss: 0.4829227328300476, g_loss: 2.7259163856506348\n","Epoch 97/2000, Step 20, d_loss: 0.41200757026672363, g_loss: 4.594778537750244\n","Epoch 97/2000, Step 21, d_loss: 0.41222378611564636, g_loss: 3.0618197917938232\n","Epoch 97/2000, Step 22, d_loss: 0.39825430512428284, g_loss: 4.19006872177124\n","Epoch 97/2000, Step 23, d_loss: 0.3755229115486145, g_loss: 3.5615642070770264\n","Epoch 97/2000, Step 24, d_loss: 0.3640845715999603, g_loss: 4.735579013824463\n","Epoch 97/2000, Step 25, d_loss: 0.3738499879837036, g_loss: 3.972856044769287\n","Epoch 97/2000, Step 26, d_loss: 0.3699601888656616, g_loss: 4.261688709259033\n","Epoch 97/2000, Step 27, d_loss: 0.3700251877307892, g_loss: 3.467911958694458\n","Epoch 97/2000, Step 28, d_loss: 0.44342756271362305, g_loss: 4.8168416023254395\n","Epoch 97/2000, Step 29, d_loss: 0.34792712330818176, g_loss: 4.264814376831055\n","Epoch 97/2000, Step 30, d_loss: 0.3645688593387604, g_loss: 5.243571758270264\n","Epoch 97/2000, Step 31, d_loss: 0.41828808188438416, g_loss: 4.086479663848877\n","Epoch 97/2000, Step 32, d_loss: 0.3575916290283203, g_loss: 4.029383182525635\n","Epoch 97/2000, Step 33, d_loss: 0.3812231123447418, g_loss: 3.2118701934814453\n","Epoch 97/2000, Step 34, d_loss: 0.359519898891449, g_loss: 2.9375815391540527\n","Epoch 97/2000, Step 35, d_loss: 0.35840466618537903, g_loss: 4.9257917404174805\n","Epoch 97/2000, Step 36, d_loss: 0.38175347447395325, g_loss: 4.301553726196289\n","Epoch 97/2000, Step 37, d_loss: 0.3802030682563782, g_loss: 4.3252058029174805\n","Epoch 97/2000, Step 38, d_loss: 0.379484623670578, g_loss: 4.046700954437256\n","Epoch 97/2000, Step 39, d_loss: 0.3757586181163788, g_loss: 4.324377536773682\n","Epoch 97/2000, Step 40, d_loss: 0.37741944193840027, g_loss: 4.378243446350098\n","Epoch 97/2000, Step 41, d_loss: 0.3969448506832123, g_loss: 3.2106151580810547\n","Epoch 97/2000, Step 42, d_loss: 0.35570067167282104, g_loss: 3.0358853340148926\n","Epoch 97/2000, Step 43, d_loss: 0.36057233810424805, g_loss: 2.8785436153411865\n","Epoch 97/2000, Step 44, d_loss: 0.36145493388175964, g_loss: 2.7095673084259033\n","Epoch 97/2000, Step 45, d_loss: 0.44022300839424133, g_loss: 3.2144064903259277\n","Epoch 97/2000, Step 46, d_loss: 0.39320820569992065, g_loss: 3.0475192070007324\n","Epoch 97/2000, Step 47, d_loss: 0.43038249015808105, g_loss: 3.905614137649536\n","Epoch 97/2000, Step 48, d_loss: 0.37984925508499146, g_loss: 4.014667987823486\n","Epoch 97/2000, Step 49, d_loss: 0.37574267387390137, g_loss: 2.9119484424591064\n","Epoch 97/2000, Step 50, d_loss: 0.3628321886062622, g_loss: 5.08799409866333\n","Epoch 97/2000, Step 51, d_loss: 0.3848227560520172, g_loss: 4.139981269836426\n","Epoch 97/2000, Step 52, d_loss: 0.40299704670906067, g_loss: 4.075502872467041\n","Epoch 97/2000, Step 53, d_loss: 0.3589436411857605, g_loss: 4.783545970916748\n","Epoch 97/2000, Step 54, d_loss: 0.364533931016922, g_loss: 4.963893413543701\n","Epoch 97/2000, Step 55, d_loss: 0.3595525324344635, g_loss: 4.146978378295898\n","Epoch 97/2000, Step 56, d_loss: 0.36235255002975464, g_loss: 4.458160877227783\n","Epoch 97/2000, Step 57, d_loss: 0.3562033772468567, g_loss: 5.289973735809326\n","Epoch 97/2000, Step 58, d_loss: 0.3493345379829407, g_loss: 6.023265838623047\n","Epoch 97/2000, Step 59, d_loss: 0.3432233929634094, g_loss: 4.902650833129883\n","Epoch 97/2000, Step 60, d_loss: 0.4356887936592102, g_loss: 4.036998748779297\n","Epoch 97/2000, Step 61, d_loss: 0.35175228118896484, g_loss: 5.162165641784668\n","Epoch 97/2000, Step 62, d_loss: 0.3662688732147217, g_loss: 4.344866752624512\n","Epoch 97/2000, Step 63, d_loss: 0.3523235023021698, g_loss: 3.4365434646606445\n","Epoch 97/2000, Step 64, d_loss: 0.3825221657752991, g_loss: 3.546010971069336\n","Epoch 97/2000, Step 65, d_loss: 0.48640871047973633, g_loss: 3.0942270755767822\n","Epoch 97/2000, Step 66, d_loss: 0.43873289227485657, g_loss: 3.463597059249878\n","Epoch 97/2000, Step 67, d_loss: 0.3561614453792572, g_loss: 4.632386207580566\n","Epoch 97/2000, Step 68, d_loss: 0.4237540364265442, g_loss: 3.826984405517578\n","Epoch 97/2000, Step 69, d_loss: 0.36530160903930664, g_loss: 5.262657165527344\n","Epoch 97/2000, Step 70, d_loss: 0.639943540096283, g_loss: 4.798211574554443\n","Epoch 97/2000, Step 71, d_loss: 0.4101827144622803, g_loss: 6.275120258331299\n","Epoch 97/2000, Step 72, d_loss: 0.38299334049224854, g_loss: 4.351009845733643\n","Epoch 97/2000, Step 73, d_loss: 0.42657899856567383, g_loss: 4.006173610687256\n","Epoch 97/2000, Step 74, d_loss: 0.38957712054252625, g_loss: 3.3815276622772217\n","Epoch 97/2000, Step 75, d_loss: 0.41527724266052246, g_loss: 3.63753342628479\n","Epoch 97/2000, Step 76, d_loss: 0.35615915060043335, g_loss: 3.658052921295166\n","Epoch 97/2000, Step 77, d_loss: 0.37291011214256287, g_loss: 4.370771408081055\n","Epoch 97/2000, Step 78, d_loss: 0.35997289419174194, g_loss: 4.37673807144165\n","Epoch 97/2000, Step 79, d_loss: 0.36430129408836365, g_loss: 3.977536916732788\n","Epoch 97/2000, Step 80, d_loss: 0.36037471890449524, g_loss: 4.579517364501953\n","Epoch 97/2000, Step 81, d_loss: 0.42200157046318054, g_loss: 3.5418455600738525\n","Epoch 97/2000, Step 82, d_loss: 0.37666916847229004, g_loss: 5.066891670227051\n","Epoch 97/2000, Step 83, d_loss: 0.37564122676849365, g_loss: 3.787912368774414\n","Epoch 97/2000, Step 84, d_loss: 0.3496670424938202, g_loss: 4.173014163970947\n","Epoch 97/2000, Step 85, d_loss: 0.42064666748046875, g_loss: 4.188488960266113\n","Epoch 97/2000, Step 86, d_loss: 0.34516242146492004, g_loss: 3.695556163787842\n","Epoch 97/2000, Step 87, d_loss: 0.41461166739463806, g_loss: 4.6041059494018555\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 98/2000, Step 1, d_loss: 0.40750089287757874, g_loss: 3.51912522315979\n","Epoch 98/2000, Step 2, d_loss: 0.3684784173965454, g_loss: 3.980311632156372\n","Epoch 98/2000, Step 3, d_loss: 0.377708375453949, g_loss: 4.231613636016846\n","Epoch 98/2000, Step 4, d_loss: 0.3526557385921478, g_loss: 4.166906356811523\n","Epoch 98/2000, Step 5, d_loss: 0.3554372191429138, g_loss: 5.112258434295654\n","Epoch 98/2000, Step 6, d_loss: 0.3891723155975342, g_loss: 4.476404190063477\n","Epoch 98/2000, Step 7, d_loss: 0.3480892479419708, g_loss: 4.459187984466553\n","Epoch 98/2000, Step 8, d_loss: 0.3793865442276001, g_loss: 3.6940526962280273\n","Epoch 98/2000, Step 9, d_loss: 0.4056764543056488, g_loss: 3.86464262008667\n","Epoch 98/2000, Step 10, d_loss: 0.38259357213974, g_loss: 3.7959866523742676\n","Epoch 98/2000, Step 11, d_loss: 0.3524858355522156, g_loss: 5.147604465484619\n","Epoch 98/2000, Step 12, d_loss: 0.3648113012313843, g_loss: 4.3193488121032715\n","Epoch 98/2000, Step 13, d_loss: 0.3494853675365448, g_loss: 3.947268486022949\n","Epoch 98/2000, Step 14, d_loss: 0.3474319577217102, g_loss: 4.29871129989624\n","Epoch 98/2000, Step 15, d_loss: 0.36280757188796997, g_loss: 3.885077953338623\n","Epoch 98/2000, Step 16, d_loss: 0.5598320960998535, g_loss: 5.37994909286499\n","Epoch 98/2000, Step 17, d_loss: 0.38720595836639404, g_loss: 2.513035535812378\n","Epoch 98/2000, Step 18, d_loss: 0.37699347734451294, g_loss: 4.041821002960205\n","Epoch 98/2000, Step 19, d_loss: 0.4397798776626587, g_loss: 3.7236125469207764\n","Epoch 98/2000, Step 20, d_loss: 0.42302438616752625, g_loss: 4.910197734832764\n","Epoch 98/2000, Step 21, d_loss: 0.42419296503067017, g_loss: 4.441451072692871\n","Epoch 98/2000, Step 22, d_loss: 0.3905044496059418, g_loss: 3.7902634143829346\n","Epoch 98/2000, Step 23, d_loss: 0.36605143547058105, g_loss: 4.6101393699646\n","Epoch 98/2000, Step 24, d_loss: 0.34919512271881104, g_loss: 4.041698932647705\n","Epoch 98/2000, Step 25, d_loss: 0.3996216952800751, g_loss: 4.806990623474121\n","Epoch 98/2000, Step 26, d_loss: 0.3641507029533386, g_loss: 4.272913455963135\n","Epoch 98/2000, Step 27, d_loss: 0.357992023229599, g_loss: 4.26450252532959\n","Epoch 98/2000, Step 28, d_loss: 0.3722723126411438, g_loss: 5.136235237121582\n","Epoch 98/2000, Step 29, d_loss: 0.3431290090084076, g_loss: 3.8031957149505615\n","Epoch 98/2000, Step 30, d_loss: 0.3496817946434021, g_loss: 3.344350814819336\n","Epoch 98/2000, Step 31, d_loss: 0.38038939237594604, g_loss: 4.391091823577881\n","Epoch 98/2000, Step 32, d_loss: 0.39886274933815, g_loss: 3.9796013832092285\n","Epoch 98/2000, Step 33, d_loss: 0.4322185516357422, g_loss: 4.016302585601807\n","Epoch 98/2000, Step 34, d_loss: 0.3829757571220398, g_loss: 3.461907386779785\n","Epoch 98/2000, Step 35, d_loss: 0.39925265312194824, g_loss: 3.7304272651672363\n","Epoch 98/2000, Step 36, d_loss: 0.3721041679382324, g_loss: 5.015422344207764\n","Epoch 98/2000, Step 37, d_loss: 0.35851219296455383, g_loss: 3.932750701904297\n","Epoch 98/2000, Step 38, d_loss: 0.38307324051856995, g_loss: 6.260960578918457\n","Epoch 98/2000, Step 39, d_loss: 0.3501295745372772, g_loss: 4.975210189819336\n","Epoch 98/2000, Step 40, d_loss: 0.3770200312137604, g_loss: 4.203452110290527\n","Epoch 98/2000, Step 41, d_loss: 0.4148775339126587, g_loss: 4.108548164367676\n","Epoch 98/2000, Step 42, d_loss: 0.39717423915863037, g_loss: 3.638404130935669\n","Epoch 98/2000, Step 43, d_loss: 0.4147236943244934, g_loss: 3.4008257389068604\n","Epoch 98/2000, Step 44, d_loss: 0.3569837212562561, g_loss: 3.4103987216949463\n","Epoch 98/2000, Step 45, d_loss: 0.3745499551296234, g_loss: 3.2335212230682373\n","Epoch 98/2000, Step 46, d_loss: 0.3718855679035187, g_loss: 3.861116409301758\n","Epoch 98/2000, Step 47, d_loss: 0.3674105405807495, g_loss: 4.766942977905273\n","Epoch 98/2000, Step 48, d_loss: 0.37349188327789307, g_loss: 3.866945743560791\n","Epoch 98/2000, Step 49, d_loss: 0.5057438015937805, g_loss: 3.9854280948638916\n","Epoch 98/2000, Step 50, d_loss: 0.35135582089424133, g_loss: 3.858065128326416\n","Epoch 98/2000, Step 51, d_loss: 0.37193962931632996, g_loss: 3.9592599868774414\n","Epoch 98/2000, Step 52, d_loss: 0.40964746475219727, g_loss: 3.2870218753814697\n","Epoch 98/2000, Step 53, d_loss: 0.3941933512687683, g_loss: 3.8131942749023438\n","Epoch 98/2000, Step 54, d_loss: 0.34950903058052063, g_loss: 2.751199960708618\n","Epoch 98/2000, Step 55, d_loss: 0.40685194730758667, g_loss: 3.828233242034912\n","Epoch 98/2000, Step 56, d_loss: 0.4260965585708618, g_loss: 4.732942581176758\n","Epoch 98/2000, Step 57, d_loss: 0.3864445090293884, g_loss: 5.076049327850342\n","Epoch 98/2000, Step 58, d_loss: 0.41359958052635193, g_loss: 5.024281978607178\n","Epoch 98/2000, Step 59, d_loss: 0.4146203100681305, g_loss: 6.61201286315918\n","Epoch 98/2000, Step 60, d_loss: 0.3966329097747803, g_loss: 4.900243759155273\n","Epoch 98/2000, Step 61, d_loss: 0.42199283838272095, g_loss: 4.163016319274902\n","Epoch 98/2000, Step 62, d_loss: 0.35991036891937256, g_loss: 4.322255611419678\n","Epoch 98/2000, Step 63, d_loss: 0.3662777543067932, g_loss: 4.227419853210449\n","Epoch 98/2000, Step 64, d_loss: 0.37301793694496155, g_loss: 4.354680061340332\n","Epoch 98/2000, Step 65, d_loss: 0.37515684962272644, g_loss: 3.931668281555176\n","Epoch 98/2000, Step 66, d_loss: 0.37268370389938354, g_loss: 3.2102978229522705\n","Epoch 98/2000, Step 67, d_loss: 0.3963252305984497, g_loss: 4.700287342071533\n","Epoch 98/2000, Step 68, d_loss: 0.3791544437408447, g_loss: 4.4691572189331055\n","Epoch 98/2000, Step 69, d_loss: 0.38862699270248413, g_loss: 4.8028740882873535\n","Epoch 98/2000, Step 70, d_loss: 0.4214872419834137, g_loss: 2.9147369861602783\n","Epoch 98/2000, Step 71, d_loss: 0.36561012268066406, g_loss: 3.731908082962036\n","Epoch 98/2000, Step 72, d_loss: 0.37922993302345276, g_loss: 4.274496078491211\n","Epoch 98/2000, Step 73, d_loss: 0.38117343187332153, g_loss: 3.8175714015960693\n","Epoch 98/2000, Step 74, d_loss: 0.4337032437324524, g_loss: 4.871846675872803\n","Epoch 98/2000, Step 75, d_loss: 0.406058132648468, g_loss: 3.703625440597534\n","Epoch 98/2000, Step 76, d_loss: 0.3679945766925812, g_loss: 5.201921463012695\n","Epoch 98/2000, Step 77, d_loss: 0.535531222820282, g_loss: 6.554323673248291\n","Epoch 98/2000, Step 78, d_loss: 0.4746920168399811, g_loss: 3.4660427570343018\n","Epoch 98/2000, Step 79, d_loss: 0.38663455843925476, g_loss: 2.6161768436431885\n","Epoch 98/2000, Step 80, d_loss: 0.38398313522338867, g_loss: 3.3588812351226807\n","Epoch 98/2000, Step 81, d_loss: 0.4898814857006073, g_loss: 3.0954267978668213\n","Epoch 98/2000, Step 82, d_loss: 0.4455493092536926, g_loss: 2.6687161922454834\n","Epoch 98/2000, Step 83, d_loss: 0.4076484441757202, g_loss: 4.441078186035156\n","Epoch 98/2000, Step 84, d_loss: 0.3599177598953247, g_loss: 3.792764186859131\n","Epoch 98/2000, Step 85, d_loss: 0.3448067903518677, g_loss: 6.748197555541992\n","Epoch 98/2000, Step 86, d_loss: 0.4668194651603699, g_loss: 5.52055549621582\n","Epoch 98/2000, Step 87, d_loss: 0.5012214183807373, g_loss: 3.757199287414551\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 99/2000, Step 1, d_loss: 0.3928907811641693, g_loss: 3.0107297897338867\n","Epoch 99/2000, Step 2, d_loss: 0.4436464309692383, g_loss: 3.040583848953247\n","Epoch 99/2000, Step 3, d_loss: 0.543307900428772, g_loss: 4.8568196296691895\n","Epoch 99/2000, Step 4, d_loss: 0.4465744197368622, g_loss: 3.670107364654541\n","Epoch 99/2000, Step 5, d_loss: 0.4458851218223572, g_loss: 2.8478848934173584\n","Epoch 99/2000, Step 6, d_loss: 0.3694186806678772, g_loss: 5.193780899047852\n","Epoch 99/2000, Step 7, d_loss: 0.4820111095905304, g_loss: 6.018852710723877\n","Epoch 99/2000, Step 8, d_loss: 0.4987359046936035, g_loss: 3.626095771789551\n","Epoch 99/2000, Step 9, d_loss: 0.3966681957244873, g_loss: 3.616060495376587\n","Epoch 99/2000, Step 10, d_loss: 0.390274316072464, g_loss: 2.145799398422241\n","Epoch 99/2000, Step 11, d_loss: 0.5344001650810242, g_loss: 4.262978553771973\n","Epoch 99/2000, Step 12, d_loss: 0.36928457021713257, g_loss: 3.196932077407837\n","Epoch 99/2000, Step 13, d_loss: 0.35471031069755554, g_loss: 3.149169921875\n","Epoch 99/2000, Step 14, d_loss: 0.3653554916381836, g_loss: 3.283130168914795\n","Epoch 99/2000, Step 15, d_loss: 0.355898380279541, g_loss: 3.896766185760498\n","Epoch 99/2000, Step 16, d_loss: 0.4752841591835022, g_loss: 5.398819923400879\n","Epoch 99/2000, Step 17, d_loss: 0.3686322867870331, g_loss: 5.056425094604492\n","Epoch 99/2000, Step 18, d_loss: 0.3604241907596588, g_loss: 3.6990692615509033\n","Epoch 99/2000, Step 19, d_loss: 0.3426010012626648, g_loss: 3.6535165309906006\n","Epoch 99/2000, Step 20, d_loss: 0.40601813793182373, g_loss: 4.323754787445068\n","Epoch 99/2000, Step 21, d_loss: 0.35356268286705017, g_loss: 4.3385396003723145\n","Epoch 99/2000, Step 22, d_loss: 0.3871859312057495, g_loss: 4.416496753692627\n","Epoch 99/2000, Step 23, d_loss: 0.3974386155605316, g_loss: 3.200479507446289\n","Epoch 99/2000, Step 24, d_loss: 0.41045525670051575, g_loss: 3.751096725463867\n","Epoch 99/2000, Step 25, d_loss: 0.48425430059432983, g_loss: 4.0405659675598145\n","Epoch 99/2000, Step 26, d_loss: 0.4030682146549225, g_loss: 2.968994379043579\n","Epoch 99/2000, Step 27, d_loss: 0.3729376494884491, g_loss: 3.1402804851531982\n","Epoch 99/2000, Step 28, d_loss: 0.446425199508667, g_loss: 3.851046562194824\n","Epoch 99/2000, Step 29, d_loss: 0.38277268409729004, g_loss: 5.573034763336182\n","Epoch 99/2000, Step 30, d_loss: 0.39317259192466736, g_loss: 2.9165115356445312\n","Epoch 99/2000, Step 31, d_loss: 0.36268672347068787, g_loss: 3.899334669113159\n","Epoch 99/2000, Step 32, d_loss: 0.370417058467865, g_loss: 3.658695697784424\n","Epoch 99/2000, Step 33, d_loss: 0.4162605404853821, g_loss: 3.529404878616333\n","Epoch 99/2000, Step 34, d_loss: 0.366567999124527, g_loss: 2.846041440963745\n","Epoch 99/2000, Step 35, d_loss: 0.3743225336074829, g_loss: 4.211093902587891\n","Epoch 99/2000, Step 36, d_loss: 0.38747069239616394, g_loss: 3.756579875946045\n","Epoch 99/2000, Step 37, d_loss: 0.39786046743392944, g_loss: 4.1431193351745605\n","Epoch 99/2000, Step 38, d_loss: 0.37724632024765015, g_loss: 4.250303268432617\n","Epoch 99/2000, Step 39, d_loss: 0.3783721923828125, g_loss: 4.569127082824707\n","Epoch 99/2000, Step 40, d_loss: 0.36546725034713745, g_loss: 5.507130146026611\n","Epoch 99/2000, Step 41, d_loss: 0.5157851576805115, g_loss: 4.651366233825684\n","Epoch 99/2000, Step 42, d_loss: 0.3673386871814728, g_loss: 2.6666669845581055\n","Epoch 99/2000, Step 43, d_loss: 0.4000382125377655, g_loss: 2.776817560195923\n","Epoch 99/2000, Step 44, d_loss: 0.3687047064304352, g_loss: 1.965063452720642\n","Epoch 99/2000, Step 45, d_loss: 0.4975782036781311, g_loss: 3.615077257156372\n","Epoch 99/2000, Step 46, d_loss: 0.42106202244758606, g_loss: 3.8970158100128174\n","Epoch 99/2000, Step 47, d_loss: 0.49954038858413696, g_loss: 2.4865565299987793\n","Epoch 99/2000, Step 48, d_loss: 0.47388118505477905, g_loss: 3.9809441566467285\n","Epoch 99/2000, Step 49, d_loss: 0.3736650347709656, g_loss: 4.702458381652832\n","Epoch 99/2000, Step 50, d_loss: 0.48365968465805054, g_loss: 5.734838485717773\n","Epoch 99/2000, Step 51, d_loss: 0.5522268414497375, g_loss: 3.406679153442383\n","Epoch 99/2000, Step 52, d_loss: 0.384693443775177, g_loss: 3.1442339420318604\n","Epoch 99/2000, Step 53, d_loss: 0.39611300826072693, g_loss: 3.4373254776000977\n","Epoch 99/2000, Step 54, d_loss: 0.42342597246170044, g_loss: 2.981060743331909\n","Epoch 99/2000, Step 55, d_loss: 0.4658474624156952, g_loss: 4.233758926391602\n","Epoch 99/2000, Step 56, d_loss: 0.41710445284843445, g_loss: 3.85331130027771\n","Epoch 99/2000, Step 57, d_loss: 0.3938036859035492, g_loss: 4.155160903930664\n","Epoch 99/2000, Step 58, d_loss: 0.4258226156234741, g_loss: 4.0606465339660645\n","Epoch 99/2000, Step 59, d_loss: 0.5184714794158936, g_loss: 4.972221851348877\n","Epoch 99/2000, Step 60, d_loss: 0.4732876718044281, g_loss: 4.325179100036621\n","Epoch 99/2000, Step 61, d_loss: 0.38199231028556824, g_loss: 2.580554246902466\n","Epoch 99/2000, Step 62, d_loss: 0.4694375991821289, g_loss: 2.1484270095825195\n","Epoch 99/2000, Step 63, d_loss: 0.48408403992652893, g_loss: 2.115854501724243\n","Epoch 99/2000, Step 64, d_loss: 0.4494389295578003, g_loss: 2.543748617172241\n","Epoch 99/2000, Step 65, d_loss: 0.45917409658432007, g_loss: 4.543417453765869\n","Epoch 99/2000, Step 66, d_loss: 0.43504196405410767, g_loss: 5.519996643066406\n","Epoch 99/2000, Step 67, d_loss: 0.3858220875263214, g_loss: 5.024390697479248\n","Epoch 99/2000, Step 68, d_loss: 0.3877348005771637, g_loss: 4.758945941925049\n","Epoch 99/2000, Step 69, d_loss: 0.48614534735679626, g_loss: 4.315365314483643\n","Epoch 99/2000, Step 70, d_loss: 0.40088629722595215, g_loss: 5.490409851074219\n","Epoch 99/2000, Step 71, d_loss: 0.47881871461868286, g_loss: 3.9549760818481445\n","Epoch 99/2000, Step 72, d_loss: 0.3868240714073181, g_loss: 3.067326545715332\n","Epoch 99/2000, Step 73, d_loss: 0.4483823776245117, g_loss: 4.111557960510254\n","Epoch 99/2000, Step 74, d_loss: 0.3784865438938141, g_loss: 2.506263017654419\n","Epoch 99/2000, Step 75, d_loss: 0.35194820165634155, g_loss: 4.0616536140441895\n","Epoch 99/2000, Step 76, d_loss: 0.38457435369491577, g_loss: 3.5251951217651367\n","Epoch 99/2000, Step 77, d_loss: 0.38566797971725464, g_loss: 3.312547445297241\n","Epoch 99/2000, Step 78, d_loss: 0.3889627456665039, g_loss: 5.37191104888916\n","Epoch 99/2000, Step 79, d_loss: 0.3791193962097168, g_loss: 4.012131690979004\n","Epoch 99/2000, Step 80, d_loss: 0.39086881279945374, g_loss: 4.022589206695557\n","Epoch 99/2000, Step 81, d_loss: 0.3634714186191559, g_loss: 3.802497148513794\n","Epoch 99/2000, Step 82, d_loss: 0.3763159215450287, g_loss: 3.59478497505188\n","Epoch 99/2000, Step 83, d_loss: 0.3836153447628021, g_loss: 3.742539167404175\n","Epoch 99/2000, Step 84, d_loss: 0.3481305241584778, g_loss: 2.961512804031372\n","Epoch 99/2000, Step 85, d_loss: 0.37920719385147095, g_loss: 4.0343017578125\n","Epoch 99/2000, Step 86, d_loss: 0.39654216170310974, g_loss: 3.424030065536499\n","Epoch 99/2000, Step 87, d_loss: 0.4040288031101227, g_loss: 3.0569138526916504\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 100/2000, Step 1, d_loss: 0.37538161873817444, g_loss: 5.186385154724121\n","Epoch 100/2000, Step 2, d_loss: 0.38900837302207947, g_loss: 4.542787075042725\n","Epoch 100/2000, Step 3, d_loss: 0.3986067771911621, g_loss: 4.604313373565674\n","Epoch 100/2000, Step 4, d_loss: 0.5933054089546204, g_loss: 4.207347869873047\n","Epoch 100/2000, Step 5, d_loss: 0.36518096923828125, g_loss: 4.633686542510986\n","Epoch 100/2000, Step 6, d_loss: 0.3983195424079895, g_loss: 4.386072635650635\n","Epoch 100/2000, Step 7, d_loss: 0.38736891746520996, g_loss: 4.004893779754639\n","Epoch 100/2000, Step 8, d_loss: 0.4557892680168152, g_loss: 3.8490679264068604\n","Epoch 100/2000, Step 9, d_loss: 0.3817220628261566, g_loss: 3.912254571914673\n","Epoch 100/2000, Step 10, d_loss: 0.36196550726890564, g_loss: 6.765190601348877\n","Epoch 100/2000, Step 11, d_loss: 0.4807792901992798, g_loss: 4.550528526306152\n","Epoch 100/2000, Step 12, d_loss: 0.41495072841644287, g_loss: 4.001094341278076\n","Epoch 100/2000, Step 13, d_loss: 0.38003161549568176, g_loss: 4.236428737640381\n","Epoch 100/2000, Step 14, d_loss: 0.4113205373287201, g_loss: 5.504568099975586\n","Epoch 100/2000, Step 15, d_loss: 0.3847445249557495, g_loss: 3.75085711479187\n","Epoch 100/2000, Step 16, d_loss: 0.42450594902038574, g_loss: 4.732837200164795\n","Epoch 100/2000, Step 17, d_loss: 0.3812655210494995, g_loss: 3.865100622177124\n","Epoch 100/2000, Step 18, d_loss: 0.3720463216304779, g_loss: 4.063958168029785\n","Epoch 100/2000, Step 19, d_loss: 0.4222766160964966, g_loss: 4.364800453186035\n","Epoch 100/2000, Step 20, d_loss: 0.39556798338890076, g_loss: 3.9616355895996094\n","Epoch 100/2000, Step 21, d_loss: 0.372355580329895, g_loss: 4.6399827003479\n","Epoch 100/2000, Step 22, d_loss: 0.36852142214775085, g_loss: 3.71842098236084\n","Epoch 100/2000, Step 23, d_loss: 0.36429476737976074, g_loss: 4.0176568031311035\n","Epoch 100/2000, Step 24, d_loss: 0.3632809519767761, g_loss: 3.0965981483459473\n","Epoch 100/2000, Step 25, d_loss: 0.35262537002563477, g_loss: 4.866246223449707\n","Epoch 100/2000, Step 26, d_loss: 0.37009942531585693, g_loss: 4.7269134521484375\n","Epoch 100/2000, Step 27, d_loss: 0.35381367802619934, g_loss: 3.760643720626831\n","Epoch 100/2000, Step 28, d_loss: 0.3695010542869568, g_loss: 3.6273722648620605\n","Epoch 100/2000, Step 29, d_loss: 0.3733901083469391, g_loss: 4.249670028686523\n","Epoch 100/2000, Step 30, d_loss: 0.3747234046459198, g_loss: 3.812208890914917\n","Epoch 100/2000, Step 31, d_loss: 0.3671407997608185, g_loss: 2.934591770172119\n","Epoch 100/2000, Step 32, d_loss: 0.4102748930454254, g_loss: 4.278401851654053\n","Epoch 100/2000, Step 33, d_loss: 0.3945819139480591, g_loss: 4.880487442016602\n","Epoch 100/2000, Step 34, d_loss: 0.3499200940132141, g_loss: 3.9828720092773438\n","Epoch 100/2000, Step 35, d_loss: 0.39586636424064636, g_loss: 4.3818230628967285\n","Epoch 100/2000, Step 36, d_loss: 0.3479522168636322, g_loss: 3.492349624633789\n","Epoch 100/2000, Step 37, d_loss: 0.3837500810623169, g_loss: 4.771280765533447\n","Epoch 100/2000, Step 38, d_loss: 0.3642461597919464, g_loss: 4.180069923400879\n","Epoch 100/2000, Step 39, d_loss: 0.38043731451034546, g_loss: 3.581268310546875\n","Epoch 100/2000, Step 40, d_loss: 0.4210513234138489, g_loss: 4.908267498016357\n","Epoch 100/2000, Step 41, d_loss: 0.43572762608528137, g_loss: 4.953464508056641\n","Epoch 100/2000, Step 42, d_loss: 0.38183847069740295, g_loss: 4.964439868927002\n","Epoch 100/2000, Step 43, d_loss: 0.3721085786819458, g_loss: 4.7446608543396\n","Epoch 100/2000, Step 44, d_loss: 0.3742426037788391, g_loss: 5.188399314880371\n","Epoch 100/2000, Step 45, d_loss: 0.3647409677505493, g_loss: 5.614985466003418\n","Epoch 100/2000, Step 46, d_loss: 0.36072251200675964, g_loss: 5.097869396209717\n","Epoch 100/2000, Step 47, d_loss: 0.349256694316864, g_loss: 4.418179988861084\n","Epoch 100/2000, Step 48, d_loss: 0.36676764488220215, g_loss: 4.273594379425049\n","Epoch 100/2000, Step 49, d_loss: 0.36030563712120056, g_loss: 3.8605148792266846\n","Epoch 100/2000, Step 50, d_loss: 0.38021886348724365, g_loss: 4.589709281921387\n","Epoch 100/2000, Step 51, d_loss: 0.3560212552547455, g_loss: 3.900700569152832\n","Epoch 100/2000, Step 52, d_loss: 0.3459916114807129, g_loss: 3.3773696422576904\n","Epoch 100/2000, Step 53, d_loss: 0.38344520330429077, g_loss: 3.771883249282837\n","Epoch 100/2000, Step 54, d_loss: 0.37559568881988525, g_loss: 4.247091293334961\n","Epoch 100/2000, Step 55, d_loss: 0.37162643671035767, g_loss: 5.216217994689941\n","Epoch 100/2000, Step 56, d_loss: 0.3537220358848572, g_loss: 4.425508975982666\n","Epoch 100/2000, Step 57, d_loss: 0.3744087517261505, g_loss: 4.222923278808594\n","Epoch 100/2000, Step 58, d_loss: 0.36203184723854065, g_loss: 7.976898193359375\n","Epoch 100/2000, Step 59, d_loss: 0.3787667751312256, g_loss: 4.955783367156982\n","Epoch 100/2000, Step 60, d_loss: 0.3730847239494324, g_loss: 6.889890670776367\n","Epoch 100/2000, Step 61, d_loss: 0.3886481821537018, g_loss: 4.744626045227051\n","Epoch 100/2000, Step 62, d_loss: 0.4089420437812805, g_loss: 3.350639820098877\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print (device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 32\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 1\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 32\n","\n","# Size of feature maps in discriminator\n","ndf = 8\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/Abstract_gallery/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/ag2/',\n","                                   transform=transforms.Compose([\n","                                   transforms.Grayscale(num_output_channels=1), # Add this line\n","                                   transforms.Resize(image_size),\n","                                   transforms.CenterCrop(image_size),\n","                                   transforms.ToTensor(),\n","                                   transforms.Normalize((0.5,), (0.5,))\n","                               ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # input is ``(nc) x 64 x 64``\n","            nn.Conv2d(1, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Conv2d(ndf , ndf * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 4, 1, 3, 1, 0, bias=False),\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input).squeeze()\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0015)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='G:/My Drive/Colab Notebooks/animals/checkpointAG.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","\n","\n","            save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/ArcwrideG_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","                current_batch_size = images.size(0)\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((current_batch_size,), 0.9))\n","\n","\n","\n","\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","\n","                d_loss_real = self.loss(real_outputs.squeeze(0), real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(current_batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images).view(-1).squeeze()\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointAG.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(2000, dataloader, resume=False)"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7TZvDVQTtN6","executionInfo":{"status":"ok","timestamp":1690605787603,"user_tz":240,"elapsed":19,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"}},"outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAA0UAAAHDCAYAAADr8bFZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5WElEQVR4nO3de1xVVf7/8TegHBQENeSmCKmleUPTgcHLmIXxLbWonMz6Kjqmk3lJ6aJmillJpZkzqVl2c2xMS83poQ6pqI+m4pszXprMS+PdnEDRBMILCuv3hz9OHgHlIBd1vZ6Px/njrLP23p+zzzqH82bvvY6HMcYIAAAAACzlWd0FAAAAAEB1IhQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAFABdmwYYM8PDy0YcOGCl/35MmT5eHhUeHrvZT9+/fLw8NDH3zwQYWtszL3Ea5elTGWriZX8v784IMP5OHhof3791dsUQDcQigCrgL79u3TiBEjdPPNN6t27dqqXbu2WrZsqeHDh+vf//53dZdXoVatWqXJkydXdxnVquhLUNHNx8dHYWFhio+P15///Gfl5uZWd4nXtJMnT2ry5MlVGryKwt6SJUuqbJvlYdvYi4yMdHm+pd2u17AGoOw8jDGmuosAbLZixQr17dtXNWrU0COPPKKoqCh5enpq586dWrZsmQ4cOKB9+/YpIiKiukutECNGjNDs2bN1PX70bNiwQd27d9f69et12223ldrvgw8+0KBBgzRlyhTdeOONOnv2rDIyMrRhwwatWbNGjRs31meffaa2bds6lzl37pzOnTsnHx+fKngm5xljdObMGdWsWVNeXl4Vss7CwkLl5+fL29tbnp6V83+5rKwsNWjQQMnJyVUWwIte+08++UR9+vSpkm2WR3nGXkWojLFUFsuXL9cvv/zivL9q1Sp99NFHev311xUYGOhs79Spk5o0aVLu7VzJ+7OgoEBnz56Vw+Go8qPBAH5Vo7oLAGy2Z88ePfTQQ4qIiFBaWppCQ0NdHn/llVc0Z86cSvvyWBHy8vLk6+tbrTUUfdGuysBQEe666y517NjReX/8+PFat26devXqpXvuuUc7duxQrVq1JEk1atRQjRpV85F97tw5FRYWytvbu8L3qaen5zX3OhW5GsZ6RXFn7F2JyhxLZZGQkOByPyMjQx999JESEhIUGRlZ6nLuvtZX8v708vKq0qAIoGRX7zctwAKvvvqq8vLy9P777xcLRNL5P7SjRo1SeHi4S/vOnTvVp08f1a9fXz4+PurYsaM+++wzlz5Fp8l89dVXSkpKUoMGDeTr66v77rtPR48eLbatv//97+ratat8fX1Vp04d9ezZU99//71Ln4EDB8rPz0979uzR3XffrTp16uiRRx6RJP3jH//Q73//ezVu3FgOh0Ph4eEaM2aMTp065bL87NmzJcnl1JUieXl5evLJJxUeHi6Hw6HmzZtr+vTpxY4qeXh4aMSIEfrrX/+qVq1ayeFwKDU1tdT9/Le//U09e/ZUWFiYHA6HmjZtqhdeeEEFBQUu/W677Ta1bt1a27dvV/fu3VW7dm01bNhQr776arF1/vjjj0pISJCvr6+CgoI0ZswYnTlzptQayur222/XxIkTdeDAAX344YfO9pKuWVizZo26dOmiunXrys/PT82bN9ezzz7r0uf06dOaPHmybr75Zvn4+Cg0NFT333+/9uzZI+nXaz2mT5+umTNnqmnTpnI4HNq+fXuJ14EUjYGDBw+qV69e8vPzU8OGDZ2v63fffafbb79dvr6+ioiI0MKFC13qKemaorLu9/z8fE2aNEkdOnRQQECAfH191bVrV61fv97ZZ//+/WrQoIEk6fnnn3eOsQuPGK1bt8451uvWrat7771XO3bscNlW0f7evn27Hn74YdWrV09dunS51EtXJnv37tXvf/971a9fX7Vr19Zvf/tbrVy5sli/N954Q61atVLt2rVVr149dezY0WVf5ubmavTo0YqMjJTD4VBQUJB69OihzZs3l7u20sbebbfdVuKRz4EDB7oEi/KOpcOHDyshIUF+fn5q0KCBnnrqqWLvzWPHjql///7y9/dX3bp1lZiYqG+//bZCTn270s81qeT3Z9Hn1PLly9W6dWs5HA61atWq2GdVSdcURUZGqlevXvryyy8VHR0tHx8fNWnSRH/5y1+K1f/vf/9b3bp1U61atdSoUSO9+OKLev/997lOCXATR4qAarRixQo1a9ZMMTExZV7m+++/V+fOndWwYUONGzdOvr6++vjjj5WQkKClS5fqvvvuc+k/cuRI1atXT8nJydq/f79mzpypESNGaPHixc4+CxYsUGJiouLj4/XKK6/o5MmTevPNN9WlSxdt2bLF5YvPuXPnFB8fry5dumj69OmqXbu2JOmTTz7RyZMnNWzYMN1www3auHGj3njjDf3444/65JNPJEl//OMf9d///ldr1qzRggULXOo0xuiee+7R+vXrNXjwYLVr106ff/65nn76aR0+fFivv/66S/9169bp448/1ogRIxQYGHjJ//p+8MEH8vPzU1JSkvz8/LRu3TpNmjRJOTk5mjZtmkvfn3/+Wf/zP/+j+++/Xw8++KCWLFmisWPHqk2bNrrrrrskSadOndIdd9yhgwcPatSoUQoLC9OCBQu0bt26sr2Il9G/f389++yzWr16tYYMGVJin++//169evVS27ZtNWXKFDkcDu3evVtfffWVs09BQYF69eqltLQ0PfTQQ3riiSeUm5urNWvWaNu2bWratKmz7/vvv6/Tp09r6NChcjgcql+/vgoLC0vcdkFBge666y797ne/06uvvqq//vWvGjFihHx9fTVhwgQ98sgjuv/++zV37lwNGDBAsbGxuvHGGy/5nMuy33NycvTOO++oX79+GjJkiHJzc/Xuu+8qPj5eGzduVLt27dSgQQO9+eabGjZsmO677z7df//9kuQ8HWzt2rW666671KRJE02ePFmnTp3SG2+8oc6dO2vz5s3FxtHvf/973XTTTZo6deoVn/KZmZmpTp066eTJkxo1apRuuOEGzZ8/X/fcc4+WLFnifO/OmzdPo0aNUp8+ffTEE0/o9OnT+ve//61vvvlGDz/8sCTpscce05IlSzRixAi1bNlSx44d05dffqkdO3bo1ltvLXeNZRl7l+PuWIqPj1dMTIymT5+utWvX6rXXXlPTpk01bNgwSeePBPfu3VsbN27UsGHD1KJFC/3tb39TYmJiuZ/nxa7kc+1SvvzySy1btkyPP/646tSpoz//+c964IEHdPDgQd1www2XXHb37t3q06ePBg8erMTERL333nsaOHCgOnTooFatWkmSDh8+rO7du8vDw0Pjx4+Xr6+v3nnnHTkcjivfKYBtDIBqkZ2dbSSZhISEYo/9/PPP5ujRo87byZMnnY/dcccdpk2bNub06dPOtsLCQtOpUydz0003Odvef/99I8nExcWZwsJCZ/uYMWOMl5eXOXHihDHGmNzcXFO3bl0zZMgQlxoyMjJMQECAS3tiYqKRZMaNG1es5gtrLJKSkmI8PDzMgQMHnG3Dhw83JX30LF++3EgyL774okt7nz59jIeHh9m9e7ezTZLx9PQ033//fbH1lKSk2v74xz+a2rVru+zHbt26GUnmL3/5i7PtzJkzJiQkxDzwwAPOtpkzZxpJ5uOPP3a25eXlmWbNmhlJZv369Zesp+i1+ec//1lqn4CAANO+fXvn/eTkZJf99vrrrxtJ5ujRo6Wu47333jOSzIwZM4o9VjQm9u3bZyQZf39/c+TIEZc+RY+9//77zraiMTB16lRn288//2xq1aplPDw8zKJFi5ztO3fuNJJMcnKys239+vXF9lFZ9/u5c+fMmTNnXGr8+eefTXBwsPnDH/7gbDt69Gix7RZp166dCQoKMseOHXO2ffvtt8bT09MMGDDA2Va0v/v161dsHSUpel6ffPJJqX1Gjx5tJJl//OMfzrbc3Fxz4403msjISFNQUGCMMebee+81rVq1uuT2AgICzPDhw8tU24XKM/a6detmunXrVqxfYmKiiYiIcN4v71iaMmWKS9/27dubDh06OO8vXbrUSDIzZ850thUUFJjbb7+92DovZ9q0aUaS2bdvX7E6ruRz7eL3pzHnP6e8vb1dPru+/fZbI8m88cYbzrai1+TCmiIiIowk88UXXzjbjhw5YhwOh3nyySedbSNHjjQeHh5my5YtzrZjx46Z+vXrF1sngEvj9DmgmuTk5EiS/Pz8ij122223qUGDBs5b0alJx48f17p16/Tggw8qNzdXWVlZysrK0rFjxxQfH6///Oc/Onz4sMu6hg4d6nJaR9euXVVQUKADBw5IOn8K1okTJ9SvXz/n+rKysuTl5aWYmBiXU5OKFP0H90IXXn+Ql5enrKwsderUScYYbdmy5bL7Y9WqVfLy8tKoUaNc2p988kkZY/T3v//dpb1bt25q2bLlZdd7cW1F+61r1646efKkdu7c6dLXz89P//u//+u87+3trejoaO3du9el1tDQUJcL6mvXrq2hQ4eWqZ6y8PPzu+RMYHXr1pV0/tTA0v4Lv3TpUgUGBmrkyJHFHrv4VJ8HHnjAedpZWTz66KMutTRv3ly+vr568MEHne3NmzdX3bp1XfZdacqy3728vOTt7S3p/NGD48eP69y5c+rYsWOZThv76aeftHXrVg0cOFD169d3trdt21Y9evTQqlWrii3z2GOPXXa9ZbVq1SpFR0e7nIbn5+enoUOHav/+/dq+fbuk8/vzxx9/1D//+c9S11W3bl198803+u9//1th9V1Y05XMQufuWLp4H3ft2tXldU9NTVXNmjVdjlx5enpq+PDh5a6xJJXxuRYXF+dyRLZt27by9/cv03uiZcuW6tq1q/N+gwYN1Lx582L7JjY2Vu3atXO21a9f33n6H4CyIxQB1aROnTqS5DIzUpG33npLa9ascTmvXzp/OoUxRhMnTnQJTUUzbUnSkSNHXJZp3Lixy/169epJOn+6kiT95z//kXT+eoKL17l69epi66tRo4YaNWpUrOaDBw86v2wWXRvQrVs3SVJ2dvZl98eBAwcUFhbm3C9FbrnlFufjF7rc6VgX+v7773XfffcpICBA/v7+atCggfML+MW1NWrUqFhgqFevnnN/FdXSrFmzYv2aN29e5pou55dffim2Ly7Ut29fde7cWY8++qiCg4P10EMP6eOPP3YJSHv27FHz5s3LdAG4O/vTx8en2JfegICAEvddQECAy74rTVn2uyTNnz9fbdu2lY+Pj2644QY1aNBAK1euLPMYk0p+nW655RZlZWUpLy/Ppd2d/VKW7Ze27QvrGzt2rPz8/BQdHa2bbrpJw4cPdzktUjp/PeK2bdsUHh6u6OhoTZ48uUxftMvicmPvcq50LJX0fgsNDXWe0lakWbNm5a7xYpX1uXbx569U8rgu77JFn0UXq8h9A9iCa4qAahIQEKDQ0FBt27at2GNF1xhdfJFs0Rfep556SvHx8SWu9+I/hqXNamT+//URRetcsGCBQkJCivW7+Au1w+EoNhteQUGBevTooePHj2vs2LFq0aKFfH19dfjwYQ0cOLDUIxlXoqwzY504cULdunWTv7+/pkyZoqZNm8rHx0ebN2/W2LFji9V2uf1VFX788UdlZ2df8otNrVq19MUXX2j9+vVauXKlUlNTtXjxYt1+++1avXq127NZuTPTWGnrvpJ9V5ZlP/zwQw0cOFAJCQl6+umnFRQUJC8vL6WkpDgnjqhoFTEDm7tuueUW7dq1SytWrFBqaqqWLl2qOXPmaNKkSXr++eclSQ8++KC6du2qTz/9VKtXr9a0adP0yiuvaNmyZc5rsMqjpLHn4eFR4mt48WQIRSpiLFW1yvpcq+z3BICKQygCqlHPnj31zjvvaOPGjYqOjr5s/6Lf0ahZs6bi4uIqpIaiUzuCgoLKvc7vvvtOP/zwg+bPn68BAwY429esWVOsb2m/wxEREaG1a9cqNzfX5b/URae3lfd3mjZs2KBjx45p2bJl+t3vfuds37dvX7nWV1TLtm3bZIxxeT67du0q9zovVDQJRWnBt4inp6fuuOMO3XHHHZoxY4amTp2qCRMmaP369c7Tdr755hudPXtWNWvWrJDaqtOSJUvUpEkTLVu2zGW/Fx0lLXKpMSaV/Drt3LlTgYGBlTrldkRERKnbvrA+SfL19VXfvn3Vt29f5efn6/7779dLL72k8ePHO6e2Dg0N1eOPP67HH39cR44c0a233qqXXnrpikJRSWOvXr16JR6FuvjobWWJiIjQ+vXrdfLkSZejRbt3767U7brzuVZdIiIiStwPlb1vgOsRp88B1eiZZ55R7dq19Yc//EGZmZnFHr/4P4JBQUG67bbb9NZbb+mnn34q1r+kqbYvJz4+Xv7+/po6darOnj1brnUW/UfzwnqNMfrTn/5UrG/Rl84TJ064tN99990qKCjQrFmzXNpff/11eXh4lPuLXkm15efna86cOeVaX1Gt//3vf7VkyRJn28mTJ/X222+Xe51F1q1bpxdeeEE33njjJa8LOH78eLG2ousKiqYGf+CBB5SVlVVsn0rX5n+bS3otv/nmG6Wnp7v0K/rifPEYCw0NVbt27TR//nyXx7Zt26bVq1fr7rvvrpzC/7+7775bGzdudKk3Ly9Pb7/9tiIjI53XyB07dsxlOW9vb7Vs2VLGGJ09e1YFBQXFTt0KCgpSWFjYFU0LX9rYa9q0qXbu3OnyWfDtt98WO6WvssTHx+vs2bOaN2+es62wsNB5rWVlcedzrbrEx8crPT1dW7dudbYdP35cf/3rX6uvKOAaxZEioBrddNNNWrhwofr166fmzZvrkUceUVRUlIwx2rdvnxYuXChPT0+Xc91nz56tLl26qE2bNhoyZIiaNGmizMxMpaen68cff9S3337rVg3+/v5688031b9/f91666166KGH1KBBAx08eFArV65U586dS/xSfaEWLVqoadOmeuqpp3T48GH5+/tr6dKlJZ4336FDB0nSqFGjFB8fLy8vLz300EPq3bu3unfvrgkTJmj//v2KiorS6tWr9be//U2jR492uVjZHZ06dVK9evWUmJioUaNGycPDQwsWLLiiUDBkyBDNmjVLAwYM0KZNmxQaGqoFCxYUu+bhcv7+979r586dOnfunDIzM7Vu3TqtWbNGERER+uyzzy75Y5dTpkzRF198oZ49eyoiIkJHjhzRnDlz1KhRI+eF/AMGDNBf/vIXJSUlaePGjeratavy8vK0du1aPf7447r33nvLvQ+qQ69evbRs2TLdd9996tmzp/bt26e5c+eqZcuWLtfm1apVSy1bttTixYt18803q379+mrdurVat26tadOm6a677lJsbKwGDx7snJI7ICDA5beMymvp0qXFJu+QpMTERI0bN04fffSR7rrrLo0aNUr169fX/PnztW/fPi1dutR5+tadd96pkJAQde7cWcHBwdqxY4dmzZqlnj17qk6dOjpx4oQaNWqkPn36KCoqSn5+flq7dq3++c9/6rXXXitTne6MvT/84Q+aMWOG4uPjNXjwYB05ckRz585Vq1atnBPGVKaEhARFR0frySef1O7du9WiRQt99tlnzn8MlHZk8Eq587lWXZ555hl9+OGH6tGjh0aOHOmckrtx48Y6fvx4pe0b4LpUhTPdASjF7t27zbBhw0yzZs2Mj4+PqVWrlmnRooV57LHHzNatW4v137NnjxkwYIAJCQkxNWvWNA0bNjS9evUyS5YscfYpberdkqZELmqPj483AQEBxsfHxzRt2tQMHDjQ/Otf/3L2SUxMNL6+viU+h+3bt5u4uDjj5+dnAgMDzZAhQ5zTz144Ze65c+fMyJEjTYMGDYyHh4fLNLa5ublmzJgxJiwszNSsWdPcdNNNZtq0aS5Tihtzfqpbd6Yj/uqrr8xvf/tbU6tWLRMWFmaeeeYZ8/nnn5c4NXRJUyFfPPWwMcYcOHDA3HPPPaZ27domMDDQPPHEEyY1NdWtKbmLbt7e3iYkJMT06NHD/OlPfzI5OTnFlrl4yt+0tDRz7733mrCwMOPt7W3CwsJMv379zA8//OCy3MmTJ82ECRPMjTfeaGrWrGlCQkJMnz59zJ49e4wxv06VPG3atGLbLG0a5ZLGQGn7LiIiwvTs2dN5v7Qpucuy3wsLC83UqVNNRESEcTgcpn379mbFihUlvj5ff/216dChg/H29i42PffatWtN586dTa1atYy/v7/p3bu32b59u8vyRfv7UlOeX6joeZV2K5qGe8+ePaZPnz6mbt26xsfHx0RHR5sVK1a4rOutt94yv/vd78wNN9xgHA6Hadq0qXn66adNdna2Meb8dOVPP/20iYqKMnXq1DG+vr4mKirKzJkz57J1lmfsGWPMhx9+aJo0aWK8vb1Nu3btzOeff17qlNxXOpZKmt766NGj5uGHHzZ16tQxAQEBZuDAgearr74yklymgb+c0qbkvtLPtdKm5C7pcyoiIsIkJiY675c2JfeF75siJU2PvmXLFtO1a1fjcDhMo0aNTEpKivnzn/9sJJmMjIzSdwYAFx7GXIPnUAAAAKstX75c9913n7788kt17ty5usu5qowePVpvvfWWfvnll6tmMgvgasc1RQAA4Kp26tQpl/sFBQV644035O/vr1tvvbWaqro6XLxvjh07pgULFqhLly4EIsANXFMEAACuaiNHjtSpU6cUGxurM2fOaNmyZfr66681derUapk2/WoSGxur2267TbfccosyMzP17rvvKicnRxMnTqzu0oBrCqfPAQCAq9rChQv12muvaffu3Tp9+rSaNWumYcOGacSIEdVdWrV79tlntWTJEv3444/y8PDQrbfequTk5Ar72QbAFm6Hoi+++ELTpk3Tpk2b9NNPP+nTTz9VQkLCJZfZsGGDkpKS9P333ys8PFzPPfecBg4ceAVlAwAAAEDFcPuaory8PEVFRZX59wH27dunnj17qnv37tq6datGjx6tRx99VJ9//rnbxQIAAABARbui0+c8PDwue6Ro7NixWrlypbZt2+Zse+ihh3TixAmlpqaWd9MAAAAAUCEqfaKF9PT0Yue1xsfHa/To0aUuc+bMGZdf5S4sLNTx48d1ww038ENkAAAAgMWMMcrNzVVYWJjzh6+vVKWHooyMDAUHB7u0BQcHKycnR6dOnSpx1piUlBQ9//zzlV0aAAAAgGvUoUOH1KhRowpZ11U5Jff48eOVlJTkvJ+dna3GjRvr0KFD8vf3r8bKAAAAAFSnnJwchYeHq06dOhW2zkoPRSEhIcrMzHRpy8zMlL+/f6m/LeBwOORwOIq1+/v7E4oAAAAAVOhlNRVzEt4lxMbGKi0tzaVtzZo1io2NrexNAwAAAMBluR2KfvnlF23dulVbt26VdH7K7a1bt+rgwYOSzp/6NmDAAGf/xx57THv37tUzzzyjnTt3as6cOfr44481ZsyYinkGAAAAAHAF3A5F//rXv9S+fXu1b99ekpSUlKT27dtr0qRJkqSffvrJGZAk6cYbb9TKlSu1Zs0aRUVF6bXXXtM777yj+Pj4CnoKAAAAAFB+V/Q7RVUlJydHAQEBys7O5poiAAAAwGKVkQ0q/ZoiAAAAALiaEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFitXKFo9uzZioyMlI+Pj2JiYrRx48ZL9p85c6aaN2+uWrVqKTw8XGPGjNHp06fLVTAAAAAAVCS3Q9HixYuVlJSk5ORkbd68WVFRUYqPj9eRI0dK7L9w4UKNGzdOycnJ2rFjh959910tXrxYzz777BUXDwAAAABXyu1QNGPGDA0ZMkSDBg1Sy5YtNXfuXNWuXVvvvfdeif2//vprde7cWQ8//LAiIyN15513ql+/fpc9ugQAAAAAVcGtUJSfn69NmzYpLi7u1xV4eiouLk7p6eklLtOpUydt2rTJGYL27t2rVatW6e677y51O2fOnFFOTo7LDQAAAAAqQw13OmdlZamgoEDBwcEu7cHBwdq5c2eJyzz88MPKyspSly5dZIzRuXPn9Nhjj13y9LmUlBQ9//zz7pQGAAAAAOVS6bPPbdiwQVOnTtWcOXO0efNmLVu2TCtXrtQLL7xQ6jLjx49Xdna283bo0KHKLhMAAACApdw6UhQYGCgvLy9lZma6tGdmZiokJKTEZSZOnKj+/fvr0UcflSS1adNGeXl5Gjp0qCZMmCBPz+K5zOFwyOFwuFMaAAAAAJSLW0eKvL291aFDB6WlpTnbCgsLlZaWptjY2BKXOXnyZLHg4+XlJUkyxrhbLwAAAABUKLeOFElSUlKSEhMT1bFjR0VHR2vmzJnKy8vToEGDJEkDBgxQw4YNlZKSIknq3bu3ZsyYofbt2ysmJka7d+/WxIkT1bt3b2c4AgAAAIDq4nYo6tu3r44ePapJkyYpIyND7dq1U2pqqnPyhYMHD7ocGXruuefk4eGh5557TocPH1aDBg3Uu3dvvfTSSxX3LAAAAACgnDzMNXAOW05OjgICApSdnS1/f//qLgcAAABANamMbFDps88BAAAAwNWMUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGrlCkWzZ89WZGSkfHx8FBMTo40bN16y/4kTJzR8+HCFhobK4XDo5ptv1qpVq8pVMAAAAABUpBruLrB48WIlJSVp7ty5iomJ0cyZMxUfH69du3YpKCioWP/8/Hz16NFDQUFBWrJkiRo2bKgDBw6obt26FVE/AAAAAFwRD2OMcWeBmJgY/eY3v9GsWbMkSYWFhQoPD9fIkSM1bty4Yv3nzp2radOmaefOnapZs2a5iszJyVFAQICys7Pl7+9frnUAAAAAuPZVRjZw6/S5/Px8bdq0SXFxcb+uwNNTcXFxSk9PL3GZzz77TLGxsRo+fLiCg4PVunVrTZ06VQUFBaVu58yZM8rJyXG5AQAAAEBlcCsUZWVlqaCgQMHBwS7twcHBysjIKHGZvXv3asmSJSooKNCqVas0ceJEvfbaa3rxxRdL3U5KSooCAgKct/DwcHfKBAAAAIAyq/TZ5woLCxUUFKS3335bHTp0UN++fTVhwgTNnTu31GXGjx+v7Oxs5+3QoUOVXSYAAAAAS7k10UJgYKC8vLyUmZnp0p6ZmamQkJASlwkNDVXNmjXl5eXlbLvllluUkZGh/Px8eXt7F1vG4XDI4XC4UxoAAAAAlItbR4q8vb3VoUMHpaWlOdsKCwuVlpam2NjYEpfp3Lmzdu/ercLCQmfbDz/8oNDQ0BIDEQAAAABUJbdPn0tKStK8efM0f/587dixQ8OGDVNeXp4GDRokSRowYIDGjx/v7D9s2DAdP35cTzzxhH744QetXLlSU6dO1fDhwyvuWQAAAABAObn9O0V9+/bV0aNHNWnSJGVkZKhdu3ZKTU11Tr5w8OBBeXr+mrXCw8P1+eefa8yYMWrbtq0aNmyoJ554QmPHjq24ZwEAAAAA5eT27xRVB36nCAAAAIB0FfxOEQAAAABcbwhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsVq5QNHv2bEVGRsrHx0cxMTHauHFjmZZbtGiRPDw8lJCQUJ7NAgAAAECFczsULV68WElJSUpOTtbmzZsVFRWl+Ph4HTly5JLL7d+/X0899ZS6du1a7mIBAAAAoKK5HYpmzJihIUOGaNCgQWrZsqXmzp2r2rVr67333it1mYKCAj3yyCN6/vnn1aRJkysqGAAAAAAqkluhKD8/X5s2bVJcXNyvK/D0VFxcnNLT00tdbsqUKQoKCtLgwYPLtJ0zZ84oJyfH5QYAAAAAlcGtUJSVlaWCggIFBwe7tAcHBysjI6PEZb788ku9++67mjdvXpm3k5KSooCAAOctPDzcnTIBAAAAoMwqdfa53Nxc9e/fX/PmzVNgYGCZlxs/fryys7Odt0OHDlVilQAAAABsVsOdzoGBgfLy8lJmZqZLe2ZmpkJCQor137Nnj/bv36/evXs72woLC89vuEYN7dq1S02bNi22nMPhkMPhcKc0AAAAACgXt44UeXt7q0OHDkpLS3O2FRYWKi0tTbGxscX6t2jRQt999522bt3qvN1zzz3q3r27tm7dymlxAAAAAKqdW0eKJCkpKUmJiYnq2LGjoqOjNXPmTOXl5WnQoEGSpAEDBqhhw4ZKSUmRj4+PWrdu7bJ83bp1JalYOwAAAABUB7dDUd++fXX06FFNmjRJGRkZateunVJTU52TLxw8eFCenpV6qRIAAAAAVBgPY4yp7iIuJycnRwEBAcrOzpa/v391lwMAAACgmlRGNuCQDgAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFitXKFo9uzZioyMlI+Pj2JiYrRx48ZS+86bN09du3ZVvXr1VK9ePcXFxV2yPwAAAABUJbdD0eLFi5WUlKTk5GRt3rxZUVFRio+P15EjR0rsv2HDBvXr10/r169Xenq6wsPDdeedd+rw4cNXXDwAAAAAXCkPY4xxZ4GYmBj95je/0axZsyRJhYWFCg8P18iRIzVu3LjLLl9QUKB69epp1qxZGjBgQJm2mZOTo4CAAGVnZ8vf39+dcgEAAABcRyojG7h1pCg/P1+bNm1SXFzcryvw9FRcXJzS09PLtI6TJ0/q7Nmzql+/fql9zpw5o5ycHJcbAAAAAFQGt0JRVlaWCgoKFBwc7NIeHBysjIyMMq1j7NixCgsLcwlWF0tJSVFAQIDzFh4e7k6ZAAAAAFBmVTr73Msvv6xFixbp008/lY+PT6n9xo8fr+zsbOft0KFDVVglAAAAAJvUcKdzYGCgvLy8lJmZ6dKemZmpkJCQSy47ffp0vfzyy1q7dq3atm17yb4Oh0MOh8Od0gAAAACgXNw6UuTt7a0OHTooLS3N2VZYWKi0tDTFxsaWutyrr76qF154QampqerYsWP5qwUAAACACubWkSJJSkpKUmJiojp27Kjo6GjNnDlTeXl5GjRokCRpwIABatiwoVJSUiRJr7zyiiZNmqSFCxcqMjLSee2Rn5+f/Pz8KvCpAAAAAID73A5Fffv21dGjRzVp0iRlZGSoXbt2Sk1NdU6+cPDgQXl6/noA6s0331R+fr769Onjsp7k5GRNnjz5yqoHAAAAgCvk9u8UVQd+pwgAAACAdBX8ThEAAAAAXG8IRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArEYoAgAAAGA1QhEAAAAAqxGKAAAAAFiNUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALAaoQgAAACA1QhFAAAAAKxGKAIAAABgNUIRAAAAAKsRigAAAABYjVAEAAAAwGqEIgAAAABWIxQBAAAAsBqhCAAAAIDVCEUAAAAArFauUDR79mxFRkbKx8dHMTEx2rhx4yX7f/LJJ2rRooV8fHzUpk0brVq1qlzFAgAAAEBFczsULV68WElJSUpOTtbmzZsVFRWl+Ph4HTlypMT+X3/9tfr166fBgwdry5YtSkhIUEJCgrZt23bFxQMAAADAlfIwxhh3FoiJidFvfvMbzZo1S5JUWFio8PBwjRw5UuPGjSvWv2/fvsrLy9OKFSucbb/97W/Vrl07zZ07t0zbzMnJUUBAgLKzs+Xv7+9OuQAAAACuI5WRDWq40zk/P1+bNm3S+PHjnW2enp6Ki4tTenp6icukp6crKSnJpS0+Pl7Lly8vdTtnzpzRmTNnnPezs7Mlnd8BAAAAAOxVlAncPLZzSW6FoqysLBUUFCg4ONilPTg4WDt37ixxmYyMjBL7Z2RklLqdlJQUPf/888Xaw8PD3SkXAAAAwHXq2LFjCggIqJB1uRWKqsr48eNdji6dOHFCEREROnjwYIU9caAkOTk5Cg8P16FDhzhVE5WKsYaqwlhDVWGsoapkZ2ercePGql+/foWt061QFBgYKC8vL2VmZrq0Z2ZmKiQkpMRlQkJC3OovSQ6HQw6Ho1h7QEAAbzJUCX9/f8YaqgRjDVWFsYaqwlhDVfH0rLhfF3JrTd7e3urQoYPS0tKcbYWFhUpLS1NsbGyJy8TGxrr0l6Q1a9aU2h8AAAAAqpLbp88lJSUpMTFRHTt2VHR0tGbOnKm8vDwNGjRIkjRgwAA1bNhQKSkpkqQnnnhC3bp102uvvaaePXtq0aJF+te//qW33367Yp8JAAAAAJSD26Gob9++Onr0qCZNmqSMjAy1a9dOqampzskUDh486HIoq1OnTlq4cKGee+45Pfvss7rpppu0fPlytW7duszbdDgcSk5OLvGUOqAiMdZQVRhrqCqMNVQVxhqqSmWMNbd/pwgAAAAAricVd3USAAAAAFyDCEUAAAAArEYoAgAAAGA1QhEAAAAAq101oWj27NmKjIyUj4+PYmJitHHjxkv2/+STT9SiRQv5+PioTZs2WrVqVRVVimudO2Nt3rx56tq1q+rVq6d69eopLi7usmMTKOLu51qRRYsWycPDQwkJCZVbIK4b7o61EydOaPjw4QoNDZXD4dDNN9/M31GUibtjbebMmWrevLlq1aql8PBwjRkzRqdPn66ianEt+uKLL9S7d2+FhYXJw8NDy5cvv+wyGzZs0K233iqHw6FmzZrpgw8+cHu7V0UoWrx4sZKSkpScnKzNmzcrKipK8fHxOnLkSIn9v/76a/Xr10+DBw/Wli1blJCQoISEBG3btq2KK8e1xt2xtmHDBvXr10/r169Xenq6wsPDdeedd+rw4cNVXDmuNe6OtSL79+/XU089pa5du1ZRpbjWuTvW8vPz1aNHD+3fv19LlizRrl27NG/ePDVs2LCKK8e1xt2xtnDhQo0bN07JycnasWOH3n33XS1evFjPPvtsFVeOa0leXp6ioqI0e/bsMvXft2+fevbsqe7du2vr1q0aPXq0Hn30UX3++efubdhcBaKjo83w4cOd9wsKCkxYWJhJSUkpsf+DDz5oevbs6dIWExNj/vjHP1Zqnbj2uTvWLnbu3DlTp04dM3/+/MoqEdeJ8oy1c+fOmU6dOpl33nnHJCYmmnvvvbcKKsW1zt2x9uabb5omTZqY/Pz8qioR1wl3x9rw4cPN7bff7tKWlJRkOnfuXKl14vohyXz66aeX7PPMM8+YVq1aubT17dvXxMfHu7Wtaj9SlJ+fr02bNikuLs7Z5unpqbi4OKWnp5e4THp6ukt/SYqPjy+1PyCVb6xd7OTJkzp79qzq169fWWXiOlDesTZlyhQFBQVp8ODBVVEmrgPlGWufffaZYmNjNXz4cAUHB6t169aaOnWqCgoKqqpsXIPKM9Y6deqkTZs2OU+x27t3r1atWqW77767SmqGHSoqF9SoyKLKIysrSwUFBQoODnZpDw4O1s6dO0tcJiMjo8T+GRkZlVYnrn3lGWsXGzt2rMLCwoq9+YALlWesffnll3r33Xe1devWKqgQ14vyjLW9e/dq3bp1euSRR7Rq1Srt3r1bjz/+uM6ePavk5OSqKBvXoPKMtYcfflhZWVnq0qWLjDE6d+6cHnvsMU6fQ4UqLRfk5OTo1KlTqlWrVpnWU+1HioBrxcsvv6xFixbp008/lY+PT3WXg+tIbm6u+vfvr3nz5ikwMLC6y8F1rrCwUEFBQXr77bfVoUMH9e3bVxMmTNDcuXOruzRcZzZs2KCpU6dqzpw52rx5s5YtW6aVK1fqhRdeqO7SgGKq/UhRYGCgvLy8lJmZ6dKemZmpkJCQEpcJCQlxqz8glW+sFZk+fbpefvllrV27Vm3btq3MMnEdcHes7dmzR/v371fv3r2dbYWFhZKkGjVqaNeuXWratGnlFo1rUnk+10JDQ1WzZk15eXk522655RZlZGQoPz9f3t7elVozrk3lGWsTJ05U//799eijj0qS2rRpo7y8PA0dOlQTJkyQpyf/m8eVKy0X+Pv7l/kokXQVHCny9vZWhw4dlJaW5mwrLCxUWlqaYmNjS1wmNjbWpb8krVmzptT+gFS+sSZJr776ql544QWlpqaqY8eOVVEqrnHujrUWLVrou+++09atW523e+65xzmTTnh4eFWWj2tIeT7XOnfurN27dzuDtyT98MMPCg0NJRChVOUZaydPniwWfIrC+Plr6IErV2G5wL05ICrHokWLjMPhMB988IHZvn27GTp0qKlbt67JyMgwxhjTv39/M27cOGf/r776ytSoUcNMnz7d7NixwyQnJ5uaNWua7777rrqeAq4R7o61l19+2Xh7e5slS5aYn376yXnLzc2trqeAa4S7Y+1izD6HsnJ3rB08eNDUqVPHjBgxwuzatcusWLHCBAUFmRdffLG6ngKuEe6OteTkZFOnTh3z0Ucfmb1795rVq1ebpk2bmgcffLC6ngKuAbm5uWbLli1my5YtRpKZMWOG2bJlizlw4IAxxphx48aZ/v37O/vv3bvX1K5d2zz99NNmx44dZvbs2cbLy8ukpqa6td2rIhQZY8wbb7xhGjdubLy9vU10dLT5v//7P+dj3bp1M4mJiS79P/74Y3PzzTcbb29v06pVK7Ny5coqrhjXKnfGWkREhJFU7JacnFz1heOa4+7n2oUIRXCHu2Pt66+/NjExMcbhcJgmTZqYl156yZw7d66Kq8a1yJ2xdvbsWTN58mTTtGlT4+PjY8LDw83jjz9ufv7556ovHNeM9evXl/jdq2hsJSYmmm7duhVbpl27dsbb29s0adLEvP/++25v18MYjl8CAAAAsFe1X1MEAAAAANWJUAQAAADAaoQiAAAAAFYjFAEAAACwGqEIAAAAgNUIRQAAAACsRigCAAAAYDVCEQAAAACrEYoAAAAAWI1QBAAAAMBqhCIAAAAAViMUAQAAALDa/wP7twa5Hli+XAAAAABJRU5ErkJggg==\n"},"metadata":{}}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1S8ZEiVmViw5Zm1Y4VUrHwWoDXK_ZjYIJ","authorship_tag":"ABX9TyPvGqrjm0vy1m7yCn2nEvjJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}