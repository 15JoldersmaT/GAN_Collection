{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlJiQI-Eaixj","outputId":"7b19a842-f240-4dc9-d329-89b1eab30324"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 31/2000, Step 71, d_loss: 0.47850242257118225, g_loss: 2.5457630157470703\n","Epoch 31/2000, Step 72, d_loss: 0.41107532382011414, g_loss: 1.2618821859359741\n","Epoch 31/2000, Step 73, d_loss: 0.42267170548439026, g_loss: 1.9665812253952026\n","Epoch 31/2000, Step 74, d_loss: 0.4404296278953552, g_loss: 3.1821959018707275\n","Epoch 31/2000, Step 75, d_loss: 0.48518580198287964, g_loss: 3.0106356143951416\n","Epoch 31/2000, Step 76, d_loss: 0.533459484577179, g_loss: 2.9588799476623535\n","Epoch 31/2000, Step 77, d_loss: 0.4064803421497345, g_loss: 4.060089111328125\n","Epoch 31/2000, Step 78, d_loss: 0.40284523367881775, g_loss: 3.9922378063201904\n","Epoch 31/2000, Step 79, d_loss: 0.3587968349456787, g_loss: 4.09423303604126\n","Epoch 31/2000, Step 80, d_loss: 0.37313002347946167, g_loss: 3.471698045730591\n","Epoch 31/2000, Step 81, d_loss: 0.49089211225509644, g_loss: 2.9845006465911865\n","Epoch 31/2000, Step 82, d_loss: 0.42364463210105896, g_loss: 3.0361578464508057\n","Epoch 31/2000, Step 83, d_loss: 0.3619973957538605, g_loss: 3.3575997352600098\n","Epoch 31/2000, Step 84, d_loss: 0.5149070024490356, g_loss: 3.3782238960266113\n","Epoch 31/2000, Step 85, d_loss: 0.39627671241760254, g_loss: 2.890468120574951\n","Epoch 31/2000, Step 86, d_loss: 0.49604520201683044, g_loss: 3.1076700687408447\n","Epoch 31/2000, Step 87, d_loss: 0.39039063453674316, g_loss: 4.850909233093262\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 32/2000, Step 1, d_loss: 0.3856275975704193, g_loss: 3.567399024963379\n","Epoch 32/2000, Step 2, d_loss: 0.4710733890533447, g_loss: 3.3863754272460938\n","Epoch 32/2000, Step 3, d_loss: 0.38705700635910034, g_loss: 3.3234496116638184\n","Epoch 32/2000, Step 4, d_loss: 0.4117776155471802, g_loss: 4.226335048675537\n","Epoch 32/2000, Step 5, d_loss: 0.40202099084854126, g_loss: 3.3955039978027344\n","Epoch 32/2000, Step 6, d_loss: 0.43768414855003357, g_loss: 3.6149253845214844\n","Epoch 32/2000, Step 7, d_loss: 0.3853907287120819, g_loss: 4.290387153625488\n","Epoch 32/2000, Step 8, d_loss: 0.39770397543907166, g_loss: 3.9140543937683105\n","Epoch 32/2000, Step 9, d_loss: 0.5302539467811584, g_loss: 3.8785998821258545\n","Epoch 32/2000, Step 10, d_loss: 0.5944868922233582, g_loss: 4.587769508361816\n","Epoch 32/2000, Step 11, d_loss: 0.44620850682258606, g_loss: 2.443866729736328\n","Epoch 32/2000, Step 12, d_loss: 0.4359470009803772, g_loss: 2.207918643951416\n","Epoch 32/2000, Step 13, d_loss: 0.5155155658721924, g_loss: 2.8581767082214355\n","Epoch 32/2000, Step 14, d_loss: 0.48775118589401245, g_loss: 2.253817081451416\n","Epoch 32/2000, Step 15, d_loss: 0.49991530179977417, g_loss: 3.1659841537475586\n","Epoch 32/2000, Step 16, d_loss: 0.448933869600296, g_loss: 3.361701726913452\n","Epoch 32/2000, Step 17, d_loss: 0.4733341932296753, g_loss: 4.553648948669434\n","Epoch 32/2000, Step 18, d_loss: 0.44469693303108215, g_loss: 3.9375627040863037\n","Epoch 32/2000, Step 19, d_loss: 0.4076840281486511, g_loss: 3.3632566928863525\n","Epoch 32/2000, Step 20, d_loss: 0.44714340567588806, g_loss: 3.027419328689575\n","Epoch 32/2000, Step 21, d_loss: 0.4337475001811981, g_loss: 2.3322904109954834\n","Epoch 32/2000, Step 22, d_loss: 0.4066222012042999, g_loss: 2.384639024734497\n","Epoch 32/2000, Step 23, d_loss: 0.44618934392929077, g_loss: 3.2152414321899414\n","Epoch 32/2000, Step 24, d_loss: 0.48180148005485535, g_loss: 2.653975486755371\n","Epoch 32/2000, Step 25, d_loss: 0.5194360613822937, g_loss: 3.6680173873901367\n","Epoch 32/2000, Step 26, d_loss: 0.4936535358428955, g_loss: 3.811345338821411\n","Epoch 32/2000, Step 27, d_loss: 0.42598333954811096, g_loss: 3.3333306312561035\n","Epoch 32/2000, Step 28, d_loss: 0.4049336910247803, g_loss: 3.5540812015533447\n","Epoch 32/2000, Step 29, d_loss: 0.41541755199432373, g_loss: 3.287731170654297\n","Epoch 32/2000, Step 30, d_loss: 0.4376099109649658, g_loss: 3.6346447467803955\n","Epoch 32/2000, Step 31, d_loss: 0.5255984663963318, g_loss: 3.3532276153564453\n","Epoch 32/2000, Step 32, d_loss: 0.43727874755859375, g_loss: 2.778998374938965\n","Epoch 32/2000, Step 33, d_loss: 0.48969438672065735, g_loss: 3.6931967735290527\n","Epoch 32/2000, Step 34, d_loss: 0.42828309535980225, g_loss: 2.578312873840332\n","Epoch 32/2000, Step 35, d_loss: 0.45534127950668335, g_loss: 4.41986083984375\n","Epoch 32/2000, Step 36, d_loss: 0.461127370595932, g_loss: 2.3057892322540283\n","Epoch 32/2000, Step 37, d_loss: 0.5163208246231079, g_loss: 2.0653905868530273\n","Epoch 32/2000, Step 38, d_loss: 0.47996675968170166, g_loss: 2.1262919902801514\n","Epoch 32/2000, Step 39, d_loss: 0.42728036642074585, g_loss: 3.131969451904297\n","Epoch 32/2000, Step 40, d_loss: 0.45838800072669983, g_loss: 3.8725979328155518\n","Epoch 32/2000, Step 41, d_loss: 0.4801501929759979, g_loss: 3.1000730991363525\n","Epoch 32/2000, Step 42, d_loss: 0.4063703119754791, g_loss: 3.6615943908691406\n","Epoch 32/2000, Step 43, d_loss: 0.7358596920967102, g_loss: 1.9683786630630493\n","Epoch 32/2000, Step 44, d_loss: 0.4265459477901459, g_loss: 2.4397854804992676\n","Epoch 32/2000, Step 45, d_loss: 0.4870087802410126, g_loss: 1.4205039739608765\n","Epoch 32/2000, Step 46, d_loss: 0.5569944977760315, g_loss: 2.8153257369995117\n","Epoch 32/2000, Step 47, d_loss: 0.475347638130188, g_loss: 3.4627397060394287\n","Epoch 32/2000, Step 48, d_loss: 0.4524398744106293, g_loss: 2.8358922004699707\n","Epoch 32/2000, Step 49, d_loss: 0.45109307765960693, g_loss: 2.9834909439086914\n","Epoch 32/2000, Step 50, d_loss: 0.5931729078292847, g_loss: 3.1962051391601562\n","Epoch 32/2000, Step 51, d_loss: 0.43795835971832275, g_loss: 3.5065805912017822\n","Epoch 32/2000, Step 52, d_loss: 0.6358801126480103, g_loss: 3.395362377166748\n","Epoch 32/2000, Step 53, d_loss: 0.5370640754699707, g_loss: 3.396169900894165\n","Epoch 32/2000, Step 54, d_loss: 0.5327286720275879, g_loss: 1.9792776107788086\n","Epoch 32/2000, Step 55, d_loss: 0.42000246047973633, g_loss: 2.905409097671509\n","Epoch 32/2000, Step 56, d_loss: 0.3808906674385071, g_loss: 3.2850499153137207\n","Epoch 32/2000, Step 57, d_loss: 0.3820795714855194, g_loss: 2.8321330547332764\n","Epoch 32/2000, Step 58, d_loss: 0.3904862105846405, g_loss: 2.6803905963897705\n","Epoch 32/2000, Step 59, d_loss: 0.38961052894592285, g_loss: 3.7700929641723633\n","Epoch 32/2000, Step 60, d_loss: 0.3966490924358368, g_loss: 3.7713565826416016\n","Epoch 32/2000, Step 61, d_loss: 0.38199394941329956, g_loss: 3.207061529159546\n","Epoch 32/2000, Step 62, d_loss: 0.38619962334632874, g_loss: 3.8810200691223145\n","Epoch 32/2000, Step 63, d_loss: 0.40898963809013367, g_loss: 3.4752838611602783\n","Epoch 32/2000, Step 64, d_loss: 0.43703603744506836, g_loss: 3.545511245727539\n","Epoch 32/2000, Step 65, d_loss: 0.39199262857437134, g_loss: 4.016462802886963\n","Epoch 32/2000, Step 66, d_loss: 0.49648305773735046, g_loss: 4.451839447021484\n","Epoch 32/2000, Step 67, d_loss: 0.39598047733306885, g_loss: 3.105196952819824\n","Epoch 32/2000, Step 68, d_loss: 0.4531446695327759, g_loss: 3.467078924179077\n","Epoch 32/2000, Step 69, d_loss: 0.3778320550918579, g_loss: 4.858414173126221\n","Epoch 32/2000, Step 70, d_loss: 0.38711288571357727, g_loss: 3.6612133979797363\n","Epoch 32/2000, Step 71, d_loss: 0.44374382495880127, g_loss: 2.90812349319458\n","Epoch 32/2000, Step 72, d_loss: 0.4015405774116516, g_loss: 3.058763265609741\n","Epoch 32/2000, Step 73, d_loss: 0.48509320616722107, g_loss: 2.9805209636688232\n","Epoch 32/2000, Step 74, d_loss: 0.40671202540397644, g_loss: 3.717647075653076\n","Epoch 32/2000, Step 75, d_loss: 0.3721201717853546, g_loss: 3.628160238265991\n","Epoch 32/2000, Step 76, d_loss: 0.37288931012153625, g_loss: 4.268820762634277\n","Epoch 32/2000, Step 77, d_loss: 0.7033007740974426, g_loss: 5.040284633636475\n","Epoch 32/2000, Step 78, d_loss: 0.417755663394928, g_loss: 3.312110424041748\n","Epoch 32/2000, Step 79, d_loss: 0.4442391097545624, g_loss: 3.2600138187408447\n","Epoch 32/2000, Step 80, d_loss: 0.42010122537612915, g_loss: 2.882723331451416\n","Epoch 32/2000, Step 81, d_loss: 0.4734646677970886, g_loss: 2.9688265323638916\n","Epoch 32/2000, Step 82, d_loss: 0.37542176246643066, g_loss: 2.8253273963928223\n","Epoch 32/2000, Step 83, d_loss: 0.45157188177108765, g_loss: 3.401805877685547\n","Epoch 32/2000, Step 84, d_loss: 0.4277845621109009, g_loss: 3.0601377487182617\n","Epoch 32/2000, Step 85, d_loss: 0.39238473773002625, g_loss: 2.865354061126709\n","Epoch 32/2000, Step 86, d_loss: 0.3968113660812378, g_loss: 3.283722162246704\n","Epoch 32/2000, Step 87, d_loss: 0.4159284234046936, g_loss: 2.8130290508270264\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 33/2000, Step 1, d_loss: 0.4431138038635254, g_loss: 3.202845335006714\n","Epoch 33/2000, Step 2, d_loss: 0.42411643266677856, g_loss: 4.154717445373535\n","Epoch 33/2000, Step 3, d_loss: 0.38147178292274475, g_loss: 3.9916553497314453\n","Epoch 33/2000, Step 4, d_loss: 0.4146448075771332, g_loss: 2.5828287601470947\n","Epoch 33/2000, Step 5, d_loss: 0.47792768478393555, g_loss: 3.134690046310425\n","Epoch 33/2000, Step 6, d_loss: 0.4747071862220764, g_loss: 3.403059959411621\n","Epoch 33/2000, Step 7, d_loss: 0.4282833933830261, g_loss: 3.2474400997161865\n","Epoch 33/2000, Step 8, d_loss: 0.42787668108940125, g_loss: 3.477783203125\n","Epoch 33/2000, Step 9, d_loss: 0.4363219141960144, g_loss: 3.6453354358673096\n","Epoch 33/2000, Step 10, d_loss: 0.3833806812763214, g_loss: 3.467853546142578\n","Epoch 33/2000, Step 11, d_loss: 0.41824281215667725, g_loss: 2.9835987091064453\n","Epoch 33/2000, Step 12, d_loss: 0.3730561435222626, g_loss: 4.043223857879639\n","Epoch 33/2000, Step 13, d_loss: 0.4697902798652649, g_loss: 3.0352344512939453\n","Epoch 33/2000, Step 14, d_loss: 0.3946995139122009, g_loss: 3.5102570056915283\n","Epoch 33/2000, Step 15, d_loss: 0.42972075939178467, g_loss: 2.1987762451171875\n","Epoch 33/2000, Step 16, d_loss: 0.39855316281318665, g_loss: 2.5884103775024414\n","Epoch 33/2000, Step 17, d_loss: 0.42494288086891174, g_loss: 2.597966432571411\n","Epoch 33/2000, Step 18, d_loss: 0.5232480764389038, g_loss: 2.9775171279907227\n","Epoch 33/2000, Step 19, d_loss: 0.4474807381629944, g_loss: 4.04026985168457\n","Epoch 33/2000, Step 20, d_loss: 0.38592028617858887, g_loss: 3.7600653171539307\n","Epoch 33/2000, Step 21, d_loss: 0.39829522371292114, g_loss: 3.679368734359741\n","Epoch 33/2000, Step 22, d_loss: 0.41283032298088074, g_loss: 3.7615010738372803\n","Epoch 33/2000, Step 23, d_loss: 0.4467637836933136, g_loss: 2.7993850708007812\n","Epoch 33/2000, Step 24, d_loss: 0.48103097081184387, g_loss: 3.5676627159118652\n","Epoch 33/2000, Step 25, d_loss: 0.6181958913803101, g_loss: 2.656791925430298\n","Epoch 33/2000, Step 26, d_loss: 0.42360085248947144, g_loss: 3.213196039199829\n","Epoch 33/2000, Step 27, d_loss: 0.3986753821372986, g_loss: 2.0854709148406982\n","Epoch 33/2000, Step 28, d_loss: 0.47656598687171936, g_loss: 3.3755557537078857\n","Epoch 33/2000, Step 29, d_loss: 0.6334677338600159, g_loss: 1.934300422668457\n","Epoch 33/2000, Step 30, d_loss: 0.5254612565040588, g_loss: 3.875739812850952\n","Epoch 33/2000, Step 31, d_loss: 0.4832504987716675, g_loss: 3.7425239086151123\n","Epoch 33/2000, Step 32, d_loss: 0.4501107633113861, g_loss: 3.287047863006592\n","Epoch 33/2000, Step 33, d_loss: 0.39283767342567444, g_loss: 4.648079872131348\n","Epoch 33/2000, Step 34, d_loss: 0.4195633828639984, g_loss: 3.987541437149048\n","Epoch 33/2000, Step 35, d_loss: 0.45324787497520447, g_loss: 3.92340087890625\n","Epoch 33/2000, Step 36, d_loss: 0.3933420479297638, g_loss: 3.56701922416687\n","Epoch 33/2000, Step 37, d_loss: 0.5422913432121277, g_loss: 3.1723344326019287\n","Epoch 33/2000, Step 38, d_loss: 0.38004910945892334, g_loss: 3.5389111042022705\n","Epoch 33/2000, Step 39, d_loss: 0.420477032661438, g_loss: 2.521615982055664\n","Epoch 33/2000, Step 40, d_loss: 0.4297701418399811, g_loss: 2.7033214569091797\n","Epoch 33/2000, Step 41, d_loss: 0.449033260345459, g_loss: 3.098626136779785\n","Epoch 33/2000, Step 42, d_loss: 0.43146735429763794, g_loss: 3.2640914916992188\n","Epoch 33/2000, Step 43, d_loss: 0.4157629609107971, g_loss: 3.660785436630249\n","Epoch 33/2000, Step 44, d_loss: 0.4356861710548401, g_loss: 4.393520832061768\n","Epoch 33/2000, Step 45, d_loss: 0.5616801977157593, g_loss: 4.426517486572266\n","Epoch 33/2000, Step 46, d_loss: 0.41307997703552246, g_loss: 3.1301043033599854\n","Epoch 33/2000, Step 47, d_loss: 0.4142182767391205, g_loss: 3.920999526977539\n","Epoch 33/2000, Step 48, d_loss: 0.4612942934036255, g_loss: 3.871027946472168\n","Epoch 33/2000, Step 49, d_loss: 0.4578549563884735, g_loss: 3.5098721981048584\n","Epoch 33/2000, Step 50, d_loss: 0.409303218126297, g_loss: 3.3135764598846436\n","Epoch 33/2000, Step 51, d_loss: 0.4781903624534607, g_loss: 3.6806507110595703\n","Epoch 33/2000, Step 52, d_loss: 0.43956682085990906, g_loss: 2.7436649799346924\n","Epoch 33/2000, Step 53, d_loss: 0.4026375114917755, g_loss: 2.9307894706726074\n","Epoch 33/2000, Step 54, d_loss: 0.40866532921791077, g_loss: 2.7676944732666016\n","Epoch 33/2000, Step 55, d_loss: 0.45110076665878296, g_loss: 2.987415075302124\n","Epoch 33/2000, Step 56, d_loss: 0.42550188302993774, g_loss: 3.2712979316711426\n","Epoch 33/2000, Step 57, d_loss: 0.463546484708786, g_loss: 3.4487016201019287\n","Epoch 33/2000, Step 58, d_loss: 0.37690335512161255, g_loss: 3.3998026847839355\n","Epoch 33/2000, Step 59, d_loss: 0.4006674587726593, g_loss: 3.4618821144104004\n","Epoch 33/2000, Step 60, d_loss: 0.45980292558670044, g_loss: 3.4882829189300537\n","Epoch 33/2000, Step 61, d_loss: 0.41310304403305054, g_loss: 3.0932021141052246\n","Epoch 33/2000, Step 62, d_loss: 0.4009551405906677, g_loss: 3.1961591243743896\n","Epoch 33/2000, Step 63, d_loss: 0.4213005304336548, g_loss: 2.6750705242156982\n","Epoch 33/2000, Step 64, d_loss: 0.430970162153244, g_loss: 2.81951642036438\n","Epoch 33/2000, Step 65, d_loss: 0.38370153307914734, g_loss: 2.4880945682525635\n","Epoch 33/2000, Step 66, d_loss: 0.5464674830436707, g_loss: 2.4521799087524414\n","Epoch 33/2000, Step 67, d_loss: 0.390116810798645, g_loss: 2.907475709915161\n","Epoch 33/2000, Step 68, d_loss: 0.5238291621208191, g_loss: 3.026111125946045\n","Epoch 33/2000, Step 69, d_loss: 0.4396055042743683, g_loss: 3.830390453338623\n","Epoch 33/2000, Step 70, d_loss: 0.37550511956214905, g_loss: 3.855563163757324\n","Epoch 33/2000, Step 71, d_loss: 0.3848070204257965, g_loss: 3.967000961303711\n","Epoch 33/2000, Step 72, d_loss: 0.3940633535385132, g_loss: 4.075066089630127\n","Epoch 33/2000, Step 73, d_loss: 0.4041118025779724, g_loss: 4.102402687072754\n","Epoch 33/2000, Step 74, d_loss: 0.4275337755680084, g_loss: 5.320584297180176\n","Epoch 33/2000, Step 75, d_loss: 0.46791255474090576, g_loss: 3.473093271255493\n","Epoch 33/2000, Step 76, d_loss: 0.3662482798099518, g_loss: 2.948009729385376\n","Epoch 33/2000, Step 77, d_loss: 0.4456802010536194, g_loss: 2.3546102046966553\n","Epoch 33/2000, Step 78, d_loss: 0.42820411920547485, g_loss: 2.0245227813720703\n","Epoch 33/2000, Step 79, d_loss: 0.5072129368782043, g_loss: 2.0168581008911133\n","Epoch 33/2000, Step 80, d_loss: 0.5880953669548035, g_loss: 3.024733304977417\n","Epoch 33/2000, Step 81, d_loss: 0.43731120228767395, g_loss: 3.3943231105804443\n","Epoch 33/2000, Step 82, d_loss: 0.4217837154865265, g_loss: 3.8492136001586914\n","Epoch 33/2000, Step 83, d_loss: 0.3881160020828247, g_loss: 3.6688685417175293\n","Epoch 33/2000, Step 84, d_loss: 0.5555899143218994, g_loss: 3.3175621032714844\n","Epoch 33/2000, Step 85, d_loss: 0.4301653504371643, g_loss: 3.830878496170044\n","Epoch 33/2000, Step 86, d_loss: 0.37496984004974365, g_loss: 4.133657932281494\n","Epoch 33/2000, Step 87, d_loss: 0.3945990204811096, g_loss: 2.508615255355835\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 34/2000, Step 1, d_loss: 0.4202669560909271, g_loss: 2.825438976287842\n","Epoch 34/2000, Step 2, d_loss: 0.4856848120689392, g_loss: 2.688570976257324\n","Epoch 34/2000, Step 3, d_loss: 0.5280703902244568, g_loss: 3.0848090648651123\n","Epoch 34/2000, Step 4, d_loss: 0.3792484700679779, g_loss: 3.1096458435058594\n","Epoch 34/2000, Step 5, d_loss: 0.4195146858692169, g_loss: 3.0531399250030518\n","Epoch 34/2000, Step 6, d_loss: 0.394179105758667, g_loss: 3.607508420944214\n","Epoch 34/2000, Step 7, d_loss: 0.3802768886089325, g_loss: 4.183251857757568\n","Epoch 34/2000, Step 8, d_loss: 0.6437557339668274, g_loss: 3.6311211585998535\n","Epoch 34/2000, Step 9, d_loss: 0.37697502970695496, g_loss: 2.507671594619751\n","Epoch 34/2000, Step 10, d_loss: 0.48038795590400696, g_loss: 2.529860734939575\n","Epoch 34/2000, Step 11, d_loss: 0.4016150236129761, g_loss: 2.7634716033935547\n","Epoch 34/2000, Step 12, d_loss: 0.413453608751297, g_loss: 2.2743945121765137\n","Epoch 34/2000, Step 13, d_loss: 0.46268707513809204, g_loss: 3.396799087524414\n","Epoch 34/2000, Step 14, d_loss: 0.5075089335441589, g_loss: 3.3428075313568115\n","Epoch 34/2000, Step 15, d_loss: 0.40385204553604126, g_loss: 3.8867979049682617\n","Epoch 34/2000, Step 16, d_loss: 0.47226351499557495, g_loss: 4.221397399902344\n","Epoch 34/2000, Step 17, d_loss: 0.37874966859817505, g_loss: 3.1032016277313232\n","Epoch 34/2000, Step 18, d_loss: 0.3871869444847107, g_loss: 4.010260581970215\n","Epoch 34/2000, Step 19, d_loss: 0.3842952847480774, g_loss: 4.406322479248047\n","Epoch 34/2000, Step 20, d_loss: 0.44937294721603394, g_loss: 3.7609777450561523\n","Epoch 34/2000, Step 21, d_loss: 0.4116077423095703, g_loss: 3.371915817260742\n","Epoch 34/2000, Step 22, d_loss: 0.41098177433013916, g_loss: 2.988131284713745\n","Epoch 34/2000, Step 23, d_loss: 0.43578609824180603, g_loss: 2.6354262828826904\n","Epoch 34/2000, Step 24, d_loss: 0.40312227606773376, g_loss: 4.216540336608887\n","Epoch 34/2000, Step 25, d_loss: 0.38762474060058594, g_loss: 3.117424964904785\n","Epoch 34/2000, Step 26, d_loss: 0.3804667592048645, g_loss: 3.6711480617523193\n","Epoch 34/2000, Step 27, d_loss: 0.49470213055610657, g_loss: 4.077338218688965\n","Epoch 34/2000, Step 28, d_loss: 0.4368860423564911, g_loss: 3.6379740238189697\n","Epoch 34/2000, Step 29, d_loss: 0.3666556179523468, g_loss: 3.2581424713134766\n","Epoch 34/2000, Step 30, d_loss: 0.4183538854122162, g_loss: 3.5070557594299316\n","Epoch 34/2000, Step 31, d_loss: 0.3846118748188019, g_loss: 3.3341586589813232\n","Epoch 34/2000, Step 32, d_loss: 0.49348440766334534, g_loss: 3.209272861480713\n","Epoch 34/2000, Step 33, d_loss: 0.39579474925994873, g_loss: 4.0242085456848145\n","Epoch 34/2000, Step 34, d_loss: 0.38619357347488403, g_loss: 3.5374228954315186\n","Epoch 34/2000, Step 35, d_loss: 0.4926338493824005, g_loss: 3.7063310146331787\n","Epoch 34/2000, Step 36, d_loss: 0.4229526221752167, g_loss: 3.7299160957336426\n","Epoch 34/2000, Step 37, d_loss: 0.3775291442871094, g_loss: 2.8862884044647217\n","Epoch 34/2000, Step 38, d_loss: 0.3842741549015045, g_loss: 2.7909464836120605\n","Epoch 34/2000, Step 39, d_loss: 0.40167179703712463, g_loss: 3.1167056560516357\n","Epoch 34/2000, Step 40, d_loss: 0.46423259377479553, g_loss: 2.776393175125122\n","Epoch 34/2000, Step 41, d_loss: 0.39475417137145996, g_loss: 2.618048906326294\n","Epoch 34/2000, Step 42, d_loss: 0.36665022373199463, g_loss: 4.003237247467041\n","Epoch 34/2000, Step 43, d_loss: 0.3964138627052307, g_loss: 4.5932512283325195\n","Epoch 34/2000, Step 44, d_loss: 0.4134712219238281, g_loss: 4.1177849769592285\n","Epoch 34/2000, Step 45, d_loss: 0.4461989402770996, g_loss: 2.8882975578308105\n","Epoch 34/2000, Step 46, d_loss: 0.4072984457015991, g_loss: 4.067799091339111\n","Epoch 34/2000, Step 47, d_loss: 0.36885806918144226, g_loss: 3.247947931289673\n","Epoch 34/2000, Step 48, d_loss: 0.4239192306995392, g_loss: 2.5792055130004883\n","Epoch 34/2000, Step 49, d_loss: 0.4670679271221161, g_loss: 3.2342112064361572\n","Epoch 34/2000, Step 50, d_loss: 0.48205506801605225, g_loss: 2.7706964015960693\n","Epoch 34/2000, Step 51, d_loss: 0.38435035943984985, g_loss: 3.5038959980010986\n","Epoch 34/2000, Step 52, d_loss: 0.38063928484916687, g_loss: 4.669904708862305\n","Epoch 34/2000, Step 53, d_loss: 0.43744274973869324, g_loss: 3.375857353210449\n","Epoch 34/2000, Step 54, d_loss: 0.43783706426620483, g_loss: 3.19032883644104\n","Epoch 34/2000, Step 55, d_loss: 0.37313297390937805, g_loss: 3.2951478958129883\n","Epoch 34/2000, Step 56, d_loss: 0.39156824350357056, g_loss: 5.059120178222656\n","Epoch 34/2000, Step 57, d_loss: 0.4148883521556854, g_loss: 4.315471649169922\n","Epoch 34/2000, Step 58, d_loss: 0.376747727394104, g_loss: 2.816375255584717\n","Epoch 34/2000, Step 59, d_loss: 0.39009976387023926, g_loss: 3.2757577896118164\n","Epoch 34/2000, Step 60, d_loss: 0.3769603669643402, g_loss: 3.011648416519165\n","Epoch 34/2000, Step 61, d_loss: 0.3513067960739136, g_loss: 3.2342000007629395\n","Epoch 34/2000, Step 62, d_loss: 0.3929356336593628, g_loss: 3.1930253505706787\n","Epoch 34/2000, Step 63, d_loss: 0.4500948190689087, g_loss: 4.755399227142334\n","Epoch 34/2000, Step 64, d_loss: 0.39490973949432373, g_loss: 2.831817388534546\n","Epoch 34/2000, Step 65, d_loss: 0.45805835723876953, g_loss: 2.809164047241211\n","Epoch 34/2000, Step 66, d_loss: 0.4173010587692261, g_loss: 2.81060528755188\n","Epoch 34/2000, Step 67, d_loss: 0.4700862169265747, g_loss: 2.276987314224243\n","Epoch 34/2000, Step 68, d_loss: 0.4189777374267578, g_loss: 3.4159793853759766\n","Epoch 34/2000, Step 69, d_loss: 0.4275631308555603, g_loss: 3.9914140701293945\n","Epoch 34/2000, Step 70, d_loss: 0.4356207549571991, g_loss: 3.8317415714263916\n","Epoch 34/2000, Step 71, d_loss: 0.4125828444957733, g_loss: 5.754695415496826\n","Epoch 34/2000, Step 72, d_loss: 0.4538602828979492, g_loss: 3.553382635116577\n","Epoch 34/2000, Step 73, d_loss: 0.4288971424102783, g_loss: 3.106830596923828\n","Epoch 34/2000, Step 74, d_loss: 0.3703932762145996, g_loss: 2.615736246109009\n","Epoch 34/2000, Step 75, d_loss: 0.3673848509788513, g_loss: 3.2151830196380615\n","Epoch 34/2000, Step 76, d_loss: 0.4546271562576294, g_loss: 2.783677816390991\n","Epoch 34/2000, Step 77, d_loss: 0.4435669183731079, g_loss: 2.768704652786255\n","Epoch 34/2000, Step 78, d_loss: 0.401508092880249, g_loss: 2.7107157707214355\n","Epoch 34/2000, Step 79, d_loss: 0.4089377522468567, g_loss: 4.331414222717285\n","Epoch 34/2000, Step 80, d_loss: 0.41582775115966797, g_loss: 3.624580144882202\n","Epoch 34/2000, Step 81, d_loss: 0.4168991446495056, g_loss: 3.4883837699890137\n","Epoch 34/2000, Step 82, d_loss: 0.4009104073047638, g_loss: 3.871183156967163\n","Epoch 34/2000, Step 83, d_loss: 0.39271795749664307, g_loss: 3.765469551086426\n","Epoch 34/2000, Step 84, d_loss: 0.4250571131706238, g_loss: 4.470564842224121\n","Epoch 34/2000, Step 85, d_loss: 0.409188449382782, g_loss: 4.546971321105957\n","Epoch 34/2000, Step 86, d_loss: 0.40570545196533203, g_loss: 4.222305774688721\n","Epoch 34/2000, Step 87, d_loss: 0.4888821840286255, g_loss: 3.3418779373168945\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 35/2000, Step 1, d_loss: 0.39061427116394043, g_loss: 3.781463384628296\n","Epoch 35/2000, Step 2, d_loss: 0.4086248576641083, g_loss: 2.85422682762146\n","Epoch 35/2000, Step 3, d_loss: 0.39903774857521057, g_loss: 3.871403932571411\n","Epoch 35/2000, Step 4, d_loss: 0.3632424473762512, g_loss: 3.6845877170562744\n","Epoch 35/2000, Step 5, d_loss: 0.4020329713821411, g_loss: 4.0136260986328125\n","Epoch 35/2000, Step 6, d_loss: 0.43660587072372437, g_loss: 3.455930709838867\n","Epoch 35/2000, Step 7, d_loss: 0.4347221851348877, g_loss: 3.0275986194610596\n","Epoch 35/2000, Step 8, d_loss: 0.46109116077423096, g_loss: 3.279507637023926\n","Epoch 35/2000, Step 9, d_loss: 0.3642464578151703, g_loss: 3.22068452835083\n","Epoch 35/2000, Step 10, d_loss: 0.3761778771877289, g_loss: 3.085191249847412\n","Epoch 35/2000, Step 11, d_loss: 0.4257327914237976, g_loss: 3.7590503692626953\n","Epoch 35/2000, Step 12, d_loss: 0.48931795358657837, g_loss: 2.4777071475982666\n","Epoch 35/2000, Step 13, d_loss: 0.43989241123199463, g_loss: 3.284783124923706\n","Epoch 35/2000, Step 14, d_loss: 0.4884568452835083, g_loss: 2.854503631591797\n","Epoch 35/2000, Step 15, d_loss: 0.4620773494243622, g_loss: 3.2889761924743652\n","Epoch 35/2000, Step 16, d_loss: 0.44325289130210876, g_loss: 3.9608571529388428\n","Epoch 35/2000, Step 17, d_loss: 0.4282011091709137, g_loss: 4.16121244430542\n","Epoch 35/2000, Step 18, d_loss: 0.41283872723579407, g_loss: 3.9654269218444824\n","Epoch 35/2000, Step 19, d_loss: 0.39174675941467285, g_loss: 3.4567322731018066\n","Epoch 35/2000, Step 20, d_loss: 0.5585999488830566, g_loss: 4.373405456542969\n","Epoch 35/2000, Step 21, d_loss: 0.4480036497116089, g_loss: 3.890181303024292\n","Epoch 35/2000, Step 22, d_loss: 0.5635724663734436, g_loss: 3.059392213821411\n","Epoch 35/2000, Step 23, d_loss: 0.4397925138473511, g_loss: 2.774184226989746\n","Epoch 35/2000, Step 24, d_loss: 0.4936208724975586, g_loss: 2.4196462631225586\n","Epoch 35/2000, Step 25, d_loss: 0.45605313777923584, g_loss: 1.9168446063995361\n","Epoch 35/2000, Step 26, d_loss: 0.47436997294425964, g_loss: 2.6858744621276855\n","Epoch 35/2000, Step 27, d_loss: 0.47614607214927673, g_loss: 3.7661707401275635\n","Epoch 35/2000, Step 28, d_loss: 0.4835977852344513, g_loss: 2.912652015686035\n","Epoch 35/2000, Step 29, d_loss: 0.4135437309741974, g_loss: 3.498136520385742\n","Epoch 35/2000, Step 30, d_loss: 0.39793652296066284, g_loss: 2.879944324493408\n","Epoch 35/2000, Step 31, d_loss: 0.46607598662376404, g_loss: 3.8303890228271484\n","Epoch 35/2000, Step 32, d_loss: 0.42422929406166077, g_loss: 3.380472183227539\n","Epoch 35/2000, Step 33, d_loss: 0.4962206482887268, g_loss: 2.9874463081359863\n","Epoch 35/2000, Step 34, d_loss: 0.45028236508369446, g_loss: 3.187854051589966\n","Epoch 35/2000, Step 35, d_loss: 0.3972291350364685, g_loss: 2.783306121826172\n","Epoch 35/2000, Step 36, d_loss: 0.4277958869934082, g_loss: 4.040450572967529\n","Epoch 35/2000, Step 37, d_loss: 0.4329856038093567, g_loss: 3.8464577198028564\n","Epoch 35/2000, Step 38, d_loss: 0.5110651850700378, g_loss: 2.972419261932373\n","Epoch 35/2000, Step 39, d_loss: 0.48262566328048706, g_loss: 3.001985549926758\n","Epoch 35/2000, Step 40, d_loss: 0.4187045991420746, g_loss: 2.9138028621673584\n","Epoch 35/2000, Step 41, d_loss: 0.40736791491508484, g_loss: 2.5603549480438232\n","Epoch 35/2000, Step 42, d_loss: 0.4538023769855499, g_loss: 1.8961926698684692\n","Epoch 35/2000, Step 43, d_loss: 0.5185133814811707, g_loss: 3.3454477787017822\n","Epoch 35/2000, Step 44, d_loss: 0.7580432295799255, g_loss: 3.9200870990753174\n","Epoch 35/2000, Step 45, d_loss: 0.42909905314445496, g_loss: 3.6785051822662354\n","Epoch 35/2000, Step 46, d_loss: 0.41333845257759094, g_loss: 4.022242546081543\n","Epoch 35/2000, Step 47, d_loss: 0.43904393911361694, g_loss: 4.044909477233887\n","Epoch 35/2000, Step 48, d_loss: 0.45662856101989746, g_loss: 3.1189749240875244\n","Epoch 35/2000, Step 49, d_loss: 0.5593604445457458, g_loss: 3.1551315784454346\n","Epoch 35/2000, Step 50, d_loss: 0.4530596137046814, g_loss: 2.851884603500366\n","Epoch 35/2000, Step 51, d_loss: 0.4024868905544281, g_loss: 3.0084455013275146\n","Epoch 35/2000, Step 52, d_loss: 0.3763980567455292, g_loss: 3.0590322017669678\n","Epoch 35/2000, Step 53, d_loss: 0.4523930251598358, g_loss: 3.26313853263855\n","Epoch 35/2000, Step 54, d_loss: 0.4848374128341675, g_loss: 3.9905402660369873\n","Epoch 35/2000, Step 55, d_loss: 0.39077436923980713, g_loss: 4.478799819946289\n","Epoch 35/2000, Step 56, d_loss: 0.39099258184432983, g_loss: 3.9417643547058105\n","Epoch 35/2000, Step 57, d_loss: 0.428453654050827, g_loss: 4.050936222076416\n","Epoch 35/2000, Step 58, d_loss: 0.4035704731941223, g_loss: 3.1573171615600586\n","Epoch 35/2000, Step 59, d_loss: 0.41064658761024475, g_loss: 2.4126222133636475\n","Epoch 35/2000, Step 60, d_loss: 0.4056854248046875, g_loss: 2.9077868461608887\n","Epoch 35/2000, Step 61, d_loss: 0.49527689814567566, g_loss: 2.58418345451355\n","Epoch 35/2000, Step 62, d_loss: 0.4143589437007904, g_loss: 4.33757209777832\n","Epoch 35/2000, Step 63, d_loss: 0.5642723441123962, g_loss: 3.708711624145508\n","Epoch 35/2000, Step 64, d_loss: 0.4113633334636688, g_loss: 2.7877197265625\n","Epoch 35/2000, Step 65, d_loss: 0.4370555579662323, g_loss: 2.8798696994781494\n","Epoch 35/2000, Step 66, d_loss: 0.5118536949157715, g_loss: 2.541616678237915\n","Epoch 35/2000, Step 67, d_loss: 0.49874061346054077, g_loss: 2.2573256492614746\n","Epoch 35/2000, Step 68, d_loss: 0.47223716974258423, g_loss: 2.837670087814331\n","Epoch 35/2000, Step 69, d_loss: 0.45040756464004517, g_loss: 3.557595729827881\n","Epoch 35/2000, Step 70, d_loss: 0.44638556241989136, g_loss: 3.389360189437866\n","Epoch 35/2000, Step 71, d_loss: 0.410309374332428, g_loss: 3.4869203567504883\n","Epoch 35/2000, Step 72, d_loss: 0.5846676826477051, g_loss: 3.653686761856079\n","Epoch 35/2000, Step 73, d_loss: 0.5325306057929993, g_loss: 2.863630533218384\n","Epoch 35/2000, Step 74, d_loss: 0.432129442691803, g_loss: 3.161259174346924\n","Epoch 35/2000, Step 75, d_loss: 0.4510398507118225, g_loss: 2.9537487030029297\n","Epoch 35/2000, Step 76, d_loss: 0.45555660128593445, g_loss: 3.1860604286193848\n","Epoch 35/2000, Step 77, d_loss: 0.4044627845287323, g_loss: 3.0411453247070312\n","Epoch 35/2000, Step 78, d_loss: 0.48467400670051575, g_loss: 3.688807964324951\n","Epoch 35/2000, Step 79, d_loss: 0.49958211183547974, g_loss: 3.2816364765167236\n","Epoch 35/2000, Step 80, d_loss: 0.37159955501556396, g_loss: 4.936956882476807\n","Epoch 35/2000, Step 81, d_loss: 0.3673051595687866, g_loss: 3.5715832710266113\n","Epoch 35/2000, Step 82, d_loss: 0.5458715558052063, g_loss: 3.654383897781372\n","Epoch 35/2000, Step 83, d_loss: 0.37593063712120056, g_loss: 2.9577877521514893\n","Epoch 35/2000, Step 84, d_loss: 0.4525947570800781, g_loss: 3.2060422897338867\n","Epoch 35/2000, Step 85, d_loss: 0.42212989926338196, g_loss: 4.447567939758301\n","Epoch 35/2000, Step 86, d_loss: 0.49654918909072876, g_loss: 2.231144428253174\n","Epoch 35/2000, Step 87, d_loss: 0.4338867962360382, g_loss: 3.660932779312134\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 36/2000, Step 1, d_loss: 0.47135046124458313, g_loss: 3.6877124309539795\n","Epoch 36/2000, Step 2, d_loss: 0.4091595709323883, g_loss: 4.0723772048950195\n","Epoch 36/2000, Step 3, d_loss: 0.41670164465904236, g_loss: 5.058506011962891\n","Epoch 36/2000, Step 4, d_loss: 0.518669605255127, g_loss: 3.170799732208252\n","Epoch 36/2000, Step 5, d_loss: 0.4256772994995117, g_loss: 2.6136586666107178\n","Epoch 36/2000, Step 6, d_loss: 0.4118022918701172, g_loss: 2.4359354972839355\n","Epoch 36/2000, Step 7, d_loss: 0.43132662773132324, g_loss: 2.1357665061950684\n","Epoch 36/2000, Step 8, d_loss: 0.4467267692089081, g_loss: 2.5901434421539307\n","Epoch 36/2000, Step 9, d_loss: 0.5263742804527283, g_loss: 3.277061700820923\n","Epoch 36/2000, Step 10, d_loss: 0.3894401490688324, g_loss: 4.83217716217041\n","Epoch 36/2000, Step 11, d_loss: 0.4064134359359741, g_loss: 3.343238115310669\n","Epoch 36/2000, Step 12, d_loss: 0.4286468029022217, g_loss: 4.404611587524414\n","Epoch 36/2000, Step 13, d_loss: 0.37544482946395874, g_loss: 3.702819585800171\n","Epoch 36/2000, Step 14, d_loss: 0.39198222756385803, g_loss: 3.4586260318756104\n","Epoch 36/2000, Step 15, d_loss: 0.5499862432479858, g_loss: 4.061790943145752\n","Epoch 36/2000, Step 16, d_loss: 0.44923651218414307, g_loss: 4.164059638977051\n","Epoch 36/2000, Step 17, d_loss: 0.39707472920417786, g_loss: 3.476485252380371\n","Epoch 36/2000, Step 18, d_loss: 0.4072877764701843, g_loss: 3.3652007579803467\n","Epoch 36/2000, Step 19, d_loss: 0.48118290305137634, g_loss: 2.9708728790283203\n","Epoch 36/2000, Step 20, d_loss: 0.43186551332473755, g_loss: 3.358787775039673\n","Epoch 36/2000, Step 21, d_loss: 0.441874623298645, g_loss: 3.8189449310302734\n","Epoch 36/2000, Step 22, d_loss: 0.38452327251434326, g_loss: 3.2738301753997803\n","Epoch 36/2000, Step 23, d_loss: 0.4225686490535736, g_loss: 3.6338956356048584\n","Epoch 36/2000, Step 24, d_loss: 0.3956531882286072, g_loss: 4.013883113861084\n","Epoch 36/2000, Step 25, d_loss: 0.3676164746284485, g_loss: 3.9433133602142334\n","Epoch 36/2000, Step 26, d_loss: 0.4605468511581421, g_loss: 4.266462802886963\n","Epoch 36/2000, Step 27, d_loss: 0.36968475580215454, g_loss: 3.665201425552368\n","Epoch 36/2000, Step 28, d_loss: 0.45465201139450073, g_loss: 3.41280198097229\n","Epoch 36/2000, Step 29, d_loss: 0.395982563495636, g_loss: 3.375075101852417\n","Epoch 36/2000, Step 30, d_loss: 0.4673103988170624, g_loss: 2.9141690731048584\n","Epoch 36/2000, Step 31, d_loss: 0.41102585196495056, g_loss: 2.9157230854034424\n","Epoch 36/2000, Step 32, d_loss: 0.4513578414916992, g_loss: 2.8124167919158936\n","Epoch 36/2000, Step 33, d_loss: 0.44586607813835144, g_loss: 5.287102699279785\n","Epoch 36/2000, Step 34, d_loss: 0.44466882944107056, g_loss: 3.4434916973114014\n","Epoch 36/2000, Step 35, d_loss: 0.37893155217170715, g_loss: 3.709522008895874\n","Epoch 36/2000, Step 36, d_loss: 0.4401663839817047, g_loss: 3.6090548038482666\n","Epoch 36/2000, Step 37, d_loss: 0.43707379698753357, g_loss: 2.9880714416503906\n","Epoch 36/2000, Step 38, d_loss: 0.37817883491516113, g_loss: 3.2152621746063232\n","Epoch 36/2000, Step 39, d_loss: 0.4855257570743561, g_loss: 3.1345276832580566\n","Epoch 36/2000, Step 40, d_loss: 0.4440358877182007, g_loss: 2.415428638458252\n","Epoch 36/2000, Step 41, d_loss: 0.41559088230133057, g_loss: 2.8410937786102295\n","Epoch 36/2000, Step 42, d_loss: 0.39822009205818176, g_loss: 3.210798978805542\n","Epoch 36/2000, Step 43, d_loss: 0.41699856519699097, g_loss: 3.4421770572662354\n","Epoch 36/2000, Step 44, d_loss: 0.5719553828239441, g_loss: 4.168176651000977\n","Epoch 36/2000, Step 45, d_loss: 0.39564967155456543, g_loss: 4.630518436431885\n","Epoch 36/2000, Step 46, d_loss: 0.4587598145008087, g_loss: 3.561450481414795\n","Epoch 36/2000, Step 47, d_loss: 0.4501494765281677, g_loss: 3.67191481590271\n","Epoch 36/2000, Step 48, d_loss: 0.40702125430107117, g_loss: 2.289919137954712\n","Epoch 36/2000, Step 49, d_loss: 0.4304736852645874, g_loss: 2.324174642562866\n","Epoch 36/2000, Step 50, d_loss: 0.44924595952033997, g_loss: 2.3966617584228516\n","Epoch 36/2000, Step 51, d_loss: 0.4164431095123291, g_loss: 2.5957863330841064\n","Epoch 36/2000, Step 52, d_loss: 0.46095961332321167, g_loss: 3.337888717651367\n","Epoch 36/2000, Step 53, d_loss: 0.38103461265563965, g_loss: 3.51505446434021\n","Epoch 36/2000, Step 54, d_loss: 0.43365538120269775, g_loss: 5.037458419799805\n","Epoch 36/2000, Step 55, d_loss: 0.36976972222328186, g_loss: 3.514273166656494\n","Epoch 36/2000, Step 56, d_loss: 0.4423472583293915, g_loss: 4.304524898529053\n","Epoch 36/2000, Step 57, d_loss: 0.4244202673435211, g_loss: 4.52368688583374\n","Epoch 36/2000, Step 58, d_loss: 0.41251707077026367, g_loss: 3.2261905670166016\n","Epoch 36/2000, Step 59, d_loss: 0.48721522092819214, g_loss: 3.5612030029296875\n","Epoch 36/2000, Step 60, d_loss: 0.4348304271697998, g_loss: 2.147083282470703\n","Epoch 36/2000, Step 61, d_loss: 0.4889755845069885, g_loss: 2.667036533355713\n","Epoch 36/2000, Step 62, d_loss: 0.4183471202850342, g_loss: 3.3053457736968994\n","Epoch 36/2000, Step 63, d_loss: 0.43670424818992615, g_loss: 3.629493474960327\n","Epoch 36/2000, Step 64, d_loss: 0.42312735319137573, g_loss: 4.148039817810059\n","Epoch 36/2000, Step 65, d_loss: 0.40872547030448914, g_loss: 2.5174548625946045\n","Epoch 36/2000, Step 66, d_loss: 0.3519534170627594, g_loss: 5.682966232299805\n","Epoch 36/2000, Step 67, d_loss: 0.3788904845714569, g_loss: 4.571852684020996\n","Epoch 36/2000, Step 68, d_loss: 0.48930418491363525, g_loss: 5.31264066696167\n","Epoch 36/2000, Step 69, d_loss: 0.4336208403110504, g_loss: 4.12633752822876\n","Epoch 36/2000, Step 70, d_loss: 0.3821879029273987, g_loss: 3.410583257675171\n","Epoch 36/2000, Step 71, d_loss: 0.37924084067344666, g_loss: 3.5327045917510986\n","Epoch 36/2000, Step 72, d_loss: 0.43321335315704346, g_loss: 3.723090648651123\n","Epoch 36/2000, Step 73, d_loss: 0.4123249351978302, g_loss: 3.6202213764190674\n","Epoch 36/2000, Step 74, d_loss: 0.41121378540992737, g_loss: 3.0137529373168945\n","Epoch 36/2000, Step 75, d_loss: 0.4940986633300781, g_loss: 3.6975955963134766\n","Epoch 36/2000, Step 76, d_loss: 0.43045300245285034, g_loss: 3.868924140930176\n","Epoch 36/2000, Step 77, d_loss: 0.4122777283191681, g_loss: 4.52439546585083\n","Epoch 36/2000, Step 78, d_loss: 0.38381099700927734, g_loss: 4.1509246826171875\n","Epoch 36/2000, Step 79, d_loss: 0.4859902560710907, g_loss: 4.263925075531006\n","Epoch 36/2000, Step 80, d_loss: 0.381608784198761, g_loss: 3.601954221725464\n","Epoch 36/2000, Step 81, d_loss: 0.38518765568733215, g_loss: 3.463926076889038\n","Epoch 36/2000, Step 82, d_loss: 0.4424538016319275, g_loss: 3.70833683013916\n","Epoch 36/2000, Step 83, d_loss: 0.35310491919517517, g_loss: 2.9461593627929688\n","Epoch 36/2000, Step 84, d_loss: 0.3912060260772705, g_loss: 3.834713935852051\n","Epoch 36/2000, Step 85, d_loss: 0.38395190238952637, g_loss: 4.1026291847229\n","Epoch 36/2000, Step 86, d_loss: 0.3563324511051178, g_loss: 4.367903232574463\n","Epoch 36/2000, Step 87, d_loss: 0.39213109016418457, g_loss: 4.109942436218262\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 37/2000, Step 1, d_loss: 0.3819178640842438, g_loss: 3.2695441246032715\n","Epoch 37/2000, Step 2, d_loss: 0.36124080419540405, g_loss: 3.2989745140075684\n","Epoch 37/2000, Step 3, d_loss: 0.4504738748073578, g_loss: 3.2954213619232178\n","Epoch 37/2000, Step 4, d_loss: 0.3995871841907501, g_loss: 2.6855928897857666\n","Epoch 37/2000, Step 5, d_loss: 0.4128870666027069, g_loss: 3.9020068645477295\n","Epoch 37/2000, Step 6, d_loss: 0.3960355222225189, g_loss: 3.012216806411743\n","Epoch 37/2000, Step 7, d_loss: 0.39482659101486206, g_loss: 4.346137523651123\n","Epoch 37/2000, Step 8, d_loss: 0.42831793427467346, g_loss: 3.880469799041748\n","Epoch 37/2000, Step 9, d_loss: 0.4343476891517639, g_loss: 4.46552038192749\n","Epoch 37/2000, Step 10, d_loss: 0.39221811294555664, g_loss: 3.166642665863037\n","Epoch 37/2000, Step 11, d_loss: 0.37880975008010864, g_loss: 2.57039737701416\n","Epoch 37/2000, Step 12, d_loss: 0.4210169315338135, g_loss: 3.803412914276123\n","Epoch 37/2000, Step 13, d_loss: 0.3930904269218445, g_loss: 4.1175150871276855\n","Epoch 37/2000, Step 14, d_loss: 0.4111630320549011, g_loss: 2.9986026287078857\n","Epoch 37/2000, Step 15, d_loss: 0.39297765493392944, g_loss: 2.7622570991516113\n","Epoch 37/2000, Step 16, d_loss: 0.376252144575119, g_loss: 3.6287975311279297\n","Epoch 37/2000, Step 17, d_loss: 0.35729947686195374, g_loss: 4.1620001792907715\n","Epoch 37/2000, Step 18, d_loss: 0.36403393745422363, g_loss: 4.590635776519775\n","Epoch 37/2000, Step 19, d_loss: 0.385329008102417, g_loss: 4.212503910064697\n","Epoch 37/2000, Step 20, d_loss: 0.38248756527900696, g_loss: 3.6252527236938477\n","Epoch 37/2000, Step 21, d_loss: 0.394001305103302, g_loss: 4.599966049194336\n","Epoch 37/2000, Step 22, d_loss: 0.44538718461990356, g_loss: 3.8284642696380615\n","Epoch 37/2000, Step 23, d_loss: 0.3986673951148987, g_loss: 3.5526680946350098\n","Epoch 37/2000, Step 24, d_loss: 0.4340275824069977, g_loss: 3.791191339492798\n","Epoch 37/2000, Step 25, d_loss: 0.37364351749420166, g_loss: 3.4553372859954834\n","Epoch 37/2000, Step 26, d_loss: 0.40788060426712036, g_loss: 4.58768892288208\n","Epoch 37/2000, Step 27, d_loss: 0.37452903389930725, g_loss: 3.94063663482666\n","Epoch 37/2000, Step 28, d_loss: 0.44379404187202454, g_loss: 3.8984334468841553\n","Epoch 37/2000, Step 29, d_loss: 0.4178924858570099, g_loss: 3.988865852355957\n","Epoch 37/2000, Step 30, d_loss: 0.4285217821598053, g_loss: 3.867594003677368\n","Epoch 37/2000, Step 31, d_loss: 0.3851180672645569, g_loss: 3.5588784217834473\n","Epoch 37/2000, Step 32, d_loss: 0.39411142468452454, g_loss: 3.2023346424102783\n","Epoch 37/2000, Step 33, d_loss: 0.4023163914680481, g_loss: 3.0177369117736816\n","Epoch 37/2000, Step 34, d_loss: 0.41725456714630127, g_loss: 2.634107828140259\n","Epoch 37/2000, Step 35, d_loss: 0.4106261730194092, g_loss: 4.391909122467041\n","Epoch 37/2000, Step 36, d_loss: 0.4646790027618408, g_loss: 4.19415807723999\n","Epoch 37/2000, Step 37, d_loss: 0.5282022953033447, g_loss: 4.146430492401123\n","Epoch 37/2000, Step 38, d_loss: 0.4302293062210083, g_loss: 3.8096272945404053\n","Epoch 37/2000, Step 39, d_loss: 0.44344639778137207, g_loss: 3.451639413833618\n","Epoch 37/2000, Step 40, d_loss: 0.38819563388824463, g_loss: 3.0838794708251953\n","Epoch 37/2000, Step 41, d_loss: 0.4907347559928894, g_loss: 3.104538679122925\n","Epoch 37/2000, Step 42, d_loss: 0.3778708875179291, g_loss: 3.2921483516693115\n","Epoch 37/2000, Step 43, d_loss: 0.37786176800727844, g_loss: 3.493297576904297\n","Epoch 37/2000, Step 44, d_loss: 0.39084190130233765, g_loss: 3.4617221355438232\n","Epoch 37/2000, Step 45, d_loss: 0.34942179918289185, g_loss: 3.5317962169647217\n","Epoch 37/2000, Step 46, d_loss: 0.3478381633758545, g_loss: 3.8578920364379883\n","Epoch 37/2000, Step 47, d_loss: 0.38632556796073914, g_loss: 3.4690768718719482\n","Epoch 37/2000, Step 48, d_loss: 0.4067395031452179, g_loss: 4.099907398223877\n","Epoch 37/2000, Step 49, d_loss: 0.3833281695842743, g_loss: 3.9390714168548584\n","Epoch 37/2000, Step 50, d_loss: 0.4236500561237335, g_loss: 3.426664352416992\n","Epoch 37/2000, Step 51, d_loss: 0.4073289632797241, g_loss: 3.818692922592163\n","Epoch 37/2000, Step 52, d_loss: 0.387534499168396, g_loss: 4.065960884094238\n","Epoch 37/2000, Step 53, d_loss: 0.3832533359527588, g_loss: 3.130946636199951\n","Epoch 37/2000, Step 54, d_loss: 0.38059675693511963, g_loss: 4.975550174713135\n","Epoch 37/2000, Step 55, d_loss: 0.37977829575538635, g_loss: 3.5590898990631104\n","Epoch 37/2000, Step 56, d_loss: 0.40175503492355347, g_loss: 3.519470691680908\n","Epoch 37/2000, Step 57, d_loss: 0.36784544587135315, g_loss: 3.959716558456421\n","Epoch 37/2000, Step 58, d_loss: 0.4436575770378113, g_loss: 4.154266834259033\n","Epoch 37/2000, Step 59, d_loss: 0.5801300406455994, g_loss: 4.4763946533203125\n","Epoch 37/2000, Step 60, d_loss: 0.3646097481250763, g_loss: 3.71689772605896\n","Epoch 37/2000, Step 61, d_loss: 0.40386104583740234, g_loss: 3.1967153549194336\n","Epoch 37/2000, Step 62, d_loss: 0.4579782485961914, g_loss: 2.664386034011841\n","Epoch 37/2000, Step 63, d_loss: 0.41554149985313416, g_loss: 2.2836086750030518\n","Epoch 37/2000, Step 64, d_loss: 0.550009548664093, g_loss: 2.630542278289795\n","Epoch 37/2000, Step 65, d_loss: 0.4299229383468628, g_loss: 3.633650064468384\n","Epoch 37/2000, Step 66, d_loss: 0.37506207823753357, g_loss: 4.041778564453125\n","Epoch 37/2000, Step 67, d_loss: 0.37480050325393677, g_loss: 3.99881649017334\n","Epoch 37/2000, Step 68, d_loss: 0.37811440229415894, g_loss: 4.440088272094727\n","Epoch 37/2000, Step 69, d_loss: 0.41299137473106384, g_loss: 3.8619179725646973\n","Epoch 37/2000, Step 70, d_loss: 0.40776005387306213, g_loss: 3.7464747428894043\n","Epoch 37/2000, Step 71, d_loss: 0.37615862488746643, g_loss: 3.5637989044189453\n","Epoch 37/2000, Step 72, d_loss: 0.45231175422668457, g_loss: 3.4527647495269775\n","Epoch 37/2000, Step 73, d_loss: 0.42013147473335266, g_loss: 3.8672218322753906\n","Epoch 37/2000, Step 74, d_loss: 0.49752986431121826, g_loss: 3.5662946701049805\n","Epoch 37/2000, Step 75, d_loss: 0.3844251036643982, g_loss: 4.281802177429199\n","Epoch 37/2000, Step 76, d_loss: 0.3817819356918335, g_loss: 4.701950550079346\n","Epoch 37/2000, Step 77, d_loss: 0.47540757060050964, g_loss: 5.080445289611816\n","Epoch 37/2000, Step 78, d_loss: 0.3983153700828552, g_loss: 3.8730640411376953\n","Epoch 37/2000, Step 79, d_loss: 0.37393608689308167, g_loss: 3.0385730266571045\n","Epoch 37/2000, Step 80, d_loss: 0.379788339138031, g_loss: 2.7243764400482178\n","Epoch 37/2000, Step 81, d_loss: 0.3746042847633362, g_loss: 3.7156643867492676\n","Epoch 37/2000, Step 82, d_loss: 0.36725103855133057, g_loss: 2.6136488914489746\n","Epoch 37/2000, Step 83, d_loss: 0.5307954549789429, g_loss: 3.5461692810058594\n","Epoch 37/2000, Step 84, d_loss: 0.3931545317173004, g_loss: 3.182931661605835\n","Epoch 37/2000, Step 85, d_loss: 0.4335780739784241, g_loss: 3.2478978633880615\n","Epoch 37/2000, Step 86, d_loss: 0.46045565605163574, g_loss: 4.387361526489258\n","Epoch 37/2000, Step 87, d_loss: 0.4157334864139557, g_loss: 3.414886236190796\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 38/2000, Step 1, d_loss: 0.44238096475601196, g_loss: 3.8816025257110596\n","Epoch 38/2000, Step 2, d_loss: 0.39805981516838074, g_loss: 3.128272294998169\n","Epoch 38/2000, Step 3, d_loss: 0.3804687559604645, g_loss: 3.0636470317840576\n","Epoch 38/2000, Step 4, d_loss: 0.3552325665950775, g_loss: 3.6494529247283936\n","Epoch 38/2000, Step 5, d_loss: 0.41875749826431274, g_loss: 4.085827827453613\n","Epoch 38/2000, Step 6, d_loss: 0.3814848065376282, g_loss: 3.254425048828125\n","Epoch 38/2000, Step 7, d_loss: 0.35752391815185547, g_loss: 4.360842704772949\n","Epoch 38/2000, Step 8, d_loss: 0.5332013368606567, g_loss: 3.9567527770996094\n","Epoch 38/2000, Step 9, d_loss: 0.35999757051467896, g_loss: 3.6468918323516846\n","Epoch 38/2000, Step 10, d_loss: 0.39177507162094116, g_loss: 3.2775604724884033\n","Epoch 38/2000, Step 11, d_loss: 0.4101491868495941, g_loss: 3.774902582168579\n","Epoch 38/2000, Step 12, d_loss: 0.39447343349456787, g_loss: 3.1519341468811035\n","Epoch 38/2000, Step 13, d_loss: 0.3950711488723755, g_loss: 4.211045742034912\n","Epoch 38/2000, Step 14, d_loss: 0.4189264178276062, g_loss: 4.060128211975098\n","Epoch 38/2000, Step 15, d_loss: 0.376276433467865, g_loss: 4.243940353393555\n","Epoch 38/2000, Step 16, d_loss: 0.4211660325527191, g_loss: 5.036846160888672\n","Epoch 38/2000, Step 17, d_loss: 0.47293221950531006, g_loss: 3.867518424987793\n","Epoch 38/2000, Step 18, d_loss: 0.4175305664539337, g_loss: 4.108895301818848\n","Epoch 38/2000, Step 19, d_loss: 0.3674216866493225, g_loss: 3.6310746669769287\n","Epoch 38/2000, Step 20, d_loss: 0.38424989581108093, g_loss: 2.9142496585845947\n","Epoch 38/2000, Step 21, d_loss: 0.44549819827079773, g_loss: 3.322798728942871\n","Epoch 38/2000, Step 22, d_loss: 0.46584004163742065, g_loss: 2.4218204021453857\n","Epoch 38/2000, Step 23, d_loss: 0.4425515830516815, g_loss: 2.765192747116089\n","Epoch 38/2000, Step 24, d_loss: 0.405863493680954, g_loss: 3.6549856662750244\n","Epoch 38/2000, Step 25, d_loss: 0.40034884214401245, g_loss: 2.587923765182495\n","Epoch 38/2000, Step 26, d_loss: 0.40953540802001953, g_loss: 3.6624908447265625\n","Epoch 38/2000, Step 27, d_loss: 0.39918139576911926, g_loss: 4.844845294952393\n","Epoch 38/2000, Step 28, d_loss: 0.4222055971622467, g_loss: 4.800902366638184\n","Epoch 38/2000, Step 29, d_loss: 0.4139443635940552, g_loss: 4.416912078857422\n","Epoch 38/2000, Step 30, d_loss: 0.3967195153236389, g_loss: 4.0230937004089355\n","Epoch 38/2000, Step 31, d_loss: 0.42354583740234375, g_loss: 3.376032829284668\n","Epoch 38/2000, Step 32, d_loss: 0.40437614917755127, g_loss: 3.2861709594726562\n","Epoch 38/2000, Step 33, d_loss: 0.35383084416389465, g_loss: 3.1231818199157715\n","Epoch 38/2000, Step 34, d_loss: 0.363835871219635, g_loss: 5.265294075012207\n","Epoch 38/2000, Step 35, d_loss: 0.4960569143295288, g_loss: 3.5272390842437744\n","Epoch 38/2000, Step 36, d_loss: 0.42257601022720337, g_loss: 3.5588226318359375\n","Epoch 38/2000, Step 37, d_loss: 0.4123882055282593, g_loss: 3.8875856399536133\n","Epoch 38/2000, Step 38, d_loss: 0.41370296478271484, g_loss: 3.612215518951416\n","Epoch 38/2000, Step 39, d_loss: 0.6827667355537415, g_loss: 3.756887197494507\n","Epoch 38/2000, Step 40, d_loss: 0.3957766592502594, g_loss: 3.8399786949157715\n","Epoch 38/2000, Step 41, d_loss: 0.39590445160865784, g_loss: 3.758610963821411\n","Epoch 38/2000, Step 42, d_loss: 0.4619392454624176, g_loss: 2.6889915466308594\n","Epoch 38/2000, Step 43, d_loss: 0.4294583201408386, g_loss: 3.2631161212921143\n","Epoch 38/2000, Step 44, d_loss: 0.4239709675312042, g_loss: 3.1848316192626953\n","Epoch 38/2000, Step 45, d_loss: 0.40010249614715576, g_loss: 3.556187629699707\n","Epoch 38/2000, Step 46, d_loss: 0.3744606375694275, g_loss: 4.378438472747803\n","Epoch 38/2000, Step 47, d_loss: 0.4431987702846527, g_loss: 5.835579872131348\n","Epoch 38/2000, Step 48, d_loss: 0.49128010869026184, g_loss: 3.852599859237671\n","Epoch 38/2000, Step 49, d_loss: 0.3871924877166748, g_loss: 3.947559118270874\n","Epoch 38/2000, Step 50, d_loss: 0.4257085919380188, g_loss: 2.867810010910034\n","Epoch 38/2000, Step 51, d_loss: 0.43437501788139343, g_loss: 3.1584348678588867\n","Epoch 38/2000, Step 52, d_loss: 0.4006654620170593, g_loss: 3.9085464477539062\n","Epoch 38/2000, Step 53, d_loss: 0.3988212049007416, g_loss: 3.4246673583984375\n","Epoch 38/2000, Step 54, d_loss: 0.3904595673084259, g_loss: 3.7518370151519775\n","Epoch 38/2000, Step 55, d_loss: 0.4061661660671234, g_loss: 4.738777160644531\n","Epoch 38/2000, Step 56, d_loss: 0.38826125860214233, g_loss: 4.801743030548096\n","Epoch 38/2000, Step 57, d_loss: 0.37158331274986267, g_loss: 4.373394966125488\n","Epoch 38/2000, Step 58, d_loss: 0.41710901260375977, g_loss: 4.7864885330200195\n","Epoch 38/2000, Step 59, d_loss: 0.4157026410102844, g_loss: 3.56634259223938\n","Epoch 38/2000, Step 60, d_loss: 0.3585432171821594, g_loss: 3.276340961456299\n","Epoch 38/2000, Step 61, d_loss: 0.39583536982536316, g_loss: 3.0510199069976807\n","Epoch 38/2000, Step 62, d_loss: 0.44051018357276917, g_loss: 2.530686140060425\n","Epoch 38/2000, Step 63, d_loss: 0.4045480489730835, g_loss: 3.2727208137512207\n","Epoch 38/2000, Step 64, d_loss: 0.35308602452278137, g_loss: 3.4243650436401367\n","Epoch 38/2000, Step 65, d_loss: 0.3740799129009247, g_loss: 3.8419225215911865\n","Epoch 38/2000, Step 66, d_loss: 0.39772066473960876, g_loss: 3.761867046356201\n","Epoch 38/2000, Step 67, d_loss: 0.39724257588386536, g_loss: 3.5309112071990967\n","Epoch 38/2000, Step 68, d_loss: 0.37458667159080505, g_loss: 3.7586069107055664\n","Epoch 38/2000, Step 69, d_loss: 0.4197007119655609, g_loss: 4.947131633758545\n","Epoch 38/2000, Step 70, d_loss: 0.4054257869720459, g_loss: 3.6155683994293213\n","Epoch 38/2000, Step 71, d_loss: 0.490080326795578, g_loss: 3.420679807662964\n","Epoch 38/2000, Step 72, d_loss: 0.3636992871761322, g_loss: 3.6868667602539062\n","Epoch 38/2000, Step 73, d_loss: 0.3810451030731201, g_loss: 3.856140613555908\n","Epoch 38/2000, Step 74, d_loss: 0.4206628203392029, g_loss: 3.5398731231689453\n","Epoch 38/2000, Step 75, d_loss: 0.5067132711410522, g_loss: 3.1961395740509033\n","Epoch 38/2000, Step 76, d_loss: 0.36793434619903564, g_loss: 3.4006896018981934\n","Epoch 38/2000, Step 77, d_loss: 0.4138849377632141, g_loss: 3.8686211109161377\n","Epoch 38/2000, Step 78, d_loss: 0.417581170797348, g_loss: 2.751987934112549\n","Epoch 38/2000, Step 79, d_loss: 0.41546329855918884, g_loss: 4.5973100662231445\n","Epoch 38/2000, Step 80, d_loss: 0.3874936103820801, g_loss: 3.1043431758880615\n","Epoch 38/2000, Step 81, d_loss: 0.5002190470695496, g_loss: 3.2867801189422607\n","Epoch 38/2000, Step 82, d_loss: 0.5083250403404236, g_loss: 3.356691360473633\n","Epoch 38/2000, Step 83, d_loss: 0.40955647826194763, g_loss: 3.849592685699463\n","Epoch 38/2000, Step 84, d_loss: 0.403488427400589, g_loss: 3.117464065551758\n","Epoch 38/2000, Step 85, d_loss: 0.39754924178123474, g_loss: 4.136411666870117\n","Epoch 38/2000, Step 86, d_loss: 0.3773728609085083, g_loss: 3.966137409210205\n","Epoch 38/2000, Step 87, d_loss: 0.3750286400318146, g_loss: 4.5226874351501465\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 39/2000, Step 1, d_loss: 0.4353964328765869, g_loss: 4.301936149597168\n","Epoch 39/2000, Step 2, d_loss: 0.3952558934688568, g_loss: 3.7959787845611572\n","Epoch 39/2000, Step 3, d_loss: 0.37920063734054565, g_loss: 3.6817071437835693\n","Epoch 39/2000, Step 4, d_loss: 0.3802501857280731, g_loss: 3.0811634063720703\n","Epoch 39/2000, Step 5, d_loss: 0.3842683732509613, g_loss: 3.2796616554260254\n","Epoch 39/2000, Step 6, d_loss: 0.381744384765625, g_loss: 3.4608523845672607\n","Epoch 39/2000, Step 7, d_loss: 0.3852265477180481, g_loss: 3.9131979942321777\n","Epoch 39/2000, Step 8, d_loss: 0.38096415996551514, g_loss: 4.486547470092773\n","Epoch 39/2000, Step 9, d_loss: 0.47865092754364014, g_loss: 3.4899535179138184\n","Epoch 39/2000, Step 10, d_loss: 0.3874610960483551, g_loss: 3.503159284591675\n","Epoch 39/2000, Step 11, d_loss: 0.3972477316856384, g_loss: 3.7845680713653564\n","Epoch 39/2000, Step 12, d_loss: 0.3779349625110626, g_loss: 4.1912641525268555\n","Epoch 39/2000, Step 13, d_loss: 0.5112422108650208, g_loss: 3.5523521900177\n","Epoch 39/2000, Step 14, d_loss: 0.502391517162323, g_loss: 3.583059787750244\n","Epoch 39/2000, Step 15, d_loss: 0.3968758285045624, g_loss: 3.322864294052124\n","Epoch 39/2000, Step 16, d_loss: 0.4236385226249695, g_loss: 3.331766366958618\n","Epoch 39/2000, Step 17, d_loss: 0.5162878632545471, g_loss: 3.097050189971924\n","Epoch 39/2000, Step 18, d_loss: 0.4983260929584503, g_loss: 3.20017671585083\n","Epoch 39/2000, Step 19, d_loss: 0.4916037917137146, g_loss: 2.074096441268921\n","Epoch 39/2000, Step 20, d_loss: 0.41731125116348267, g_loss: 3.4256553649902344\n","Epoch 39/2000, Step 21, d_loss: 0.455704927444458, g_loss: 3.6696925163269043\n","Epoch 39/2000, Step 22, d_loss: 0.4757716655731201, g_loss: 4.990324974060059\n","Epoch 39/2000, Step 23, d_loss: 0.5168440341949463, g_loss: 3.5075454711914062\n","Epoch 39/2000, Step 24, d_loss: 0.5407549738883972, g_loss: 3.2263355255126953\n","Epoch 39/2000, Step 25, d_loss: 0.41424545645713806, g_loss: 2.854651927947998\n","Epoch 39/2000, Step 26, d_loss: 0.45196107029914856, g_loss: 2.2473952770233154\n","Epoch 39/2000, Step 27, d_loss: 0.5286012291908264, g_loss: 2.440584897994995\n","Epoch 39/2000, Step 28, d_loss: 0.4563089609146118, g_loss: 3.8610177040100098\n","Epoch 39/2000, Step 29, d_loss: 0.4875260293483734, g_loss: 2.4829018115997314\n","Epoch 39/2000, Step 30, d_loss: 0.4389774799346924, g_loss: 3.2612810134887695\n","Epoch 39/2000, Step 31, d_loss: 0.49213773012161255, g_loss: 3.20627498626709\n","Epoch 39/2000, Step 32, d_loss: 0.3937225937843323, g_loss: 3.9890658855438232\n","Epoch 39/2000, Step 33, d_loss: 0.416713684797287, g_loss: 3.5383784770965576\n","Epoch 39/2000, Step 34, d_loss: 0.3625747263431549, g_loss: 3.99949049949646\n","Epoch 39/2000, Step 35, d_loss: 0.47223830223083496, g_loss: 4.273842811584473\n","Epoch 39/2000, Step 36, d_loss: 0.3977688252925873, g_loss: 3.630155563354492\n","Epoch 39/2000, Step 37, d_loss: 0.38749951124191284, g_loss: 3.5810089111328125\n","Epoch 39/2000, Step 38, d_loss: 0.4594358503818512, g_loss: 2.440718412399292\n","Epoch 39/2000, Step 39, d_loss: 0.4323076903820038, g_loss: 3.197341203689575\n","Epoch 39/2000, Step 40, d_loss: 0.4457944631576538, g_loss: 3.4719698429107666\n","Epoch 39/2000, Step 41, d_loss: 0.4408068060874939, g_loss: 3.480804920196533\n","Epoch 39/2000, Step 42, d_loss: 0.4879235327243805, g_loss: 3.5029728412628174\n","Epoch 39/2000, Step 43, d_loss: 0.4256521165370941, g_loss: 3.982302665710449\n","Epoch 39/2000, Step 44, d_loss: 0.40264779329299927, g_loss: 4.783807277679443\n","Epoch 39/2000, Step 45, d_loss: 0.4474412500858307, g_loss: 3.554807186126709\n","Epoch 39/2000, Step 46, d_loss: 0.513627290725708, g_loss: 3.9286789894104004\n","Epoch 39/2000, Step 47, d_loss: 0.46179938316345215, g_loss: 2.903787136077881\n","Epoch 39/2000, Step 48, d_loss: 0.40524351596832275, g_loss: 3.083364248275757\n","Epoch 39/2000, Step 49, d_loss: 0.41731470823287964, g_loss: 2.972487688064575\n","Epoch 39/2000, Step 50, d_loss: 0.4303487241268158, g_loss: 4.753093242645264\n","Epoch 39/2000, Step 51, d_loss: 0.40342065691947937, g_loss: 4.289660453796387\n","Epoch 39/2000, Step 52, d_loss: 0.46566519141197205, g_loss: 3.7322983741760254\n","Epoch 39/2000, Step 53, d_loss: 0.41403308510780334, g_loss: 3.0711846351623535\n","Epoch 39/2000, Step 54, d_loss: 0.41437646746635437, g_loss: 2.7423887252807617\n","Epoch 39/2000, Step 55, d_loss: 0.44847190380096436, g_loss: 2.773111343383789\n","Epoch 39/2000, Step 56, d_loss: 0.5336558222770691, g_loss: 3.63224720954895\n","Epoch 39/2000, Step 57, d_loss: 0.49734362959861755, g_loss: 3.6679251194000244\n","Epoch 39/2000, Step 58, d_loss: 0.39372193813323975, g_loss: 2.8973100185394287\n","Epoch 39/2000, Step 59, d_loss: 0.4566466510295868, g_loss: 3.455622911453247\n","Epoch 39/2000, Step 60, d_loss: 0.4707548916339874, g_loss: 4.079920291900635\n","Epoch 39/2000, Step 61, d_loss: 0.3694143295288086, g_loss: 3.540203094482422\n","Epoch 39/2000, Step 62, d_loss: 0.4224100410938263, g_loss: 4.220625877380371\n","Epoch 39/2000, Step 63, d_loss: 0.5141796469688416, g_loss: 3.510338306427002\n","Epoch 39/2000, Step 64, d_loss: 0.3817080557346344, g_loss: 3.5958046913146973\n","Epoch 39/2000, Step 65, d_loss: 0.371261328458786, g_loss: 3.3658597469329834\n","Epoch 39/2000, Step 66, d_loss: 0.3971012234687805, g_loss: 2.8633127212524414\n","Epoch 39/2000, Step 67, d_loss: 0.45502254366874695, g_loss: 3.0478899478912354\n","Epoch 39/2000, Step 68, d_loss: 0.388797402381897, g_loss: 3.4165444374084473\n","Epoch 39/2000, Step 69, d_loss: 0.42416176199913025, g_loss: 3.2231502532958984\n","Epoch 39/2000, Step 70, d_loss: 0.3978787660598755, g_loss: 3.7842488288879395\n","Epoch 39/2000, Step 71, d_loss: 0.47563663125038147, g_loss: 3.8514676094055176\n","Epoch 39/2000, Step 72, d_loss: 0.3925991952419281, g_loss: 3.5272490978240967\n","Epoch 39/2000, Step 73, d_loss: 0.37112733721733093, g_loss: 3.4958536624908447\n","Epoch 39/2000, Step 74, d_loss: 0.43124136328697205, g_loss: 3.165107250213623\n","Epoch 39/2000, Step 75, d_loss: 0.39144977927207947, g_loss: 2.893578290939331\n","Epoch 39/2000, Step 76, d_loss: 0.4027833938598633, g_loss: 3.6991126537323\n","Epoch 39/2000, Step 77, d_loss: 0.438982754945755, g_loss: 3.8667354583740234\n","Epoch 39/2000, Step 78, d_loss: 0.36490628123283386, g_loss: 4.557187080383301\n","Epoch 39/2000, Step 79, d_loss: 0.3825802206993103, g_loss: 4.592103004455566\n","Epoch 39/2000, Step 80, d_loss: 0.4836488664150238, g_loss: 3.3039870262145996\n","Epoch 39/2000, Step 81, d_loss: 0.3997853398323059, g_loss: 4.001092910766602\n","Epoch 39/2000, Step 82, d_loss: 0.39486178755760193, g_loss: 3.811277389526367\n","Epoch 39/2000, Step 83, d_loss: 0.4154987037181854, g_loss: 4.079120635986328\n","Epoch 39/2000, Step 84, d_loss: 0.38300642371177673, g_loss: 3.81706166267395\n","Epoch 39/2000, Step 85, d_loss: 0.408221960067749, g_loss: 3.6338090896606445\n","Epoch 39/2000, Step 86, d_loss: 0.3885197639465332, g_loss: 4.121892929077148\n","Epoch 39/2000, Step 87, d_loss: 0.44398051500320435, g_loss: 3.1985461711883545\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 40/2000, Step 1, d_loss: 0.3961126506328583, g_loss: 3.559966802597046\n","Epoch 40/2000, Step 2, d_loss: 0.36860689520835876, g_loss: 3.118577718734741\n","Epoch 40/2000, Step 3, d_loss: 0.4085157513618469, g_loss: 3.8389084339141846\n","Epoch 40/2000, Step 4, d_loss: 0.36183083057403564, g_loss: 3.781306028366089\n","Epoch 40/2000, Step 5, d_loss: 0.3664023280143738, g_loss: 2.906511068344116\n","Epoch 40/2000, Step 6, d_loss: 0.3700146973133087, g_loss: 3.521475076675415\n","Epoch 40/2000, Step 7, d_loss: 0.3838386535644531, g_loss: 4.029368877410889\n","Epoch 40/2000, Step 8, d_loss: 0.5684935450553894, g_loss: 4.147668361663818\n","Epoch 40/2000, Step 9, d_loss: 0.42139771580696106, g_loss: 3.444554328918457\n","Epoch 40/2000, Step 10, d_loss: 0.3962520956993103, g_loss: 2.8205983638763428\n","Epoch 40/2000, Step 11, d_loss: 0.45908814668655396, g_loss: 3.184236526489258\n","Epoch 40/2000, Step 12, d_loss: 0.40128010511398315, g_loss: 2.9599721431732178\n","Epoch 40/2000, Step 13, d_loss: 0.3933984339237213, g_loss: 4.364181041717529\n","Epoch 40/2000, Step 14, d_loss: 0.3697469234466553, g_loss: 3.3977677822113037\n","Epoch 40/2000, Step 15, d_loss: 0.35420751571655273, g_loss: 4.392401695251465\n","Epoch 40/2000, Step 16, d_loss: 0.4605649411678314, g_loss: 4.565739631652832\n","Epoch 40/2000, Step 17, d_loss: 0.3516436815261841, g_loss: 3.4809980392456055\n","Epoch 40/2000, Step 18, d_loss: 0.3650079369544983, g_loss: 2.5810508728027344\n","Epoch 40/2000, Step 19, d_loss: 0.39823782444000244, g_loss: 3.533421039581299\n","Epoch 40/2000, Step 20, d_loss: 0.39611122012138367, g_loss: 3.430799722671509\n","Epoch 40/2000, Step 21, d_loss: 0.39392775297164917, g_loss: 3.2400410175323486\n","Epoch 40/2000, Step 22, d_loss: 0.4215852916240692, g_loss: 3.2244181632995605\n","Epoch 40/2000, Step 23, d_loss: 0.3978259861469269, g_loss: 3.367189884185791\n","Epoch 40/2000, Step 24, d_loss: 0.4354288876056671, g_loss: 3.596113920211792\n","Epoch 40/2000, Step 25, d_loss: 0.4365827441215515, g_loss: 3.3513429164886475\n","Epoch 40/2000, Step 26, d_loss: 0.39795246720314026, g_loss: 3.147770643234253\n","Epoch 40/2000, Step 27, d_loss: 0.4074067175388336, g_loss: 3.296278715133667\n","Epoch 40/2000, Step 28, d_loss: 0.39192602038383484, g_loss: 3.3452603816986084\n","Epoch 40/2000, Step 29, d_loss: 0.4596174359321594, g_loss: 4.184587001800537\n","Epoch 40/2000, Step 30, d_loss: 0.5519925355911255, g_loss: 3.96439528465271\n","Epoch 40/2000, Step 31, d_loss: 0.4652321934700012, g_loss: 3.390876293182373\n","Epoch 40/2000, Step 32, d_loss: 0.42189913988113403, g_loss: 3.4908530712127686\n","Epoch 40/2000, Step 33, d_loss: 0.39089012145996094, g_loss: 3.819265365600586\n","Epoch 40/2000, Step 34, d_loss: 0.4827508330345154, g_loss: 4.169617652893066\n","Epoch 40/2000, Step 35, d_loss: 0.40504589676856995, g_loss: 3.5090460777282715\n","Epoch 40/2000, Step 36, d_loss: 0.4129087030887604, g_loss: 3.1924617290496826\n","Epoch 40/2000, Step 37, d_loss: 0.3596257269382477, g_loss: 3.3226871490478516\n","Epoch 40/2000, Step 38, d_loss: 0.4114716649055481, g_loss: 3.6481552124023438\n","Epoch 40/2000, Step 39, d_loss: 0.4020712673664093, g_loss: 4.12662935256958\n","Epoch 40/2000, Step 40, d_loss: 0.4093226194381714, g_loss: 3.6046504974365234\n","Epoch 40/2000, Step 41, d_loss: 0.41398313641548157, g_loss: 3.622653007507324\n","Epoch 40/2000, Step 42, d_loss: 0.38646090030670166, g_loss: 5.022802352905273\n","Epoch 40/2000, Step 43, d_loss: 0.3793388903141022, g_loss: 4.7756571769714355\n","Epoch 40/2000, Step 44, d_loss: 0.45155495405197144, g_loss: 4.920917987823486\n","Epoch 40/2000, Step 45, d_loss: 0.7081826329231262, g_loss: 3.56288480758667\n","Epoch 40/2000, Step 46, d_loss: 0.4819882810115814, g_loss: 3.1071548461914062\n","Epoch 40/2000, Step 47, d_loss: 0.5191055536270142, g_loss: 2.20841383934021\n","Epoch 40/2000, Step 48, d_loss: 0.5064058899879456, g_loss: 2.5675785541534424\n","Epoch 40/2000, Step 49, d_loss: 0.45312657952308655, g_loss: 2.7588233947753906\n","Epoch 40/2000, Step 50, d_loss: 0.4581924080848694, g_loss: 2.436300754547119\n","Epoch 40/2000, Step 51, d_loss: 0.40715229511260986, g_loss: 3.2971837520599365\n","Epoch 40/2000, Step 52, d_loss: 0.4299115836620331, g_loss: 3.6416640281677246\n","Epoch 40/2000, Step 53, d_loss: 0.4267197549343109, g_loss: 3.974461317062378\n","Epoch 40/2000, Step 54, d_loss: 0.5668109059333801, g_loss: 2.6974103450775146\n","Epoch 40/2000, Step 55, d_loss: 0.4561817944049835, g_loss: 2.607727527618408\n","Epoch 40/2000, Step 56, d_loss: 0.4404534697532654, g_loss: 2.5287728309631348\n","Epoch 40/2000, Step 57, d_loss: 0.43751928210258484, g_loss: 2.9279701709747314\n","Epoch 40/2000, Step 58, d_loss: 0.4774141311645508, g_loss: 3.414895534515381\n","Epoch 40/2000, Step 59, d_loss: 0.49611571431159973, g_loss: 3.6575229167938232\n","Epoch 40/2000, Step 60, d_loss: 0.40605002641677856, g_loss: 3.5235328674316406\n","Epoch 40/2000, Step 61, d_loss: 0.5125513076782227, g_loss: 5.040886878967285\n","Epoch 40/2000, Step 62, d_loss: 0.4050145745277405, g_loss: 4.077661037445068\n","Epoch 40/2000, Step 63, d_loss: 0.4363082945346832, g_loss: 4.052898406982422\n","Epoch 40/2000, Step 64, d_loss: 0.469390332698822, g_loss: 3.060609817504883\n","Epoch 40/2000, Step 65, d_loss: 0.3860536813735962, g_loss: 2.4329516887664795\n","Epoch 40/2000, Step 66, d_loss: 0.4474725127220154, g_loss: 2.466733694076538\n","Epoch 40/2000, Step 67, d_loss: 0.474196195602417, g_loss: 2.119380235671997\n","Epoch 40/2000, Step 68, d_loss: 0.4714341163635254, g_loss: 3.0243170261383057\n","Epoch 40/2000, Step 69, d_loss: 0.43831637501716614, g_loss: 4.265491962432861\n","Epoch 40/2000, Step 70, d_loss: 0.4033257067203522, g_loss: 3.346259117126465\n","Epoch 40/2000, Step 71, d_loss: 0.3893328011035919, g_loss: 5.249287128448486\n","Epoch 40/2000, Step 72, d_loss: 0.45445874333381653, g_loss: 3.671525716781616\n","Epoch 40/2000, Step 73, d_loss: 0.6029880046844482, g_loss: 3.924344778060913\n","Epoch 40/2000, Step 74, d_loss: 0.39074042439460754, g_loss: 2.588010311126709\n","Epoch 40/2000, Step 75, d_loss: 0.3997829258441925, g_loss: 2.991492986679077\n","Epoch 40/2000, Step 76, d_loss: 0.40415093302726746, g_loss: 2.468513250350952\n","Epoch 40/2000, Step 77, d_loss: 0.4419543743133545, g_loss: 2.256955623626709\n","Epoch 40/2000, Step 78, d_loss: 0.43258070945739746, g_loss: 3.3279600143432617\n","Epoch 40/2000, Step 79, d_loss: 0.38876038789749146, g_loss: 3.823539972305298\n","Epoch 40/2000, Step 80, d_loss: 0.4060325622558594, g_loss: 4.121689319610596\n","Epoch 40/2000, Step 81, d_loss: 0.4251781702041626, g_loss: 4.375637054443359\n","Epoch 40/2000, Step 82, d_loss: 0.4261550307273865, g_loss: 4.326735973358154\n","Epoch 40/2000, Step 83, d_loss: 0.5894110798835754, g_loss: 3.520491123199463\n","Epoch 40/2000, Step 84, d_loss: 0.45253786444664, g_loss: 2.8235080242156982\n","Epoch 40/2000, Step 85, d_loss: 0.4057944118976593, g_loss: 2.9923295974731445\n","Epoch 40/2000, Step 86, d_loss: 0.460294246673584, g_loss: 2.2410900592803955\n","Epoch 40/2000, Step 87, d_loss: 0.42492589354515076, g_loss: 3.62568736076355\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 41/2000, Step 1, d_loss: 0.47400790452957153, g_loss: 3.529794692993164\n","Epoch 41/2000, Step 2, d_loss: 0.43155351281166077, g_loss: 3.27941632270813\n","Epoch 41/2000, Step 3, d_loss: 0.40826576948165894, g_loss: 3.917358160018921\n","Epoch 41/2000, Step 4, d_loss: 0.42147400975227356, g_loss: 3.740950345993042\n","Epoch 41/2000, Step 5, d_loss: 0.5810306072235107, g_loss: 2.0672411918640137\n","Epoch 41/2000, Step 6, d_loss: 0.39900240302085876, g_loss: 2.930922508239746\n","Epoch 41/2000, Step 7, d_loss: 0.4153890013694763, g_loss: 3.087841272354126\n","Epoch 41/2000, Step 8, d_loss: 0.5040420889854431, g_loss: 2.420623779296875\n","Epoch 41/2000, Step 9, d_loss: 0.4551975727081299, g_loss: 2.51524019241333\n","Epoch 41/2000, Step 10, d_loss: 0.4714396297931671, g_loss: 3.517812490463257\n","Epoch 41/2000, Step 11, d_loss: 0.374335378408432, g_loss: 4.68632173538208\n","Epoch 41/2000, Step 12, d_loss: 0.43154147267341614, g_loss: 4.005280017852783\n","Epoch 41/2000, Step 13, d_loss: 0.37824803590774536, g_loss: 4.1301398277282715\n","Epoch 41/2000, Step 14, d_loss: 0.37018445134162903, g_loss: 4.201763153076172\n","Epoch 41/2000, Step 15, d_loss: 0.4450581967830658, g_loss: 3.987886667251587\n","Epoch 41/2000, Step 16, d_loss: 0.38029202818870544, g_loss: 4.118540287017822\n","Epoch 41/2000, Step 17, d_loss: 0.45788824558258057, g_loss: 3.9745054244995117\n","Epoch 41/2000, Step 18, d_loss: 0.3998335599899292, g_loss: 2.819960832595825\n","Epoch 41/2000, Step 19, d_loss: 0.4639732241630554, g_loss: 2.910648822784424\n","Epoch 41/2000, Step 20, d_loss: 0.4121606945991516, g_loss: 3.1993632316589355\n","Epoch 41/2000, Step 21, d_loss: 0.4055839478969574, g_loss: 3.793466806411743\n","Epoch 41/2000, Step 22, d_loss: 0.360751211643219, g_loss: 3.585143566131592\n","Epoch 41/2000, Step 23, d_loss: 0.37189921736717224, g_loss: 4.084555149078369\n","Epoch 41/2000, Step 24, d_loss: 0.39410400390625, g_loss: 3.4728856086730957\n","Epoch 41/2000, Step 25, d_loss: 0.36444297432899475, g_loss: 4.418750762939453\n","Epoch 41/2000, Step 26, d_loss: 0.4161865711212158, g_loss: 4.651844024658203\n","Epoch 41/2000, Step 27, d_loss: 0.38133835792541504, g_loss: 4.035283088684082\n","Epoch 41/2000, Step 28, d_loss: 0.3869357109069824, g_loss: 4.451096057891846\n","Epoch 41/2000, Step 29, d_loss: 0.4414111375808716, g_loss: 3.684281587600708\n","Epoch 41/2000, Step 30, d_loss: 0.4182596802711487, g_loss: 3.1114139556884766\n","Epoch 41/2000, Step 31, d_loss: 0.42131122946739197, g_loss: 3.8341498374938965\n","Epoch 41/2000, Step 32, d_loss: 0.4355764091014862, g_loss: 2.7749271392822266\n","Epoch 41/2000, Step 33, d_loss: 0.45681530237197876, g_loss: 3.6857895851135254\n","Epoch 41/2000, Step 34, d_loss: 0.4955957531929016, g_loss: 4.977668285369873\n","Epoch 41/2000, Step 35, d_loss: 0.40782713890075684, g_loss: 3.534541368484497\n","Epoch 41/2000, Step 36, d_loss: 0.4122503995895386, g_loss: 3.5229272842407227\n","Epoch 41/2000, Step 37, d_loss: 0.41639184951782227, g_loss: 3.6500749588012695\n","Epoch 41/2000, Step 38, d_loss: 0.46664008498191833, g_loss: 3.8544256687164307\n","Epoch 41/2000, Step 39, d_loss: 0.5543091893196106, g_loss: 3.288773775100708\n","Epoch 41/2000, Step 40, d_loss: 0.378505676984787, g_loss: 3.59830379486084\n","Epoch 41/2000, Step 41, d_loss: 0.42586907744407654, g_loss: 3.1382768154144287\n","Epoch 41/2000, Step 42, d_loss: 0.461813360452652, g_loss: 4.755228042602539\n","Epoch 41/2000, Step 43, d_loss: 0.49117279052734375, g_loss: 3.6929500102996826\n","Epoch 41/2000, Step 44, d_loss: 0.45823928713798523, g_loss: 4.388846397399902\n","Epoch 41/2000, Step 45, d_loss: 0.42408308386802673, g_loss: 5.158726215362549\n","Epoch 41/2000, Step 46, d_loss: 0.3754029870033264, g_loss: 3.6119930744171143\n","Epoch 41/2000, Step 47, d_loss: 0.3831533193588257, g_loss: 3.793703079223633\n","Epoch 41/2000, Step 48, d_loss: 0.5668788552284241, g_loss: 4.058877944946289\n","Epoch 41/2000, Step 49, d_loss: 0.43866994976997375, g_loss: 3.406475305557251\n","Epoch 41/2000, Step 50, d_loss: 0.3737933337688446, g_loss: 3.3683454990386963\n","Epoch 41/2000, Step 51, d_loss: 0.4575677514076233, g_loss: 2.8343353271484375\n","Epoch 41/2000, Step 52, d_loss: 0.374705046415329, g_loss: 2.589641571044922\n","Epoch 41/2000, Step 53, d_loss: 0.40811359882354736, g_loss: 2.4541327953338623\n","Epoch 41/2000, Step 54, d_loss: 0.39687761664390564, g_loss: 2.940707206726074\n","Epoch 41/2000, Step 55, d_loss: 0.40936726331710815, g_loss: 2.491076946258545\n","Epoch 41/2000, Step 56, d_loss: 0.455020010471344, g_loss: 4.101292610168457\n","Epoch 41/2000, Step 57, d_loss: 0.4254756569862366, g_loss: 4.077709674835205\n","Epoch 41/2000, Step 58, d_loss: 0.38634195923805237, g_loss: 4.447827339172363\n","Epoch 41/2000, Step 59, d_loss: 0.3727708160877228, g_loss: 4.764612197875977\n","Epoch 41/2000, Step 60, d_loss: 0.427196204662323, g_loss: 4.468072414398193\n","Epoch 41/2000, Step 61, d_loss: 0.461783766746521, g_loss: 3.419578790664673\n","Epoch 41/2000, Step 62, d_loss: 0.41027817130088806, g_loss: 3.0537397861480713\n","Epoch 41/2000, Step 63, d_loss: 0.4222829043865204, g_loss: 3.1459946632385254\n","Epoch 41/2000, Step 64, d_loss: 0.4126911461353302, g_loss: 3.0690293312072754\n","Epoch 41/2000, Step 65, d_loss: 0.415120393037796, g_loss: 2.9827563762664795\n","Epoch 41/2000, Step 66, d_loss: 0.5046599507331848, g_loss: 3.6324257850646973\n","Epoch 41/2000, Step 67, d_loss: 0.46420997381210327, g_loss: 3.0253586769104004\n","Epoch 41/2000, Step 68, d_loss: 0.3942243158817291, g_loss: 3.972749710083008\n","Epoch 41/2000, Step 69, d_loss: 0.6862293481826782, g_loss: 3.8335611820220947\n","Epoch 41/2000, Step 70, d_loss: 0.4177597463130951, g_loss: 3.318620443344116\n","Epoch 41/2000, Step 71, d_loss: 0.42816177010536194, g_loss: 2.262305736541748\n","Epoch 41/2000, Step 72, d_loss: 0.6404237747192383, g_loss: 1.8563036918640137\n","Epoch 41/2000, Step 73, d_loss: 0.5853838920593262, g_loss: 1.9910539388656616\n","Epoch 41/2000, Step 74, d_loss: 0.6235480308532715, g_loss: 3.1492955684661865\n","Epoch 41/2000, Step 75, d_loss: 0.49167799949645996, g_loss: 3.5629334449768066\n","Epoch 41/2000, Step 76, d_loss: 0.5082619190216064, g_loss: 4.147680282592773\n","Epoch 41/2000, Step 77, d_loss: 0.47261616587638855, g_loss: 3.9177002906799316\n","Epoch 41/2000, Step 78, d_loss: 0.5770053863525391, g_loss: 5.11297607421875\n","Epoch 41/2000, Step 79, d_loss: 0.47792455554008484, g_loss: 3.8269598484039307\n","Epoch 41/2000, Step 80, d_loss: 0.44690775871276855, g_loss: 2.793707847595215\n","Epoch 41/2000, Step 81, d_loss: 0.45752739906311035, g_loss: 2.533858299255371\n","Epoch 41/2000, Step 82, d_loss: 0.48940330743789673, g_loss: 2.6990368366241455\n","Epoch 41/2000, Step 83, d_loss: 0.433758407831192, g_loss: 3.5754964351654053\n","Epoch 41/2000, Step 84, d_loss: 0.39861705899238586, g_loss: 2.3286118507385254\n","Epoch 41/2000, Step 85, d_loss: 0.48127853870391846, g_loss: 4.035362243652344\n","Epoch 41/2000, Step 86, d_loss: 0.43071964383125305, g_loss: 2.8354744911193848\n","Epoch 41/2000, Step 87, d_loss: 0.3668059706687927, g_loss: 3.492727756500244\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 42/2000, Step 1, d_loss: 0.3860544264316559, g_loss: 4.135503768920898\n","Epoch 42/2000, Step 2, d_loss: 0.3782322108745575, g_loss: 4.738583564758301\n","Epoch 42/2000, Step 3, d_loss: 0.43014177680015564, g_loss: 4.830933094024658\n","Epoch 42/2000, Step 4, d_loss: 0.44230207800865173, g_loss: 5.038601398468018\n","Epoch 42/2000, Step 5, d_loss: 0.38101449608802795, g_loss: 3.5149500370025635\n","Epoch 42/2000, Step 6, d_loss: 0.4008582830429077, g_loss: 3.051138162612915\n","Epoch 42/2000, Step 7, d_loss: 0.44177156686782837, g_loss: 2.3812637329101562\n","Epoch 42/2000, Step 8, d_loss: 0.42390406131744385, g_loss: 2.965725898742676\n","Epoch 42/2000, Step 9, d_loss: 0.4207223355770111, g_loss: 3.20200514793396\n","Epoch 42/2000, Step 10, d_loss: 0.4867773652076721, g_loss: 3.3932442665100098\n","Epoch 42/2000, Step 11, d_loss: 0.4877215623855591, g_loss: 3.584995746612549\n","Epoch 42/2000, Step 12, d_loss: 0.3654312789440155, g_loss: 3.777632474899292\n","Epoch 42/2000, Step 13, d_loss: 0.39106157422065735, g_loss: 4.297740936279297\n","Epoch 42/2000, Step 14, d_loss: 0.4205668270587921, g_loss: 3.7596631050109863\n","Epoch 42/2000, Step 15, d_loss: 0.3970237970352173, g_loss: 4.07447624206543\n","Epoch 42/2000, Step 16, d_loss: 0.40586671233177185, g_loss: 2.7399351596832275\n","Epoch 42/2000, Step 17, d_loss: 0.3903493881225586, g_loss: 2.88008713722229\n","Epoch 42/2000, Step 18, d_loss: 0.41782647371292114, g_loss: 3.0236880779266357\n","Epoch 42/2000, Step 19, d_loss: 0.4217975437641144, g_loss: 2.4154422283172607\n","Epoch 42/2000, Step 20, d_loss: 0.38968366384506226, g_loss: 3.9108219146728516\n","Epoch 42/2000, Step 21, d_loss: 0.444970965385437, g_loss: 3.0270917415618896\n","Epoch 42/2000, Step 22, d_loss: 0.3994658291339874, g_loss: 2.517411708831787\n","Epoch 42/2000, Step 23, d_loss: 0.40269920229911804, g_loss: 3.3823752403259277\n","Epoch 42/2000, Step 24, d_loss: 0.4155365824699402, g_loss: 3.1113052368164062\n","Epoch 42/2000, Step 25, d_loss: 0.4637921452522278, g_loss: 4.398661136627197\n","Epoch 42/2000, Step 26, d_loss: 0.4079461395740509, g_loss: 4.443294048309326\n","Epoch 42/2000, Step 27, d_loss: 0.3794369697570801, g_loss: 4.313039779663086\n","Epoch 42/2000, Step 28, d_loss: 0.5708674192428589, g_loss: 3.636349678039551\n","Epoch 42/2000, Step 29, d_loss: 0.38372567296028137, g_loss: 3.7380354404449463\n","Epoch 42/2000, Step 30, d_loss: 0.43252527713775635, g_loss: 3.3830482959747314\n","Epoch 42/2000, Step 31, d_loss: 0.5700485110282898, g_loss: 3.2250325679779053\n","Epoch 42/2000, Step 32, d_loss: 0.3760438561439514, g_loss: 2.3525073528289795\n","Epoch 42/2000, Step 33, d_loss: 0.44940972328186035, g_loss: 2.5369784832000732\n","Epoch 42/2000, Step 34, d_loss: 0.40033337473869324, g_loss: 2.306342840194702\n","Epoch 42/2000, Step 35, d_loss: 0.4816005229949951, g_loss: 3.207310914993286\n","Epoch 42/2000, Step 36, d_loss: 0.5405628681182861, g_loss: 3.3810014724731445\n","Epoch 42/2000, Step 37, d_loss: 0.44112858176231384, g_loss: 3.3872318267822266\n","Epoch 42/2000, Step 38, d_loss: 0.43529558181762695, g_loss: 4.89501953125\n","Epoch 42/2000, Step 39, d_loss: 0.4556872248649597, g_loss: 5.001821041107178\n","Epoch 42/2000, Step 40, d_loss: 0.4391830861568451, g_loss: 4.875383377075195\n","Epoch 42/2000, Step 41, d_loss: 0.40067243576049805, g_loss: 4.0598039627075195\n","Epoch 42/2000, Step 42, d_loss: 0.45189356803894043, g_loss: 3.6986124515533447\n","Epoch 42/2000, Step 43, d_loss: 0.4107232689857483, g_loss: 3.4196791648864746\n","Epoch 42/2000, Step 44, d_loss: 0.41059863567352295, g_loss: 4.6884026527404785\n","Epoch 42/2000, Step 45, d_loss: 0.3912312686443329, g_loss: 4.064583778381348\n","Epoch 42/2000, Step 46, d_loss: 0.3810987174510956, g_loss: 4.758460521697998\n","Epoch 42/2000, Step 47, d_loss: 0.47907063364982605, g_loss: 3.7363393306732178\n","Epoch 42/2000, Step 48, d_loss: 0.40671882033348083, g_loss: 4.217534065246582\n","Epoch 42/2000, Step 49, d_loss: 0.4375378489494324, g_loss: 3.176645517349243\n","Epoch 42/2000, Step 50, d_loss: 0.3791056275367737, g_loss: 3.455291748046875\n","Epoch 42/2000, Step 51, d_loss: 0.44079306721687317, g_loss: 3.614224910736084\n","Epoch 42/2000, Step 52, d_loss: 0.3683279752731323, g_loss: 3.679593086242676\n","Epoch 42/2000, Step 53, d_loss: 0.39641061425209045, g_loss: 4.298387050628662\n","Epoch 42/2000, Step 54, d_loss: 0.4373241066932678, g_loss: 3.371513843536377\n","Epoch 42/2000, Step 55, d_loss: 0.3652051091194153, g_loss: 3.334352731704712\n","Epoch 42/2000, Step 56, d_loss: 0.3748035728931427, g_loss: 3.9969708919525146\n","Epoch 42/2000, Step 57, d_loss: 0.3534400761127472, g_loss: 3.5932886600494385\n","Epoch 42/2000, Step 58, d_loss: 0.3633754253387451, g_loss: 3.7344160079956055\n","Epoch 42/2000, Step 59, d_loss: 0.37459123134613037, g_loss: 3.943103551864624\n","Epoch 42/2000, Step 60, d_loss: 0.4036869406700134, g_loss: 3.8157567977905273\n","Epoch 42/2000, Step 61, d_loss: 0.3595386743545532, g_loss: 4.545342922210693\n","Epoch 42/2000, Step 62, d_loss: 0.3848823606967926, g_loss: 4.010140419006348\n","Epoch 42/2000, Step 63, d_loss: 0.5648584365844727, g_loss: 3.5628416538238525\n","Epoch 42/2000, Step 64, d_loss: 0.3577420711517334, g_loss: 3.4615418910980225\n","Epoch 42/2000, Step 65, d_loss: 0.4114893078804016, g_loss: 3.6541922092437744\n","Epoch 42/2000, Step 66, d_loss: 0.4040350914001465, g_loss: 3.2252206802368164\n","Epoch 42/2000, Step 67, d_loss: 0.4433329105377197, g_loss: 3.733996868133545\n","Epoch 42/2000, Step 68, d_loss: 0.38000184297561646, g_loss: 3.4476306438446045\n","Epoch 42/2000, Step 69, d_loss: 0.41141876578330994, g_loss: 4.067907810211182\n","Epoch 42/2000, Step 70, d_loss: 0.3952830135822296, g_loss: 3.7696962356567383\n","Epoch 42/2000, Step 71, d_loss: 0.5260043144226074, g_loss: 3.6750645637512207\n","Epoch 42/2000, Step 72, d_loss: 0.34325265884399414, g_loss: 3.33134126663208\n","Epoch 42/2000, Step 73, d_loss: 0.42170390486717224, g_loss: 2.839284658432007\n","Epoch 42/2000, Step 74, d_loss: 0.44521564245224, g_loss: 3.4317638874053955\n","Epoch 42/2000, Step 75, d_loss: 0.45650503039360046, g_loss: 3.1030044555664062\n","Epoch 42/2000, Step 76, d_loss: 0.4265952408313751, g_loss: 3.695016622543335\n","Epoch 42/2000, Step 77, d_loss: 0.4276476204395294, g_loss: 3.1843478679656982\n","Epoch 42/2000, Step 78, d_loss: 0.4185749292373657, g_loss: 4.399994373321533\n","Epoch 42/2000, Step 79, d_loss: 0.3724422752857208, g_loss: 5.265107154846191\n","Epoch 42/2000, Step 80, d_loss: 0.4280591607093811, g_loss: 3.932973623275757\n","Epoch 42/2000, Step 81, d_loss: 0.37165385484695435, g_loss: 3.910858392715454\n","Epoch 42/2000, Step 82, d_loss: 0.46971866488456726, g_loss: 3.669165849685669\n","Epoch 42/2000, Step 83, d_loss: 0.4087319076061249, g_loss: 3.5417535305023193\n","Epoch 42/2000, Step 84, d_loss: 0.39589476585388184, g_loss: 3.2285685539245605\n","Epoch 42/2000, Step 85, d_loss: 0.3696202337741852, g_loss: 2.3526463508605957\n","Epoch 42/2000, Step 86, d_loss: 0.4433332085609436, g_loss: 3.0374889373779297\n","Epoch 42/2000, Step 87, d_loss: 0.40163302421569824, g_loss: 3.7928836345672607\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 43/2000, Step 1, d_loss: 0.35891348123550415, g_loss: 3.194535970687866\n","Epoch 43/2000, Step 2, d_loss: 0.3955081105232239, g_loss: 4.09984827041626\n","Epoch 43/2000, Step 3, d_loss: 0.40607714653015137, g_loss: 4.4823737144470215\n","Epoch 43/2000, Step 4, d_loss: 0.428737074136734, g_loss: 4.167296409606934\n","Epoch 43/2000, Step 5, d_loss: 0.3662166893482208, g_loss: 3.530688524246216\n","Epoch 43/2000, Step 6, d_loss: 0.3512049913406372, g_loss: 3.801909923553467\n","Epoch 43/2000, Step 7, d_loss: 0.3992421329021454, g_loss: 3.2527973651885986\n","Epoch 43/2000, Step 8, d_loss: 0.3942505717277527, g_loss: 3.0955073833465576\n","Epoch 43/2000, Step 9, d_loss: 0.4098763167858124, g_loss: 2.417121171951294\n","Epoch 43/2000, Step 10, d_loss: 0.39591795206069946, g_loss: 3.2174673080444336\n","Epoch 43/2000, Step 11, d_loss: 0.40540972352027893, g_loss: 2.7795956134796143\n","Epoch 43/2000, Step 12, d_loss: 0.37507760524749756, g_loss: 3.707770824432373\n","Epoch 43/2000, Step 13, d_loss: 0.3561380207538605, g_loss: 3.6734633445739746\n","Epoch 43/2000, Step 14, d_loss: 0.3968553841114044, g_loss: 3.896512269973755\n","Epoch 43/2000, Step 15, d_loss: 0.4214852452278137, g_loss: 5.103862285614014\n","Epoch 43/2000, Step 16, d_loss: 0.4149690568447113, g_loss: 3.551072835922241\n","Epoch 43/2000, Step 17, d_loss: 0.3627075254917145, g_loss: 3.7821860313415527\n","Epoch 43/2000, Step 18, d_loss: 0.43974414467811584, g_loss: 4.534982204437256\n","Epoch 43/2000, Step 19, d_loss: 0.4581305682659149, g_loss: 3.32258939743042\n","Epoch 43/2000, Step 20, d_loss: 0.4390142560005188, g_loss: 3.577223062515259\n","Epoch 43/2000, Step 21, d_loss: 0.37054404616355896, g_loss: 3.612748622894287\n","Epoch 43/2000, Step 22, d_loss: 0.3450968861579895, g_loss: 3.232731342315674\n","Epoch 43/2000, Step 23, d_loss: 0.41026997566223145, g_loss: 4.469797611236572\n","Epoch 43/2000, Step 24, d_loss: 0.5248757600784302, g_loss: 3.772202491760254\n","Epoch 43/2000, Step 25, d_loss: 0.4262339770793915, g_loss: 3.6789321899414062\n","Epoch 43/2000, Step 26, d_loss: 0.4123365879058838, g_loss: 3.1501519680023193\n","Epoch 43/2000, Step 27, d_loss: 0.42456406354904175, g_loss: 2.6603808403015137\n","Epoch 43/2000, Step 28, d_loss: 0.3947460949420929, g_loss: 4.1005330085754395\n","Epoch 43/2000, Step 29, d_loss: 0.45283541083335876, g_loss: 4.434609889984131\n","Epoch 43/2000, Step 30, d_loss: 0.3696943521499634, g_loss: 3.134347438812256\n","Epoch 43/2000, Step 31, d_loss: 0.4319780468940735, g_loss: 3.2603201866149902\n","Epoch 43/2000, Step 32, d_loss: 0.5667046904563904, g_loss: 3.2991127967834473\n","Epoch 43/2000, Step 33, d_loss: 0.4580233097076416, g_loss: 3.4342830181121826\n","Epoch 43/2000, Step 34, d_loss: 0.45040836930274963, g_loss: 3.0928361415863037\n","Epoch 43/2000, Step 35, d_loss: 0.46438068151474, g_loss: 2.2692952156066895\n","Epoch 43/2000, Step 36, d_loss: 0.584613025188446, g_loss: 3.024916410446167\n","Epoch 43/2000, Step 37, d_loss: 0.4723139703273773, g_loss: 3.8138208389282227\n","Epoch 43/2000, Step 38, d_loss: 0.48050493001937866, g_loss: 3.126479387283325\n","Epoch 43/2000, Step 39, d_loss: 0.40760400891304016, g_loss: 4.156125545501709\n","Epoch 43/2000, Step 40, d_loss: 0.39124226570129395, g_loss: 3.5109100341796875\n","Epoch 43/2000, Step 41, d_loss: 0.41427111625671387, g_loss: 3.5095839500427246\n","Epoch 43/2000, Step 42, d_loss: 0.49250808358192444, g_loss: 3.6287424564361572\n","Epoch 43/2000, Step 43, d_loss: 0.4028274416923523, g_loss: 3.4953014850616455\n","Epoch 43/2000, Step 44, d_loss: 0.41077423095703125, g_loss: 3.5559611320495605\n","Epoch 43/2000, Step 45, d_loss: 0.4596322178840637, g_loss: 2.8189961910247803\n","Epoch 43/2000, Step 46, d_loss: 0.4533064365386963, g_loss: 4.330860614776611\n","Epoch 43/2000, Step 47, d_loss: 0.4381510019302368, g_loss: 4.0735249519348145\n","Epoch 43/2000, Step 48, d_loss: 0.4042694866657257, g_loss: 3.5533218383789062\n","Epoch 43/2000, Step 49, d_loss: 0.3759687840938568, g_loss: 4.088185787200928\n","Epoch 43/2000, Step 50, d_loss: 0.39329832792282104, g_loss: 3.36551570892334\n","Epoch 43/2000, Step 51, d_loss: 0.37719956040382385, g_loss: 4.2409162521362305\n","Epoch 43/2000, Step 52, d_loss: 0.3818686902523041, g_loss: 3.687532901763916\n","Epoch 43/2000, Step 53, d_loss: 0.41260504722595215, g_loss: 3.805039644241333\n","Epoch 43/2000, Step 54, d_loss: 0.3702395260334015, g_loss: 3.35699462890625\n","Epoch 43/2000, Step 55, d_loss: 0.45776480436325073, g_loss: 4.413699150085449\n","Epoch 43/2000, Step 56, d_loss: 0.3881228566169739, g_loss: 4.045758247375488\n","Epoch 43/2000, Step 57, d_loss: 0.3767210841178894, g_loss: 3.4759790897369385\n","Epoch 43/2000, Step 58, d_loss: 0.4021913707256317, g_loss: 3.4138009548187256\n","Epoch 43/2000, Step 59, d_loss: 0.40415462851524353, g_loss: 4.026888847351074\n","Epoch 43/2000, Step 60, d_loss: 0.38078564405441284, g_loss: 3.6815876960754395\n","Epoch 43/2000, Step 61, d_loss: 0.4073650538921356, g_loss: 3.339282989501953\n","Epoch 43/2000, Step 62, d_loss: 0.38859304785728455, g_loss: 3.504023790359497\n","Epoch 43/2000, Step 63, d_loss: 0.44463902711868286, g_loss: 2.676060676574707\n","Epoch 43/2000, Step 64, d_loss: 0.39150142669677734, g_loss: 2.634166717529297\n","Epoch 43/2000, Step 65, d_loss: 0.425589919090271, g_loss: 3.1080563068389893\n","Epoch 43/2000, Step 66, d_loss: 0.4405531883239746, g_loss: 3.3630483150482178\n","Epoch 43/2000, Step 67, d_loss: 0.46908196806907654, g_loss: 3.7483158111572266\n","Epoch 43/2000, Step 68, d_loss: 0.41422367095947266, g_loss: 4.252718448638916\n","Epoch 43/2000, Step 69, d_loss: 0.3914533853530884, g_loss: 4.208607196807861\n","Epoch 43/2000, Step 70, d_loss: 0.3667401373386383, g_loss: 3.7803804874420166\n","Epoch 43/2000, Step 71, d_loss: 0.4482162594795227, g_loss: 4.037240028381348\n","Epoch 43/2000, Step 72, d_loss: 0.37849515676498413, g_loss: 3.5708179473876953\n","Epoch 43/2000, Step 73, d_loss: 0.3869055509567261, g_loss: 3.99163556098938\n","Epoch 43/2000, Step 74, d_loss: 0.4575994610786438, g_loss: 3.8036866188049316\n","Epoch 43/2000, Step 75, d_loss: 0.41702109575271606, g_loss: 4.485562801361084\n","Epoch 43/2000, Step 76, d_loss: 0.3864366412162781, g_loss: 3.2059595584869385\n","Epoch 43/2000, Step 77, d_loss: 0.4192698299884796, g_loss: 3.5786542892456055\n","Epoch 43/2000, Step 78, d_loss: 0.4148506820201874, g_loss: 3.576716184616089\n","Epoch 43/2000, Step 79, d_loss: 0.382890522480011, g_loss: 4.405155658721924\n","Epoch 43/2000, Step 80, d_loss: 0.3806338310241699, g_loss: 3.9711525440216064\n","Epoch 43/2000, Step 81, d_loss: 0.37718141078948975, g_loss: 3.6218934059143066\n","Epoch 43/2000, Step 82, d_loss: 0.3653494715690613, g_loss: 4.171300888061523\n","Epoch 43/2000, Step 83, d_loss: 0.44030892848968506, g_loss: 3.2978463172912598\n","Epoch 43/2000, Step 84, d_loss: 0.41366714239120483, g_loss: 3.6560451984405518\n","Epoch 43/2000, Step 85, d_loss: 0.39395755529403687, g_loss: 3.6566202640533447\n","Epoch 43/2000, Step 86, d_loss: 0.3790242373943329, g_loss: 3.8913543224334717\n","Epoch 43/2000, Step 87, d_loss: 0.4144626259803772, g_loss: 3.8023838996887207\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 44/2000, Step 1, d_loss: 0.4019973874092102, g_loss: 5.243423938751221\n","Epoch 44/2000, Step 2, d_loss: 0.39759719371795654, g_loss: 3.7633919715881348\n","Epoch 44/2000, Step 3, d_loss: 0.4469102621078491, g_loss: 3.823045253753662\n","Epoch 44/2000, Step 4, d_loss: 0.3611343801021576, g_loss: 4.710371494293213\n","Epoch 44/2000, Step 5, d_loss: 0.36383387446403503, g_loss: 2.8281660079956055\n","Epoch 44/2000, Step 6, d_loss: 0.4477720260620117, g_loss: 3.5350537300109863\n","Epoch 44/2000, Step 7, d_loss: 0.4034357964992523, g_loss: 3.060214042663574\n","Epoch 44/2000, Step 8, d_loss: 0.4371434152126312, g_loss: 3.2730648517608643\n","Epoch 44/2000, Step 9, d_loss: 0.40138569474220276, g_loss: 4.050422191619873\n","Epoch 44/2000, Step 10, d_loss: 0.36887919902801514, g_loss: 5.082551002502441\n","Epoch 44/2000, Step 11, d_loss: 0.37143927812576294, g_loss: 4.846635341644287\n","Epoch 44/2000, Step 12, d_loss: 0.42507150769233704, g_loss: 4.304233074188232\n","Epoch 44/2000, Step 13, d_loss: 0.36545777320861816, g_loss: 3.876673936843872\n","Epoch 44/2000, Step 14, d_loss: 0.5151771903038025, g_loss: 3.017713785171509\n","Epoch 44/2000, Step 15, d_loss: 0.3973024785518646, g_loss: 2.564424991607666\n","Epoch 44/2000, Step 16, d_loss: 0.39513614773750305, g_loss: 3.074220895767212\n","Epoch 44/2000, Step 17, d_loss: 0.4441876709461212, g_loss: 2.9329898357391357\n","Epoch 44/2000, Step 18, d_loss: 0.511991560459137, g_loss: 2.982116222381592\n","Epoch 44/2000, Step 19, d_loss: 0.44574031233787537, g_loss: 3.567223072052002\n","Epoch 44/2000, Step 20, d_loss: 0.43749016523361206, g_loss: 3.279798984527588\n","Epoch 44/2000, Step 21, d_loss: 0.3817747235298157, g_loss: 3.856046199798584\n","Epoch 44/2000, Step 22, d_loss: 0.35758641362190247, g_loss: 4.515422821044922\n","Epoch 44/2000, Step 23, d_loss: 0.39101558923721313, g_loss: 4.364551544189453\n","Epoch 44/2000, Step 24, d_loss: 0.4938204288482666, g_loss: 4.928493976593018\n","Epoch 44/2000, Step 25, d_loss: 0.48545029759407043, g_loss: 3.662710189819336\n","Epoch 44/2000, Step 26, d_loss: 0.38127756118774414, g_loss: 3.910911798477173\n","Epoch 44/2000, Step 27, d_loss: 0.39099517464637756, g_loss: 3.1580417156219482\n","Epoch 44/2000, Step 28, d_loss: 0.4628913402557373, g_loss: 3.2301113605499268\n","Epoch 44/2000, Step 29, d_loss: 0.5211993455886841, g_loss: 4.002910137176514\n","Epoch 44/2000, Step 30, d_loss: 0.5122219324111938, g_loss: 2.753159284591675\n","Epoch 44/2000, Step 31, d_loss: 0.4518384337425232, g_loss: 3.298835515975952\n","Epoch 44/2000, Step 32, d_loss: 0.37105458974838257, g_loss: 3.2782177925109863\n","Epoch 44/2000, Step 33, d_loss: 0.41942882537841797, g_loss: 3.7787721157073975\n","Epoch 44/2000, Step 34, d_loss: 0.40949469804763794, g_loss: 3.719008684158325\n","Epoch 44/2000, Step 35, d_loss: 0.41726988554000854, g_loss: 4.078637599945068\n","Epoch 44/2000, Step 36, d_loss: 0.40883633494377136, g_loss: 4.191575050354004\n","Epoch 44/2000, Step 37, d_loss: 0.4766184091567993, g_loss: 3.6962478160858154\n","Epoch 44/2000, Step 38, d_loss: 0.36648479104042053, g_loss: 3.042715549468994\n","Epoch 44/2000, Step 39, d_loss: 0.45992347598075867, g_loss: 4.336795806884766\n","Epoch 44/2000, Step 40, d_loss: 0.39671552181243896, g_loss: 2.695232629776001\n","Epoch 44/2000, Step 41, d_loss: 0.48069411516189575, g_loss: 3.1012165546417236\n","Epoch 44/2000, Step 42, d_loss: 0.550432026386261, g_loss: 3.7619285583496094\n","Epoch 44/2000, Step 43, d_loss: 0.4801775813102722, g_loss: 4.781290531158447\n","Epoch 44/2000, Step 44, d_loss: 0.5014511942863464, g_loss: 3.465933084487915\n","Epoch 44/2000, Step 45, d_loss: 0.3897917866706848, g_loss: 6.109029769897461\n","Epoch 44/2000, Step 46, d_loss: 0.6723383069038391, g_loss: 5.170686721801758\n","Epoch 44/2000, Step 47, d_loss: 0.5881937146186829, g_loss: 3.9526209831237793\n","Epoch 44/2000, Step 48, d_loss: 0.3902377784252167, g_loss: 3.072723388671875\n","Epoch 44/2000, Step 49, d_loss: 0.4190825819969177, g_loss: 3.1682114601135254\n","Epoch 44/2000, Step 50, d_loss: 0.4855826795101166, g_loss: 2.2326951026916504\n","Epoch 44/2000, Step 51, d_loss: 0.503692626953125, g_loss: 3.1541688442230225\n","Epoch 44/2000, Step 52, d_loss: 0.4193487763404846, g_loss: 3.5286166667938232\n","Epoch 44/2000, Step 53, d_loss: 0.38347601890563965, g_loss: 3.5283122062683105\n","Epoch 44/2000, Step 54, d_loss: 0.3980841040611267, g_loss: 4.549324035644531\n","Epoch 44/2000, Step 55, d_loss: 0.369267076253891, g_loss: 4.615018844604492\n","Epoch 44/2000, Step 56, d_loss: 0.5075695514678955, g_loss: 3.614443778991699\n","Epoch 44/2000, Step 57, d_loss: 0.4060893654823303, g_loss: 2.485374689102173\n","Epoch 44/2000, Step 58, d_loss: 0.4689784049987793, g_loss: 4.400783061981201\n","Epoch 44/2000, Step 59, d_loss: 0.41428494453430176, g_loss: 5.040664196014404\n","Epoch 44/2000, Step 60, d_loss: 0.3997868001461029, g_loss: 3.419965982437134\n","Epoch 44/2000, Step 61, d_loss: 0.3852953314781189, g_loss: 4.532660484313965\n","Epoch 44/2000, Step 62, d_loss: 0.4387907087802887, g_loss: 4.040122032165527\n","Epoch 44/2000, Step 63, d_loss: 0.46285563707351685, g_loss: 4.5886640548706055\n","Epoch 44/2000, Step 64, d_loss: 0.3879525065422058, g_loss: 2.8315491676330566\n","Epoch 44/2000, Step 65, d_loss: 0.3810955584049225, g_loss: 3.535381317138672\n","Epoch 44/2000, Step 66, d_loss: 0.46752068400382996, g_loss: 3.4084653854370117\n","Epoch 44/2000, Step 67, d_loss: 0.44623932242393494, g_loss: 4.218522548675537\n","Epoch 44/2000, Step 68, d_loss: 0.372067928314209, g_loss: 4.153297424316406\n","Epoch 44/2000, Step 69, d_loss: 0.37411680817604065, g_loss: 4.48597526550293\n","Epoch 44/2000, Step 70, d_loss: 0.35976505279541016, g_loss: 5.643850326538086\n","Epoch 44/2000, Step 71, d_loss: 0.4379572570323944, g_loss: 5.546918869018555\n","Epoch 44/2000, Step 72, d_loss: 0.36459484696388245, g_loss: 3.9874818325042725\n","Epoch 44/2000, Step 73, d_loss: 0.5016275644302368, g_loss: 4.779148578643799\n","Epoch 44/2000, Step 74, d_loss: 0.3737773597240448, g_loss: 4.711137771606445\n","Epoch 44/2000, Step 75, d_loss: 0.4157271385192871, g_loss: 4.480153560638428\n","Epoch 44/2000, Step 76, d_loss: 0.4244959354400635, g_loss: 2.947619915008545\n","Epoch 44/2000, Step 77, d_loss: 0.42942193150520325, g_loss: 2.7989213466644287\n","Epoch 44/2000, Step 78, d_loss: 0.394879549741745, g_loss: 4.056009769439697\n","Epoch 44/2000, Step 79, d_loss: 0.4265885651111603, g_loss: 3.4068849086761475\n","Epoch 44/2000, Step 80, d_loss: 0.3608424663543701, g_loss: 3.019691228866577\n","Epoch 44/2000, Step 81, d_loss: 0.369743674993515, g_loss: 4.4757795333862305\n","Epoch 44/2000, Step 82, d_loss: 0.37242990732192993, g_loss: 4.037004470825195\n","Epoch 44/2000, Step 83, d_loss: 0.36219653487205505, g_loss: 4.072463512420654\n","Epoch 44/2000, Step 84, d_loss: 0.3744783401489258, g_loss: 3.7665820121765137\n","Epoch 44/2000, Step 85, d_loss: 0.3986189663410187, g_loss: 5.38652229309082\n","Epoch 44/2000, Step 86, d_loss: 0.3936903476715088, g_loss: 4.304234027862549\n","Epoch 44/2000, Step 87, d_loss: 0.4254535138607025, g_loss: 3.7905707359313965\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 45/2000, Step 1, d_loss: 0.3696517050266266, g_loss: 2.5189404487609863\n","Epoch 45/2000, Step 2, d_loss: 0.37371572852134705, g_loss: 3.6083757877349854\n","Epoch 45/2000, Step 3, d_loss: 0.40937539935112, g_loss: 3.1883232593536377\n","Epoch 45/2000, Step 4, d_loss: 0.38581275939941406, g_loss: 3.8476266860961914\n","Epoch 45/2000, Step 5, d_loss: 0.44184091687202454, g_loss: 4.116063117980957\n","Epoch 45/2000, Step 6, d_loss: 0.42020589113235474, g_loss: 3.5940892696380615\n","Epoch 45/2000, Step 7, d_loss: 0.39387115836143494, g_loss: 4.047816276550293\n","Epoch 45/2000, Step 8, d_loss: 0.3811054527759552, g_loss: 3.1079201698303223\n","Epoch 45/2000, Step 9, d_loss: 0.38365447521209717, g_loss: 4.206434726715088\n","Epoch 45/2000, Step 10, d_loss: 0.3460387885570526, g_loss: 3.288727045059204\n","Epoch 45/2000, Step 11, d_loss: 0.3742831349372864, g_loss: 5.466506004333496\n","Epoch 45/2000, Step 12, d_loss: 0.4997592866420746, g_loss: 5.059442520141602\n","Epoch 45/2000, Step 13, d_loss: 0.36444389820098877, g_loss: 4.9496283531188965\n","Epoch 45/2000, Step 14, d_loss: 0.35288554430007935, g_loss: 4.2236738204956055\n","Epoch 45/2000, Step 15, d_loss: 0.4440150558948517, g_loss: 3.331796407699585\n","Epoch 45/2000, Step 16, d_loss: 0.38006874918937683, g_loss: 4.052420616149902\n","Epoch 45/2000, Step 17, d_loss: 0.429581880569458, g_loss: 2.674433708190918\n","Epoch 45/2000, Step 18, d_loss: 0.3732656240463257, g_loss: 2.9229936599731445\n","Epoch 45/2000, Step 19, d_loss: 0.39296743273735046, g_loss: 3.3807315826416016\n","Epoch 45/2000, Step 20, d_loss: 0.41159582138061523, g_loss: 3.7137789726257324\n","Epoch 45/2000, Step 21, d_loss: 0.3726379871368408, g_loss: 4.404228210449219\n","Epoch 45/2000, Step 22, d_loss: 0.47159990668296814, g_loss: 4.463764667510986\n","Epoch 45/2000, Step 23, d_loss: 0.3748823404312134, g_loss: 3.552741050720215\n","Epoch 45/2000, Step 24, d_loss: 0.3945053517818451, g_loss: 4.01054573059082\n","Epoch 45/2000, Step 25, d_loss: 0.3658924400806427, g_loss: 4.19094705581665\n","Epoch 45/2000, Step 26, d_loss: 0.4917603135108948, g_loss: 3.051309108734131\n","Epoch 45/2000, Step 27, d_loss: 0.36999642848968506, g_loss: 3.0319342613220215\n","Epoch 45/2000, Step 28, d_loss: 0.40367504954338074, g_loss: 3.1629645824432373\n","Epoch 45/2000, Step 29, d_loss: 0.39003369212150574, g_loss: 3.054178476333618\n","Epoch 45/2000, Step 30, d_loss: 0.3801148235797882, g_loss: 4.479926586151123\n","Epoch 45/2000, Step 31, d_loss: 0.39948928356170654, g_loss: 2.880152702331543\n","Epoch 45/2000, Step 32, d_loss: 0.4014017879962921, g_loss: 3.528895139694214\n","Epoch 45/2000, Step 33, d_loss: 0.3940422236919403, g_loss: 2.917557954788208\n","Epoch 45/2000, Step 34, d_loss: 0.3619617223739624, g_loss: 4.256737232208252\n","Epoch 45/2000, Step 35, d_loss: 0.42518025636672974, g_loss: 4.373098373413086\n","Epoch 45/2000, Step 36, d_loss: 0.5092451572418213, g_loss: 3.6040685176849365\n","Epoch 45/2000, Step 37, d_loss: 0.35863056778907776, g_loss: 3.315232276916504\n","Epoch 45/2000, Step 38, d_loss: 0.37600693106651306, g_loss: 4.577187538146973\n","Epoch 45/2000, Step 39, d_loss: 0.48626500368118286, g_loss: 4.008666515350342\n","Epoch 45/2000, Step 40, d_loss: 0.4068148732185364, g_loss: 3.6587564945220947\n","Epoch 45/2000, Step 41, d_loss: 0.4358122646808624, g_loss: 3.767890453338623\n","Epoch 45/2000, Step 42, d_loss: 0.4311963617801666, g_loss: 5.6226959228515625\n","Epoch 45/2000, Step 43, d_loss: 0.4067050516605377, g_loss: 4.1702728271484375\n","Epoch 45/2000, Step 44, d_loss: 0.400301456451416, g_loss: 5.187160015106201\n","Epoch 45/2000, Step 45, d_loss: 0.3924492597579956, g_loss: 3.5936367511749268\n","Epoch 45/2000, Step 46, d_loss: 0.35827070474624634, g_loss: 4.659434795379639\n","Epoch 45/2000, Step 47, d_loss: 0.37808018922805786, g_loss: 3.5549919605255127\n","Epoch 45/2000, Step 48, d_loss: 0.37348371744155884, g_loss: 4.614109516143799\n","Epoch 45/2000, Step 49, d_loss: 0.3686852753162384, g_loss: 3.1595957279205322\n","Epoch 45/2000, Step 50, d_loss: 0.3824232816696167, g_loss: 3.1887195110321045\n","Epoch 45/2000, Step 51, d_loss: 0.4449194669723511, g_loss: 4.759406089782715\n","Epoch 45/2000, Step 52, d_loss: 0.4035172164440155, g_loss: 3.6357929706573486\n","Epoch 45/2000, Step 53, d_loss: 0.3491661250591278, g_loss: 3.8901896476745605\n","Epoch 45/2000, Step 54, d_loss: 0.39433515071868896, g_loss: 4.113529682159424\n","Epoch 45/2000, Step 55, d_loss: 0.4761543571949005, g_loss: 4.613996505737305\n","Epoch 45/2000, Step 56, d_loss: 0.4139626622200012, g_loss: 4.090888023376465\n","Epoch 45/2000, Step 57, d_loss: 0.3776335120201111, g_loss: 3.6199474334716797\n","Epoch 45/2000, Step 58, d_loss: 0.4994937479496002, g_loss: 3.5004031658172607\n","Epoch 45/2000, Step 59, d_loss: 0.39089235663414, g_loss: 3.227764368057251\n","Epoch 45/2000, Step 60, d_loss: 0.43402448296546936, g_loss: 4.072793960571289\n","Epoch 45/2000, Step 61, d_loss: 0.40685710310935974, g_loss: 2.511805534362793\n","Epoch 45/2000, Step 62, d_loss: 0.44606107473373413, g_loss: 2.9102771282196045\n","Epoch 45/2000, Step 63, d_loss: 0.513861894607544, g_loss: 3.7269320487976074\n","Epoch 45/2000, Step 64, d_loss: 0.40054845809936523, g_loss: 3.4192960262298584\n","Epoch 45/2000, Step 65, d_loss: 0.4187290370464325, g_loss: 4.293022632598877\n","Epoch 45/2000, Step 66, d_loss: 0.431427925825119, g_loss: 4.323015213012695\n","Epoch 45/2000, Step 67, d_loss: 0.4650494456291199, g_loss: 4.928377151489258\n","Epoch 45/2000, Step 68, d_loss: 0.4168827533721924, g_loss: 4.113739490509033\n","Epoch 45/2000, Step 69, d_loss: 0.4075193405151367, g_loss: 2.7532904148101807\n","Epoch 45/2000, Step 70, d_loss: 0.42242375016212463, g_loss: 3.2195169925689697\n","Epoch 45/2000, Step 71, d_loss: 0.4855473041534424, g_loss: 3.498732805252075\n","Epoch 45/2000, Step 72, d_loss: 0.3899421989917755, g_loss: 3.210955858230591\n","Epoch 45/2000, Step 73, d_loss: 0.3673853278160095, g_loss: 3.886798143386841\n","Epoch 45/2000, Step 74, d_loss: 0.4484708905220032, g_loss: 4.70609188079834\n","Epoch 45/2000, Step 75, d_loss: 0.3641767203807831, g_loss: 5.062259197235107\n","Epoch 45/2000, Step 76, d_loss: 0.4767630696296692, g_loss: 4.307642936706543\n","Epoch 45/2000, Step 77, d_loss: 0.44879966974258423, g_loss: 4.0783371925354\n","Epoch 45/2000, Step 78, d_loss: 0.352582186460495, g_loss: 3.9088096618652344\n","Epoch 45/2000, Step 79, d_loss: 0.3927123546600342, g_loss: 3.1357715129852295\n","Epoch 45/2000, Step 80, d_loss: 0.39470258355140686, g_loss: 5.987716197967529\n","Epoch 45/2000, Step 81, d_loss: 0.3860376179218292, g_loss: 5.054562568664551\n","Epoch 45/2000, Step 82, d_loss: 0.385666161775589, g_loss: 4.3272576332092285\n","Epoch 45/2000, Step 83, d_loss: 0.3700396418571472, g_loss: 3.8908066749572754\n","Epoch 45/2000, Step 84, d_loss: 0.345938503742218, g_loss: 4.784379482269287\n","Epoch 45/2000, Step 85, d_loss: 0.42389798164367676, g_loss: 4.17854642868042\n","Epoch 45/2000, Step 86, d_loss: 0.40862688422203064, g_loss: 4.400914669036865\n","Epoch 45/2000, Step 87, d_loss: 0.4119957387447357, g_loss: 3.4577910900115967\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 46/2000, Step 1, d_loss: 0.3785795271396637, g_loss: 2.974928855895996\n","Epoch 46/2000, Step 2, d_loss: 0.4282739758491516, g_loss: 2.3940775394439697\n","Epoch 46/2000, Step 3, d_loss: 0.46240586042404175, g_loss: 1.8699530363082886\n","Epoch 46/2000, Step 4, d_loss: 0.4548117518424988, g_loss: 3.778294563293457\n","Epoch 46/2000, Step 5, d_loss: 0.41381460428237915, g_loss: 3.382272720336914\n","Epoch 46/2000, Step 6, d_loss: 0.41296839714050293, g_loss: 3.483504056930542\n","Epoch 46/2000, Step 7, d_loss: 0.37637171149253845, g_loss: 4.723616600036621\n","Epoch 46/2000, Step 8, d_loss: 0.3704914152622223, g_loss: 5.836649417877197\n","Epoch 46/2000, Step 9, d_loss: 0.8673534393310547, g_loss: 4.748892784118652\n","Epoch 46/2000, Step 10, d_loss: 0.39496707916259766, g_loss: 3.5206644535064697\n","Epoch 46/2000, Step 11, d_loss: 0.4060268700122833, g_loss: 2.8781321048736572\n","Epoch 46/2000, Step 12, d_loss: 0.4204443097114563, g_loss: 2.363527297973633\n","Epoch 46/2000, Step 13, d_loss: 0.5308389663696289, g_loss: 2.5050313472747803\n","Epoch 46/2000, Step 14, d_loss: 0.6530818343162537, g_loss: 2.55531644821167\n","Epoch 46/2000, Step 15, d_loss: 0.42469239234924316, g_loss: 3.6475136280059814\n","Epoch 46/2000, Step 16, d_loss: 0.4178503751754761, g_loss: 4.359054088592529\n","Epoch 46/2000, Step 17, d_loss: 0.5924205183982849, g_loss: 4.840324878692627\n","Epoch 46/2000, Step 18, d_loss: 0.3917481601238251, g_loss: 3.894026041030884\n","Epoch 46/2000, Step 19, d_loss: 0.5056747198104858, g_loss: 3.1396257877349854\n","Epoch 46/2000, Step 20, d_loss: 0.3993307948112488, g_loss: 2.669278860092163\n","Epoch 46/2000, Step 21, d_loss: 0.4123970568180084, g_loss: 3.1781458854675293\n","Epoch 46/2000, Step 22, d_loss: 0.36242735385894775, g_loss: 3.5119335651397705\n","Epoch 46/2000, Step 23, d_loss: 0.4595649242401123, g_loss: 3.6762847900390625\n","Epoch 46/2000, Step 24, d_loss: 0.38816922903060913, g_loss: 3.1900556087493896\n","Epoch 46/2000, Step 25, d_loss: 0.42036888003349304, g_loss: 3.324193239212036\n","Epoch 46/2000, Step 26, d_loss: 0.4326592683792114, g_loss: 3.3355822563171387\n","Epoch 46/2000, Step 27, d_loss: 0.4160357415676117, g_loss: 3.0328261852264404\n","Epoch 46/2000, Step 28, d_loss: 0.3830062747001648, g_loss: 2.6205856800079346\n","Epoch 46/2000, Step 29, d_loss: 0.3924145996570587, g_loss: 4.075734615325928\n","Epoch 46/2000, Step 30, d_loss: 0.44382041692733765, g_loss: 2.951807737350464\n","Epoch 46/2000, Step 31, d_loss: 0.4624652564525604, g_loss: 4.40482759475708\n","Epoch 46/2000, Step 32, d_loss: 0.42972373962402344, g_loss: 3.2280805110931396\n","Epoch 46/2000, Step 33, d_loss: 0.37064969539642334, g_loss: 3.3434019088745117\n","Epoch 46/2000, Step 34, d_loss: 0.3873160779476166, g_loss: 3.2297825813293457\n","Epoch 46/2000, Step 35, d_loss: 0.4247255325317383, g_loss: 2.12516188621521\n","Epoch 46/2000, Step 36, d_loss: 0.4936857521533966, g_loss: 3.2714877128601074\n","Epoch 46/2000, Step 37, d_loss: 0.4380471408367157, g_loss: 2.402524709701538\n","Epoch 46/2000, Step 38, d_loss: 0.5388602614402771, g_loss: 2.999413013458252\n","Epoch 46/2000, Step 39, d_loss: 0.40052059292793274, g_loss: 4.592175006866455\n","Epoch 46/2000, Step 40, d_loss: 0.4477303624153137, g_loss: 4.962394714355469\n","Epoch 46/2000, Step 41, d_loss: 0.5738170742988586, g_loss: 4.104197025299072\n","Epoch 46/2000, Step 42, d_loss: 0.4968976676464081, g_loss: 3.717470169067383\n","Epoch 46/2000, Step 43, d_loss: 0.39101743698120117, g_loss: 2.897528648376465\n","Epoch 46/2000, Step 44, d_loss: 0.47073662281036377, g_loss: 3.26564884185791\n","Epoch 46/2000, Step 45, d_loss: 0.5393581986427307, g_loss: 2.5543930530548096\n","Epoch 46/2000, Step 46, d_loss: 0.4587947130203247, g_loss: 3.140104293823242\n","Epoch 46/2000, Step 47, d_loss: 0.4235721826553345, g_loss: 3.0653693675994873\n","Epoch 46/2000, Step 48, d_loss: 0.4915909171104431, g_loss: 2.671933174133301\n","Epoch 46/2000, Step 49, d_loss: 0.46673035621643066, g_loss: 3.8270750045776367\n","Epoch 46/2000, Step 50, d_loss: 0.4080166518688202, g_loss: 4.391984462738037\n","Epoch 46/2000, Step 51, d_loss: 0.4482455551624298, g_loss: 3.314657688140869\n","Epoch 46/2000, Step 52, d_loss: 0.45921939611434937, g_loss: 3.7339725494384766\n","Epoch 46/2000, Step 53, d_loss: 0.3889913260936737, g_loss: 3.192039728164673\n","Epoch 46/2000, Step 54, d_loss: 0.3780474066734314, g_loss: 3.9871878623962402\n","Epoch 46/2000, Step 55, d_loss: 0.42983725666999817, g_loss: 4.250268936157227\n","Epoch 46/2000, Step 56, d_loss: 0.4660974144935608, g_loss: 3.3389718532562256\n","Epoch 46/2000, Step 57, d_loss: 0.4045315682888031, g_loss: 3.981774091720581\n","Epoch 46/2000, Step 58, d_loss: 0.39432668685913086, g_loss: 4.065878868103027\n","Epoch 46/2000, Step 59, d_loss: 0.37195536494255066, g_loss: 3.598734140396118\n","Epoch 46/2000, Step 60, d_loss: 0.45505866408348083, g_loss: 3.7989821434020996\n","Epoch 46/2000, Step 61, d_loss: 0.45780149102211, g_loss: 4.351500034332275\n","Epoch 46/2000, Step 62, d_loss: 0.43888452649116516, g_loss: 3.7622263431549072\n","Epoch 46/2000, Step 63, d_loss: 0.44736677408218384, g_loss: 4.584479331970215\n","Epoch 46/2000, Step 64, d_loss: 0.3889767825603485, g_loss: 4.70616340637207\n","Epoch 46/2000, Step 65, d_loss: 0.3751177191734314, g_loss: 4.458292007446289\n","Epoch 46/2000, Step 66, d_loss: 0.37430840730667114, g_loss: 3.7276763916015625\n","Epoch 46/2000, Step 67, d_loss: 0.505725085735321, g_loss: 3.887712001800537\n","Epoch 46/2000, Step 68, d_loss: 0.4768342077732086, g_loss: 3.6199145317077637\n","Epoch 46/2000, Step 69, d_loss: 0.4388315975666046, g_loss: 2.9320406913757324\n","Epoch 46/2000, Step 70, d_loss: 0.43459853529930115, g_loss: 2.380997896194458\n","Epoch 46/2000, Step 71, d_loss: 0.47667306661605835, g_loss: 2.6823060512542725\n","Epoch 46/2000, Step 72, d_loss: 0.44100815057754517, g_loss: 3.5726685523986816\n","Epoch 46/2000, Step 73, d_loss: 0.4007328748703003, g_loss: 4.016801357269287\n","Epoch 46/2000, Step 74, d_loss: 0.44373205304145813, g_loss: 2.8361966609954834\n","Epoch 46/2000, Step 75, d_loss: 0.38459330797195435, g_loss: 3.6720521450042725\n","Epoch 46/2000, Step 76, d_loss: 0.4806484878063202, g_loss: 3.780768632888794\n","Epoch 46/2000, Step 77, d_loss: 0.39306309819221497, g_loss: 3.7578728199005127\n","Epoch 46/2000, Step 78, d_loss: 0.4099699854850769, g_loss: 3.1229071617126465\n","Epoch 46/2000, Step 79, d_loss: 0.4070211350917816, g_loss: 3.2943172454833984\n","Epoch 46/2000, Step 80, d_loss: 0.40367376804351807, g_loss: 3.7373569011688232\n","Epoch 46/2000, Step 81, d_loss: 0.43264591693878174, g_loss: 3.9089150428771973\n","Epoch 46/2000, Step 82, d_loss: 0.3797683119773865, g_loss: 3.0931034088134766\n","Epoch 46/2000, Step 83, d_loss: 0.39527395367622375, g_loss: 3.8400046825408936\n","Epoch 46/2000, Step 84, d_loss: 0.3786846697330475, g_loss: 5.141147613525391\n","Epoch 46/2000, Step 85, d_loss: 0.49233993887901306, g_loss: 3.1057536602020264\n","Epoch 46/2000, Step 86, d_loss: 0.4577352702617645, g_loss: 2.74666690826416\n","Epoch 46/2000, Step 87, d_loss: 0.4001319408416748, g_loss: 2.029554843902588\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 47/2000, Step 1, d_loss: 0.6635115146636963, g_loss: 2.355821132659912\n","Epoch 47/2000, Step 2, d_loss: 0.4270033538341522, g_loss: 2.217926263809204\n","Epoch 47/2000, Step 3, d_loss: 0.453016459941864, g_loss: 2.2010324001312256\n","Epoch 47/2000, Step 4, d_loss: 0.4679618775844574, g_loss: 3.603306293487549\n","Epoch 47/2000, Step 5, d_loss: 0.4012846052646637, g_loss: 3.866358518600464\n","Epoch 47/2000, Step 6, d_loss: 0.40142327547073364, g_loss: 4.664561748504639\n","Epoch 47/2000, Step 7, d_loss: 0.5015839338302612, g_loss: 4.718475818634033\n","Epoch 47/2000, Step 8, d_loss: 0.4781915843486786, g_loss: 5.536346912384033\n","Epoch 47/2000, Step 9, d_loss: 0.5172725319862366, g_loss: 3.9403841495513916\n","Epoch 47/2000, Step 10, d_loss: 0.3614116609096527, g_loss: 3.2722625732421875\n","Epoch 47/2000, Step 11, d_loss: 0.4194149971008301, g_loss: 2.4119462966918945\n","Epoch 47/2000, Step 12, d_loss: 0.5538344383239746, g_loss: 2.1522202491760254\n","Epoch 47/2000, Step 13, d_loss: 0.5416247844696045, g_loss: 2.3207995891571045\n","Epoch 47/2000, Step 14, d_loss: 0.43341711163520813, g_loss: 2.9969794750213623\n","Epoch 47/2000, Step 15, d_loss: 0.3729649484157562, g_loss: 4.2283854484558105\n","Epoch 47/2000, Step 16, d_loss: 0.42579934000968933, g_loss: 3.3925588130950928\n","Epoch 47/2000, Step 17, d_loss: 0.4046357572078705, g_loss: 4.169735431671143\n","Epoch 47/2000, Step 18, d_loss: 0.46181386709213257, g_loss: 4.137730121612549\n","Epoch 47/2000, Step 19, d_loss: 0.41246768832206726, g_loss: 4.264666557312012\n","Epoch 47/2000, Step 20, d_loss: 0.37732067704200745, g_loss: 3.2663280963897705\n","Epoch 47/2000, Step 21, d_loss: 0.3923228681087494, g_loss: 3.4673359394073486\n","Epoch 47/2000, Step 22, d_loss: 0.4306388199329376, g_loss: 3.51900053024292\n","Epoch 47/2000, Step 23, d_loss: 0.4121270477771759, g_loss: 3.5389227867126465\n","Epoch 47/2000, Step 24, d_loss: 0.7261040210723877, g_loss: 4.386161804199219\n","Epoch 47/2000, Step 25, d_loss: 0.4111284613609314, g_loss: 3.682177782058716\n","Epoch 47/2000, Step 26, d_loss: 0.4485955834388733, g_loss: 3.2750582695007324\n","Epoch 47/2000, Step 27, d_loss: 0.44719091057777405, g_loss: 2.6642849445343018\n","Epoch 47/2000, Step 28, d_loss: 0.5031588077545166, g_loss: 4.554184913635254\n","Epoch 47/2000, Step 29, d_loss: 0.4185631275177002, g_loss: 4.433192253112793\n","Epoch 47/2000, Step 30, d_loss: 0.36527180671691895, g_loss: 3.5328919887542725\n","Epoch 47/2000, Step 31, d_loss: 0.3752758502960205, g_loss: 4.671029567718506\n","Epoch 47/2000, Step 32, d_loss: 0.4226343631744385, g_loss: 2.4502053260803223\n","Epoch 47/2000, Step 33, d_loss: 0.4166455864906311, g_loss: 3.578953742980957\n","Epoch 47/2000, Step 34, d_loss: 0.4002876281738281, g_loss: 3.5259172916412354\n","Epoch 47/2000, Step 35, d_loss: 0.38560882210731506, g_loss: 3.9436421394348145\n","Epoch 47/2000, Step 36, d_loss: 0.42807576060295105, g_loss: 3.3381428718566895\n","Epoch 47/2000, Step 37, d_loss: 0.4978574514389038, g_loss: 4.219266891479492\n","Epoch 47/2000, Step 38, d_loss: 0.3959512710571289, g_loss: 3.893831491470337\n","Epoch 47/2000, Step 39, d_loss: 0.44401755928993225, g_loss: 4.763217449188232\n","Epoch 47/2000, Step 40, d_loss: 0.4090290665626526, g_loss: 3.8287739753723145\n","Epoch 47/2000, Step 41, d_loss: 0.45339828729629517, g_loss: 4.361801624298096\n","Epoch 47/2000, Step 42, d_loss: 0.45891615748405457, g_loss: 3.8358771800994873\n","Epoch 47/2000, Step 43, d_loss: 0.4105883538722992, g_loss: 3.062464952468872\n","Epoch 47/2000, Step 44, d_loss: 0.39970308542251587, g_loss: 3.6260528564453125\n","Epoch 47/2000, Step 45, d_loss: 0.43342727422714233, g_loss: 3.1595842838287354\n","Epoch 47/2000, Step 46, d_loss: 0.41412264108657837, g_loss: 3.2558066844940186\n","Epoch 47/2000, Step 47, d_loss: 0.42875778675079346, g_loss: 3.999992847442627\n","Epoch 47/2000, Step 48, d_loss: 0.4051267206668854, g_loss: 4.339317321777344\n","Epoch 47/2000, Step 49, d_loss: 0.4340874254703522, g_loss: 3.8577702045440674\n","Epoch 47/2000, Step 50, d_loss: 0.3780801594257355, g_loss: 4.21697998046875\n","Epoch 47/2000, Step 51, d_loss: 0.4016757011413574, g_loss: 3.7301888465881348\n","Epoch 47/2000, Step 52, d_loss: 0.37019675970077515, g_loss: 4.279578685760498\n","Epoch 47/2000, Step 53, d_loss: 0.4239289164543152, g_loss: 4.04303503036499\n","Epoch 47/2000, Step 54, d_loss: 0.36537179350852966, g_loss: 3.3955109119415283\n","Epoch 47/2000, Step 55, d_loss: 0.38786381483078003, g_loss: 3.786684989929199\n","Epoch 47/2000, Step 56, d_loss: 0.39040085673332214, g_loss: 4.8052592277526855\n","Epoch 47/2000, Step 57, d_loss: 0.5998176336288452, g_loss: 4.154988765716553\n","Epoch 47/2000, Step 58, d_loss: 0.3724028766155243, g_loss: 3.3186047077178955\n","Epoch 47/2000, Step 59, d_loss: 0.44541236758232117, g_loss: 4.047041893005371\n","Epoch 47/2000, Step 60, d_loss: 0.40438607335090637, g_loss: 2.6847450733184814\n","Epoch 47/2000, Step 61, d_loss: 0.4852374494075775, g_loss: 2.7115516662597656\n","Epoch 47/2000, Step 62, d_loss: 0.613664448261261, g_loss: 2.227125883102417\n","Epoch 47/2000, Step 63, d_loss: 0.5880656838417053, g_loss: 2.6732282638549805\n","Epoch 47/2000, Step 64, d_loss: 0.49390819668769836, g_loss: 4.267095565795898\n","Epoch 47/2000, Step 65, d_loss: 0.4565522074699402, g_loss: 4.3076958656311035\n","Epoch 47/2000, Step 66, d_loss: 0.4011712670326233, g_loss: 4.733796119689941\n","Epoch 47/2000, Step 67, d_loss: 0.5035478472709656, g_loss: 5.553903579711914\n","Epoch 47/2000, Step 68, d_loss: 0.6628971695899963, g_loss: 4.053783893585205\n","Epoch 47/2000, Step 69, d_loss: 0.49536722898483276, g_loss: 3.043588161468506\n","Epoch 47/2000, Step 70, d_loss: 0.4045996069908142, g_loss: 2.643968343734741\n","Epoch 47/2000, Step 71, d_loss: 0.3815040588378906, g_loss: 2.6527459621429443\n","Epoch 47/2000, Step 72, d_loss: 0.45556697249412537, g_loss: 2.980515718460083\n","Epoch 47/2000, Step 73, d_loss: 0.42591971158981323, g_loss: 2.9765148162841797\n","Epoch 47/2000, Step 74, d_loss: 0.4288367033004761, g_loss: 3.856955051422119\n","Epoch 47/2000, Step 75, d_loss: 0.4026090204715729, g_loss: 3.6616766452789307\n","Epoch 47/2000, Step 76, d_loss: 0.5173224210739136, g_loss: 5.431804180145264\n","Epoch 47/2000, Step 77, d_loss: 0.4106835722923279, g_loss: 3.690098524093628\n","Epoch 47/2000, Step 78, d_loss: 0.4180743992328644, g_loss: 3.890605926513672\n","Epoch 47/2000, Step 79, d_loss: 0.416597843170166, g_loss: 3.129218578338623\n","Epoch 47/2000, Step 80, d_loss: 0.421436607837677, g_loss: 3.7282352447509766\n","Epoch 47/2000, Step 81, d_loss: 0.4148634672164917, g_loss: 4.094531536102295\n","Epoch 47/2000, Step 82, d_loss: 0.44304823875427246, g_loss: 4.062830924987793\n","Epoch 47/2000, Step 83, d_loss: 0.38345837593078613, g_loss: 3.7283267974853516\n","Epoch 47/2000, Step 84, d_loss: 0.3988352417945862, g_loss: 2.623737335205078\n","Epoch 47/2000, Step 85, d_loss: 0.38148099184036255, g_loss: 3.626619338989258\n","Epoch 47/2000, Step 86, d_loss: 0.4338189959526062, g_loss: 4.319262981414795\n","Epoch 47/2000, Step 87, d_loss: 0.41703611612319946, g_loss: 3.3011012077331543\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 48/2000, Step 1, d_loss: 0.37127700448036194, g_loss: 3.908243417739868\n","Epoch 48/2000, Step 2, d_loss: 0.4221043884754181, g_loss: 4.116992473602295\n","Epoch 48/2000, Step 3, d_loss: 0.3879636824131012, g_loss: 3.260315179824829\n","Epoch 48/2000, Step 4, d_loss: 0.5204437971115112, g_loss: 3.6430981159210205\n","Epoch 48/2000, Step 5, d_loss: 0.42101919651031494, g_loss: 3.12045955657959\n","Epoch 48/2000, Step 6, d_loss: 0.41385531425476074, g_loss: 3.231813669204712\n","Epoch 48/2000, Step 7, d_loss: 0.38338571786880493, g_loss: 2.8000428676605225\n","Epoch 48/2000, Step 8, d_loss: 0.4512549638748169, g_loss: 4.084327697753906\n","Epoch 48/2000, Step 9, d_loss: 0.4521019458770752, g_loss: 3.861966609954834\n","Epoch 48/2000, Step 10, d_loss: 0.4359290301799774, g_loss: 4.277430057525635\n","Epoch 48/2000, Step 11, d_loss: 0.3895827531814575, g_loss: 4.272143363952637\n","Epoch 48/2000, Step 12, d_loss: 0.41523125767707825, g_loss: 4.411174297332764\n","Epoch 48/2000, Step 13, d_loss: 0.38665375113487244, g_loss: 4.484572887420654\n","Epoch 48/2000, Step 14, d_loss: 0.5261960029602051, g_loss: 3.960844039916992\n","Epoch 48/2000, Step 15, d_loss: 0.4132373034954071, g_loss: 4.429624080657959\n","Epoch 48/2000, Step 16, d_loss: 0.37064483761787415, g_loss: 3.3042755126953125\n","Epoch 48/2000, Step 17, d_loss: 0.4258902966976166, g_loss: 3.913313627243042\n","Epoch 48/2000, Step 18, d_loss: 0.49168917536735535, g_loss: 3.6192164421081543\n","Epoch 48/2000, Step 19, d_loss: 0.5887154340744019, g_loss: 2.785188913345337\n","Epoch 48/2000, Step 20, d_loss: 0.4483344554901123, g_loss: 3.2916131019592285\n","Epoch 48/2000, Step 21, d_loss: 0.4351420998573303, g_loss: 2.5186522006988525\n","Epoch 48/2000, Step 22, d_loss: 0.4063981771469116, g_loss: 3.098921298980713\n","Epoch 48/2000, Step 23, d_loss: 0.4289851486682892, g_loss: 3.9639687538146973\n","Epoch 48/2000, Step 24, d_loss: 0.51323401927948, g_loss: 3.4873361587524414\n","Epoch 48/2000, Step 25, d_loss: 0.4863120913505554, g_loss: 4.995660305023193\n","Epoch 48/2000, Step 26, d_loss: 0.4331699013710022, g_loss: 3.4210145473480225\n","Epoch 48/2000, Step 27, d_loss: 0.41692718863487244, g_loss: 3.9052319526672363\n","Epoch 48/2000, Step 28, d_loss: 0.4439239799976349, g_loss: 4.647222995758057\n","Epoch 48/2000, Step 29, d_loss: 0.4699205756187439, g_loss: 4.020841598510742\n","Epoch 48/2000, Step 30, d_loss: 0.410235196352005, g_loss: 4.537014961242676\n","Epoch 48/2000, Step 31, d_loss: 0.6425681710243225, g_loss: 3.850844383239746\n","Epoch 48/2000, Step 32, d_loss: 0.43231135606765747, g_loss: 2.758875846862793\n","Epoch 48/2000, Step 33, d_loss: 0.4200901389122009, g_loss: 2.962353467941284\n","Epoch 48/2000, Step 34, d_loss: 0.5887641906738281, g_loss: 2.1341185569763184\n","Epoch 48/2000, Step 35, d_loss: 0.5116471648216248, g_loss: 2.892518997192383\n","Epoch 48/2000, Step 36, d_loss: 0.49255627393722534, g_loss: 4.30302619934082\n","Epoch 48/2000, Step 37, d_loss: 0.39656803011894226, g_loss: 3.4718832969665527\n","Epoch 48/2000, Step 38, d_loss: 0.39611393213272095, g_loss: 3.888026237487793\n","Epoch 48/2000, Step 39, d_loss: 0.5690338015556335, g_loss: 4.641168117523193\n","Epoch 48/2000, Step 40, d_loss: 0.4346446692943573, g_loss: 3.5200905799865723\n","Epoch 48/2000, Step 41, d_loss: 0.42976242303848267, g_loss: 2.6684041023254395\n","Epoch 48/2000, Step 42, d_loss: 0.44492214918136597, g_loss: 3.0885345935821533\n","Epoch 48/2000, Step 43, d_loss: 0.4302637279033661, g_loss: 1.8051979541778564\n","Epoch 48/2000, Step 44, d_loss: 0.490902841091156, g_loss: 3.1999993324279785\n","Epoch 48/2000, Step 45, d_loss: 0.5933466553688049, g_loss: 3.8700850009918213\n","Epoch 48/2000, Step 46, d_loss: 0.5333975553512573, g_loss: 3.846705675125122\n","Epoch 48/2000, Step 47, d_loss: 0.40126270055770874, g_loss: 3.348351001739502\n","Epoch 48/2000, Step 48, d_loss: 0.45968663692474365, g_loss: 3.3718883991241455\n","Epoch 48/2000, Step 49, d_loss: 0.39004284143447876, g_loss: 4.663847923278809\n","Epoch 48/2000, Step 50, d_loss: 0.43893054127693176, g_loss: 3.246731996536255\n","Epoch 48/2000, Step 51, d_loss: 0.4753885269165039, g_loss: 4.4065728187561035\n","Epoch 48/2000, Step 52, d_loss: 0.39998969435691833, g_loss: 3.6756985187530518\n","Epoch 48/2000, Step 53, d_loss: 0.36703401803970337, g_loss: 5.328121662139893\n","Epoch 48/2000, Step 54, d_loss: 0.5020356774330139, g_loss: 3.0045175552368164\n","Epoch 48/2000, Step 55, d_loss: 0.36544468998908997, g_loss: 3.1466774940490723\n","Epoch 48/2000, Step 56, d_loss: 0.39816048741340637, g_loss: 2.7742719650268555\n","Epoch 48/2000, Step 57, d_loss: 0.40300339460372925, g_loss: 3.0547423362731934\n","Epoch 48/2000, Step 58, d_loss: 0.4445081353187561, g_loss: 3.8473687171936035\n","Epoch 48/2000, Step 59, d_loss: 0.4155976474285126, g_loss: 2.8950982093811035\n","Epoch 48/2000, Step 60, d_loss: 0.37573447823524475, g_loss: 4.703973293304443\n","Epoch 48/2000, Step 61, d_loss: 0.37166157364845276, g_loss: 5.53511381149292\n","Epoch 48/2000, Step 62, d_loss: 0.39898407459259033, g_loss: 4.7151689529418945\n","Epoch 48/2000, Step 63, d_loss: 0.35058796405792236, g_loss: 4.436021327972412\n","Epoch 48/2000, Step 64, d_loss: 0.4233943521976471, g_loss: 3.941013813018799\n","Epoch 48/2000, Step 65, d_loss: 0.3971232771873474, g_loss: 3.257734537124634\n","Epoch 48/2000, Step 66, d_loss: 0.40804028511047363, g_loss: 3.8136186599731445\n","Epoch 48/2000, Step 67, d_loss: 0.39552682638168335, g_loss: 3.555913209915161\n","Epoch 48/2000, Step 68, d_loss: 0.41731947660446167, g_loss: 2.867675542831421\n","Epoch 48/2000, Step 69, d_loss: 0.43253087997436523, g_loss: 3.1054134368896484\n","Epoch 48/2000, Step 70, d_loss: 0.41887199878692627, g_loss: 2.484419107437134\n","Epoch 48/2000, Step 71, d_loss: 0.3961053192615509, g_loss: 4.493120193481445\n","Epoch 48/2000, Step 72, d_loss: 0.4176405668258667, g_loss: 3.017646074295044\n","Epoch 48/2000, Step 73, d_loss: 0.4385712146759033, g_loss: 4.316890716552734\n","Epoch 48/2000, Step 74, d_loss: 0.39612826704978943, g_loss: 4.195576190948486\n","Epoch 48/2000, Step 75, d_loss: 0.39946943521499634, g_loss: 3.9879040718078613\n","Epoch 48/2000, Step 76, d_loss: 0.39505434036254883, g_loss: 3.2781026363372803\n","Epoch 48/2000, Step 77, d_loss: 0.3971717655658722, g_loss: 3.7201006412506104\n","Epoch 48/2000, Step 78, d_loss: 0.3882037103176117, g_loss: 3.277545690536499\n","Epoch 48/2000, Step 79, d_loss: 0.4265258014202118, g_loss: 3.3742635250091553\n","Epoch 48/2000, Step 80, d_loss: 0.40185582637786865, g_loss: 3.889009714126587\n","Epoch 48/2000, Step 81, d_loss: 0.3866082727909088, g_loss: 3.2195897102355957\n","Epoch 48/2000, Step 82, d_loss: 0.3893848657608032, g_loss: 3.899277687072754\n","Epoch 48/2000, Step 83, d_loss: 0.42637017369270325, g_loss: 3.5170586109161377\n","Epoch 48/2000, Step 84, d_loss: 0.36618348956108093, g_loss: 4.447111129760742\n","Epoch 48/2000, Step 85, d_loss: 0.3780628442764282, g_loss: 4.745880603790283\n","Epoch 48/2000, Step 86, d_loss: 0.3983086049556732, g_loss: 4.410037040710449\n","Epoch 48/2000, Step 87, d_loss: 0.4389200210571289, g_loss: 3.1222293376922607\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 49/2000, Step 1, d_loss: 0.41026169061660767, g_loss: 3.0038886070251465\n","Epoch 49/2000, Step 2, d_loss: 0.4067312479019165, g_loss: 4.527552127838135\n","Epoch 49/2000, Step 3, d_loss: 0.4334685504436493, g_loss: 4.303745269775391\n","Epoch 49/2000, Step 4, d_loss: 0.39543014764785767, g_loss: 3.8033251762390137\n","Epoch 49/2000, Step 5, d_loss: 0.4544154107570648, g_loss: 3.9332690238952637\n","Epoch 49/2000, Step 6, d_loss: 0.4269044101238251, g_loss: 3.7442305088043213\n","Epoch 49/2000, Step 7, d_loss: 0.363560289144516, g_loss: 4.5225348472595215\n","Epoch 49/2000, Step 8, d_loss: 0.3953600525856018, g_loss: 3.6962122917175293\n","Epoch 49/2000, Step 9, d_loss: 0.45150119066238403, g_loss: 3.4981276988983154\n","Epoch 49/2000, Step 10, d_loss: 0.4368073344230652, g_loss: 2.7910616397857666\n","Epoch 49/2000, Step 11, d_loss: 0.4353657066822052, g_loss: 3.7036211490631104\n","Epoch 49/2000, Step 12, d_loss: 0.38461756706237793, g_loss: 3.5633656978607178\n","Epoch 49/2000, Step 13, d_loss: 0.3622336685657501, g_loss: 4.284125328063965\n","Epoch 49/2000, Step 14, d_loss: 0.3728075921535492, g_loss: 4.700709819793701\n","Epoch 49/2000, Step 15, d_loss: 0.4969910681247711, g_loss: 4.182957649230957\n","Epoch 49/2000, Step 16, d_loss: 0.4072609841823578, g_loss: 4.08243989944458\n","Epoch 49/2000, Step 17, d_loss: 0.4052947163581848, g_loss: 3.0887303352355957\n","Epoch 49/2000, Step 18, d_loss: 0.41105177998542786, g_loss: 3.2961363792419434\n","Epoch 49/2000, Step 19, d_loss: 0.3879213333129883, g_loss: 4.423178195953369\n","Epoch 49/2000, Step 20, d_loss: 0.359778493642807, g_loss: 4.068548679351807\n","Epoch 49/2000, Step 21, d_loss: 0.3740159273147583, g_loss: 4.330682754516602\n","Epoch 49/2000, Step 22, d_loss: 0.365646094083786, g_loss: 4.042399883270264\n","Epoch 49/2000, Step 23, d_loss: 0.3871033787727356, g_loss: 4.227755546569824\n","Epoch 49/2000, Step 24, d_loss: 0.3917854428291321, g_loss: 4.365646839141846\n","Epoch 49/2000, Step 25, d_loss: 0.38671356439590454, g_loss: 3.458281993865967\n","Epoch 49/2000, Step 26, d_loss: 0.43510472774505615, g_loss: 3.8719301223754883\n","Epoch 49/2000, Step 27, d_loss: 0.37067240476608276, g_loss: 4.22143030166626\n","Epoch 49/2000, Step 28, d_loss: 0.39332690834999084, g_loss: 4.142456531524658\n","Epoch 49/2000, Step 29, d_loss: 0.3865731656551361, g_loss: 3.46903920173645\n","Epoch 49/2000, Step 30, d_loss: 0.3785579800605774, g_loss: 4.257354259490967\n","Epoch 49/2000, Step 31, d_loss: 0.3856809437274933, g_loss: 4.096451759338379\n","Epoch 49/2000, Step 32, d_loss: 0.40590277314186096, g_loss: 3.7609703540802\n","Epoch 49/2000, Step 33, d_loss: 0.44285157322883606, g_loss: 4.068551063537598\n","Epoch 49/2000, Step 34, d_loss: 0.362513929605484, g_loss: 4.318593502044678\n","Epoch 49/2000, Step 35, d_loss: 0.3679535984992981, g_loss: 3.9108033180236816\n","Epoch 49/2000, Step 36, d_loss: 0.3797890841960907, g_loss: 3.987741470336914\n","Epoch 49/2000, Step 37, d_loss: 0.4090011417865753, g_loss: 4.594010829925537\n","Epoch 49/2000, Step 38, d_loss: 0.4021698236465454, g_loss: 4.225159645080566\n","Epoch 49/2000, Step 39, d_loss: 0.4782150983810425, g_loss: 3.648848056793213\n","Epoch 49/2000, Step 40, d_loss: 0.42413026094436646, g_loss: 3.1761155128479004\n","Epoch 49/2000, Step 41, d_loss: 0.4444117546081543, g_loss: 2.85479736328125\n","Epoch 49/2000, Step 42, d_loss: 0.4919208884239197, g_loss: 3.657510280609131\n","Epoch 49/2000, Step 43, d_loss: 0.38895463943481445, g_loss: 3.547612428665161\n","Epoch 49/2000, Step 44, d_loss: 0.4055194854736328, g_loss: 4.032284736633301\n","Epoch 49/2000, Step 45, d_loss: 0.38766035437583923, g_loss: 4.990201950073242\n","Epoch 49/2000, Step 46, d_loss: 0.40502646565437317, g_loss: 4.263010501861572\n","Epoch 49/2000, Step 47, d_loss: 0.6290301084518433, g_loss: 4.055061340332031\n","Epoch 49/2000, Step 48, d_loss: 0.3764580190181732, g_loss: 3.460413694381714\n","Epoch 49/2000, Step 49, d_loss: 0.39120185375213623, g_loss: 2.9553146362304688\n","Epoch 49/2000, Step 50, d_loss: 0.459506630897522, g_loss: 2.796165704727173\n","Epoch 49/2000, Step 51, d_loss: 0.43728742003440857, g_loss: 3.358236312866211\n","Epoch 49/2000, Step 52, d_loss: 0.3972755968570709, g_loss: 3.1422061920166016\n","Epoch 49/2000, Step 53, d_loss: 0.3818058669567108, g_loss: 3.3936681747436523\n","Epoch 49/2000, Step 54, d_loss: 0.4211099147796631, g_loss: 4.027113437652588\n","Epoch 49/2000, Step 55, d_loss: 0.40880170464515686, g_loss: 4.738739967346191\n","Epoch 49/2000, Step 56, d_loss: 0.38174542784690857, g_loss: 4.259554386138916\n","Epoch 49/2000, Step 57, d_loss: 0.42012453079223633, g_loss: 4.675283908843994\n","Epoch 49/2000, Step 58, d_loss: 0.4022139012813568, g_loss: 5.119058609008789\n","Epoch 49/2000, Step 59, d_loss: 0.37614792585372925, g_loss: 3.5151500701904297\n","Epoch 49/2000, Step 60, d_loss: 0.38458454608917236, g_loss: 3.0846660137176514\n","Epoch 49/2000, Step 61, d_loss: 0.3736685812473297, g_loss: 3.108375072479248\n","Epoch 49/2000, Step 62, d_loss: 0.42337849736213684, g_loss: 3.335158586502075\n","Epoch 49/2000, Step 63, d_loss: 0.4369964897632599, g_loss: 3.3380064964294434\n","Epoch 49/2000, Step 64, d_loss: 0.3616921901702881, g_loss: 3.3658854961395264\n","Epoch 49/2000, Step 65, d_loss: 0.40348634123802185, g_loss: 4.634305000305176\n","Epoch 49/2000, Step 66, d_loss: 0.3621300458908081, g_loss: 4.1215901374816895\n","Epoch 49/2000, Step 67, d_loss: 0.5908835530281067, g_loss: 4.317138671875\n","Epoch 49/2000, Step 68, d_loss: 0.45587947964668274, g_loss: 3.7070860862731934\n","Epoch 49/2000, Step 69, d_loss: 0.3718518614768982, g_loss: 3.568338394165039\n","Epoch 49/2000, Step 70, d_loss: 0.3794710636138916, g_loss: 2.7669994831085205\n","Epoch 49/2000, Step 71, d_loss: 0.46084293723106384, g_loss: 2.9505765438079834\n","Epoch 49/2000, Step 72, d_loss: 0.45852479338645935, g_loss: 3.2598581314086914\n","Epoch 49/2000, Step 73, d_loss: 0.44318145513534546, g_loss: 2.6305994987487793\n","Epoch 49/2000, Step 74, d_loss: 0.44197726249694824, g_loss: 3.682621479034424\n","Epoch 49/2000, Step 75, d_loss: 0.3985567092895508, g_loss: 3.876335382461548\n","Epoch 49/2000, Step 76, d_loss: 0.4675290584564209, g_loss: 4.632621765136719\n","Epoch 49/2000, Step 77, d_loss: 0.3987491726875305, g_loss: 4.07834529876709\n","Epoch 49/2000, Step 78, d_loss: 0.3949147164821625, g_loss: 4.450723648071289\n","Epoch 49/2000, Step 79, d_loss: 0.4220938980579376, g_loss: 4.236144065856934\n","Epoch 49/2000, Step 80, d_loss: 0.4091562330722809, g_loss: 3.4833896160125732\n","Epoch 49/2000, Step 81, d_loss: 0.3962636888027191, g_loss: 3.4157397747039795\n","Epoch 49/2000, Step 82, d_loss: 0.41696280241012573, g_loss: 2.69816255569458\n","Epoch 49/2000, Step 83, d_loss: 0.41537731885910034, g_loss: 3.393272876739502\n","Epoch 49/2000, Step 84, d_loss: 0.41301649808883667, g_loss: 3.542109727859497\n","Epoch 49/2000, Step 85, d_loss: 0.41495412588119507, g_loss: 3.220975637435913\n","Epoch 49/2000, Step 86, d_loss: 0.3837886154651642, g_loss: 3.6356420516967773\n","Epoch 49/2000, Step 87, d_loss: 0.3852522373199463, g_loss: 3.9968340396881104\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 50/2000, Step 1, d_loss: 0.35580915212631226, g_loss: 5.122292518615723\n","Epoch 50/2000, Step 2, d_loss: 0.3650985658168793, g_loss: 5.1690592765808105\n","Epoch 50/2000, Step 3, d_loss: 0.5422585606575012, g_loss: 5.0486249923706055\n","Epoch 50/2000, Step 4, d_loss: 0.3563952147960663, g_loss: 3.5474393367767334\n","Epoch 50/2000, Step 5, d_loss: 0.3680843710899353, g_loss: 4.855849742889404\n","Epoch 50/2000, Step 6, d_loss: 0.3807743191719055, g_loss: 2.9974422454833984\n","Epoch 50/2000, Step 7, d_loss: 0.4283761978149414, g_loss: 3.1184606552124023\n","Epoch 50/2000, Step 8, d_loss: 0.3732338547706604, g_loss: 2.945240020751953\n","Epoch 50/2000, Step 9, d_loss: 0.3772309124469757, g_loss: 3.43326735496521\n","Epoch 50/2000, Step 10, d_loss: 0.35970160365104675, g_loss: 5.903687000274658\n","Epoch 50/2000, Step 11, d_loss: 0.37382814288139343, g_loss: 3.8603289127349854\n","Epoch 50/2000, Step 12, d_loss: 0.37171944975852966, g_loss: 4.295559883117676\n","Epoch 50/2000, Step 13, d_loss: 0.3846851587295532, g_loss: 4.541711807250977\n","Epoch 50/2000, Step 14, d_loss: 0.39941737055778503, g_loss: 4.684687614440918\n","Epoch 50/2000, Step 15, d_loss: 0.36879226565361023, g_loss: 3.726720094680786\n","Epoch 50/2000, Step 16, d_loss: 0.38605624437332153, g_loss: 3.605955123901367\n","Epoch 50/2000, Step 17, d_loss: 0.3681707978248596, g_loss: 3.28749680519104\n","Epoch 50/2000, Step 18, d_loss: 0.3799899220466614, g_loss: 4.162862300872803\n","Epoch 50/2000, Step 19, d_loss: 0.4114789068698883, g_loss: 3.1320598125457764\n","Epoch 50/2000, Step 20, d_loss: 0.3841884434223175, g_loss: 3.305203437805176\n","Epoch 50/2000, Step 21, d_loss: 0.397373229265213, g_loss: 3.850451707839966\n","Epoch 50/2000, Step 22, d_loss: 0.3777530789375305, g_loss: 3.9372639656066895\n","Epoch 50/2000, Step 23, d_loss: 0.37656572461128235, g_loss: 3.9778029918670654\n","Epoch 50/2000, Step 24, d_loss: 0.4099353849887848, g_loss: 3.4396092891693115\n","Epoch 50/2000, Step 25, d_loss: 0.38174334168434143, g_loss: 4.818658828735352\n","Epoch 50/2000, Step 26, d_loss: 0.35004058480262756, g_loss: 3.934774875640869\n","Epoch 50/2000, Step 27, d_loss: 0.4057270586490631, g_loss: 3.7464599609375\n","Epoch 50/2000, Step 28, d_loss: 0.3670637607574463, g_loss: 3.9253029823303223\n","Epoch 50/2000, Step 29, d_loss: 0.43180522322654724, g_loss: 3.8263256549835205\n","Epoch 50/2000, Step 30, d_loss: 0.3698645234107971, g_loss: 3.0271005630493164\n","Epoch 50/2000, Step 31, d_loss: 0.3697279393672943, g_loss: 3.5139825344085693\n","Epoch 50/2000, Step 32, d_loss: 0.38122764229774475, g_loss: 3.316009283065796\n","Epoch 50/2000, Step 33, d_loss: 0.45754164457321167, g_loss: 4.61109733581543\n","Epoch 50/2000, Step 34, d_loss: 0.37927696108818054, g_loss: 3.5224618911743164\n","Epoch 50/2000, Step 35, d_loss: 0.4470937252044678, g_loss: 3.826352834701538\n","Epoch 50/2000, Step 36, d_loss: 0.38611331582069397, g_loss: 4.019175052642822\n","Epoch 50/2000, Step 37, d_loss: 0.3649694621562958, g_loss: 3.887843370437622\n","Epoch 50/2000, Step 38, d_loss: 0.42122215032577515, g_loss: 4.2775726318359375\n","Epoch 50/2000, Step 39, d_loss: 0.3753969073295593, g_loss: 4.162474155426025\n","Epoch 50/2000, Step 40, d_loss: 0.40352270007133484, g_loss: 4.642074108123779\n","Epoch 50/2000, Step 41, d_loss: 0.41134700179100037, g_loss: 4.390000820159912\n","Epoch 50/2000, Step 42, d_loss: 0.3615512549877167, g_loss: 3.7979416847229004\n","Epoch 50/2000, Step 43, d_loss: 0.39769431948661804, g_loss: 3.2356791496276855\n","Epoch 50/2000, Step 44, d_loss: 0.3944312632083893, g_loss: 4.124359130859375\n","Epoch 50/2000, Step 45, d_loss: 0.40028294920921326, g_loss: 4.396421432495117\n","Epoch 50/2000, Step 46, d_loss: 0.43600010871887207, g_loss: 4.609588623046875\n","Epoch 50/2000, Step 47, d_loss: 0.3656296133995056, g_loss: 4.802768230438232\n","Epoch 50/2000, Step 48, d_loss: 0.3671470880508423, g_loss: 4.100860595703125\n","Epoch 50/2000, Step 49, d_loss: 0.4004763066768646, g_loss: 4.04527473449707\n","Epoch 50/2000, Step 50, d_loss: 0.5412880182266235, g_loss: 4.050391674041748\n","Epoch 50/2000, Step 51, d_loss: 0.3694344460964203, g_loss: 3.4787726402282715\n","Epoch 50/2000, Step 52, d_loss: 0.36816489696502686, g_loss: 3.5845530033111572\n","Epoch 50/2000, Step 53, d_loss: 0.3701104521751404, g_loss: 4.513829708099365\n","Epoch 50/2000, Step 54, d_loss: 0.38037729263305664, g_loss: 4.054407119750977\n","Epoch 50/2000, Step 55, d_loss: 0.4071638286113739, g_loss: 3.9597854614257812\n","Epoch 50/2000, Step 56, d_loss: 0.3727104067802429, g_loss: 3.8627617359161377\n","Epoch 50/2000, Step 57, d_loss: 0.4127713739871979, g_loss: 3.6872525215148926\n","Epoch 50/2000, Step 58, d_loss: 0.34605830907821655, g_loss: 4.768636226654053\n","Epoch 50/2000, Step 59, d_loss: 0.4510217308998108, g_loss: 4.838291168212891\n","Epoch 50/2000, Step 60, d_loss: 0.41761723160743713, g_loss: 3.9621007442474365\n","Epoch 50/2000, Step 61, d_loss: 0.3582783639431, g_loss: 3.955609083175659\n","Epoch 50/2000, Step 62, d_loss: 0.3747384548187256, g_loss: 3.330463409423828\n","Epoch 50/2000, Step 63, d_loss: 0.3682853579521179, g_loss: 3.576450824737549\n","Epoch 50/2000, Step 64, d_loss: 0.43324926495552063, g_loss: 3.3622968196868896\n","Epoch 50/2000, Step 65, d_loss: 0.4092389643192291, g_loss: 3.733722686767578\n","Epoch 50/2000, Step 66, d_loss: 0.3837593197822571, g_loss: 4.554503917694092\n","Epoch 50/2000, Step 67, d_loss: 0.3785431385040283, g_loss: 5.264503002166748\n","Epoch 50/2000, Step 68, d_loss: 0.34881392121315, g_loss: 4.762136459350586\n","Epoch 50/2000, Step 69, d_loss: 0.39985355734825134, g_loss: 3.5904834270477295\n","Epoch 50/2000, Step 70, d_loss: 0.4038461148738861, g_loss: 3.5428712368011475\n","Epoch 50/2000, Step 71, d_loss: 0.398505300283432, g_loss: 4.145564556121826\n","Epoch 50/2000, Step 72, d_loss: 0.39004984498023987, g_loss: 4.022602558135986\n","Epoch 50/2000, Step 73, d_loss: 0.41151607036590576, g_loss: 3.6137096881866455\n","Epoch 50/2000, Step 74, d_loss: 0.359664648771286, g_loss: 4.465147018432617\n","Epoch 50/2000, Step 75, d_loss: 0.37437349557876587, g_loss: 3.460493326187134\n","Epoch 50/2000, Step 76, d_loss: 0.42615726590156555, g_loss: 4.632850646972656\n","Epoch 50/2000, Step 77, d_loss: 0.3912196159362793, g_loss: 4.111906051635742\n","Epoch 50/2000, Step 78, d_loss: 0.3910536766052246, g_loss: 4.002560138702393\n","Epoch 50/2000, Step 79, d_loss: 0.3801676034927368, g_loss: 3.7100868225097656\n","Epoch 50/2000, Step 80, d_loss: 0.4031745195388794, g_loss: 5.005110740661621\n","Epoch 50/2000, Step 81, d_loss: 0.37250179052352905, g_loss: 4.781971454620361\n","Epoch 50/2000, Step 82, d_loss: 0.38900086283683777, g_loss: 3.8419666290283203\n","Epoch 50/2000, Step 83, d_loss: 0.4148505628108978, g_loss: 4.258571147918701\n","Epoch 50/2000, Step 84, d_loss: 0.3782128691673279, g_loss: 3.6461668014526367\n","Epoch 50/2000, Step 85, d_loss: 0.4067137837409973, g_loss: 3.5612270832061768\n","Epoch 50/2000, Step 86, d_loss: 0.44230374693870544, g_loss: 3.987844467163086\n","Epoch 50/2000, Step 87, d_loss: 0.42872029542922974, g_loss: 3.5810790061950684\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 51/2000, Step 1, d_loss: 0.3658882975578308, g_loss: 4.928677558898926\n","Epoch 51/2000, Step 2, d_loss: 0.36501866579055786, g_loss: 4.681661605834961\n","Epoch 51/2000, Step 3, d_loss: 0.559110701084137, g_loss: 3.700117349624634\n","Epoch 51/2000, Step 4, d_loss: 0.4364932179450989, g_loss: 3.703634738922119\n","Epoch 51/2000, Step 5, d_loss: 0.389370858669281, g_loss: 3.7965147495269775\n","Epoch 51/2000, Step 6, d_loss: 0.4120376408100128, g_loss: 3.732988119125366\n","Epoch 51/2000, Step 7, d_loss: 0.42584726214408875, g_loss: 4.444854259490967\n","Epoch 51/2000, Step 8, d_loss: 0.39921560883522034, g_loss: 3.3992598056793213\n","Epoch 51/2000, Step 9, d_loss: 0.4340529441833496, g_loss: 3.3663153648376465\n","Epoch 51/2000, Step 10, d_loss: 0.3535521328449249, g_loss: 3.531182289123535\n","Epoch 51/2000, Step 11, d_loss: 0.413088321685791, g_loss: 3.357121706008911\n","Epoch 51/2000, Step 12, d_loss: 0.4341660439968109, g_loss: 3.9605765342712402\n","Epoch 51/2000, Step 13, d_loss: 0.4024891257286072, g_loss: 4.3334760665893555\n","Epoch 51/2000, Step 14, d_loss: 0.41714242100715637, g_loss: 4.809863090515137\n","Epoch 51/2000, Step 15, d_loss: 0.3611659109592438, g_loss: 3.813267946243286\n","Epoch 51/2000, Step 16, d_loss: 0.40662384033203125, g_loss: 4.040627479553223\n","Epoch 51/2000, Step 17, d_loss: 0.3987526595592499, g_loss: 3.554074764251709\n","Epoch 51/2000, Step 18, d_loss: 0.40826499462127686, g_loss: 3.7571332454681396\n","Epoch 51/2000, Step 19, d_loss: 0.4059562683105469, g_loss: 3.427023410797119\n","Epoch 51/2000, Step 20, d_loss: 0.4140041470527649, g_loss: 3.457843780517578\n","Epoch 51/2000, Step 21, d_loss: 0.43005168437957764, g_loss: 3.1034388542175293\n","Epoch 51/2000, Step 22, d_loss: 0.4520781338214874, g_loss: 3.3605217933654785\n","Epoch 51/2000, Step 23, d_loss: 0.4279078245162964, g_loss: 4.5237040519714355\n","Epoch 51/2000, Step 24, d_loss: 0.37843525409698486, g_loss: 3.7725210189819336\n","Epoch 51/2000, Step 25, d_loss: 0.43800055980682373, g_loss: 4.173961639404297\n","Epoch 51/2000, Step 26, d_loss: 0.4119294285774231, g_loss: 3.6961965560913086\n","Epoch 51/2000, Step 27, d_loss: 0.3837093114852905, g_loss: 4.151976585388184\n","Epoch 51/2000, Step 28, d_loss: 0.37348079681396484, g_loss: 4.436083793640137\n","Epoch 51/2000, Step 29, d_loss: 0.3717420697212219, g_loss: 2.831299304962158\n","Epoch 51/2000, Step 30, d_loss: 0.3794184923171997, g_loss: 3.51106858253479\n","Epoch 51/2000, Step 31, d_loss: 0.4285335838794708, g_loss: 3.6551215648651123\n","Epoch 51/2000, Step 32, d_loss: 0.42295509576797485, g_loss: 3.967757225036621\n","Epoch 51/2000, Step 33, d_loss: 0.474311888217926, g_loss: 3.8646583557128906\n","Epoch 51/2000, Step 34, d_loss: 0.3654683232307434, g_loss: 4.715611457824707\n","Epoch 51/2000, Step 35, d_loss: 0.5104603171348572, g_loss: 2.9059031009674072\n","Epoch 51/2000, Step 36, d_loss: 0.43529337644577026, g_loss: 3.6003825664520264\n","Epoch 51/2000, Step 37, d_loss: 0.3762751817703247, g_loss: 4.186763286590576\n","Epoch 51/2000, Step 38, d_loss: 0.4093422293663025, g_loss: 3.2349321842193604\n","Epoch 51/2000, Step 39, d_loss: 0.4398326575756073, g_loss: 3.7997677326202393\n","Epoch 51/2000, Step 40, d_loss: 0.4016056954860687, g_loss: 3.5129482746124268\n","Epoch 51/2000, Step 41, d_loss: 0.391507625579834, g_loss: 3.122610092163086\n","Epoch 51/2000, Step 42, d_loss: 0.4111931025981903, g_loss: 4.054880619049072\n","Epoch 51/2000, Step 43, d_loss: 0.3791428208351135, g_loss: 4.068039417266846\n","Epoch 51/2000, Step 44, d_loss: 0.4399142265319824, g_loss: 3.9350264072418213\n","Epoch 51/2000, Step 45, d_loss: 0.386022686958313, g_loss: 3.6418354511260986\n","Epoch 51/2000, Step 46, d_loss: 0.3561263382434845, g_loss: 3.845489263534546\n","Epoch 51/2000, Step 47, d_loss: 0.4075866639614105, g_loss: 3.8259150981903076\n","Epoch 51/2000, Step 48, d_loss: 0.37500467896461487, g_loss: 5.8727898597717285\n","Epoch 51/2000, Step 49, d_loss: 0.44496679306030273, g_loss: 3.734644889831543\n","Epoch 51/2000, Step 50, d_loss: 0.36897027492523193, g_loss: 3.6334428787231445\n","Epoch 51/2000, Step 51, d_loss: 0.3903252184391022, g_loss: 3.641016721725464\n","Epoch 51/2000, Step 52, d_loss: 0.3820308446884155, g_loss: 4.284811019897461\n","Epoch 51/2000, Step 53, d_loss: 0.40364018082618713, g_loss: 3.0614209175109863\n","Epoch 51/2000, Step 54, d_loss: 0.4084768295288086, g_loss: 2.9406816959381104\n","Epoch 51/2000, Step 55, d_loss: 0.4431934952735901, g_loss: 4.0892133712768555\n","Epoch 51/2000, Step 56, d_loss: 0.37582579255104065, g_loss: 3.0877275466918945\n","Epoch 51/2000, Step 57, d_loss: 0.44087469577789307, g_loss: 3.5295395851135254\n","Epoch 51/2000, Step 58, d_loss: 0.41160517930984497, g_loss: 3.235673666000366\n","Epoch 51/2000, Step 59, d_loss: 0.3802119493484497, g_loss: 3.7950756549835205\n","Epoch 51/2000, Step 60, d_loss: 0.3887551724910736, g_loss: 4.643219470977783\n","Epoch 51/2000, Step 61, d_loss: 0.3804013729095459, g_loss: 3.461228370666504\n","Epoch 51/2000, Step 62, d_loss: 0.45793232321739197, g_loss: 3.7263343334198\n","Epoch 51/2000, Step 63, d_loss: 0.409217894077301, g_loss: 3.618978977203369\n","Epoch 51/2000, Step 64, d_loss: 0.3872789740562439, g_loss: 3.286060333251953\n","Epoch 51/2000, Step 65, d_loss: 0.3946332633495331, g_loss: 3.5415196418762207\n","Epoch 51/2000, Step 66, d_loss: 0.41593578457832336, g_loss: 3.8920083045959473\n","Epoch 51/2000, Step 67, d_loss: 0.4465121030807495, g_loss: 4.028332233428955\n","Epoch 51/2000, Step 68, d_loss: 0.45550310611724854, g_loss: 4.939656734466553\n","Epoch 51/2000, Step 69, d_loss: 0.36683890223503113, g_loss: 4.678413391113281\n","Epoch 51/2000, Step 70, d_loss: 0.4743199050426483, g_loss: 3.495171070098877\n","Epoch 51/2000, Step 71, d_loss: 0.39706265926361084, g_loss: 3.2918448448181152\n","Epoch 51/2000, Step 72, d_loss: 0.41294538974761963, g_loss: 2.9883291721343994\n","Epoch 51/2000, Step 73, d_loss: 0.41845256090164185, g_loss: 2.7579336166381836\n","Epoch 51/2000, Step 74, d_loss: 0.36442893743515015, g_loss: 4.31409215927124\n","Epoch 51/2000, Step 75, d_loss: 0.3765997290611267, g_loss: 4.214226245880127\n","Epoch 51/2000, Step 76, d_loss: 0.38998356461524963, g_loss: 3.5944344997406006\n","Epoch 51/2000, Step 77, d_loss: 0.38633906841278076, g_loss: 3.4372167587280273\n","Epoch 51/2000, Step 78, d_loss: 0.38875192403793335, g_loss: 4.26971435546875\n","Epoch 51/2000, Step 79, d_loss: 0.3797726035118103, g_loss: 5.38694429397583\n","Epoch 51/2000, Step 80, d_loss: 0.4877317547798157, g_loss: 4.7512617111206055\n","Epoch 51/2000, Step 81, d_loss: 0.4369066059589386, g_loss: 4.220016956329346\n","Epoch 51/2000, Step 82, d_loss: 0.4324963688850403, g_loss: 3.6034774780273438\n","Epoch 51/2000, Step 83, d_loss: 0.382448673248291, g_loss: 3.156869888305664\n","Epoch 51/2000, Step 84, d_loss: 0.4860439896583557, g_loss: 2.6267619132995605\n","Epoch 51/2000, Step 85, d_loss: 0.4066846966743469, g_loss: 2.9917097091674805\n","Epoch 51/2000, Step 86, d_loss: 0.48240381479263306, g_loss: 3.468627691268921\n","Epoch 51/2000, Step 87, d_loss: 0.4232034981250763, g_loss: 3.7447590827941895\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 52/2000, Step 1, d_loss: 0.4263233244419098, g_loss: 3.603672981262207\n","Epoch 52/2000, Step 2, d_loss: 0.38353702425956726, g_loss: 4.031347274780273\n","Epoch 52/2000, Step 3, d_loss: 0.47382763028144836, g_loss: 3.9183895587921143\n","Epoch 52/2000, Step 4, d_loss: 0.35758739709854126, g_loss: 4.432894706726074\n","Epoch 52/2000, Step 5, d_loss: 0.567868709564209, g_loss: 3.4108877182006836\n","Epoch 52/2000, Step 6, d_loss: 0.4508984386920929, g_loss: 2.525423049926758\n","Epoch 52/2000, Step 7, d_loss: 0.44059014320373535, g_loss: 3.2162394523620605\n","Epoch 52/2000, Step 8, d_loss: 0.44619208574295044, g_loss: 2.6430206298828125\n","Epoch 52/2000, Step 9, d_loss: 0.3823026716709137, g_loss: 4.828610897064209\n","Epoch 52/2000, Step 10, d_loss: 0.4074033200740814, g_loss: 4.629454612731934\n","Epoch 52/2000, Step 11, d_loss: 0.3991308808326721, g_loss: 3.7552332878112793\n","Epoch 52/2000, Step 12, d_loss: 0.3824346363544464, g_loss: 4.030130863189697\n","Epoch 52/2000, Step 13, d_loss: 0.3782992362976074, g_loss: 4.7662763595581055\n","Epoch 52/2000, Step 14, d_loss: 0.3833628296852112, g_loss: 4.6845526695251465\n","Epoch 52/2000, Step 15, d_loss: 0.406024307012558, g_loss: 4.716876029968262\n","Epoch 52/2000, Step 16, d_loss: 0.3756606876850128, g_loss: 2.560626745223999\n","Epoch 52/2000, Step 17, d_loss: 0.41622477769851685, g_loss: 2.832749605178833\n","Epoch 52/2000, Step 18, d_loss: 0.4387565553188324, g_loss: 2.708496332168579\n","Epoch 52/2000, Step 19, d_loss: 0.47040531039237976, g_loss: 2.405229091644287\n","Epoch 52/2000, Step 20, d_loss: 0.48226356506347656, g_loss: 3.341503381729126\n","Epoch 52/2000, Step 21, d_loss: 0.387673020362854, g_loss: 4.315998554229736\n","Epoch 52/2000, Step 22, d_loss: 0.4078387916088104, g_loss: 4.509236812591553\n","Epoch 52/2000, Step 23, d_loss: 0.4045415222644806, g_loss: 4.228677272796631\n","Epoch 52/2000, Step 24, d_loss: 0.4262474775314331, g_loss: 3.7141451835632324\n","Epoch 52/2000, Step 25, d_loss: 0.4267579913139343, g_loss: 4.031702518463135\n","Epoch 52/2000, Step 26, d_loss: 0.3842003345489502, g_loss: 3.2900826930999756\n","Epoch 52/2000, Step 27, d_loss: 0.42742371559143066, g_loss: 3.164771318435669\n","Epoch 52/2000, Step 28, d_loss: 0.4210954010486603, g_loss: 3.5618033409118652\n","Epoch 52/2000, Step 29, d_loss: 0.43637317419052124, g_loss: 3.345729112625122\n","Epoch 52/2000, Step 30, d_loss: 0.35810723900794983, g_loss: 4.321657657623291\n","Epoch 52/2000, Step 31, d_loss: 0.37139731645584106, g_loss: 4.496469020843506\n","Epoch 52/2000, Step 32, d_loss: 0.3683985769748688, g_loss: 4.3004984855651855\n","Epoch 52/2000, Step 33, d_loss: 0.3446301817893982, g_loss: 4.960691452026367\n","Epoch 52/2000, Step 34, d_loss: 0.34953370690345764, g_loss: 4.127415657043457\n","Epoch 52/2000, Step 35, d_loss: 0.5141636729240417, g_loss: 4.880127429962158\n","Epoch 52/2000, Step 36, d_loss: 0.394529789686203, g_loss: 3.521289587020874\n","Epoch 52/2000, Step 37, d_loss: 0.3723703920841217, g_loss: 4.081376075744629\n","Epoch 52/2000, Step 38, d_loss: 0.4502079486846924, g_loss: 2.788529396057129\n","Epoch 52/2000, Step 39, d_loss: 0.383134663105011, g_loss: 3.255676746368408\n","Epoch 52/2000, Step 40, d_loss: 0.4063386023044586, g_loss: 2.947833776473999\n","Epoch 52/2000, Step 41, d_loss: 0.3822990953922272, g_loss: 3.7115371227264404\n","Epoch 52/2000, Step 42, d_loss: 0.40734779834747314, g_loss: 4.140539169311523\n","Epoch 52/2000, Step 43, d_loss: 0.5255767107009888, g_loss: 4.465943336486816\n","Epoch 52/2000, Step 44, d_loss: 0.4123036861419678, g_loss: 3.4655373096466064\n","Epoch 52/2000, Step 45, d_loss: 0.43865618109703064, g_loss: 3.866076707839966\n","Epoch 52/2000, Step 46, d_loss: 0.42114946246147156, g_loss: 3.2294936180114746\n","Epoch 52/2000, Step 47, d_loss: 0.41878268122673035, g_loss: 2.8447539806365967\n","Epoch 52/2000, Step 48, d_loss: 0.41984739899635315, g_loss: 2.875274658203125\n","Epoch 52/2000, Step 49, d_loss: 0.3978605568408966, g_loss: 2.768040418624878\n","Epoch 52/2000, Step 50, d_loss: 0.47743454575538635, g_loss: 3.5802972316741943\n","Epoch 52/2000, Step 51, d_loss: 0.4878028929233551, g_loss: 3.536536931991577\n","Epoch 52/2000, Step 52, d_loss: 0.37863728404045105, g_loss: 3.6743788719177246\n","Epoch 52/2000, Step 53, d_loss: 0.35475972294807434, g_loss: 4.673482894897461\n","Epoch 52/2000, Step 54, d_loss: 0.4134597182273865, g_loss: 5.0286431312561035\n","Epoch 52/2000, Step 55, d_loss: 0.420797199010849, g_loss: 3.8899319171905518\n","Epoch 52/2000, Step 56, d_loss: 0.5417545437812805, g_loss: 3.8478455543518066\n","Epoch 52/2000, Step 57, d_loss: 0.4612944424152374, g_loss: 3.032839775085449\n","Epoch 52/2000, Step 58, d_loss: 0.40847888588905334, g_loss: 2.3117706775665283\n","Epoch 52/2000, Step 59, d_loss: 0.49379870295524597, g_loss: 2.5853846073150635\n","Epoch 52/2000, Step 60, d_loss: 0.516598105430603, g_loss: 2.2560901641845703\n","Epoch 52/2000, Step 61, d_loss: 0.46534112095832825, g_loss: 2.7307913303375244\n","Epoch 52/2000, Step 62, d_loss: 0.46871623396873474, g_loss: 3.0890145301818848\n","Epoch 52/2000, Step 63, d_loss: 0.39826837182044983, g_loss: 4.4816436767578125\n","Epoch 52/2000, Step 64, d_loss: 0.4277816712856293, g_loss: 4.626910209655762\n","Epoch 52/2000, Step 65, d_loss: 0.7109917402267456, g_loss: 4.388697147369385\n","Epoch 52/2000, Step 66, d_loss: 0.4069972038269043, g_loss: 4.066835403442383\n","Epoch 52/2000, Step 67, d_loss: 0.4309116005897522, g_loss: 2.6371097564697266\n","Epoch 52/2000, Step 68, d_loss: 0.47317588329315186, g_loss: 2.4832663536071777\n","Epoch 52/2000, Step 69, d_loss: 0.6734438538551331, g_loss: 2.8817784786224365\n","Epoch 52/2000, Step 70, d_loss: 0.522460401058197, g_loss: 3.3695571422576904\n","Epoch 52/2000, Step 71, d_loss: 0.41764187812805176, g_loss: 3.583014488220215\n","Epoch 52/2000, Step 72, d_loss: 0.4009723961353302, g_loss: 4.96646785736084\n","Epoch 52/2000, Step 73, d_loss: 0.3820982873439789, g_loss: 4.523316383361816\n","Epoch 52/2000, Step 74, d_loss: 0.454537034034729, g_loss: 4.626912593841553\n","Epoch 52/2000, Step 75, d_loss: 0.5116098523139954, g_loss: 4.055530548095703\n","Epoch 52/2000, Step 76, d_loss: 0.43676257133483887, g_loss: 3.44696307182312\n","Epoch 52/2000, Step 77, d_loss: 0.38828757405281067, g_loss: 2.2089381217956543\n","Epoch 52/2000, Step 78, d_loss: 0.636905312538147, g_loss: 2.3303513526916504\n","Epoch 52/2000, Step 79, d_loss: 0.5393957495689392, g_loss: 3.081047773361206\n","Epoch 52/2000, Step 80, d_loss: 0.5036727786064148, g_loss: 2.788527488708496\n","Epoch 52/2000, Step 81, d_loss: 0.4411841332912445, g_loss: 3.72914457321167\n","Epoch 52/2000, Step 82, d_loss: 0.3870551288127899, g_loss: 5.215818405151367\n","Epoch 52/2000, Step 83, d_loss: 0.37158387899398804, g_loss: 4.599918365478516\n","Epoch 52/2000, Step 84, d_loss: 0.4380941390991211, g_loss: 4.265025615692139\n","Epoch 52/2000, Step 85, d_loss: 0.5029476284980774, g_loss: 3.993530511856079\n","Epoch 52/2000, Step 86, d_loss: 0.4695160984992981, g_loss: 4.206795692443848\n","Epoch 52/2000, Step 87, d_loss: 0.40052056312561035, g_loss: 3.7450287342071533\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 53/2000, Step 1, d_loss: 0.536780834197998, g_loss: 3.102853536605835\n","Epoch 53/2000, Step 2, d_loss: 0.4908324182033539, g_loss: 2.606029748916626\n","Epoch 53/2000, Step 3, d_loss: 0.528972864151001, g_loss: 1.9551568031311035\n","Epoch 53/2000, Step 4, d_loss: 0.5369648933410645, g_loss: 2.398359775543213\n","Epoch 53/2000, Step 5, d_loss: 0.4353409707546234, g_loss: 3.1094119548797607\n","Epoch 53/2000, Step 6, d_loss: 0.3832424581050873, g_loss: 3.8196170330047607\n","Epoch 53/2000, Step 7, d_loss: 0.4051906168460846, g_loss: 4.950170040130615\n","Epoch 53/2000, Step 8, d_loss: 0.4022006094455719, g_loss: 5.855454444885254\n","Epoch 53/2000, Step 9, d_loss: 0.7397745847702026, g_loss: 3.4758572578430176\n","Epoch 53/2000, Step 10, d_loss: 0.4605322778224945, g_loss: 3.3240678310394287\n","Epoch 53/2000, Step 11, d_loss: 0.46830683946609497, g_loss: 2.511723041534424\n","Epoch 53/2000, Step 12, d_loss: 0.494430810213089, g_loss: 2.5597341060638428\n","Epoch 53/2000, Step 13, d_loss: 0.601241409778595, g_loss: 2.9261996746063232\n","Epoch 53/2000, Step 14, d_loss: 0.5226632356643677, g_loss: 2.7142648696899414\n","Epoch 53/2000, Step 15, d_loss: 0.4097166955471039, g_loss: 3.4947357177734375\n","Epoch 53/2000, Step 16, d_loss: 0.39576074481010437, g_loss: 3.9255361557006836\n","Epoch 53/2000, Step 17, d_loss: 0.38137009739875793, g_loss: 4.225235939025879\n","Epoch 53/2000, Step 18, d_loss: 0.4738416075706482, g_loss: 5.779322624206543\n","Epoch 53/2000, Step 19, d_loss: 0.3945356607437134, g_loss: 5.381499767303467\n","Epoch 53/2000, Step 20, d_loss: 0.4915826916694641, g_loss: 4.505945205688477\n","Epoch 53/2000, Step 21, d_loss: 0.3696872293949127, g_loss: 5.054999351501465\n","Epoch 53/2000, Step 22, d_loss: 0.39940738677978516, g_loss: 2.6078622341156006\n","Epoch 53/2000, Step 23, d_loss: 0.5048174858093262, g_loss: 2.816431760787964\n","Epoch 53/2000, Step 24, d_loss: 0.4266170859336853, g_loss: 2.5729267597198486\n","Epoch 53/2000, Step 25, d_loss: 0.37400296330451965, g_loss: 2.1390156745910645\n","Epoch 53/2000, Step 26, d_loss: 0.46122628450393677, g_loss: 4.982284069061279\n","Epoch 53/2000, Step 27, d_loss: 0.4774901270866394, g_loss: 2.910731315612793\n","Epoch 53/2000, Step 28, d_loss: 0.42823654413223267, g_loss: 3.5199291706085205\n","Epoch 53/2000, Step 29, d_loss: 0.44181588292121887, g_loss: 4.023802280426025\n","Epoch 53/2000, Step 30, d_loss: 0.36146849393844604, g_loss: 4.388327598571777\n","Epoch 53/2000, Step 31, d_loss: 0.41973790526390076, g_loss: 3.9363460540771484\n","Epoch 53/2000, Step 32, d_loss: 0.38729244470596313, g_loss: 4.337827205657959\n","Epoch 53/2000, Step 33, d_loss: 0.5058313608169556, g_loss: 4.526421070098877\n","Epoch 53/2000, Step 34, d_loss: 0.45301830768585205, g_loss: 2.905700206756592\n","Epoch 53/2000, Step 35, d_loss: 0.4586184024810791, g_loss: 3.5020511150360107\n","Epoch 53/2000, Step 36, d_loss: 0.43521711230278015, g_loss: 3.806304931640625\n","Epoch 53/2000, Step 37, d_loss: 0.3638567626476288, g_loss: 5.2790937423706055\n","Epoch 53/2000, Step 38, d_loss: 0.5262574553489685, g_loss: 4.884370803833008\n","Epoch 53/2000, Step 39, d_loss: 0.45612913370132446, g_loss: 3.2413296699523926\n","Epoch 53/2000, Step 40, d_loss: 0.4602934420108795, g_loss: 2.2668991088867188\n","Epoch 53/2000, Step 41, d_loss: 0.41241106390953064, g_loss: 2.3208651542663574\n","Epoch 53/2000, Step 42, d_loss: 0.48887503147125244, g_loss: 2.687779188156128\n","Epoch 53/2000, Step 43, d_loss: 0.4255979359149933, g_loss: 2.9200713634490967\n","Epoch 53/2000, Step 44, d_loss: 0.44854483008384705, g_loss: 3.2235395908355713\n","Epoch 53/2000, Step 45, d_loss: 0.3979840874671936, g_loss: 4.313199043273926\n","Epoch 53/2000, Step 46, d_loss: 0.43528294563293457, g_loss: 5.19606351852417\n","Epoch 53/2000, Step 47, d_loss: 0.37236589193344116, g_loss: 4.197813987731934\n","Epoch 53/2000, Step 48, d_loss: 0.3902621269226074, g_loss: 4.498265266418457\n","Epoch 53/2000, Step 49, d_loss: 0.48673927783966064, g_loss: 4.322398662567139\n","Epoch 53/2000, Step 50, d_loss: 0.41908127069473267, g_loss: 5.183357238769531\n","Epoch 53/2000, Step 51, d_loss: 0.41457921266555786, g_loss: 3.046962261199951\n","Epoch 53/2000, Step 52, d_loss: 0.4618963897228241, g_loss: 3.129333257675171\n","Epoch 53/2000, Step 53, d_loss: 0.4247845411300659, g_loss: 3.5188310146331787\n","Epoch 53/2000, Step 54, d_loss: 0.40653398633003235, g_loss: 2.961085796356201\n","Epoch 53/2000, Step 55, d_loss: 0.44578927755355835, g_loss: 2.636643171310425\n","Epoch 53/2000, Step 56, d_loss: 0.49422991275787354, g_loss: 3.4357197284698486\n","Epoch 53/2000, Step 57, d_loss: 0.5036876201629639, g_loss: 3.2276711463928223\n","Epoch 53/2000, Step 58, d_loss: 0.43684694170951843, g_loss: 3.830122709274292\n","Epoch 53/2000, Step 59, d_loss: 0.4427637457847595, g_loss: 4.435751914978027\n","Epoch 53/2000, Step 60, d_loss: 0.4092797338962555, g_loss: 3.888002634048462\n","Epoch 53/2000, Step 61, d_loss: 0.4074411988258362, g_loss: 4.035359859466553\n","Epoch 53/2000, Step 62, d_loss: 0.4376593828201294, g_loss: 4.269706726074219\n","Epoch 53/2000, Step 63, d_loss: 0.4027249217033386, g_loss: 2.8688323497772217\n","Epoch 53/2000, Step 64, d_loss: 0.4167560636997223, g_loss: 3.4303181171417236\n","Epoch 53/2000, Step 65, d_loss: 0.3715484142303467, g_loss: 3.871572971343994\n","Epoch 53/2000, Step 66, d_loss: 0.4099296033382416, g_loss: 4.46187686920166\n","Epoch 53/2000, Step 67, d_loss: 0.3800174593925476, g_loss: 3.8653345108032227\n","Epoch 53/2000, Step 68, d_loss: 0.3760175406932831, g_loss: 4.028994083404541\n","Epoch 53/2000, Step 69, d_loss: 0.37476831674575806, g_loss: 4.501758098602295\n","Epoch 53/2000, Step 70, d_loss: 0.45021554827690125, g_loss: 3.99444317817688\n","Epoch 53/2000, Step 71, d_loss: 0.3503996431827545, g_loss: 3.9992053508758545\n","Epoch 53/2000, Step 72, d_loss: 0.38036710023880005, g_loss: 3.1738088130950928\n","Epoch 53/2000, Step 73, d_loss: 0.4137629270553589, g_loss: 3.6368367671966553\n","Epoch 53/2000, Step 74, d_loss: 0.39131057262420654, g_loss: 3.097721576690674\n","Epoch 53/2000, Step 75, d_loss: 0.4548966586589813, g_loss: 3.7265472412109375\n","Epoch 53/2000, Step 76, d_loss: 0.4105181097984314, g_loss: 3.9756553173065186\n","Epoch 53/2000, Step 77, d_loss: 0.38064876198768616, g_loss: 4.5848798751831055\n","Epoch 53/2000, Step 78, d_loss: 0.4161982536315918, g_loss: 3.200847864151001\n","Epoch 53/2000, Step 79, d_loss: 0.38228586316108704, g_loss: 3.8696224689483643\n","Epoch 53/2000, Step 80, d_loss: 0.4051932096481323, g_loss: 3.7684648036956787\n","Epoch 53/2000, Step 81, d_loss: 0.37972450256347656, g_loss: 3.5839121341705322\n","Epoch 53/2000, Step 82, d_loss: 0.37902337312698364, g_loss: 2.6773412227630615\n","Epoch 53/2000, Step 83, d_loss: 0.39111149311065674, g_loss: 3.54219651222229\n","Epoch 53/2000, Step 84, d_loss: 0.3842625916004181, g_loss: 3.6398444175720215\n","Epoch 53/2000, Step 85, d_loss: 0.3928709030151367, g_loss: 5.163143157958984\n","Epoch 53/2000, Step 86, d_loss: 0.3774876296520233, g_loss: 5.68746280670166\n","Epoch 53/2000, Step 87, d_loss: 0.3885921239852905, g_loss: 5.103066921234131\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 54/2000, Step 1, d_loss: 0.384290874004364, g_loss: 4.325602054595947\n","Epoch 54/2000, Step 2, d_loss: 0.3930480182170868, g_loss: 5.131326198577881\n","Epoch 54/2000, Step 3, d_loss: 0.35079437494277954, g_loss: 3.866218090057373\n","Epoch 54/2000, Step 4, d_loss: 0.3692336678504944, g_loss: 3.8927688598632812\n","Epoch 54/2000, Step 5, d_loss: 0.4130655527114868, g_loss: 3.8467488288879395\n","Epoch 54/2000, Step 6, d_loss: 0.3577781319618225, g_loss: 4.336358070373535\n","Epoch 54/2000, Step 7, d_loss: 0.3571186661720276, g_loss: 4.229557991027832\n","Epoch 54/2000, Step 8, d_loss: 0.3985971510410309, g_loss: 3.2925093173980713\n","Epoch 54/2000, Step 9, d_loss: 0.3658820390701294, g_loss: 3.2947428226470947\n","Epoch 54/2000, Step 10, d_loss: 0.3792615532875061, g_loss: 3.9186360836029053\n","Epoch 54/2000, Step 11, d_loss: 0.368198424577713, g_loss: 3.404620885848999\n","Epoch 54/2000, Step 12, d_loss: 0.3530868887901306, g_loss: 4.93140172958374\n","Epoch 54/2000, Step 13, d_loss: 0.48725277185440063, g_loss: 3.8362722396850586\n","Epoch 54/2000, Step 14, d_loss: 0.3689922094345093, g_loss: 3.999540090560913\n","Epoch 54/2000, Step 15, d_loss: 0.3838157057762146, g_loss: 3.4228878021240234\n","Epoch 54/2000, Step 16, d_loss: 0.3976355791091919, g_loss: 3.1180946826934814\n","Epoch 54/2000, Step 17, d_loss: 0.41884076595306396, g_loss: 4.503734588623047\n","Epoch 54/2000, Step 18, d_loss: 0.4259676933288574, g_loss: 3.7236080169677734\n","Epoch 54/2000, Step 19, d_loss: 0.37163445353507996, g_loss: 4.700393199920654\n","Epoch 54/2000, Step 20, d_loss: 0.37841758131980896, g_loss: 3.6790013313293457\n","Epoch 54/2000, Step 21, d_loss: 0.4066784977912903, g_loss: 5.797961235046387\n","Epoch 54/2000, Step 22, d_loss: 0.5740254521369934, g_loss: 3.928184747695923\n","Epoch 54/2000, Step 23, d_loss: 0.365774542093277, g_loss: 3.2240865230560303\n","Epoch 54/2000, Step 24, d_loss: 0.40684080123901367, g_loss: 3.240041971206665\n","Epoch 54/2000, Step 25, d_loss: 0.459825724363327, g_loss: 2.83398699760437\n","Epoch 54/2000, Step 26, d_loss: 0.3900614082813263, g_loss: 3.6935126781463623\n","Epoch 54/2000, Step 27, d_loss: 0.4042375683784485, g_loss: 3.3193273544311523\n","Epoch 54/2000, Step 28, d_loss: 0.4067067503929138, g_loss: 3.3817460536956787\n","Epoch 54/2000, Step 29, d_loss: 0.38661524653434753, g_loss: 3.875694990158081\n","Epoch 54/2000, Step 30, d_loss: 0.3895106911659241, g_loss: 4.4209489822387695\n","Epoch 54/2000, Step 31, d_loss: 0.3911866843700409, g_loss: 4.434272289276123\n","Epoch 54/2000, Step 32, d_loss: 0.435462087392807, g_loss: 3.759805202484131\n","Epoch 54/2000, Step 33, d_loss: 0.5435261130332947, g_loss: 3.1859841346740723\n","Epoch 54/2000, Step 34, d_loss: 0.4330628514289856, g_loss: 2.7239582538604736\n","Epoch 54/2000, Step 35, d_loss: 0.564346432685852, g_loss: 2.5600333213806152\n","Epoch 54/2000, Step 36, d_loss: 0.6422502994537354, g_loss: 2.114074468612671\n","Epoch 54/2000, Step 37, d_loss: 0.43758681416511536, g_loss: 3.9165897369384766\n","Epoch 54/2000, Step 38, d_loss: 0.352794349193573, g_loss: 4.209637641906738\n","Epoch 54/2000, Step 39, d_loss: 0.4389498829841614, g_loss: 3.8276009559631348\n","Epoch 54/2000, Step 40, d_loss: 0.4038577973842621, g_loss: 4.5027875900268555\n","Epoch 54/2000, Step 41, d_loss: 0.5583304166793823, g_loss: 5.414613723754883\n","Epoch 54/2000, Step 42, d_loss: 0.4437723457813263, g_loss: 3.487149715423584\n","Epoch 54/2000, Step 43, d_loss: 0.41814786195755005, g_loss: 2.892580270767212\n","Epoch 54/2000, Step 44, d_loss: 0.4582921266555786, g_loss: 4.025459289550781\n","Epoch 54/2000, Step 45, d_loss: 0.494834840297699, g_loss: 3.4883532524108887\n","Epoch 54/2000, Step 46, d_loss: 0.4998483657836914, g_loss: 3.1177978515625\n","Epoch 54/2000, Step 47, d_loss: 0.5332129597663879, g_loss: 3.661607027053833\n","Epoch 54/2000, Step 48, d_loss: 0.47474563121795654, g_loss: 3.9141933917999268\n","Epoch 54/2000, Step 49, d_loss: 0.4037349820137024, g_loss: 3.667951822280884\n","Epoch 54/2000, Step 50, d_loss: 0.41897785663604736, g_loss: 4.5028557777404785\n","Epoch 54/2000, Step 51, d_loss: 0.4111000597476959, g_loss: 4.281169891357422\n","Epoch 54/2000, Step 52, d_loss: 0.6655377149581909, g_loss: 3.672088861465454\n","Epoch 54/2000, Step 53, d_loss: 0.4108002185821533, g_loss: 2.7885184288024902\n","Epoch 54/2000, Step 54, d_loss: 0.4291635751724243, g_loss: 3.4320666790008545\n","Epoch 54/2000, Step 55, d_loss: 0.6132572889328003, g_loss: 2.346968173980713\n","Epoch 54/2000, Step 56, d_loss: 0.5446845293045044, g_loss: 2.0546467304229736\n","Epoch 54/2000, Step 57, d_loss: 0.427883505821228, g_loss: 2.960371732711792\n","Epoch 54/2000, Step 58, d_loss: 0.4724575877189636, g_loss: 3.1468114852905273\n","Epoch 54/2000, Step 59, d_loss: 0.4042963981628418, g_loss: 3.9013924598693848\n","Epoch 54/2000, Step 60, d_loss: 0.37573301792144775, g_loss: 4.01190710067749\n","Epoch 54/2000, Step 61, d_loss: 0.4117632806301117, g_loss: 4.367956161499023\n","Epoch 54/2000, Step 62, d_loss: 0.4462100863456726, g_loss: 4.405377388000488\n","Epoch 54/2000, Step 63, d_loss: 0.4544408321380615, g_loss: 3.7546868324279785\n","Epoch 54/2000, Step 64, d_loss: 0.38050058484077454, g_loss: 3.763657331466675\n","Epoch 54/2000, Step 65, d_loss: 0.3944288492202759, g_loss: 2.969048261642456\n","Epoch 54/2000, Step 66, d_loss: 0.3659597635269165, g_loss: 2.922003984451294\n","Epoch 54/2000, Step 67, d_loss: 0.39972007274627686, g_loss: 2.864109516143799\n","Epoch 54/2000, Step 68, d_loss: 0.48506516218185425, g_loss: 2.9224250316619873\n","Epoch 54/2000, Step 69, d_loss: 0.41393759846687317, g_loss: 3.2716116905212402\n","Epoch 54/2000, Step 70, d_loss: 0.41289710998535156, g_loss: 3.828209161758423\n","Epoch 54/2000, Step 71, d_loss: 0.3853585720062256, g_loss: 4.114007472991943\n","Epoch 54/2000, Step 72, d_loss: 0.3593493700027466, g_loss: 3.9219653606414795\n","Epoch 54/2000, Step 73, d_loss: 0.3745785057544708, g_loss: 4.297283172607422\n","Epoch 54/2000, Step 74, d_loss: 0.374286413192749, g_loss: 4.507761001586914\n","Epoch 54/2000, Step 75, d_loss: 0.48194366693496704, g_loss: 4.149017810821533\n","Epoch 54/2000, Step 76, d_loss: 0.3665921092033386, g_loss: 4.0528059005737305\n","Epoch 54/2000, Step 77, d_loss: 0.3865491449832916, g_loss: 4.269167900085449\n","Epoch 54/2000, Step 78, d_loss: 0.4043015241622925, g_loss: 3.7081923484802246\n","Epoch 54/2000, Step 79, d_loss: 0.4124358594417572, g_loss: 2.806443452835083\n","Epoch 54/2000, Step 80, d_loss: 0.3914915919303894, g_loss: 2.5993809700012207\n","Epoch 54/2000, Step 81, d_loss: 0.40152618288993835, g_loss: 3.125641345977783\n","Epoch 54/2000, Step 82, d_loss: 0.41774696111679077, g_loss: 3.419161319732666\n","Epoch 54/2000, Step 83, d_loss: 0.4409165680408478, g_loss: 4.409712314605713\n","Epoch 54/2000, Step 84, d_loss: 0.41545721888542175, g_loss: 3.493126392364502\n","Epoch 54/2000, Step 85, d_loss: 0.3655477464199066, g_loss: 4.271631240844727\n","Epoch 54/2000, Step 86, d_loss: 0.37566256523132324, g_loss: 4.285411834716797\n","Epoch 54/2000, Step 87, d_loss: 0.3871402442455292, g_loss: 5.256741523742676\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 55/2000, Step 1, d_loss: 0.386751264333725, g_loss: 5.12042236328125\n","Epoch 55/2000, Step 2, d_loss: 0.377698689699173, g_loss: 4.212160110473633\n","Epoch 55/2000, Step 3, d_loss: 0.37545377016067505, g_loss: 4.9528374671936035\n","Epoch 55/2000, Step 4, d_loss: 0.356399267911911, g_loss: 3.387169599533081\n","Epoch 55/2000, Step 5, d_loss: 0.35923880338668823, g_loss: 4.303117275238037\n","Epoch 55/2000, Step 6, d_loss: 0.36816972494125366, g_loss: 4.013792037963867\n","Epoch 55/2000, Step 7, d_loss: 0.38941818475723267, g_loss: 4.467853546142578\n","Epoch 55/2000, Step 8, d_loss: 0.3551679253578186, g_loss: 4.181422233581543\n","Epoch 55/2000, Step 9, d_loss: 0.38921281695365906, g_loss: 4.362502574920654\n","Epoch 55/2000, Step 10, d_loss: 0.36805206537246704, g_loss: 3.698540687561035\n","Epoch 55/2000, Step 11, d_loss: 0.44934573769569397, g_loss: 3.6108412742614746\n","Epoch 55/2000, Step 12, d_loss: 0.36542338132858276, g_loss: 2.802276849746704\n","Epoch 55/2000, Step 13, d_loss: 0.38206052780151367, g_loss: 2.9836933612823486\n","Epoch 55/2000, Step 14, d_loss: 0.46004629135131836, g_loss: 3.5910322666168213\n","Epoch 55/2000, Step 15, d_loss: 0.40741804242134094, g_loss: 3.609036922454834\n","Epoch 55/2000, Step 16, d_loss: 0.3889721930027008, g_loss: 3.3324363231658936\n","Epoch 55/2000, Step 17, d_loss: 0.38673290610313416, g_loss: 3.9703636169433594\n","Epoch 55/2000, Step 18, d_loss: 0.3489993214607239, g_loss: 4.915265083312988\n","Epoch 55/2000, Step 19, d_loss: 0.4654659032821655, g_loss: 4.843905448913574\n","Epoch 55/2000, Step 20, d_loss: 0.36832255125045776, g_loss: 5.512439250946045\n","Epoch 55/2000, Step 21, d_loss: 0.3827919065952301, g_loss: 4.41504430770874\n","Epoch 55/2000, Step 22, d_loss: 0.5463088154792786, g_loss: 3.768918037414551\n","Epoch 55/2000, Step 23, d_loss: 0.369983434677124, g_loss: 3.8929409980773926\n","Epoch 55/2000, Step 24, d_loss: 0.3857983946800232, g_loss: 3.965623378753662\n","Epoch 55/2000, Step 25, d_loss: 0.44650986790657043, g_loss: 3.309847593307495\n","Epoch 55/2000, Step 26, d_loss: 0.4204820692539215, g_loss: 3.1371071338653564\n","Epoch 55/2000, Step 27, d_loss: 0.3910130262374878, g_loss: 3.799769163131714\n","Epoch 55/2000, Step 28, d_loss: 0.4142313301563263, g_loss: 5.858441352844238\n","Epoch 55/2000, Step 29, d_loss: 0.4083796441555023, g_loss: 5.050556659698486\n","Epoch 55/2000, Step 30, d_loss: 0.3769666850566864, g_loss: 5.469877243041992\n","Epoch 55/2000, Step 31, d_loss: 0.3898051083087921, g_loss: 4.441415309906006\n","Epoch 55/2000, Step 32, d_loss: 0.3879876732826233, g_loss: 4.695914268493652\n","Epoch 55/2000, Step 33, d_loss: 0.4123527407646179, g_loss: 5.035789489746094\n","Epoch 55/2000, Step 34, d_loss: 0.3734888434410095, g_loss: 4.7089619636535645\n","Epoch 55/2000, Step 35, d_loss: 0.373454749584198, g_loss: 3.9507949352264404\n","Epoch 55/2000, Step 36, d_loss: 0.36983293294906616, g_loss: 3.0110461711883545\n","Epoch 55/2000, Step 37, d_loss: 0.4087674021720886, g_loss: 3.334273338317871\n","Epoch 55/2000, Step 38, d_loss: 0.4661746025085449, g_loss: 3.199399709701538\n","Epoch 55/2000, Step 39, d_loss: 0.3558070659637451, g_loss: 2.918473243713379\n","Epoch 55/2000, Step 40, d_loss: 0.4179847836494446, g_loss: 4.337729454040527\n","Epoch 55/2000, Step 41, d_loss: 0.3744525611400604, g_loss: 4.568607330322266\n","Epoch 55/2000, Step 42, d_loss: 0.3426985740661621, g_loss: 5.657515048980713\n","Epoch 55/2000, Step 43, d_loss: 0.35447362065315247, g_loss: 4.949244499206543\n","Epoch 55/2000, Step 44, d_loss: 0.49085870385169983, g_loss: 4.291110038757324\n","Epoch 55/2000, Step 45, d_loss: 0.3598737418651581, g_loss: 3.899320602416992\n","Epoch 55/2000, Step 46, d_loss: 0.3679382801055908, g_loss: 3.8912384510040283\n","Epoch 55/2000, Step 47, d_loss: 0.42175543308258057, g_loss: 3.546600580215454\n","Epoch 55/2000, Step 48, d_loss: 0.3774351179599762, g_loss: 3.154726982116699\n","Epoch 55/2000, Step 49, d_loss: 0.39869076013565063, g_loss: 3.1477389335632324\n","Epoch 55/2000, Step 50, d_loss: 0.40379634499549866, g_loss: 3.087184190750122\n","Epoch 55/2000, Step 51, d_loss: 0.4015180170536041, g_loss: 3.9580864906311035\n","Epoch 55/2000, Step 52, d_loss: 0.45477136969566345, g_loss: 3.6260151863098145\n","Epoch 55/2000, Step 53, d_loss: 0.42448294162750244, g_loss: 3.986332416534424\n","Epoch 55/2000, Step 54, d_loss: 0.4445267617702484, g_loss: 3.2172739505767822\n","Epoch 55/2000, Step 55, d_loss: 0.36183446645736694, g_loss: 5.328649044036865\n","Epoch 55/2000, Step 56, d_loss: 0.3821912109851837, g_loss: 4.014067649841309\n","Epoch 55/2000, Step 57, d_loss: 0.44078412652015686, g_loss: 3.470085620880127\n","Epoch 55/2000, Step 58, d_loss: 0.4112377464771271, g_loss: 3.6277549266815186\n","Epoch 55/2000, Step 59, d_loss: 0.3817683756351471, g_loss: 3.882913112640381\n","Epoch 55/2000, Step 60, d_loss: 0.3741244673728943, g_loss: 3.8064029216766357\n","Epoch 55/2000, Step 61, d_loss: 0.42204549908638, g_loss: 4.714237689971924\n","Epoch 55/2000, Step 62, d_loss: 0.4714440703392029, g_loss: 3.3565239906311035\n","Epoch 55/2000, Step 63, d_loss: 0.37477579712867737, g_loss: 4.584997177124023\n","Epoch 55/2000, Step 64, d_loss: 0.4288822114467621, g_loss: 4.234011173248291\n","Epoch 55/2000, Step 65, d_loss: 0.36879077553749084, g_loss: 2.9730522632598877\n","Epoch 55/2000, Step 66, d_loss: 0.4487895965576172, g_loss: 3.2757070064544678\n","Epoch 55/2000, Step 67, d_loss: 0.41234105825424194, g_loss: 3.283334970474243\n","Epoch 55/2000, Step 68, d_loss: 0.44238609075546265, g_loss: 3.3860538005828857\n","Epoch 55/2000, Step 69, d_loss: 0.4443207085132599, g_loss: 3.6820785999298096\n","Epoch 55/2000, Step 70, d_loss: 0.44921088218688965, g_loss: 4.550482273101807\n","Epoch 55/2000, Step 71, d_loss: 0.3793763816356659, g_loss: 3.3188223838806152\n","Epoch 55/2000, Step 72, d_loss: 0.43611234426498413, g_loss: 4.865679740905762\n","Epoch 55/2000, Step 73, d_loss: 0.38039541244506836, g_loss: 3.487326145172119\n","Epoch 55/2000, Step 74, d_loss: 0.37236288189888, g_loss: 3.2627742290496826\n","Epoch 55/2000, Step 75, d_loss: 0.3808594346046448, g_loss: 4.106695175170898\n","Epoch 55/2000, Step 76, d_loss: 0.35901325941085815, g_loss: 4.419309616088867\n","Epoch 55/2000, Step 77, d_loss: 0.3633214235305786, g_loss: 4.60832405090332\n","Epoch 55/2000, Step 78, d_loss: 0.5263381600379944, g_loss: 3.2404088973999023\n","Epoch 55/2000, Step 79, d_loss: 0.3906419575214386, g_loss: 3.181396484375\n","Epoch 55/2000, Step 80, d_loss: 0.45136648416519165, g_loss: 3.355381965637207\n","Epoch 55/2000, Step 81, d_loss: 0.47648969292640686, g_loss: 2.7659361362457275\n","Epoch 55/2000, Step 82, d_loss: 0.43178480863571167, g_loss: 3.3449759483337402\n","Epoch 55/2000, Step 83, d_loss: 0.4481815695762634, g_loss: 3.4526119232177734\n","Epoch 55/2000, Step 84, d_loss: 0.3555840253829956, g_loss: 3.0634005069732666\n","Epoch 55/2000, Step 85, d_loss: 0.3682164251804352, g_loss: 4.489898681640625\n","Epoch 55/2000, Step 86, d_loss: 0.3966727554798126, g_loss: 4.329924583435059\n","Epoch 55/2000, Step 87, d_loss: 0.362652063369751, g_loss: 4.846794605255127\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 56/2000, Step 1, d_loss: 0.36765214800834656, g_loss: 3.8526878356933594\n","Epoch 56/2000, Step 2, d_loss: 0.407856285572052, g_loss: 4.330146789550781\n","Epoch 56/2000, Step 3, d_loss: 0.36153870820999146, g_loss: 3.4607253074645996\n","Epoch 56/2000, Step 4, d_loss: 0.3780374228954315, g_loss: 4.184427261352539\n","Epoch 56/2000, Step 5, d_loss: 0.37952712178230286, g_loss: 3.32844614982605\n","Epoch 56/2000, Step 6, d_loss: 0.37431326508522034, g_loss: 2.768296957015991\n","Epoch 56/2000, Step 7, d_loss: 0.37788382172584534, g_loss: 3.648651361465454\n","Epoch 56/2000, Step 8, d_loss: 0.48387032747268677, g_loss: 3.611455202102661\n","Epoch 56/2000, Step 9, d_loss: 0.3938358724117279, g_loss: 3.2284786701202393\n","Epoch 56/2000, Step 10, d_loss: 0.38197803497314453, g_loss: 4.335714340209961\n","Epoch 56/2000, Step 11, d_loss: 0.3590618073940277, g_loss: 3.931077480316162\n","Epoch 56/2000, Step 12, d_loss: 0.3699827194213867, g_loss: 5.295191287994385\n","Epoch 56/2000, Step 13, d_loss: 0.3684564232826233, g_loss: 5.39495325088501\n","Epoch 56/2000, Step 14, d_loss: 0.6362172365188599, g_loss: 4.527890205383301\n","Epoch 56/2000, Step 15, d_loss: 0.35746434330940247, g_loss: 4.405876636505127\n","Epoch 56/2000, Step 16, d_loss: 0.36569684743881226, g_loss: 3.3055238723754883\n","Epoch 56/2000, Step 17, d_loss: 0.3431742489337921, g_loss: 4.395850658416748\n","Epoch 56/2000, Step 18, d_loss: 0.42447471618652344, g_loss: 3.2939844131469727\n","Epoch 56/2000, Step 19, d_loss: 0.38639312982559204, g_loss: 3.1821353435516357\n","Epoch 56/2000, Step 20, d_loss: 0.37671199440956116, g_loss: 3.4713754653930664\n","Epoch 56/2000, Step 21, d_loss: 0.40193232893943787, g_loss: 3.661525011062622\n","Epoch 56/2000, Step 22, d_loss: 0.35791730880737305, g_loss: 4.623831748962402\n","Epoch 56/2000, Step 23, d_loss: 0.37263786792755127, g_loss: 3.2177131175994873\n","Epoch 56/2000, Step 24, d_loss: 0.38843783736228943, g_loss: 3.8345792293548584\n","Epoch 56/2000, Step 25, d_loss: 0.39269283413887024, g_loss: 4.174878120422363\n","Epoch 56/2000, Step 26, d_loss: 0.40714651346206665, g_loss: 4.387640476226807\n","Epoch 56/2000, Step 27, d_loss: 0.3593326508998871, g_loss: 3.9809257984161377\n","Epoch 56/2000, Step 28, d_loss: 0.5199999213218689, g_loss: 4.302662372589111\n","Epoch 56/2000, Step 29, d_loss: 0.3674975037574768, g_loss: 3.5887763500213623\n","Epoch 56/2000, Step 30, d_loss: 0.4025052785873413, g_loss: 3.7384679317474365\n","Epoch 56/2000, Step 31, d_loss: 0.3895755708217621, g_loss: 3.3258912563323975\n","Epoch 56/2000, Step 32, d_loss: 0.4454572796821594, g_loss: 3.3825435638427734\n","Epoch 56/2000, Step 33, d_loss: 0.39428186416625977, g_loss: 3.7665746212005615\n","Epoch 56/2000, Step 34, d_loss: 0.471355676651001, g_loss: 2.849687099456787\n","Epoch 56/2000, Step 35, d_loss: 0.5108160972595215, g_loss: 2.920468330383301\n","Epoch 56/2000, Step 36, d_loss: 0.47237685322761536, g_loss: 2.972665548324585\n","Epoch 56/2000, Step 37, d_loss: 0.5312142968177795, g_loss: 3.969599485397339\n","Epoch 56/2000, Step 38, d_loss: 0.4549518823623657, g_loss: 3.690497636795044\n","Epoch 56/2000, Step 39, d_loss: 0.49717891216278076, g_loss: 4.345462799072266\n","Epoch 56/2000, Step 40, d_loss: 0.46312734484672546, g_loss: 3.794138193130493\n","Epoch 56/2000, Step 41, d_loss: 0.37723490595817566, g_loss: 3.785405397415161\n","Epoch 56/2000, Step 42, d_loss: 0.38852959871292114, g_loss: 3.947296380996704\n","Epoch 56/2000, Step 43, d_loss: 0.39745017886161804, g_loss: 4.206794738769531\n","Epoch 56/2000, Step 44, d_loss: 0.4370584785938263, g_loss: 5.654752254486084\n","Epoch 56/2000, Step 45, d_loss: 0.4118306636810303, g_loss: 3.8906798362731934\n","Epoch 56/2000, Step 46, d_loss: 0.41972440481185913, g_loss: 4.257346153259277\n","Epoch 56/2000, Step 47, d_loss: 0.3809092044830322, g_loss: 4.177295684814453\n","Epoch 56/2000, Step 48, d_loss: 0.4959772825241089, g_loss: 3.118595600128174\n","Epoch 56/2000, Step 49, d_loss: 0.42296692728996277, g_loss: 3.448288679122925\n","Epoch 56/2000, Step 50, d_loss: 0.44965118169784546, g_loss: 3.20821475982666\n","Epoch 56/2000, Step 51, d_loss: 0.4178197979927063, g_loss: 3.5630342960357666\n","Epoch 56/2000, Step 52, d_loss: 0.4136776924133301, g_loss: 3.8498663902282715\n","Epoch 56/2000, Step 53, d_loss: 0.4101658761501312, g_loss: 3.8413302898406982\n","Epoch 56/2000, Step 54, d_loss: 0.3974953889846802, g_loss: 3.6611762046813965\n","Epoch 56/2000, Step 55, d_loss: 0.47661587595939636, g_loss: 2.799358606338501\n","Epoch 56/2000, Step 56, d_loss: 0.3632257878780365, g_loss: 3.8177952766418457\n","Epoch 56/2000, Step 57, d_loss: 0.38440680503845215, g_loss: 3.898399829864502\n","Epoch 56/2000, Step 58, d_loss: 0.4038354456424713, g_loss: 4.389703273773193\n","Epoch 56/2000, Step 59, d_loss: 0.4647698402404785, g_loss: 3.2981793880462646\n","Epoch 56/2000, Step 60, d_loss: 0.3974648714065552, g_loss: 5.295288562774658\n","Epoch 56/2000, Step 61, d_loss: 0.3625275194644928, g_loss: 4.1582417488098145\n","Epoch 56/2000, Step 62, d_loss: 0.40204834938049316, g_loss: 4.1803998947143555\n","Epoch 56/2000, Step 63, d_loss: 0.40705370903015137, g_loss: 3.4263627529144287\n","Epoch 56/2000, Step 64, d_loss: 0.38963332772254944, g_loss: 4.281893253326416\n","Epoch 56/2000, Step 65, d_loss: 0.3872418999671936, g_loss: 3.91705322265625\n","Epoch 56/2000, Step 66, d_loss: 0.37079930305480957, g_loss: 4.091063499450684\n","Epoch 56/2000, Step 67, d_loss: 0.3612057566642761, g_loss: 4.3786492347717285\n","Epoch 56/2000, Step 68, d_loss: 0.43878117203712463, g_loss: 3.639984607696533\n","Epoch 56/2000, Step 69, d_loss: 0.40780818462371826, g_loss: 3.7099664211273193\n","Epoch 56/2000, Step 70, d_loss: 0.36283978819847107, g_loss: 3.958009958267212\n","Epoch 56/2000, Step 71, d_loss: 0.38000935316085815, g_loss: 4.046542644500732\n","Epoch 56/2000, Step 72, d_loss: 0.36368420720100403, g_loss: 3.2840170860290527\n","Epoch 56/2000, Step 73, d_loss: 0.39081189036369324, g_loss: 3.7588419914245605\n","Epoch 56/2000, Step 74, d_loss: 0.3940649926662445, g_loss: 4.30053186416626\n","Epoch 56/2000, Step 75, d_loss: 0.4477032721042633, g_loss: 3.7453176975250244\n","Epoch 56/2000, Step 76, d_loss: 0.3894272446632385, g_loss: 4.065424919128418\n","Epoch 56/2000, Step 77, d_loss: 0.40839987993240356, g_loss: 3.8074278831481934\n","Epoch 56/2000, Step 78, d_loss: 0.39212509989738464, g_loss: 3.7822041511535645\n","Epoch 56/2000, Step 79, d_loss: 0.4735100269317627, g_loss: 3.7341971397399902\n","Epoch 56/2000, Step 80, d_loss: 0.3539913296699524, g_loss: 3.1839818954467773\n","Epoch 56/2000, Step 81, d_loss: 0.40198656916618347, g_loss: 2.9886231422424316\n","Epoch 56/2000, Step 82, d_loss: 0.42612555623054504, g_loss: 3.3314085006713867\n","Epoch 56/2000, Step 83, d_loss: 0.38206225633621216, g_loss: 4.882948875427246\n","Epoch 56/2000, Step 84, d_loss: 0.39554017782211304, g_loss: 3.967435359954834\n","Epoch 56/2000, Step 85, d_loss: 0.372281014919281, g_loss: 4.019434452056885\n","Epoch 56/2000, Step 86, d_loss: 0.3761979341506958, g_loss: 4.585562229156494\n","Epoch 56/2000, Step 87, d_loss: 0.36087241768836975, g_loss: 3.9686739444732666\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 57/2000, Step 1, d_loss: 0.45400378108024597, g_loss: 4.266639709472656\n","Epoch 57/2000, Step 2, d_loss: 0.36921426653862, g_loss: 4.516731262207031\n","Epoch 57/2000, Step 3, d_loss: 0.3503528833389282, g_loss: 3.3655965328216553\n","Epoch 57/2000, Step 4, d_loss: 0.38070112466812134, g_loss: 3.337090253829956\n","Epoch 57/2000, Step 5, d_loss: 0.4271150529384613, g_loss: 3.404543876647949\n","Epoch 57/2000, Step 6, d_loss: 0.4153425991535187, g_loss: 3.391040563583374\n","Epoch 57/2000, Step 7, d_loss: 0.40189141035079956, g_loss: 4.620281219482422\n","Epoch 57/2000, Step 8, d_loss: 0.37536177039146423, g_loss: 4.277233600616455\n","Epoch 57/2000, Step 9, d_loss: 0.3713257312774658, g_loss: 4.408553600311279\n","Epoch 57/2000, Step 10, d_loss: 0.36712053418159485, g_loss: 4.504010200500488\n","Epoch 57/2000, Step 11, d_loss: 0.4364894926548004, g_loss: 4.9975810050964355\n","Epoch 57/2000, Step 12, d_loss: 0.3584350347518921, g_loss: 4.972821235656738\n","Epoch 57/2000, Step 13, d_loss: 0.45688605308532715, g_loss: 3.857022762298584\n","Epoch 57/2000, Step 14, d_loss: 0.3623465895652771, g_loss: 3.3747406005859375\n","Epoch 57/2000, Step 15, d_loss: 0.3841606676578522, g_loss: 2.886371612548828\n","Epoch 57/2000, Step 16, d_loss: 0.38719841837882996, g_loss: 3.2514588832855225\n","Epoch 57/2000, Step 17, d_loss: 0.42748329043388367, g_loss: 2.1005380153656006\n","Epoch 57/2000, Step 18, d_loss: 0.38485777378082275, g_loss: 2.989114284515381\n","Epoch 57/2000, Step 19, d_loss: 0.451031357049942, g_loss: 3.7360031604766846\n","Epoch 57/2000, Step 20, d_loss: 0.38214796781539917, g_loss: 4.373290538787842\n","Epoch 57/2000, Step 21, d_loss: 0.39389440417289734, g_loss: 4.942760467529297\n","Epoch 57/2000, Step 22, d_loss: 0.3542207181453705, g_loss: 4.904690265655518\n","Epoch 57/2000, Step 23, d_loss: 0.38549819588661194, g_loss: 4.4811553955078125\n","Epoch 57/2000, Step 24, d_loss: 0.4342295527458191, g_loss: 5.4955854415893555\n","Epoch 57/2000, Step 25, d_loss: 0.4140632152557373, g_loss: 5.176502227783203\n","Epoch 57/2000, Step 26, d_loss: 0.4039786159992218, g_loss: 3.8322083950042725\n","Epoch 57/2000, Step 27, d_loss: 0.4943370521068573, g_loss: 3.1488678455352783\n","Epoch 57/2000, Step 28, d_loss: 0.4003329277038574, g_loss: 4.200634479522705\n","Epoch 57/2000, Step 29, d_loss: 0.3921797275543213, g_loss: 2.463845729827881\n","Epoch 57/2000, Step 30, d_loss: 0.4437812268733978, g_loss: 2.0402867794036865\n","Epoch 57/2000, Step 31, d_loss: 0.50002121925354, g_loss: 2.309004783630371\n","Epoch 57/2000, Step 32, d_loss: 0.4581896662712097, g_loss: 4.290167331695557\n","Epoch 57/2000, Step 33, d_loss: 0.4475231170654297, g_loss: 4.6814703941345215\n","Epoch 57/2000, Step 34, d_loss: 0.4309719502925873, g_loss: 4.04465913772583\n","Epoch 57/2000, Step 35, d_loss: 0.38284459710121155, g_loss: 4.672700881958008\n","Epoch 57/2000, Step 36, d_loss: 0.37447136640548706, g_loss: 5.121641635894775\n","Epoch 57/2000, Step 37, d_loss: 0.40024563670158386, g_loss: 4.750283718109131\n","Epoch 57/2000, Step 38, d_loss: 0.506337583065033, g_loss: 4.3157782554626465\n","Epoch 57/2000, Step 39, d_loss: 0.42121392488479614, g_loss: 4.548708915710449\n","Epoch 57/2000, Step 40, d_loss: 0.400625616312027, g_loss: 3.6194183826446533\n","Epoch 57/2000, Step 41, d_loss: 0.39073488116264343, g_loss: 3.6995370388031006\n","Epoch 57/2000, Step 42, d_loss: 0.4198022782802582, g_loss: 3.239813804626465\n","Epoch 57/2000, Step 43, d_loss: 0.4016358554363251, g_loss: 3.0387773513793945\n","Epoch 57/2000, Step 44, d_loss: 0.4275767505168915, g_loss: 3.3906331062316895\n","Epoch 57/2000, Step 45, d_loss: 0.3936559855937958, g_loss: 4.468167781829834\n","Epoch 57/2000, Step 46, d_loss: 0.41301482915878296, g_loss: 3.220212697982788\n","Epoch 57/2000, Step 47, d_loss: 0.4639902114868164, g_loss: 5.444477081298828\n","Epoch 57/2000, Step 48, d_loss: 0.41329213976860046, g_loss: 3.4057111740112305\n","Epoch 57/2000, Step 49, d_loss: 0.37795490026474, g_loss: 2.684352397918701\n","Epoch 57/2000, Step 50, d_loss: 0.4944026470184326, g_loss: 2.7183737754821777\n","Epoch 57/2000, Step 51, d_loss: 0.40436315536499023, g_loss: 3.10577654838562\n","Epoch 57/2000, Step 52, d_loss: 0.45601972937583923, g_loss: 4.0571675300598145\n","Epoch 57/2000, Step 53, d_loss: 0.39502179622650146, g_loss: 4.087117671966553\n","Epoch 57/2000, Step 54, d_loss: 0.3802608847618103, g_loss: 5.367267608642578\n","Epoch 57/2000, Step 55, d_loss: 0.4642133116722107, g_loss: 4.197004795074463\n","Epoch 57/2000, Step 56, d_loss: 0.383217453956604, g_loss: 4.778153419494629\n","Epoch 57/2000, Step 57, d_loss: 0.6157942414283752, g_loss: 4.584278106689453\n","Epoch 57/2000, Step 58, d_loss: 0.38617339730262756, g_loss: 3.121558666229248\n","Epoch 57/2000, Step 59, d_loss: 0.41627195477485657, g_loss: 2.9602015018463135\n","Epoch 57/2000, Step 60, d_loss: 0.47043895721435547, g_loss: 2.6897616386413574\n","Epoch 57/2000, Step 61, d_loss: 0.435449481010437, g_loss: 2.7519359588623047\n","Epoch 57/2000, Step 62, d_loss: 0.43868187069892883, g_loss: 3.6097517013549805\n","Epoch 57/2000, Step 63, d_loss: 0.38340336084365845, g_loss: 3.352358341217041\n","Epoch 57/2000, Step 64, d_loss: 0.37400272488594055, g_loss: 5.184338092803955\n","Epoch 57/2000, Step 65, d_loss: 0.43739891052246094, g_loss: 4.6751837730407715\n","Epoch 57/2000, Step 66, d_loss: 0.46688491106033325, g_loss: 3.7002532482147217\n","Epoch 57/2000, Step 67, d_loss: 0.38306477665901184, g_loss: 4.667232036590576\n","Epoch 57/2000, Step 68, d_loss: 0.3631595969200134, g_loss: 3.655519962310791\n","Epoch 57/2000, Step 69, d_loss: 0.3734433054924011, g_loss: 3.876429557800293\n","Epoch 57/2000, Step 70, d_loss: 0.39261889457702637, g_loss: 5.330840110778809\n","Epoch 57/2000, Step 71, d_loss: 0.358786940574646, g_loss: 3.364938497543335\n","Epoch 57/2000, Step 72, d_loss: 0.36161333322525024, g_loss: 4.540544033050537\n","Epoch 57/2000, Step 73, d_loss: 0.5723403096199036, g_loss: 4.259825229644775\n","Epoch 57/2000, Step 74, d_loss: 0.3984850347042084, g_loss: 3.849915027618408\n","Epoch 57/2000, Step 75, d_loss: 0.44122424721717834, g_loss: 3.814371347427368\n","Epoch 57/2000, Step 76, d_loss: 0.41948696970939636, g_loss: 3.02673077583313\n","Epoch 57/2000, Step 77, d_loss: 0.48712438344955444, g_loss: 4.012558460235596\n","Epoch 57/2000, Step 78, d_loss: 0.43352413177490234, g_loss: 3.247138500213623\n","Epoch 57/2000, Step 79, d_loss: 0.4241737723350525, g_loss: 2.996485471725464\n","Epoch 57/2000, Step 80, d_loss: 0.3866921663284302, g_loss: 3.3856098651885986\n","Epoch 57/2000, Step 81, d_loss: 0.37853631377220154, g_loss: 3.770838975906372\n","Epoch 57/2000, Step 82, d_loss: 0.48279711604118347, g_loss: 3.7508866786956787\n","Epoch 57/2000, Step 83, d_loss: 0.35311418771743774, g_loss: 5.121496677398682\n","Epoch 57/2000, Step 84, d_loss: 0.37507539987564087, g_loss: 4.460536479949951\n","Epoch 57/2000, Step 85, d_loss: 0.38500991463661194, g_loss: 3.987013339996338\n","Epoch 57/2000, Step 86, d_loss: 0.40151697397232056, g_loss: 3.8899998664855957\n","Epoch 57/2000, Step 87, d_loss: 0.40247732400894165, g_loss: 5.0806050300598145\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 58/2000, Step 1, d_loss: 0.4017907977104187, g_loss: 4.101795673370361\n","Epoch 58/2000, Step 2, d_loss: 0.4005025029182434, g_loss: 4.076409339904785\n","Epoch 58/2000, Step 3, d_loss: 0.40179988741874695, g_loss: 4.533187389373779\n","Epoch 58/2000, Step 4, d_loss: 0.45419371128082275, g_loss: 3.3345115184783936\n","Epoch 58/2000, Step 5, d_loss: 0.39931491017341614, g_loss: 4.254373073577881\n","Epoch 58/2000, Step 6, d_loss: 0.48223307728767395, g_loss: 4.6020612716674805\n","Epoch 58/2000, Step 7, d_loss: 0.3973226547241211, g_loss: 4.269063949584961\n","Epoch 58/2000, Step 8, d_loss: 0.3503463566303253, g_loss: 4.044398307800293\n","Epoch 58/2000, Step 9, d_loss: 0.40648287534713745, g_loss: 3.9696125984191895\n","Epoch 58/2000, Step 10, d_loss: 0.40781500935554504, g_loss: 5.7490620613098145\n","Epoch 58/2000, Step 11, d_loss: 0.696140468120575, g_loss: 3.5385935306549072\n","Epoch 58/2000, Step 12, d_loss: 0.37783026695251465, g_loss: 2.624549150466919\n","Epoch 58/2000, Step 13, d_loss: 0.4025515019893646, g_loss: 3.9780240058898926\n","Epoch 58/2000, Step 14, d_loss: 0.5647088289260864, g_loss: 3.5453343391418457\n","Epoch 58/2000, Step 15, d_loss: 0.501750648021698, g_loss: 2.6684348583221436\n","Epoch 58/2000, Step 16, d_loss: 0.49927642941474915, g_loss: 3.7108991146087646\n","Epoch 58/2000, Step 17, d_loss: 0.4454343914985657, g_loss: 3.3313610553741455\n","Epoch 58/2000, Step 18, d_loss: 0.4110579490661621, g_loss: 5.281856536865234\n","Epoch 58/2000, Step 19, d_loss: 0.3754160404205322, g_loss: 4.628476619720459\n","Epoch 58/2000, Step 20, d_loss: 0.383199006319046, g_loss: 4.268441677093506\n","Epoch 58/2000, Step 21, d_loss: 0.3761849105358124, g_loss: 5.1270952224731445\n","Epoch 58/2000, Step 22, d_loss: 0.49763059616088867, g_loss: 5.1022419929504395\n","Epoch 58/2000, Step 23, d_loss: 0.48705747723579407, g_loss: 3.3218348026275635\n","Epoch 58/2000, Step 24, d_loss: 0.35554057359695435, g_loss: 3.4084699153900146\n","Epoch 58/2000, Step 25, d_loss: 0.3992980122566223, g_loss: 3.052603006362915\n","Epoch 58/2000, Step 26, d_loss: 0.5134578943252563, g_loss: 2.311152458190918\n","Epoch 58/2000, Step 27, d_loss: 0.44059014320373535, g_loss: 3.6556310653686523\n","Epoch 58/2000, Step 28, d_loss: 0.46208012104034424, g_loss: 4.681329250335693\n","Epoch 58/2000, Step 29, d_loss: 0.42929479479789734, g_loss: 3.7325711250305176\n","Epoch 58/2000, Step 30, d_loss: 0.44576340913772583, g_loss: 3.9199674129486084\n","Epoch 58/2000, Step 31, d_loss: 0.4081348478794098, g_loss: 3.0350944995880127\n","Epoch 58/2000, Step 32, d_loss: 0.36788105964660645, g_loss: 3.572702646255493\n","Epoch 58/2000, Step 33, d_loss: 0.3728112280368805, g_loss: 5.026491165161133\n","Epoch 58/2000, Step 34, d_loss: 0.4204052686691284, g_loss: 4.202662944793701\n","Epoch 58/2000, Step 35, d_loss: 0.44779568910598755, g_loss: 3.300201177597046\n","Epoch 58/2000, Step 36, d_loss: 0.4332945942878723, g_loss: 3.4557971954345703\n","Epoch 58/2000, Step 37, d_loss: 0.3999224007129669, g_loss: 4.368078231811523\n","Epoch 58/2000, Step 38, d_loss: 0.37024223804473877, g_loss: 4.555247783660889\n","Epoch 58/2000, Step 39, d_loss: 0.36536386609077454, g_loss: 3.9124908447265625\n","Epoch 58/2000, Step 40, d_loss: 0.42560529708862305, g_loss: 4.08871603012085\n","Epoch 58/2000, Step 41, d_loss: 0.5922135710716248, g_loss: 4.102304458618164\n","Epoch 58/2000, Step 42, d_loss: 0.38078442215919495, g_loss: 2.7619850635528564\n","Epoch 58/2000, Step 43, d_loss: 0.4441627264022827, g_loss: 2.8092422485351562\n","Epoch 58/2000, Step 44, d_loss: 0.4125288426876068, g_loss: 2.695110321044922\n","Epoch 58/2000, Step 45, d_loss: 0.4178576171398163, g_loss: 2.436607837677002\n","Epoch 58/2000, Step 46, d_loss: 0.40533050894737244, g_loss: 3.8358359336853027\n","Epoch 58/2000, Step 47, d_loss: 0.4127482771873474, g_loss: 4.352899074554443\n","Epoch 58/2000, Step 48, d_loss: 0.40909263491630554, g_loss: 3.6282765865325928\n","Epoch 58/2000, Step 49, d_loss: 0.3706863522529602, g_loss: 3.517949104309082\n","Epoch 58/2000, Step 50, d_loss: 0.5161110162734985, g_loss: 4.027167320251465\n","Epoch 58/2000, Step 51, d_loss: 0.3917844593524933, g_loss: 4.204765319824219\n","Epoch 58/2000, Step 52, d_loss: 0.4187967777252197, g_loss: 3.7818970680236816\n","Epoch 58/2000, Step 53, d_loss: 0.3763278126716614, g_loss: 4.758732318878174\n","Epoch 58/2000, Step 54, d_loss: 0.3935607969760895, g_loss: 4.198051929473877\n","Epoch 58/2000, Step 55, d_loss: 0.4026511013507843, g_loss: 4.052634239196777\n","Epoch 58/2000, Step 56, d_loss: 0.46837130188941956, g_loss: 4.756130695343018\n","Epoch 58/2000, Step 57, d_loss: 0.3891080319881439, g_loss: 3.8268308639526367\n","Epoch 58/2000, Step 58, d_loss: 0.37682777643203735, g_loss: 2.9239702224731445\n","Epoch 58/2000, Step 59, d_loss: 0.4308320879936218, g_loss: 3.076460599899292\n","Epoch 58/2000, Step 60, d_loss: 0.440352201461792, g_loss: 5.448751926422119\n","Epoch 58/2000, Step 61, d_loss: 0.4445485770702362, g_loss: 2.278214454650879\n","Epoch 58/2000, Step 62, d_loss: 0.42029720544815063, g_loss: 3.00022029876709\n","Epoch 58/2000, Step 63, d_loss: 0.3993097245693207, g_loss: 4.879892826080322\n","Epoch 58/2000, Step 64, d_loss: 0.4406513273715973, g_loss: 3.6799540519714355\n","Epoch 58/2000, Step 65, d_loss: 0.38837721943855286, g_loss: 4.248082637786865\n","Epoch 58/2000, Step 66, d_loss: 0.37877997756004333, g_loss: 4.790539741516113\n","Epoch 58/2000, Step 67, d_loss: 0.3818027973175049, g_loss: 4.820533752441406\n","Epoch 58/2000, Step 68, d_loss: 0.402952641248703, g_loss: 4.684687614440918\n","Epoch 58/2000, Step 69, d_loss: 0.39802223443984985, g_loss: 4.194651126861572\n","Epoch 58/2000, Step 70, d_loss: 0.3708096742630005, g_loss: 2.706678628921509\n","Epoch 58/2000, Step 71, d_loss: 0.43415528535842896, g_loss: 3.2504286766052246\n","Epoch 58/2000, Step 72, d_loss: 0.389775812625885, g_loss: 3.026679754257202\n","Epoch 58/2000, Step 73, d_loss: 0.3985058069229126, g_loss: 4.162424087524414\n","Epoch 58/2000, Step 74, d_loss: 0.3597947955131531, g_loss: 3.8550477027893066\n","Epoch 58/2000, Step 75, d_loss: 0.35919642448425293, g_loss: 4.17760705947876\n","Epoch 58/2000, Step 76, d_loss: 0.3824425935745239, g_loss: 4.747196197509766\n","Epoch 58/2000, Step 77, d_loss: 0.34705090522766113, g_loss: 3.5357561111450195\n","Epoch 58/2000, Step 78, d_loss: 0.37111562490463257, g_loss: 4.0958099365234375\n","Epoch 58/2000, Step 79, d_loss: 0.3638373613357544, g_loss: 5.561428070068359\n","Epoch 58/2000, Step 80, d_loss: 0.36881646513938904, g_loss: 3.5524001121520996\n","Epoch 58/2000, Step 81, d_loss: 0.3684098720550537, g_loss: 3.615978956222534\n","Epoch 58/2000, Step 82, d_loss: 0.3799363672733307, g_loss: 3.9610378742218018\n","Epoch 58/2000, Step 83, d_loss: 0.388583242893219, g_loss: 4.230458736419678\n","Epoch 58/2000, Step 84, d_loss: 0.3695037066936493, g_loss: 4.0846710205078125\n","Epoch 58/2000, Step 85, d_loss: 0.359659880399704, g_loss: 5.86636209487915\n","Epoch 58/2000, Step 86, d_loss: 0.3652585446834564, g_loss: 4.330808162689209\n","Epoch 58/2000, Step 87, d_loss: 0.4347418546676636, g_loss: 5.190943241119385\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 59/2000, Step 1, d_loss: 0.43759554624557495, g_loss: 3.707242965698242\n","Epoch 59/2000, Step 2, d_loss: 0.4085090756416321, g_loss: 3.319164514541626\n","Epoch 59/2000, Step 3, d_loss: 0.3595907688140869, g_loss: 3.5712337493896484\n","Epoch 59/2000, Step 4, d_loss: 0.41668909788131714, g_loss: 3.9986913204193115\n","Epoch 59/2000, Step 5, d_loss: 0.4492662847042084, g_loss: 3.8201141357421875\n","Epoch 59/2000, Step 6, d_loss: 0.39446595311164856, g_loss: 4.5965495109558105\n","Epoch 59/2000, Step 7, d_loss: 0.43579378724098206, g_loss: 3.935469150543213\n","Epoch 59/2000, Step 8, d_loss: 0.3962910771369934, g_loss: 3.7689995765686035\n","Epoch 59/2000, Step 9, d_loss: 0.46922916173934937, g_loss: 3.9795150756835938\n","Epoch 59/2000, Step 10, d_loss: 0.4074276089668274, g_loss: 3.8411412239074707\n","Epoch 59/2000, Step 11, d_loss: 0.4437691867351532, g_loss: 4.07873010635376\n","Epoch 59/2000, Step 12, d_loss: 0.4114763140678406, g_loss: 4.082205295562744\n","Epoch 59/2000, Step 13, d_loss: 0.4293521046638489, g_loss: 2.5393569469451904\n","Epoch 59/2000, Step 14, d_loss: 0.4011581242084503, g_loss: 4.006645202636719\n","Epoch 59/2000, Step 15, d_loss: 0.4428791105747223, g_loss: 3.0702993869781494\n","Epoch 59/2000, Step 16, d_loss: 0.418639600276947, g_loss: 3.4811174869537354\n","Epoch 59/2000, Step 17, d_loss: 0.4214775562286377, g_loss: 2.8681728839874268\n","Epoch 59/2000, Step 18, d_loss: 0.35684734582901, g_loss: 3.673457145690918\n","Epoch 59/2000, Step 19, d_loss: 0.3994532823562622, g_loss: 3.174128532409668\n","Epoch 59/2000, Step 20, d_loss: 0.4180854558944702, g_loss: 3.701910972595215\n","Epoch 59/2000, Step 21, d_loss: 0.38619476556777954, g_loss: 5.00535774230957\n","Epoch 59/2000, Step 22, d_loss: 0.47885850071907043, g_loss: 3.777862787246704\n","Epoch 59/2000, Step 23, d_loss: 0.4134279787540436, g_loss: 4.2298054695129395\n","Epoch 59/2000, Step 24, d_loss: 0.35833075642585754, g_loss: 4.438808441162109\n","Epoch 59/2000, Step 25, d_loss: 0.36942222714424133, g_loss: 4.285096645355225\n","Epoch 59/2000, Step 26, d_loss: 0.4095005691051483, g_loss: 5.400877475738525\n","Epoch 59/2000, Step 27, d_loss: 0.42527416348457336, g_loss: 4.998864650726318\n","Epoch 59/2000, Step 28, d_loss: 0.3633873760700226, g_loss: 4.594208717346191\n","Epoch 59/2000, Step 29, d_loss: 0.3894185721874237, g_loss: 4.343904972076416\n","Epoch 59/2000, Step 30, d_loss: 0.38209912180900574, g_loss: 5.362086772918701\n","Epoch 59/2000, Step 31, d_loss: 0.36381688714027405, g_loss: 3.604583263397217\n","Epoch 59/2000, Step 32, d_loss: 0.37106597423553467, g_loss: 3.7263741493225098\n","Epoch 59/2000, Step 33, d_loss: 0.389385849237442, g_loss: 3.936401605606079\n","Epoch 59/2000, Step 34, d_loss: 0.39566943049430847, g_loss: 4.950158596038818\n","Epoch 59/2000, Step 35, d_loss: 0.3668251633644104, g_loss: 4.392890930175781\n","Epoch 59/2000, Step 36, d_loss: 0.35636839270591736, g_loss: 4.225699424743652\n","Epoch 59/2000, Step 37, d_loss: 0.35733896493911743, g_loss: 4.216541767120361\n","Epoch 59/2000, Step 38, d_loss: 0.4110513925552368, g_loss: 3.967646837234497\n","Epoch 59/2000, Step 39, d_loss: 0.3563581705093384, g_loss: 3.5991294384002686\n","Epoch 59/2000, Step 40, d_loss: 0.3695372939109802, g_loss: 4.468822956085205\n","Epoch 59/2000, Step 41, d_loss: 0.3517740070819855, g_loss: 3.5910773277282715\n","Epoch 59/2000, Step 42, d_loss: 0.3778635859489441, g_loss: 4.726744651794434\n","Epoch 59/2000, Step 43, d_loss: 0.3610036075115204, g_loss: 3.3776111602783203\n","Epoch 59/2000, Step 44, d_loss: 0.3881046772003174, g_loss: 3.8954005241394043\n","Epoch 59/2000, Step 45, d_loss: 0.40544435381889343, g_loss: 3.7199771404266357\n","Epoch 59/2000, Step 46, d_loss: 0.36993736028671265, g_loss: 3.8233916759490967\n","Epoch 59/2000, Step 47, d_loss: 0.39391395449638367, g_loss: 3.7435147762298584\n","Epoch 59/2000, Step 48, d_loss: 0.4149593114852905, g_loss: 3.6926426887512207\n","Epoch 59/2000, Step 49, d_loss: 0.36818215250968933, g_loss: 3.829446792602539\n","Epoch 59/2000, Step 50, d_loss: 0.36822518706321716, g_loss: 4.1142897605896\n","Epoch 59/2000, Step 51, d_loss: 0.3888222873210907, g_loss: 3.8687093257904053\n","Epoch 59/2000, Step 52, d_loss: 0.39423102140426636, g_loss: 4.0528340339660645\n","Epoch 59/2000, Step 53, d_loss: 0.4152233898639679, g_loss: 3.7400546073913574\n","Epoch 59/2000, Step 54, d_loss: 0.35548636317253113, g_loss: 4.3336076736450195\n","Epoch 59/2000, Step 55, d_loss: 0.47176697850227356, g_loss: 5.853409290313721\n","Epoch 59/2000, Step 56, d_loss: 0.3751281499862671, g_loss: 4.3184614181518555\n","Epoch 59/2000, Step 57, d_loss: 0.35739049315452576, g_loss: 4.175710201263428\n","Epoch 59/2000, Step 58, d_loss: 0.36233723163604736, g_loss: 4.250357151031494\n","Epoch 59/2000, Step 59, d_loss: 0.36098989844322205, g_loss: 4.033970832824707\n","Epoch 59/2000, Step 60, d_loss: 0.3617120087146759, g_loss: 4.080944538116455\n","Epoch 59/2000, Step 61, d_loss: 0.391044557094574, g_loss: 3.7068915367126465\n","Epoch 59/2000, Step 62, d_loss: 0.38568586111068726, g_loss: 3.7024803161621094\n","Epoch 59/2000, Step 63, d_loss: 0.3597174882888794, g_loss: 3.831423044204712\n","Epoch 59/2000, Step 64, d_loss: 0.3601350486278534, g_loss: 4.104542255401611\n","Epoch 59/2000, Step 65, d_loss: 0.369702011346817, g_loss: 3.7931857109069824\n","Epoch 59/2000, Step 66, d_loss: 0.4267846345901489, g_loss: 4.388057708740234\n","Epoch 59/2000, Step 67, d_loss: 0.38811570405960083, g_loss: 3.5898633003234863\n","Epoch 59/2000, Step 68, d_loss: 0.3624362349510193, g_loss: 4.802937030792236\n","Epoch 59/2000, Step 69, d_loss: 0.39898139238357544, g_loss: 3.6890156269073486\n","Epoch 59/2000, Step 70, d_loss: 0.4197079539299011, g_loss: 4.30373477935791\n","Epoch 59/2000, Step 71, d_loss: 0.3575989305973053, g_loss: 4.113277435302734\n","Epoch 59/2000, Step 72, d_loss: 0.40229547023773193, g_loss: 4.263177871704102\n","Epoch 59/2000, Step 73, d_loss: 0.4309766888618469, g_loss: 5.5990095138549805\n","Epoch 59/2000, Step 74, d_loss: 0.3572121858596802, g_loss: 5.606007099151611\n","Epoch 59/2000, Step 75, d_loss: 0.3579561412334442, g_loss: 4.014278411865234\n","Epoch 59/2000, Step 76, d_loss: 0.3939851224422455, g_loss: 3.984147071838379\n","Epoch 59/2000, Step 77, d_loss: 0.4359714686870575, g_loss: 3.375653028488159\n","Epoch 59/2000, Step 78, d_loss: 0.3838384747505188, g_loss: 3.47702956199646\n","Epoch 59/2000, Step 79, d_loss: 0.4226459860801697, g_loss: 3.8122637271881104\n","Epoch 59/2000, Step 80, d_loss: 0.406830757856369, g_loss: 2.561208724975586\n","Epoch 59/2000, Step 81, d_loss: 0.40171074867248535, g_loss: 3.7686216831207275\n","Epoch 59/2000, Step 82, d_loss: 0.3707783818244934, g_loss: 3.14892315864563\n","Epoch 59/2000, Step 83, d_loss: 0.37399008870124817, g_loss: 3.7717511653900146\n","Epoch 59/2000, Step 84, d_loss: 0.3687489628791809, g_loss: 4.3844194412231445\n","Epoch 59/2000, Step 85, d_loss: 0.36727890372276306, g_loss: 4.780545234680176\n","Epoch 59/2000, Step 86, d_loss: 0.39334964752197266, g_loss: 4.739819049835205\n","Epoch 59/2000, Step 87, d_loss: 0.35165008902549744, g_loss: 4.918771743774414\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 60/2000, Step 1, d_loss: 0.35119056701660156, g_loss: 4.6504807472229\n","Epoch 60/2000, Step 2, d_loss: 0.4098071753978729, g_loss: 4.774444103240967\n","Epoch 60/2000, Step 3, d_loss: 0.4156011641025543, g_loss: 4.727188587188721\n","Epoch 60/2000, Step 4, d_loss: 0.38014695048332214, g_loss: 3.528561592102051\n","Epoch 60/2000, Step 5, d_loss: 0.43746310472488403, g_loss: 3.155041456222534\n","Epoch 60/2000, Step 6, d_loss: 0.38900789618492126, g_loss: 3.2357852458953857\n","Epoch 60/2000, Step 7, d_loss: 0.4207152724266052, g_loss: 2.504607915878296\n","Epoch 60/2000, Step 8, d_loss: 0.4641455113887787, g_loss: 3.5128695964813232\n","Epoch 60/2000, Step 9, d_loss: 0.43898308277130127, g_loss: 3.511556625366211\n","Epoch 60/2000, Step 10, d_loss: 0.36578893661499023, g_loss: 3.468111991882324\n","Epoch 60/2000, Step 11, d_loss: 0.4119704067707062, g_loss: 3.471029043197632\n","Epoch 60/2000, Step 12, d_loss: 0.37904879450798035, g_loss: 4.567965030670166\n","Epoch 60/2000, Step 13, d_loss: 0.43659061193466187, g_loss: 4.554398059844971\n","Epoch 60/2000, Step 14, d_loss: 0.49660080671310425, g_loss: 4.764477252960205\n","Epoch 60/2000, Step 15, d_loss: 0.3748670816421509, g_loss: 4.7878241539001465\n","Epoch 60/2000, Step 16, d_loss: 0.38603731989860535, g_loss: 4.365700721740723\n","Epoch 60/2000, Step 17, d_loss: 0.40679019689559937, g_loss: 3.593925952911377\n","Epoch 60/2000, Step 18, d_loss: 0.4124739170074463, g_loss: 2.901034355163574\n","Epoch 60/2000, Step 19, d_loss: 0.38595670461654663, g_loss: 3.532074213027954\n","Epoch 60/2000, Step 20, d_loss: 0.373462051153183, g_loss: 3.7581229209899902\n","Epoch 60/2000, Step 21, d_loss: 0.4008280634880066, g_loss: 5.194904804229736\n","Epoch 60/2000, Step 22, d_loss: 0.4085042476654053, g_loss: 4.591437339782715\n","Epoch 60/2000, Step 23, d_loss: 0.3865409195423126, g_loss: 3.760382652282715\n","Epoch 60/2000, Step 24, d_loss: 0.40060198307037354, g_loss: 3.2536911964416504\n","Epoch 60/2000, Step 25, d_loss: 0.402815043926239, g_loss: 3.1526389122009277\n","Epoch 60/2000, Step 26, d_loss: 0.3948575258255005, g_loss: 3.2081868648529053\n","Epoch 60/2000, Step 27, d_loss: 0.3936265707015991, g_loss: 3.4450480937957764\n","Epoch 60/2000, Step 28, d_loss: 0.39902570843696594, g_loss: 3.5820653438568115\n","Epoch 60/2000, Step 29, d_loss: 0.41921529173851013, g_loss: 3.595731735229492\n","Epoch 60/2000, Step 30, d_loss: 0.4121105670928955, g_loss: 3.682185411453247\n","Epoch 60/2000, Step 31, d_loss: 0.4250164330005646, g_loss: 3.8328750133514404\n","Epoch 60/2000, Step 32, d_loss: 0.37977170944213867, g_loss: 4.778883934020996\n","Epoch 60/2000, Step 33, d_loss: 0.387917697429657, g_loss: 5.626196384429932\n","Epoch 60/2000, Step 34, d_loss: 0.43171730637550354, g_loss: 4.4605913162231445\n","Epoch 60/2000, Step 35, d_loss: 0.4440920352935791, g_loss: 4.93220853805542\n","Epoch 60/2000, Step 36, d_loss: 0.35399073362350464, g_loss: 3.946659803390503\n","Epoch 60/2000, Step 37, d_loss: 0.423630952835083, g_loss: 3.9798529148101807\n","Epoch 60/2000, Step 38, d_loss: 0.395373672246933, g_loss: 3.129884719848633\n","Epoch 60/2000, Step 39, d_loss: 0.37701088190078735, g_loss: 3.399580955505371\n","Epoch 60/2000, Step 40, d_loss: 0.3712528645992279, g_loss: 3.285128116607666\n","Epoch 60/2000, Step 41, d_loss: 0.4041149616241455, g_loss: 4.304945945739746\n","Epoch 60/2000, Step 42, d_loss: 0.3560106158256531, g_loss: 4.184600353240967\n","Epoch 60/2000, Step 43, d_loss: 0.3908812701702118, g_loss: 3.7734458446502686\n","Epoch 60/2000, Step 44, d_loss: 0.3736382722854614, g_loss: 4.146075248718262\n","Epoch 60/2000, Step 45, d_loss: 0.37632569670677185, g_loss: 3.7895987033843994\n","Epoch 60/2000, Step 46, d_loss: 0.3768457770347595, g_loss: 3.847877264022827\n","Epoch 60/2000, Step 47, d_loss: 0.3915623128414154, g_loss: 3.9550423622131348\n","Epoch 60/2000, Step 48, d_loss: 0.36183249950408936, g_loss: 4.558570861816406\n","Epoch 60/2000, Step 49, d_loss: 0.36312341690063477, g_loss: 4.021261215209961\n","Epoch 60/2000, Step 50, d_loss: 0.35235193371772766, g_loss: 3.807371139526367\n","Epoch 60/2000, Step 51, d_loss: 0.38603270053863525, g_loss: 3.7484261989593506\n","Epoch 60/2000, Step 52, d_loss: 0.3623141944408417, g_loss: 4.757500171661377\n","Epoch 60/2000, Step 53, d_loss: 0.47032833099365234, g_loss: 4.094526767730713\n","Epoch 60/2000, Step 54, d_loss: 0.36461228132247925, g_loss: 4.578275680541992\n","Epoch 60/2000, Step 55, d_loss: 0.3945085108280182, g_loss: 3.8862650394439697\n","Epoch 60/2000, Step 56, d_loss: 0.38186484575271606, g_loss: 4.19337272644043\n","Epoch 60/2000, Step 57, d_loss: 0.36830800771713257, g_loss: 4.330101013183594\n","Epoch 60/2000, Step 58, d_loss: 0.3700416088104248, g_loss: 4.716893672943115\n","Epoch 60/2000, Step 59, d_loss: 0.3772861659526825, g_loss: 4.678715705871582\n","Epoch 60/2000, Step 60, d_loss: 0.37476015090942383, g_loss: 5.058172225952148\n","Epoch 60/2000, Step 61, d_loss: 0.3829425871372223, g_loss: 3.9707112312316895\n","Epoch 60/2000, Step 62, d_loss: 0.3876010477542877, g_loss: 4.514564514160156\n","Epoch 60/2000, Step 63, d_loss: 0.4543142020702362, g_loss: 5.084338188171387\n","Epoch 60/2000, Step 64, d_loss: 0.35797473788261414, g_loss: 3.55698299407959\n","Epoch 60/2000, Step 65, d_loss: 0.3919544517993927, g_loss: 3.8821301460266113\n","Epoch 60/2000, Step 66, d_loss: 0.4679165184497833, g_loss: 3.4582717418670654\n","Epoch 60/2000, Step 67, d_loss: 0.4158272445201874, g_loss: 3.043428897857666\n","Epoch 60/2000, Step 68, d_loss: 0.42348557710647583, g_loss: 4.225169658660889\n","Epoch 60/2000, Step 69, d_loss: 0.3837848901748657, g_loss: 3.2006218433380127\n","Epoch 60/2000, Step 70, d_loss: 0.4090939164161682, g_loss: 3.8097035884857178\n","Epoch 60/2000, Step 71, d_loss: 0.3892853856086731, g_loss: 4.074585914611816\n","Epoch 60/2000, Step 72, d_loss: 0.3581421673297882, g_loss: 4.189998626708984\n","Epoch 60/2000, Step 73, d_loss: 0.4274638891220093, g_loss: 4.629701137542725\n","Epoch 60/2000, Step 74, d_loss: 0.35354840755462646, g_loss: 4.003048419952393\n","Epoch 60/2000, Step 75, d_loss: 0.38537973165512085, g_loss: 4.004220962524414\n","Epoch 60/2000, Step 76, d_loss: 0.37995797395706177, g_loss: 4.2719292640686035\n","Epoch 60/2000, Step 77, d_loss: 0.39218762516975403, g_loss: 4.12313175201416\n","Epoch 60/2000, Step 78, d_loss: 0.3661351203918457, g_loss: 3.806558847427368\n","Epoch 60/2000, Step 79, d_loss: 0.38326552510261536, g_loss: 4.985198974609375\n","Epoch 60/2000, Step 80, d_loss: 0.3753601014614105, g_loss: 4.9811530113220215\n","Epoch 60/2000, Step 81, d_loss: 0.3751867115497589, g_loss: 3.7464754581451416\n","Epoch 60/2000, Step 82, d_loss: 0.3681226968765259, g_loss: 3.667353630065918\n","Epoch 60/2000, Step 83, d_loss: 0.37304142117500305, g_loss: 4.564328193664551\n","Epoch 60/2000, Step 84, d_loss: 0.3579346239566803, g_loss: 4.156307697296143\n","Epoch 60/2000, Step 85, d_loss: 0.38334333896636963, g_loss: 4.47464656829834\n","Epoch 60/2000, Step 86, d_loss: 0.3758828341960907, g_loss: 4.073633670806885\n","Epoch 60/2000, Step 87, d_loss: 0.40236228704452515, g_loss: 3.2037932872772217\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 61/2000, Step 1, d_loss: 0.3864865005016327, g_loss: 2.8418993949890137\n","Epoch 61/2000, Step 2, d_loss: 0.4401744604110718, g_loss: 3.865691900253296\n","Epoch 61/2000, Step 3, d_loss: 0.44441163539886475, g_loss: 2.912336587905884\n","Epoch 61/2000, Step 4, d_loss: 0.38402169942855835, g_loss: 4.422644138336182\n","Epoch 61/2000, Step 5, d_loss: 0.39714857935905457, g_loss: 3.529292106628418\n","Epoch 61/2000, Step 6, d_loss: 0.41265803575515747, g_loss: 4.050314903259277\n","Epoch 61/2000, Step 7, d_loss: 0.3607635796070099, g_loss: 4.603222370147705\n","Epoch 61/2000, Step 8, d_loss: 0.3629886507987976, g_loss: 5.491800308227539\n","Epoch 61/2000, Step 9, d_loss: 0.35953763127326965, g_loss: 4.925413131713867\n","Epoch 61/2000, Step 10, d_loss: 0.38406190276145935, g_loss: 5.043630599975586\n","Epoch 61/2000, Step 11, d_loss: 0.3885059058666229, g_loss: 4.78460693359375\n","Epoch 61/2000, Step 12, d_loss: 0.4029642343521118, g_loss: 4.299527168273926\n","Epoch 61/2000, Step 13, d_loss: 0.3820877969264984, g_loss: 4.195014953613281\n","Epoch 61/2000, Step 14, d_loss: 0.3847532272338867, g_loss: 4.728693962097168\n","Epoch 61/2000, Step 15, d_loss: 0.388800710439682, g_loss: 3.2738547325134277\n","Epoch 61/2000, Step 16, d_loss: 0.3587479293346405, g_loss: 3.656071901321411\n","Epoch 61/2000, Step 17, d_loss: 0.44373762607574463, g_loss: 3.705014228820801\n","Epoch 61/2000, Step 18, d_loss: 0.41102635860443115, g_loss: 4.173862457275391\n","Epoch 61/2000, Step 19, d_loss: 0.40364646911621094, g_loss: 3.5110208988189697\n","Epoch 61/2000, Step 20, d_loss: 0.395364373922348, g_loss: 4.561651229858398\n","Epoch 61/2000, Step 21, d_loss: 0.42331528663635254, g_loss: 3.725454330444336\n","Epoch 61/2000, Step 22, d_loss: 0.39094969630241394, g_loss: 3.919609785079956\n","Epoch 61/2000, Step 23, d_loss: 0.3764566481113434, g_loss: 4.416486740112305\n","Epoch 61/2000, Step 24, d_loss: 0.3470967411994934, g_loss: 4.751648902893066\n","Epoch 61/2000, Step 25, d_loss: 0.4157819151878357, g_loss: 4.214343070983887\n","Epoch 61/2000, Step 26, d_loss: 0.3820706009864807, g_loss: 4.505722999572754\n","Epoch 61/2000, Step 27, d_loss: 0.40640392899513245, g_loss: 4.549012660980225\n","Epoch 61/2000, Step 28, d_loss: 0.3717723786830902, g_loss: 3.690180540084839\n","Epoch 61/2000, Step 29, d_loss: 0.3526915907859802, g_loss: 4.494784355163574\n","Epoch 61/2000, Step 30, d_loss: 0.3742499053478241, g_loss: 4.134705543518066\n","Epoch 61/2000, Step 31, d_loss: 0.35519006848335266, g_loss: 4.947354316711426\n","Epoch 61/2000, Step 32, d_loss: 0.38895556330680847, g_loss: 3.9364376068115234\n","Epoch 61/2000, Step 33, d_loss: 0.37784454226493835, g_loss: 3.484203338623047\n","Epoch 61/2000, Step 34, d_loss: 0.3495776653289795, g_loss: 3.631953716278076\n","Epoch 61/2000, Step 35, d_loss: 0.38909974694252014, g_loss: 3.347536563873291\n","Epoch 61/2000, Step 36, d_loss: 0.41191229224205017, g_loss: 3.604116678237915\n","Epoch 61/2000, Step 37, d_loss: 0.46365654468536377, g_loss: 3.9205830097198486\n","Epoch 61/2000, Step 38, d_loss: 0.3957524299621582, g_loss: 4.443350315093994\n","Epoch 61/2000, Step 39, d_loss: 0.4138997197151184, g_loss: 5.191466808319092\n","Epoch 61/2000, Step 40, d_loss: 0.3590483069419861, g_loss: 5.0320281982421875\n","Epoch 61/2000, Step 41, d_loss: 0.4285266399383545, g_loss: 4.9001288414001465\n","Epoch 61/2000, Step 42, d_loss: 0.4544503390789032, g_loss: 4.841206073760986\n","Epoch 61/2000, Step 43, d_loss: 0.37156936526298523, g_loss: 3.860905408859253\n","Epoch 61/2000, Step 44, d_loss: 0.38980695605278015, g_loss: 3.8199350833892822\n","Epoch 61/2000, Step 45, d_loss: 0.36503472924232483, g_loss: 3.7521615028381348\n","Epoch 61/2000, Step 46, d_loss: 0.40188780426979065, g_loss: 3.1494994163513184\n","Epoch 61/2000, Step 47, d_loss: 0.42220497131347656, g_loss: 4.168941974639893\n","Epoch 61/2000, Step 48, d_loss: 0.37486132979393005, g_loss: 4.893370151519775\n","Epoch 61/2000, Step 49, d_loss: 0.36050620675086975, g_loss: 4.8010993003845215\n","Epoch 61/2000, Step 50, d_loss: 0.3699014186859131, g_loss: 5.090068817138672\n","Epoch 61/2000, Step 51, d_loss: 0.39663711190223694, g_loss: 4.9052815437316895\n","Epoch 61/2000, Step 52, d_loss: 0.4934694468975067, g_loss: 4.798910140991211\n","Epoch 61/2000, Step 53, d_loss: 0.35947301983833313, g_loss: 4.326638221740723\n","Epoch 61/2000, Step 54, d_loss: 0.39019471406936646, g_loss: 3.7763259410858154\n","Epoch 61/2000, Step 55, d_loss: 0.40830475091934204, g_loss: 2.904191493988037\n","Epoch 61/2000, Step 56, d_loss: 0.5003588795661926, g_loss: 3.4940319061279297\n","Epoch 61/2000, Step 57, d_loss: 0.4055159389972687, g_loss: 3.032977342605591\n","Epoch 61/2000, Step 58, d_loss: 0.3815475404262543, g_loss: 5.491218090057373\n","Epoch 61/2000, Step 59, d_loss: 0.38807255029678345, g_loss: 4.248281478881836\n","Epoch 61/2000, Step 60, d_loss: 0.37485992908477783, g_loss: 5.0269269943237305\n","Epoch 61/2000, Step 61, d_loss: 0.36355099081993103, g_loss: 4.719837665557861\n","Epoch 61/2000, Step 62, d_loss: 0.5177500247955322, g_loss: 4.3238139152526855\n","Epoch 61/2000, Step 63, d_loss: 0.3732958137989044, g_loss: 5.340611934661865\n","Epoch 61/2000, Step 64, d_loss: 0.459063857793808, g_loss: 4.025937080383301\n","Epoch 61/2000, Step 65, d_loss: 0.402567982673645, g_loss: 3.9587881565093994\n","Epoch 61/2000, Step 66, d_loss: 0.43471530079841614, g_loss: 3.1338374614715576\n","Epoch 61/2000, Step 67, d_loss: 0.382150262594223, g_loss: 3.165963888168335\n","Epoch 61/2000, Step 68, d_loss: 0.4496643841266632, g_loss: 2.8867340087890625\n","Epoch 61/2000, Step 69, d_loss: 0.42437195777893066, g_loss: 3.284513473510742\n","Epoch 61/2000, Step 70, d_loss: 0.47213834524154663, g_loss: 3.4815807342529297\n","Epoch 61/2000, Step 71, d_loss: 0.4791037142276764, g_loss: 4.122247695922852\n","Epoch 61/2000, Step 72, d_loss: 0.43712443113327026, g_loss: 4.750828266143799\n","Epoch 61/2000, Step 73, d_loss: 0.36589837074279785, g_loss: 4.412671089172363\n","Epoch 61/2000, Step 74, d_loss: 0.4090876281261444, g_loss: 5.238203525543213\n","Epoch 61/2000, Step 75, d_loss: 0.43715304136276245, g_loss: 4.549503326416016\n","Epoch 61/2000, Step 76, d_loss: 0.3923293650150299, g_loss: 4.25687313079834\n","Epoch 61/2000, Step 77, d_loss: 0.36650916934013367, g_loss: 3.160456895828247\n","Epoch 61/2000, Step 78, d_loss: 0.43004751205444336, g_loss: 3.7465131282806396\n","Epoch 61/2000, Step 79, d_loss: 0.3723844885826111, g_loss: 3.395242929458618\n","Epoch 61/2000, Step 80, d_loss: 0.4311383366584778, g_loss: 3.5883748531341553\n","Epoch 61/2000, Step 81, d_loss: 0.36827751994132996, g_loss: 3.8336567878723145\n","Epoch 61/2000, Step 82, d_loss: 0.37653347849845886, g_loss: 4.66695499420166\n","Epoch 61/2000, Step 83, d_loss: 0.3922024667263031, g_loss: 4.826559543609619\n","Epoch 61/2000, Step 84, d_loss: 0.3871740698814392, g_loss: 3.469744920730591\n","Epoch 61/2000, Step 85, d_loss: 0.371660441160202, g_loss: 3.8805150985717773\n","Epoch 61/2000, Step 86, d_loss: 0.3624776303768158, g_loss: 3.6580662727355957\n","Epoch 61/2000, Step 87, d_loss: 0.3859325647354126, g_loss: 3.2917540073394775\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 62/2000, Step 1, d_loss: 0.38728567957878113, g_loss: 3.4595887660980225\n","Epoch 62/2000, Step 2, d_loss: 0.3766854405403137, g_loss: 3.5056934356689453\n","Epoch 62/2000, Step 3, d_loss: 0.417097270488739, g_loss: 4.068789482116699\n","Epoch 62/2000, Step 4, d_loss: 0.3580242693424225, g_loss: 4.517518520355225\n","Epoch 62/2000, Step 5, d_loss: 0.3858492374420166, g_loss: 4.239376068115234\n","Epoch 62/2000, Step 6, d_loss: 0.41031181812286377, g_loss: 3.5265564918518066\n","Epoch 62/2000, Step 7, d_loss: 0.37892386317253113, g_loss: 4.067244052886963\n","Epoch 62/2000, Step 8, d_loss: 0.3459863066673279, g_loss: 4.15769624710083\n","Epoch 62/2000, Step 9, d_loss: 0.4648200571537018, g_loss: 5.062582492828369\n","Epoch 62/2000, Step 10, d_loss: 0.4859095811843872, g_loss: 4.6273322105407715\n","Epoch 62/2000, Step 11, d_loss: 0.40583640336990356, g_loss: 3.7244131565093994\n","Epoch 62/2000, Step 12, d_loss: 0.4581960439682007, g_loss: 2.704821825027466\n","Epoch 62/2000, Step 13, d_loss: 0.4603947103023529, g_loss: 3.17877197265625\n","Epoch 62/2000, Step 14, d_loss: 0.42993372678756714, g_loss: 3.8415071964263916\n","Epoch 62/2000, Step 15, d_loss: 0.37189409136772156, g_loss: 3.960829496383667\n","Epoch 62/2000, Step 16, d_loss: 0.4061046838760376, g_loss: 4.0394439697265625\n","Epoch 62/2000, Step 17, d_loss: 0.4373481869697571, g_loss: 4.752457141876221\n","Epoch 62/2000, Step 18, d_loss: 0.3658334016799927, g_loss: 4.179397106170654\n","Epoch 62/2000, Step 19, d_loss: 0.36002013087272644, g_loss: 5.728470325469971\n","Epoch 62/2000, Step 20, d_loss: 0.41799411177635193, g_loss: 4.6614179611206055\n","Epoch 62/2000, Step 21, d_loss: 0.4005584716796875, g_loss: 3.569502353668213\n","Epoch 62/2000, Step 22, d_loss: 0.4167003333568573, g_loss: 4.451015472412109\n","Epoch 62/2000, Step 23, d_loss: 0.4110513925552368, g_loss: 4.826275825500488\n","Epoch 62/2000, Step 24, d_loss: 0.3624194860458374, g_loss: 4.46360969543457\n","Epoch 62/2000, Step 25, d_loss: 0.3767024278640747, g_loss: 3.7328643798828125\n","Epoch 62/2000, Step 26, d_loss: 0.37542712688446045, g_loss: 4.253578186035156\n","Epoch 62/2000, Step 27, d_loss: 0.379657506942749, g_loss: 4.052831172943115\n","Epoch 62/2000, Step 28, d_loss: 0.4011770486831665, g_loss: 4.900745868682861\n","Epoch 62/2000, Step 29, d_loss: 0.3720123767852783, g_loss: 5.056443214416504\n","Epoch 62/2000, Step 30, d_loss: 0.3669721186161041, g_loss: 3.4046473503112793\n","Epoch 62/2000, Step 31, d_loss: 0.35818466544151306, g_loss: 4.852307319641113\n","Epoch 62/2000, Step 32, d_loss: 0.3689038157463074, g_loss: 3.8381340503692627\n","Epoch 62/2000, Step 33, d_loss: 0.3873670995235443, g_loss: 3.6061415672302246\n","Epoch 62/2000, Step 34, d_loss: 0.37028685212135315, g_loss: 4.721916198730469\n","Epoch 62/2000, Step 35, d_loss: 0.3754896819591522, g_loss: 5.065600872039795\n","Epoch 62/2000, Step 36, d_loss: 0.36008450388908386, g_loss: 4.612678527832031\n","Epoch 62/2000, Step 37, d_loss: 0.35704469680786133, g_loss: 4.491902828216553\n","Epoch 62/2000, Step 38, d_loss: 0.4115520417690277, g_loss: 4.264409065246582\n","Epoch 62/2000, Step 39, d_loss: 0.45322710275650024, g_loss: 4.466963291168213\n","Epoch 62/2000, Step 40, d_loss: 0.4578395485877991, g_loss: 2.991591215133667\n","Epoch 62/2000, Step 41, d_loss: 0.36514297127723694, g_loss: 3.8606207370758057\n","Epoch 62/2000, Step 42, d_loss: 0.3585386276245117, g_loss: 4.401236057281494\n","Epoch 62/2000, Step 43, d_loss: 0.39726096391677856, g_loss: 3.2785608768463135\n","Epoch 62/2000, Step 44, d_loss: 0.4103659391403198, g_loss: 3.4728639125823975\n","Epoch 62/2000, Step 45, d_loss: 0.3811940848827362, g_loss: 3.6374778747558594\n","Epoch 62/2000, Step 46, d_loss: 0.40476518869400024, g_loss: 4.084476947784424\n","Epoch 62/2000, Step 47, d_loss: 0.36789318919181824, g_loss: 4.442294597625732\n","Epoch 62/2000, Step 48, d_loss: 0.3908540606498718, g_loss: 4.467317581176758\n","Epoch 62/2000, Step 49, d_loss: 0.3824300765991211, g_loss: 5.592004776000977\n","Epoch 62/2000, Step 50, d_loss: 0.3732108175754547, g_loss: 4.643161773681641\n","Epoch 62/2000, Step 51, d_loss: 0.3645873963832855, g_loss: 4.961399078369141\n","Epoch 62/2000, Step 52, d_loss: 0.5376480221748352, g_loss: 4.463874340057373\n","Epoch 62/2000, Step 53, d_loss: 0.4600915312767029, g_loss: 5.253026962280273\n","Epoch 62/2000, Step 54, d_loss: 0.4054296016693115, g_loss: 3.0551857948303223\n","Epoch 62/2000, Step 55, d_loss: 0.488614559173584, g_loss: 3.4423460960388184\n","Epoch 62/2000, Step 56, d_loss: 0.4410911202430725, g_loss: 3.4511570930480957\n","Epoch 62/2000, Step 57, d_loss: 0.48755261301994324, g_loss: 3.7604739665985107\n","Epoch 62/2000, Step 58, d_loss: 0.39036479592323303, g_loss: 3.3436522483825684\n","Epoch 62/2000, Step 59, d_loss: 0.3945852816104889, g_loss: 5.044404983520508\n","Epoch 62/2000, Step 60, d_loss: 0.36877861618995667, g_loss: 4.566922187805176\n","Epoch 62/2000, Step 61, d_loss: 0.6450315117835999, g_loss: 3.845456838607788\n","Epoch 62/2000, Step 62, d_loss: 0.3840903341770172, g_loss: 3.4248156547546387\n","Epoch 62/2000, Step 63, d_loss: 0.42442992329597473, g_loss: 3.1507105827331543\n","Epoch 62/2000, Step 64, d_loss: 0.452756404876709, g_loss: 4.597178936004639\n","Epoch 62/2000, Step 65, d_loss: 0.434395968914032, g_loss: 4.652448654174805\n","Epoch 62/2000, Step 66, d_loss: 0.40497639775276184, g_loss: 3.6054649353027344\n","Epoch 62/2000, Step 67, d_loss: 0.37982818484306335, g_loss: 4.247165203094482\n","Epoch 62/2000, Step 68, d_loss: 0.3602534234523773, g_loss: 4.746157169342041\n","Epoch 62/2000, Step 69, d_loss: 0.41884085536003113, g_loss: 4.667618274688721\n","Epoch 62/2000, Step 70, d_loss: 0.46184679865837097, g_loss: 3.9674861431121826\n","Epoch 62/2000, Step 71, d_loss: 0.41607216000556946, g_loss: 3.73281192779541\n","Epoch 62/2000, Step 72, d_loss: 0.352999210357666, g_loss: 3.3636419773101807\n","Epoch 62/2000, Step 73, d_loss: 0.4816571772098541, g_loss: 4.357332706451416\n","Epoch 62/2000, Step 74, d_loss: 0.41933441162109375, g_loss: 2.940826416015625\n","Epoch 62/2000, Step 75, d_loss: 0.42630088329315186, g_loss: 3.991262197494507\n","Epoch 62/2000, Step 76, d_loss: 0.40885886549949646, g_loss: 3.2768709659576416\n","Epoch 62/2000, Step 77, d_loss: 0.4198814332485199, g_loss: 3.7493863105773926\n","Epoch 62/2000, Step 78, d_loss: 0.36221256852149963, g_loss: 3.791065216064453\n","Epoch 62/2000, Step 79, d_loss: 0.39363327622413635, g_loss: 4.428520202636719\n","Epoch 62/2000, Step 80, d_loss: 0.3689984083175659, g_loss: 4.438047409057617\n","Epoch 62/2000, Step 81, d_loss: 0.4018121659755707, g_loss: 4.943691730499268\n","Epoch 62/2000, Step 82, d_loss: 0.3659168481826782, g_loss: 3.9023942947387695\n","Epoch 62/2000, Step 83, d_loss: 0.3687351644039154, g_loss: 4.571126461029053\n","Epoch 62/2000, Step 84, d_loss: 0.42848291993141174, g_loss: 4.024837493896484\n","Epoch 62/2000, Step 85, d_loss: 0.4134458303451538, g_loss: 3.3049190044403076\n","Epoch 62/2000, Step 86, d_loss: 0.41989806294441223, g_loss: 3.145963668823242\n","Epoch 62/2000, Step 87, d_loss: 0.5064841508865356, g_loss: 3.081354856491089\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 63/2000, Step 1, d_loss: 0.4759752154350281, g_loss: 2.7468373775482178\n","Epoch 63/2000, Step 2, d_loss: 0.4189978241920471, g_loss: 3.637857675552368\n","Epoch 63/2000, Step 3, d_loss: 0.3918684124946594, g_loss: 4.285373210906982\n","Epoch 63/2000, Step 4, d_loss: 0.3906969428062439, g_loss: 3.7829625606536865\n","Epoch 63/2000, Step 5, d_loss: 0.43766820430755615, g_loss: 4.06335973739624\n","Epoch 63/2000, Step 6, d_loss: 0.3970162868499756, g_loss: 3.7203238010406494\n","Epoch 63/2000, Step 7, d_loss: 0.36770325899124146, g_loss: 3.4303622245788574\n","Epoch 63/2000, Step 8, d_loss: 0.42663776874542236, g_loss: 3.5867228507995605\n","Epoch 63/2000, Step 9, d_loss: 0.48039165139198303, g_loss: 3.709724187850952\n","Epoch 63/2000, Step 10, d_loss: 0.41063934564590454, g_loss: 2.629554033279419\n","Epoch 63/2000, Step 11, d_loss: 0.44051048159599304, g_loss: 3.3948960304260254\n","Epoch 63/2000, Step 12, d_loss: 0.46074748039245605, g_loss: 3.761871576309204\n","Epoch 63/2000, Step 13, d_loss: 0.37859901785850525, g_loss: 4.225757598876953\n","Epoch 63/2000, Step 14, d_loss: 0.3746107220649719, g_loss: 4.373725891113281\n","Epoch 63/2000, Step 15, d_loss: 0.47801733016967773, g_loss: 4.028057098388672\n","Epoch 63/2000, Step 16, d_loss: 0.4327366054058075, g_loss: 3.601022481918335\n","Epoch 63/2000, Step 17, d_loss: 0.36747151613235474, g_loss: 3.75591778755188\n","Epoch 63/2000, Step 18, d_loss: 0.44779133796691895, g_loss: 3.852689027786255\n","Epoch 63/2000, Step 19, d_loss: 0.40364131331443787, g_loss: 2.6170859336853027\n","Epoch 63/2000, Step 20, d_loss: 0.48848065733909607, g_loss: 2.5553112030029297\n","Epoch 63/2000, Step 21, d_loss: 0.4578667879104614, g_loss: 3.189833402633667\n","Epoch 63/2000, Step 22, d_loss: 0.4244825541973114, g_loss: 3.7904255390167236\n","Epoch 63/2000, Step 23, d_loss: 0.3841083347797394, g_loss: 3.7001500129699707\n","Epoch 63/2000, Step 24, d_loss: 0.36577364802360535, g_loss: 4.571200847625732\n","Epoch 63/2000, Step 25, d_loss: 0.3915783166885376, g_loss: 5.254210472106934\n","Epoch 63/2000, Step 26, d_loss: 0.3563084602355957, g_loss: 4.566843032836914\n","Epoch 63/2000, Step 27, d_loss: 0.42190781235694885, g_loss: 4.415947437286377\n","Epoch 63/2000, Step 28, d_loss: 0.45465052127838135, g_loss: 5.169629096984863\n","Epoch 63/2000, Step 29, d_loss: 0.39998066425323486, g_loss: 4.130654811859131\n","Epoch 63/2000, Step 30, d_loss: 0.3973953425884247, g_loss: 3.7028026580810547\n","Epoch 63/2000, Step 31, d_loss: 0.39332863688468933, g_loss: 3.529560089111328\n","Epoch 63/2000, Step 32, d_loss: 0.3823792040348053, g_loss: 3.690825939178467\n","Epoch 63/2000, Step 33, d_loss: 0.38627228140830994, g_loss: 3.536682605743408\n","Epoch 63/2000, Step 34, d_loss: 0.36956119537353516, g_loss: 3.7720582485198975\n","Epoch 63/2000, Step 35, d_loss: 0.4337432086467743, g_loss: 3.8058412075042725\n","Epoch 63/2000, Step 36, d_loss: 0.39805012941360474, g_loss: 3.874743938446045\n","Epoch 63/2000, Step 37, d_loss: 0.3929896652698517, g_loss: 4.630771160125732\n","Epoch 63/2000, Step 38, d_loss: 0.3770899176597595, g_loss: 4.968306541442871\n","Epoch 63/2000, Step 39, d_loss: 0.44676515460014343, g_loss: 5.021903038024902\n","Epoch 63/2000, Step 40, d_loss: 0.48454827070236206, g_loss: 4.438807487487793\n","Epoch 63/2000, Step 41, d_loss: 0.3695947229862213, g_loss: 3.761775493621826\n","Epoch 63/2000, Step 42, d_loss: 0.3542785942554474, g_loss: 3.8804688453674316\n","Epoch 63/2000, Step 43, d_loss: 0.3838171064853668, g_loss: 3.811000108718872\n","Epoch 63/2000, Step 44, d_loss: 0.37392938137054443, g_loss: 3.8536624908447266\n","Epoch 63/2000, Step 45, d_loss: 0.39618879556655884, g_loss: 4.586731433868408\n","Epoch 63/2000, Step 46, d_loss: 0.42030054330825806, g_loss: 3.9271080493927\n","Epoch 63/2000, Step 47, d_loss: 0.38839006423950195, g_loss: 3.8143556118011475\n","Epoch 63/2000, Step 48, d_loss: 0.3707377314567566, g_loss: 5.712534427642822\n","Epoch 63/2000, Step 49, d_loss: 0.5340644121170044, g_loss: 5.217044830322266\n","Epoch 63/2000, Step 50, d_loss: 0.37607747316360474, g_loss: 3.885390043258667\n","Epoch 63/2000, Step 51, d_loss: 0.37832847237586975, g_loss: 3.8913943767547607\n","Epoch 63/2000, Step 52, d_loss: 0.37492647767066956, g_loss: 3.7134690284729004\n","Epoch 63/2000, Step 53, d_loss: 0.38028833270072937, g_loss: 4.00539493560791\n","Epoch 63/2000, Step 54, d_loss: 0.3867206275463104, g_loss: 4.5333662033081055\n","Epoch 63/2000, Step 55, d_loss: 0.37990349531173706, g_loss: 3.776611566543579\n","Epoch 63/2000, Step 56, d_loss: 0.41971492767333984, g_loss: 4.059569835662842\n","Epoch 63/2000, Step 57, d_loss: 0.3664761781692505, g_loss: 6.580456733703613\n","Epoch 63/2000, Step 58, d_loss: 0.3557531535625458, g_loss: 4.200465679168701\n","Epoch 63/2000, Step 59, d_loss: 0.4044760465621948, g_loss: 4.608612060546875\n","Epoch 63/2000, Step 60, d_loss: 0.38791701197624207, g_loss: 4.67547082901001\n","Epoch 63/2000, Step 61, d_loss: 0.35683566331863403, g_loss: 5.355400562286377\n","Epoch 63/2000, Step 62, d_loss: 0.36774224042892456, g_loss: 4.8212103843688965\n","Epoch 63/2000, Step 63, d_loss: 0.3821062445640564, g_loss: 3.5301003456115723\n","Epoch 63/2000, Step 64, d_loss: 0.3817429840564728, g_loss: 3.944530487060547\n","Epoch 63/2000, Step 65, d_loss: 0.3692484200000763, g_loss: 4.389759540557861\n","Epoch 63/2000, Step 66, d_loss: 0.3624585270881653, g_loss: 3.619910478591919\n","Epoch 63/2000, Step 67, d_loss: 0.3696836829185486, g_loss: 3.545819044113159\n","Epoch 63/2000, Step 68, d_loss: 0.3665749728679657, g_loss: 4.3240132331848145\n","Epoch 63/2000, Step 69, d_loss: 0.393658846616745, g_loss: 3.7714247703552246\n","Epoch 63/2000, Step 70, d_loss: 0.5380823612213135, g_loss: 4.229172229766846\n","Epoch 63/2000, Step 71, d_loss: 0.38849300146102905, g_loss: 4.501884460449219\n","Epoch 63/2000, Step 72, d_loss: 0.406854510307312, g_loss: 3.624962091445923\n","Epoch 63/2000, Step 73, d_loss: 0.3963708281517029, g_loss: 2.691077470779419\n","Epoch 63/2000, Step 74, d_loss: 0.44849422574043274, g_loss: 2.9257514476776123\n","Epoch 63/2000, Step 75, d_loss: 0.4218895435333252, g_loss: 3.7110488414764404\n","Epoch 63/2000, Step 76, d_loss: 0.3903680145740509, g_loss: 4.301940441131592\n","Epoch 63/2000, Step 77, d_loss: 0.35593926906585693, g_loss: 5.0584940910339355\n","Epoch 63/2000, Step 78, d_loss: 0.3554902970790863, g_loss: 5.16996955871582\n","Epoch 63/2000, Step 79, d_loss: 0.36068546772003174, g_loss: 5.00869607925415\n","Epoch 63/2000, Step 80, d_loss: 0.530998170375824, g_loss: 5.258920669555664\n","Epoch 63/2000, Step 81, d_loss: 0.41299957036972046, g_loss: 4.170402526855469\n","Epoch 63/2000, Step 82, d_loss: 0.37009891867637634, g_loss: 3.9740705490112305\n","Epoch 63/2000, Step 83, d_loss: 0.39189237356185913, g_loss: 4.60031270980835\n","Epoch 63/2000, Step 84, d_loss: 0.42044511437416077, g_loss: 3.7944960594177246\n","Epoch 63/2000, Step 85, d_loss: 0.4089525640010834, g_loss: 3.0439414978027344\n","Epoch 63/2000, Step 86, d_loss: 0.4382190406322479, g_loss: 3.1936306953430176\n","Epoch 63/2000, Step 87, d_loss: 0.41321754455566406, g_loss: 3.653358221054077\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 64/2000, Step 1, d_loss: 0.3849869668483734, g_loss: 3.574774742126465\n","Epoch 64/2000, Step 2, d_loss: 0.39276373386383057, g_loss: 5.488668918609619\n","Epoch 64/2000, Step 3, d_loss: 0.3739023804664612, g_loss: 5.200008392333984\n","Epoch 64/2000, Step 4, d_loss: 0.41981980204582214, g_loss: 5.151874542236328\n","Epoch 64/2000, Step 5, d_loss: 0.47763583064079285, g_loss: 5.054580211639404\n","Epoch 64/2000, Step 6, d_loss: 0.39874812960624695, g_loss: 4.079907417297363\n","Epoch 64/2000, Step 7, d_loss: 0.3823068141937256, g_loss: 3.0238213539123535\n","Epoch 64/2000, Step 8, d_loss: 0.36532506346702576, g_loss: 3.52785587310791\n","Epoch 64/2000, Step 9, d_loss: 0.43285566568374634, g_loss: 3.863321304321289\n","Epoch 64/2000, Step 10, d_loss: 0.38285401463508606, g_loss: 2.8863487243652344\n","Epoch 64/2000, Step 11, d_loss: 0.39225083589553833, g_loss: 3.726457357406616\n","Epoch 64/2000, Step 12, d_loss: 0.4120982587337494, g_loss: 3.7815535068511963\n","Epoch 64/2000, Step 13, d_loss: 0.39004427194595337, g_loss: 4.523019313812256\n","Epoch 64/2000, Step 14, d_loss: 0.35641878843307495, g_loss: 4.1739726066589355\n","Epoch 64/2000, Step 15, d_loss: 0.38117802143096924, g_loss: 5.042872905731201\n","Epoch 64/2000, Step 16, d_loss: 0.36196914315223694, g_loss: 4.127075672149658\n","Epoch 64/2000, Step 17, d_loss: 0.35138919949531555, g_loss: 5.80494499206543\n","Epoch 64/2000, Step 18, d_loss: 0.37307584285736084, g_loss: 4.331933975219727\n","Epoch 64/2000, Step 19, d_loss: 0.3615400493144989, g_loss: 4.387959957122803\n","Epoch 64/2000, Step 20, d_loss: 0.4058288335800171, g_loss: 5.037503242492676\n","Epoch 64/2000, Step 21, d_loss: 0.36574915051460266, g_loss: 4.657260417938232\n","Epoch 64/2000, Step 22, d_loss: 0.3730916678905487, g_loss: 4.760497093200684\n","Epoch 64/2000, Step 23, d_loss: 0.37050044536590576, g_loss: 3.4870409965515137\n","Epoch 64/2000, Step 24, d_loss: 0.40922945737838745, g_loss: 2.9762117862701416\n","Epoch 64/2000, Step 25, d_loss: 0.40111225843429565, g_loss: 3.7263753414154053\n","Epoch 64/2000, Step 26, d_loss: 0.418291300535202, g_loss: 4.504376411437988\n","Epoch 64/2000, Step 27, d_loss: 0.36315107345581055, g_loss: 4.740556240081787\n","Epoch 64/2000, Step 28, d_loss: 0.38197436928749084, g_loss: 4.996084690093994\n","Epoch 64/2000, Step 29, d_loss: 0.34204617142677307, g_loss: 4.5884270668029785\n","Epoch 64/2000, Step 30, d_loss: 0.4792126715183258, g_loss: 5.779535293579102\n","Epoch 64/2000, Step 31, d_loss: 0.4803936183452606, g_loss: 3.9912028312683105\n","Epoch 64/2000, Step 32, d_loss: 0.3762989640235901, g_loss: 2.7994158267974854\n","Epoch 64/2000, Step 33, d_loss: 0.4459567070007324, g_loss: 3.4111201763153076\n","Epoch 64/2000, Step 34, d_loss: 0.3956473767757416, g_loss: 2.544343948364258\n","Epoch 64/2000, Step 35, d_loss: 0.4203401505947113, g_loss: 2.8145430088043213\n","Epoch 64/2000, Step 36, d_loss: 0.431576132774353, g_loss: 3.795090436935425\n","Epoch 64/2000, Step 37, d_loss: 0.3625386655330658, g_loss: 3.6623990535736084\n","Epoch 64/2000, Step 38, d_loss: 0.3823854923248291, g_loss: 4.051550388336182\n","Epoch 64/2000, Step 39, d_loss: 0.35888999700546265, g_loss: 5.398931980133057\n","Epoch 64/2000, Step 40, d_loss: 0.4091740548610687, g_loss: 4.276989459991455\n","Epoch 64/2000, Step 41, d_loss: 0.41922539472579956, g_loss: 5.098870277404785\n","Epoch 64/2000, Step 42, d_loss: 0.4078163206577301, g_loss: 3.683176040649414\n","Epoch 64/2000, Step 43, d_loss: 0.38132140040397644, g_loss: 4.059720039367676\n","Epoch 64/2000, Step 44, d_loss: 0.4098467528820038, g_loss: 3.450054168701172\n","Epoch 64/2000, Step 45, d_loss: 0.3935907185077667, g_loss: 4.684947490692139\n","Epoch 64/2000, Step 46, d_loss: 0.4053897261619568, g_loss: 3.3154196739196777\n","Epoch 64/2000, Step 47, d_loss: 0.40652012825012207, g_loss: 3.3098537921905518\n","Epoch 64/2000, Step 48, d_loss: 0.3965545892715454, g_loss: 3.894817590713501\n","Epoch 64/2000, Step 49, d_loss: 0.36607062816619873, g_loss: 4.565417289733887\n","Epoch 64/2000, Step 50, d_loss: 0.36508819460868835, g_loss: 6.144678115844727\n","Epoch 64/2000, Step 51, d_loss: 0.5058357119560242, g_loss: 4.731502056121826\n","Epoch 64/2000, Step 52, d_loss: 0.5436788201332092, g_loss: 3.801180362701416\n","Epoch 64/2000, Step 53, d_loss: 0.39796167612075806, g_loss: 3.6266818046569824\n","Epoch 64/2000, Step 54, d_loss: 0.3957901895046234, g_loss: 3.4248971939086914\n","Epoch 64/2000, Step 55, d_loss: 0.4928070604801178, g_loss: 3.5129477977752686\n","Epoch 64/2000, Step 56, d_loss: 0.43961235880851746, g_loss: 2.703178644180298\n","Epoch 64/2000, Step 57, d_loss: 0.47737187147140503, g_loss: 3.0532896518707275\n","Epoch 64/2000, Step 58, d_loss: 0.4006710350513458, g_loss: 3.866309881210327\n","Epoch 64/2000, Step 59, d_loss: 0.36352166533470154, g_loss: 4.723935127258301\n","Epoch 64/2000, Step 60, d_loss: 0.3500419557094574, g_loss: 5.133494853973389\n","Epoch 64/2000, Step 61, d_loss: 0.40785300731658936, g_loss: 5.089607238769531\n","Epoch 64/2000, Step 62, d_loss: 0.4151282012462616, g_loss: 4.559711456298828\n","Epoch 64/2000, Step 63, d_loss: 0.37195885181427, g_loss: 4.213647365570068\n","Epoch 64/2000, Step 64, d_loss: 0.36848917603492737, g_loss: 3.6292927265167236\n","Epoch 64/2000, Step 65, d_loss: 0.3870845139026642, g_loss: 4.043547630310059\n","Epoch 64/2000, Step 66, d_loss: 0.39327943325042725, g_loss: 4.212438106536865\n","Epoch 64/2000, Step 67, d_loss: 0.3758391737937927, g_loss: 4.633241653442383\n","Epoch 64/2000, Step 68, d_loss: 0.37767472863197327, g_loss: 4.151440620422363\n","Epoch 64/2000, Step 69, d_loss: 0.39896997809410095, g_loss: 3.549659252166748\n","Epoch 64/2000, Step 70, d_loss: 0.38968193531036377, g_loss: 5.623309135437012\n","Epoch 64/2000, Step 71, d_loss: 0.3984847068786621, g_loss: 5.01230525970459\n","Epoch 64/2000, Step 72, d_loss: 0.3822924494743347, g_loss: 4.321310997009277\n","Epoch 64/2000, Step 73, d_loss: 0.3536076247692108, g_loss: 4.746766567230225\n","Epoch 64/2000, Step 74, d_loss: 0.4982678294181824, g_loss: 4.004205226898193\n","Epoch 64/2000, Step 75, d_loss: 0.3922381103038788, g_loss: 3.8330721855163574\n","Epoch 64/2000, Step 76, d_loss: 0.37079161405563354, g_loss: 3.5582005977630615\n","Epoch 64/2000, Step 77, d_loss: 0.3980062007904053, g_loss: 3.809572696685791\n","Epoch 64/2000, Step 78, d_loss: 0.412756085395813, g_loss: 3.7113261222839355\n","Epoch 64/2000, Step 79, d_loss: 0.3864905834197998, g_loss: 3.0925495624542236\n","Epoch 64/2000, Step 80, d_loss: 0.3874964714050293, g_loss: 3.5038912296295166\n","Epoch 64/2000, Step 81, d_loss: 0.372649222612381, g_loss: 3.972414493560791\n","Epoch 64/2000, Step 82, d_loss: 0.3864859640598297, g_loss: 4.182956218719482\n","Epoch 64/2000, Step 83, d_loss: 0.39669695496559143, g_loss: 5.095696449279785\n","Epoch 64/2000, Step 84, d_loss: 0.36429375410079956, g_loss: 3.8536176681518555\n","Epoch 64/2000, Step 85, d_loss: 0.38871195912361145, g_loss: 4.018056392669678\n","Epoch 64/2000, Step 86, d_loss: 0.39499789476394653, g_loss: 3.864117383956909\n","Epoch 64/2000, Step 87, d_loss: 0.36566510796546936, g_loss: 3.785677194595337\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 65/2000, Step 1, d_loss: 0.40421393513679504, g_loss: 4.517256736755371\n","Epoch 65/2000, Step 2, d_loss: 0.397756427526474, g_loss: 3.858269453048706\n","Epoch 65/2000, Step 3, d_loss: 0.36378875374794006, g_loss: 4.010443687438965\n","Epoch 65/2000, Step 4, d_loss: 0.3805634677410126, g_loss: 4.704721450805664\n","Epoch 65/2000, Step 5, d_loss: 0.3804989755153656, g_loss: 3.640049934387207\n","Epoch 65/2000, Step 6, d_loss: 0.35945644974708557, g_loss: 4.206237316131592\n","Epoch 65/2000, Step 7, d_loss: 0.3968545198440552, g_loss: 3.9603629112243652\n","Epoch 65/2000, Step 8, d_loss: 0.3876398801803589, g_loss: 3.642693281173706\n","Epoch 65/2000, Step 9, d_loss: 0.3820638656616211, g_loss: 3.6825575828552246\n","Epoch 65/2000, Step 10, d_loss: 0.37791502475738525, g_loss: 3.6332664489746094\n","Epoch 65/2000, Step 11, d_loss: 0.3623179793357849, g_loss: 3.7861990928649902\n","Epoch 65/2000, Step 12, d_loss: 0.3675803542137146, g_loss: 4.349839687347412\n","Epoch 65/2000, Step 13, d_loss: 0.36591294407844543, g_loss: 4.135776519775391\n","Epoch 65/2000, Step 14, d_loss: 0.37627744674682617, g_loss: 3.4496426582336426\n","Epoch 65/2000, Step 15, d_loss: 0.40271127223968506, g_loss: 3.6507327556610107\n","Epoch 65/2000, Step 16, d_loss: 0.43066513538360596, g_loss: 3.5927271842956543\n","Epoch 65/2000, Step 17, d_loss: 0.3963419795036316, g_loss: 4.42857027053833\n","Epoch 65/2000, Step 18, d_loss: 0.402622789144516, g_loss: 5.670351982116699\n","Epoch 65/2000, Step 19, d_loss: 0.35989752411842346, g_loss: 4.418729305267334\n","Epoch 65/2000, Step 20, d_loss: 0.35987189412117004, g_loss: 4.5502705574035645\n","Epoch 65/2000, Step 21, d_loss: 0.48951229453086853, g_loss: 5.6759185791015625\n","Epoch 65/2000, Step 22, d_loss: 0.38325369358062744, g_loss: 4.189591884613037\n","Epoch 65/2000, Step 23, d_loss: 0.38815924525260925, g_loss: 3.7066280841827393\n","Epoch 65/2000, Step 24, d_loss: 0.41369977593421936, g_loss: 3.1043715476989746\n","Epoch 65/2000, Step 25, d_loss: 0.3896179497241974, g_loss: 3.755502462387085\n","Epoch 65/2000, Step 26, d_loss: 0.43081575632095337, g_loss: 3.906214952468872\n","Epoch 65/2000, Step 27, d_loss: 0.36230340600013733, g_loss: 4.449527740478516\n","Epoch 65/2000, Step 28, d_loss: 0.3693426847457886, g_loss: 4.225532531738281\n","Epoch 65/2000, Step 29, d_loss: 0.3832235336303711, g_loss: 5.062455654144287\n","Epoch 65/2000, Step 30, d_loss: 0.3774116337299347, g_loss: 4.00383996963501\n","Epoch 65/2000, Step 31, d_loss: 0.37465089559555054, g_loss: 5.000931262969971\n","Epoch 65/2000, Step 32, d_loss: 0.3558165431022644, g_loss: 5.041863441467285\n","Epoch 65/2000, Step 33, d_loss: 0.5883191227912903, g_loss: 4.903154373168945\n","Epoch 65/2000, Step 34, d_loss: 0.3981226980686188, g_loss: 3.8328442573547363\n","Epoch 65/2000, Step 35, d_loss: 0.35806506872177124, g_loss: 3.6200618743896484\n","Epoch 65/2000, Step 36, d_loss: 0.40623903274536133, g_loss: 3.2609870433807373\n","Epoch 65/2000, Step 37, d_loss: 0.4338325262069702, g_loss: 3.3523857593536377\n","Epoch 65/2000, Step 38, d_loss: 0.4441325068473816, g_loss: 4.156121253967285\n","Epoch 65/2000, Step 39, d_loss: 0.39178887009620667, g_loss: 3.822357654571533\n","Epoch 65/2000, Step 40, d_loss: 0.3769364058971405, g_loss: 4.246922016143799\n","Epoch 65/2000, Step 41, d_loss: 0.4731823801994324, g_loss: 3.9944052696228027\n","Epoch 65/2000, Step 42, d_loss: 0.4262073338031769, g_loss: 4.056390285491943\n","Epoch 65/2000, Step 43, d_loss: 0.421664834022522, g_loss: 3.7144711017608643\n","Epoch 65/2000, Step 44, d_loss: 0.4746869206428528, g_loss: 3.789889335632324\n","Epoch 65/2000, Step 45, d_loss: 0.3909648656845093, g_loss: 3.3151001930236816\n","Epoch 65/2000, Step 46, d_loss: 0.3918083608150482, g_loss: 3.1013119220733643\n","Epoch 65/2000, Step 47, d_loss: 0.4386172890663147, g_loss: 3.9250600337982178\n","Epoch 65/2000, Step 48, d_loss: 0.42142826318740845, g_loss: 3.770159959793091\n","Epoch 65/2000, Step 49, d_loss: 0.39198771119117737, g_loss: 3.716702461242676\n","Epoch 65/2000, Step 50, d_loss: 0.3961330056190491, g_loss: 5.59760856628418\n","Epoch 65/2000, Step 51, d_loss: 0.3575517237186432, g_loss: 4.168907165527344\n","Epoch 65/2000, Step 52, d_loss: 0.37625378370285034, g_loss: 5.019637584686279\n","Epoch 65/2000, Step 53, d_loss: 0.4037664830684662, g_loss: 4.610260486602783\n","Epoch 65/2000, Step 54, d_loss: 0.36834901571273804, g_loss: 6.465368270874023\n","Epoch 65/2000, Step 55, d_loss: 0.4038611352443695, g_loss: 5.636863708496094\n","Epoch 65/2000, Step 56, d_loss: 0.38440993428230286, g_loss: 3.4863412380218506\n","Epoch 65/2000, Step 57, d_loss: 0.40908893942832947, g_loss: 3.7903904914855957\n","Epoch 65/2000, Step 58, d_loss: 0.38204672932624817, g_loss: 3.4148519039154053\n","Epoch 65/2000, Step 59, d_loss: 0.4065166413784027, g_loss: 3.07212233543396\n","Epoch 65/2000, Step 60, d_loss: 0.4178503453731537, g_loss: 3.3545749187469482\n","Epoch 65/2000, Step 61, d_loss: 0.4118022322654724, g_loss: 3.5014259815216064\n","Epoch 65/2000, Step 62, d_loss: 0.48548954725265503, g_loss: 3.3435964584350586\n","Epoch 65/2000, Step 63, d_loss: 0.35704946517944336, g_loss: 3.6710381507873535\n","Epoch 65/2000, Step 64, d_loss: 0.3676341474056244, g_loss: 4.213720321655273\n","Epoch 65/2000, Step 65, d_loss: 0.3675849139690399, g_loss: 4.417287826538086\n","Epoch 65/2000, Step 66, d_loss: 0.37015557289123535, g_loss: 4.888851642608643\n","Epoch 65/2000, Step 67, d_loss: 0.38100701570510864, g_loss: 4.3564534187316895\n","Epoch 65/2000, Step 68, d_loss: 0.3651193082332611, g_loss: 4.811025142669678\n","Epoch 65/2000, Step 69, d_loss: 0.3570024371147156, g_loss: 4.911579132080078\n","Epoch 65/2000, Step 70, d_loss: 0.3627219498157501, g_loss: 4.397154808044434\n","Epoch 65/2000, Step 71, d_loss: 0.3944444954395294, g_loss: 4.2816853523254395\n","Epoch 65/2000, Step 72, d_loss: 0.37660595774650574, g_loss: 3.9217796325683594\n","Epoch 65/2000, Step 73, d_loss: 0.35548651218414307, g_loss: 3.8067102432250977\n","Epoch 65/2000, Step 74, d_loss: 0.35104650259017944, g_loss: 4.576061725616455\n","Epoch 65/2000, Step 75, d_loss: 0.4130781292915344, g_loss: 3.832759380340576\n","Epoch 65/2000, Step 76, d_loss: 0.42133402824401855, g_loss: 2.842808723449707\n","Epoch 65/2000, Step 77, d_loss: 0.3678251802921295, g_loss: 4.548153877258301\n","Epoch 65/2000, Step 78, d_loss: 0.4076021611690521, g_loss: 4.046746253967285\n","Epoch 65/2000, Step 79, d_loss: 0.39742547273635864, g_loss: 4.529611587524414\n","Epoch 65/2000, Step 80, d_loss: 0.3959672451019287, g_loss: 3.558835983276367\n","Epoch 65/2000, Step 81, d_loss: 0.4176836609840393, g_loss: 4.178317546844482\n","Epoch 65/2000, Step 82, d_loss: 0.43456992506980896, g_loss: 3.098440170288086\n","Epoch 65/2000, Step 83, d_loss: 0.3614921271800995, g_loss: 3.8206748962402344\n","Epoch 65/2000, Step 84, d_loss: 0.5117116570472717, g_loss: 3.4505908489227295\n","Epoch 65/2000, Step 85, d_loss: 0.4203900098800659, g_loss: 3.947948932647705\n","Epoch 65/2000, Step 86, d_loss: 0.4029247760772705, g_loss: 4.407288551330566\n","Epoch 65/2000, Step 87, d_loss: 0.38612422347068787, g_loss: 5.02644681930542\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 66/2000, Step 1, d_loss: 0.3866150677204132, g_loss: 4.984220504760742\n","Epoch 66/2000, Step 2, d_loss: 0.36611223220825195, g_loss: 5.381679534912109\n","Epoch 66/2000, Step 3, d_loss: 0.40565022826194763, g_loss: 5.250744819641113\n","Epoch 66/2000, Step 4, d_loss: 0.4053148627281189, g_loss: 5.521976470947266\n","Epoch 66/2000, Step 5, d_loss: 0.40041807293891907, g_loss: 4.278863906860352\n","Epoch 66/2000, Step 6, d_loss: 0.4019329845905304, g_loss: 3.665252923965454\n","Epoch 66/2000, Step 7, d_loss: 0.3724908232688904, g_loss: 3.712268114089966\n","Epoch 66/2000, Step 8, d_loss: 0.39982956647872925, g_loss: 3.6345744132995605\n","Epoch 66/2000, Step 9, d_loss: 0.3898999094963074, g_loss: 4.638950347900391\n","Epoch 66/2000, Step 10, d_loss: 0.37676453590393066, g_loss: 4.636152267456055\n","Epoch 66/2000, Step 11, d_loss: 0.3986094892024994, g_loss: 4.114376544952393\n","Epoch 66/2000, Step 12, d_loss: 0.36135923862457275, g_loss: 4.731256008148193\n","Epoch 66/2000, Step 13, d_loss: 0.3665529787540436, g_loss: 4.795149803161621\n","Epoch 66/2000, Step 14, d_loss: 0.3815820813179016, g_loss: 4.706296443939209\n","Epoch 66/2000, Step 15, d_loss: 0.3673132658004761, g_loss: 4.486506938934326\n","Epoch 66/2000, Step 16, d_loss: 0.37775886058807373, g_loss: 3.7443554401397705\n","Epoch 66/2000, Step 17, d_loss: 0.355761855840683, g_loss: 4.099142074584961\n","Epoch 66/2000, Step 18, d_loss: 0.3712204396724701, g_loss: 3.9779484272003174\n","Epoch 66/2000, Step 19, d_loss: 0.372833251953125, g_loss: 3.904857873916626\n","Epoch 66/2000, Step 20, d_loss: 0.43658745288848877, g_loss: 3.878093957901001\n","Epoch 66/2000, Step 21, d_loss: 0.4113817811012268, g_loss: 4.02911901473999\n","Epoch 66/2000, Step 22, d_loss: 0.3532356321811676, g_loss: 4.15884256362915\n","Epoch 66/2000, Step 23, d_loss: 0.39304256439208984, g_loss: 4.048104763031006\n","Epoch 66/2000, Step 24, d_loss: 0.3632286489009857, g_loss: 3.669102430343628\n","Epoch 66/2000, Step 25, d_loss: 0.3884608745574951, g_loss: 4.010360240936279\n","Epoch 66/2000, Step 26, d_loss: 0.3714645504951477, g_loss: 4.676638126373291\n","Epoch 66/2000, Step 27, d_loss: 0.35934096574783325, g_loss: 4.485004901885986\n","Epoch 66/2000, Step 28, d_loss: 0.3828434646129608, g_loss: 4.127888202667236\n","Epoch 66/2000, Step 29, d_loss: 0.3987654745578766, g_loss: 4.371711730957031\n","Epoch 66/2000, Step 30, d_loss: 0.36021849513053894, g_loss: 4.741611480712891\n","Epoch 66/2000, Step 31, d_loss: 0.3501860797405243, g_loss: 4.108954429626465\n","Epoch 66/2000, Step 32, d_loss: 0.36392566561698914, g_loss: 4.011475086212158\n","Epoch 66/2000, Step 33, d_loss: 0.4029584228992462, g_loss: 4.244441032409668\n","Epoch 66/2000, Step 34, d_loss: 0.35845550894737244, g_loss: 3.4941093921661377\n","Epoch 66/2000, Step 35, d_loss: 0.35248714685440063, g_loss: 4.261602401733398\n","Epoch 66/2000, Step 36, d_loss: 0.37504640221595764, g_loss: 4.131435394287109\n","Epoch 66/2000, Step 37, d_loss: 0.4227656126022339, g_loss: 3.968287944793701\n","Epoch 66/2000, Step 38, d_loss: 0.3754143714904785, g_loss: 3.847564220428467\n","Epoch 66/2000, Step 39, d_loss: 0.3626959025859833, g_loss: 3.8020479679107666\n","Epoch 66/2000, Step 40, d_loss: 0.3710957169532776, g_loss: 4.418565273284912\n","Epoch 66/2000, Step 41, d_loss: 0.3677123486995697, g_loss: 4.831999778747559\n","Epoch 66/2000, Step 42, d_loss: 0.36501944065093994, g_loss: 3.54552960395813\n","Epoch 66/2000, Step 43, d_loss: 0.3840848207473755, g_loss: 3.737943649291992\n","Epoch 66/2000, Step 44, d_loss: 0.38874417543411255, g_loss: 3.40907883644104\n","Epoch 66/2000, Step 45, d_loss: 0.39368143677711487, g_loss: 4.210939407348633\n","Epoch 66/2000, Step 46, d_loss: 0.36796361207962036, g_loss: 3.5387113094329834\n","Epoch 66/2000, Step 47, d_loss: 0.36837974190711975, g_loss: 3.000617027282715\n","Epoch 66/2000, Step 48, d_loss: 0.3877228796482086, g_loss: 3.8091535568237305\n","Epoch 66/2000, Step 49, d_loss: 0.4731982946395874, g_loss: 4.366581916809082\n","Epoch 66/2000, Step 50, d_loss: 0.3456120789051056, g_loss: 4.0075531005859375\n","Epoch 66/2000, Step 51, d_loss: 0.38188087940216064, g_loss: 4.2307353019714355\n","Epoch 66/2000, Step 52, d_loss: 0.3706076741218567, g_loss: 4.02721643447876\n","Epoch 66/2000, Step 53, d_loss: 0.3915612995624542, g_loss: 4.649115562438965\n","Epoch 66/2000, Step 54, d_loss: 0.35102054476737976, g_loss: 3.8925888538360596\n","Epoch 66/2000, Step 55, d_loss: 0.4633100926876068, g_loss: 4.281357288360596\n","Epoch 66/2000, Step 56, d_loss: 0.4069651961326599, g_loss: 3.4908273220062256\n","Epoch 66/2000, Step 57, d_loss: 0.4833849370479584, g_loss: 3.6688003540039062\n","Epoch 66/2000, Step 58, d_loss: 0.41754746437072754, g_loss: 2.971081495285034\n","Epoch 66/2000, Step 59, d_loss: 0.40881192684173584, g_loss: 3.3142693042755127\n","Epoch 66/2000, Step 60, d_loss: 0.4580206274986267, g_loss: 3.504875659942627\n","Epoch 66/2000, Step 61, d_loss: 0.407993346452713, g_loss: 4.518580436706543\n","Epoch 66/2000, Step 62, d_loss: 0.3627626895904541, g_loss: 4.083279132843018\n","Epoch 66/2000, Step 63, d_loss: 0.38476595282554626, g_loss: 5.3127360343933105\n","Epoch 66/2000, Step 64, d_loss: 0.3452529013156891, g_loss: 5.860256195068359\n","Epoch 66/2000, Step 65, d_loss: 0.43677031993865967, g_loss: 6.692636489868164\n","Epoch 66/2000, Step 66, d_loss: 0.482707142829895, g_loss: 4.305660247802734\n","Epoch 66/2000, Step 67, d_loss: 0.42437323927879333, g_loss: 4.314899921417236\n","Epoch 66/2000, Step 68, d_loss: 0.3836013972759247, g_loss: 3.4672179222106934\n","Epoch 66/2000, Step 69, d_loss: 0.4325697422027588, g_loss: 2.5643653869628906\n","Epoch 66/2000, Step 70, d_loss: 0.5393968224525452, g_loss: 3.1905527114868164\n","Epoch 66/2000, Step 71, d_loss: 0.41894012689590454, g_loss: 3.251757860183716\n","Epoch 66/2000, Step 72, d_loss: 0.4533098042011261, g_loss: 4.0058488845825195\n","Epoch 66/2000, Step 73, d_loss: 0.40599367022514343, g_loss: 4.85697078704834\n","Epoch 66/2000, Step 74, d_loss: 0.36497095227241516, g_loss: 5.248558044433594\n","Epoch 66/2000, Step 75, d_loss: 0.5046189427375793, g_loss: 4.6919145584106445\n","Epoch 66/2000, Step 76, d_loss: 0.4188983738422394, g_loss: 5.387360095977783\n","Epoch 66/2000, Step 77, d_loss: 0.40512728691101074, g_loss: 4.659599304199219\n","Epoch 66/2000, Step 78, d_loss: 0.35820385813713074, g_loss: 3.389261245727539\n","Epoch 66/2000, Step 79, d_loss: 0.37993574142456055, g_loss: 3.5361058712005615\n","Epoch 66/2000, Step 80, d_loss: 0.3714894652366638, g_loss: 3.3685247898101807\n","Epoch 66/2000, Step 81, d_loss: 0.4051375389099121, g_loss: 3.2715511322021484\n","Epoch 66/2000, Step 82, d_loss: 0.35115963220596313, g_loss: 4.28776216506958\n","Epoch 66/2000, Step 83, d_loss: 0.3708859980106354, g_loss: 3.464277505874634\n","Epoch 66/2000, Step 84, d_loss: 0.3663359582424164, g_loss: 4.689436435699463\n","Epoch 66/2000, Step 85, d_loss: 0.35986313223838806, g_loss: 5.113582611083984\n","Epoch 66/2000, Step 86, d_loss: 0.424183189868927, g_loss: 4.358375549316406\n","Epoch 66/2000, Step 87, d_loss: 0.39620542526245117, g_loss: 4.674798488616943\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 67/2000, Step 1, d_loss: 0.37992560863494873, g_loss: 4.15973424911499\n","Epoch 67/2000, Step 2, d_loss: 0.35856369137763977, g_loss: 4.409312725067139\n","Epoch 67/2000, Step 3, d_loss: 0.370651513338089, g_loss: 3.8998656272888184\n","Epoch 67/2000, Step 4, d_loss: 0.3815735876560211, g_loss: 4.500062465667725\n","Epoch 67/2000, Step 5, d_loss: 0.4113162159919739, g_loss: 3.9942638874053955\n","Epoch 67/2000, Step 6, d_loss: 0.3663692772388458, g_loss: 4.107654094696045\n","Epoch 67/2000, Step 7, d_loss: 0.36229807138442993, g_loss: 4.371638298034668\n","Epoch 67/2000, Step 8, d_loss: 0.36227548122406006, g_loss: 4.530603885650635\n","Epoch 67/2000, Step 9, d_loss: 0.3719066083431244, g_loss: 3.804173707962036\n","Epoch 67/2000, Step 10, d_loss: 0.4488942623138428, g_loss: 4.0949625968933105\n","Epoch 67/2000, Step 11, d_loss: 0.36263883113861084, g_loss: 3.179077386856079\n","Epoch 67/2000, Step 12, d_loss: 0.3815998136997223, g_loss: 3.0785577297210693\n","Epoch 67/2000, Step 13, d_loss: 0.41133755445480347, g_loss: 3.234006881713867\n","Epoch 67/2000, Step 14, d_loss: 0.36341363191604614, g_loss: 3.6174659729003906\n","Epoch 67/2000, Step 15, d_loss: 0.4220522940158844, g_loss: 4.281043529510498\n","Epoch 67/2000, Step 16, d_loss: 0.3830455243587494, g_loss: 4.022106647491455\n","Epoch 67/2000, Step 17, d_loss: 0.366013765335083, g_loss: 3.36431622505188\n","Epoch 67/2000, Step 18, d_loss: 0.3694049119949341, g_loss: 4.567413806915283\n","Epoch 67/2000, Step 19, d_loss: 0.37760627269744873, g_loss: 4.969923496246338\n","Epoch 67/2000, Step 20, d_loss: 0.38778167963027954, g_loss: 4.609286785125732\n","Epoch 67/2000, Step 21, d_loss: 0.38247475028038025, g_loss: 3.8621764183044434\n","Epoch 67/2000, Step 22, d_loss: 0.37332695722579956, g_loss: 3.662813186645508\n","Epoch 67/2000, Step 23, d_loss: 0.38004595041275024, g_loss: 4.299976348876953\n","Epoch 67/2000, Step 24, d_loss: 0.36589616537094116, g_loss: 4.057586193084717\n","Epoch 67/2000, Step 25, d_loss: 0.39328715205192566, g_loss: 3.888890027999878\n","Epoch 67/2000, Step 26, d_loss: 0.3603743314743042, g_loss: 4.4669342041015625\n","Epoch 67/2000, Step 27, d_loss: 0.35276269912719727, g_loss: 5.331154823303223\n","Epoch 67/2000, Step 28, d_loss: 0.42207393050193787, g_loss: 4.587102890014648\n","Epoch 67/2000, Step 29, d_loss: 0.3546134829521179, g_loss: 3.930164337158203\n","Epoch 67/2000, Step 30, d_loss: 0.3735959529876709, g_loss: 3.2890994548797607\n","Epoch 67/2000, Step 31, d_loss: 0.4022156298160553, g_loss: 4.376991271972656\n","Epoch 67/2000, Step 32, d_loss: 0.3893333673477173, g_loss: 4.97373104095459\n","Epoch 67/2000, Step 33, d_loss: 0.3849293887615204, g_loss: 3.516932249069214\n","Epoch 67/2000, Step 34, d_loss: 0.37066352367401123, g_loss: 3.54443359375\n","Epoch 67/2000, Step 35, d_loss: 0.3545403778553009, g_loss: 4.031405925750732\n","Epoch 67/2000, Step 36, d_loss: 0.39880359172821045, g_loss: 4.200547695159912\n","Epoch 67/2000, Step 37, d_loss: 0.3630352318286896, g_loss: 3.8160953521728516\n","Epoch 67/2000, Step 38, d_loss: 0.3827337622642517, g_loss: 5.121673583984375\n","Epoch 67/2000, Step 39, d_loss: 0.3711179196834564, g_loss: 4.173272609710693\n","Epoch 67/2000, Step 40, d_loss: 0.36286652088165283, g_loss: 3.7885167598724365\n","Epoch 67/2000, Step 41, d_loss: 0.424746572971344, g_loss: 3.971632719039917\n","Epoch 67/2000, Step 42, d_loss: 0.3952561914920807, g_loss: 4.633701801300049\n","Epoch 67/2000, Step 43, d_loss: 0.38466620445251465, g_loss: 3.3574635982513428\n","Epoch 67/2000, Step 44, d_loss: 0.3830831050872803, g_loss: 3.6427783966064453\n","Epoch 67/2000, Step 45, d_loss: 0.43555155396461487, g_loss: 4.082828044891357\n","Epoch 67/2000, Step 46, d_loss: 0.4062507152557373, g_loss: 3.2932262420654297\n","Epoch 67/2000, Step 47, d_loss: 0.41421470046043396, g_loss: 3.8039801120758057\n","Epoch 67/2000, Step 48, d_loss: 0.40986549854278564, g_loss: 3.6643407344818115\n","Epoch 67/2000, Step 49, d_loss: 0.4190104901790619, g_loss: 4.540837287902832\n","Epoch 67/2000, Step 50, d_loss: 0.3928883373737335, g_loss: 3.430140495300293\n","Epoch 67/2000, Step 51, d_loss: 0.3558812737464905, g_loss: 5.717728137969971\n","Epoch 67/2000, Step 52, d_loss: 0.44407686591148376, g_loss: 5.411473751068115\n","Epoch 67/2000, Step 53, d_loss: 0.37545323371887207, g_loss: 4.192901134490967\n","Epoch 67/2000, Step 54, d_loss: 0.3970714211463928, g_loss: 4.300898551940918\n","Epoch 67/2000, Step 55, d_loss: 0.3594987988471985, g_loss: 3.7219271659851074\n","Epoch 67/2000, Step 56, d_loss: 0.3936181366443634, g_loss: 4.3546576499938965\n","Epoch 67/2000, Step 57, d_loss: 0.42075884342193604, g_loss: 3.881526470184326\n","Epoch 67/2000, Step 58, d_loss: 0.4380459785461426, g_loss: 4.154181480407715\n","Epoch 67/2000, Step 59, d_loss: 0.40980714559555054, g_loss: 3.5750441551208496\n","Epoch 67/2000, Step 60, d_loss: 0.37294161319732666, g_loss: 3.704620599746704\n","Epoch 67/2000, Step 61, d_loss: 0.3669118583202362, g_loss: 2.994889497756958\n","Epoch 67/2000, Step 62, d_loss: 0.39105090498924255, g_loss: 4.1450605392456055\n","Epoch 67/2000, Step 63, d_loss: 0.3886975646018982, g_loss: 3.855659246444702\n","Epoch 67/2000, Step 64, d_loss: 0.39245185256004333, g_loss: 3.9330196380615234\n","Epoch 67/2000, Step 65, d_loss: 0.37893643975257874, g_loss: 5.640960693359375\n","Epoch 67/2000, Step 66, d_loss: 0.38199758529663086, g_loss: 4.756130218505859\n","Epoch 67/2000, Step 67, d_loss: 0.35948073863983154, g_loss: 4.919258117675781\n","Epoch 67/2000, Step 68, d_loss: 0.5153664946556091, g_loss: 5.07480525970459\n","Epoch 67/2000, Step 69, d_loss: 0.3559390902519226, g_loss: 3.8014421463012695\n","Epoch 67/2000, Step 70, d_loss: 0.3742627799510956, g_loss: 3.1798200607299805\n","Epoch 67/2000, Step 71, d_loss: 0.4022235870361328, g_loss: 3.294560194015503\n","Epoch 67/2000, Step 72, d_loss: 0.39531439542770386, g_loss: 3.2517940998077393\n","Epoch 67/2000, Step 73, d_loss: 0.4437479078769684, g_loss: 4.532892227172852\n","Epoch 67/2000, Step 74, d_loss: 0.365694135427475, g_loss: 3.2233529090881348\n","Epoch 67/2000, Step 75, d_loss: 0.3679264485836029, g_loss: 3.933940887451172\n","Epoch 67/2000, Step 76, d_loss: 0.3518125116825104, g_loss: 5.063957214355469\n","Epoch 67/2000, Step 77, d_loss: 0.3569490611553192, g_loss: 5.143051624298096\n","Epoch 67/2000, Step 78, d_loss: 0.3528149724006653, g_loss: 4.571991443634033\n","Epoch 67/2000, Step 79, d_loss: 0.36916935443878174, g_loss: 4.403524398803711\n","Epoch 67/2000, Step 80, d_loss: 0.49036353826522827, g_loss: 5.681325435638428\n","Epoch 67/2000, Step 81, d_loss: 0.3759068548679352, g_loss: 5.208602428436279\n","Epoch 67/2000, Step 82, d_loss: 0.44549959897994995, g_loss: 2.8134360313415527\n","Epoch 67/2000, Step 83, d_loss: 0.44081318378448486, g_loss: 2.4562437534332275\n","Epoch 67/2000, Step 84, d_loss: 0.5188949704170227, g_loss: 2.689418077468872\n","Epoch 67/2000, Step 85, d_loss: 0.46850407123565674, g_loss: 2.9662492275238037\n","Epoch 67/2000, Step 86, d_loss: 0.533699631690979, g_loss: 4.707696914672852\n","Epoch 67/2000, Step 87, d_loss: 0.3890690505504608, g_loss: 5.395785331726074\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 68/2000, Step 1, d_loss: 0.5112120509147644, g_loss: 5.027080059051514\n","Epoch 68/2000, Step 2, d_loss: 0.357479065656662, g_loss: 3.994492769241333\n","Epoch 68/2000, Step 3, d_loss: 0.35478711128234863, g_loss: 5.153683185577393\n","Epoch 68/2000, Step 4, d_loss: 0.35255593061447144, g_loss: 4.672924518585205\n","Epoch 68/2000, Step 5, d_loss: 0.4340091049671173, g_loss: 5.084765434265137\n","Epoch 68/2000, Step 6, d_loss: 0.402920126914978, g_loss: 3.737032413482666\n","Epoch 68/2000, Step 7, d_loss: 0.3744086027145386, g_loss: 3.70216703414917\n","Epoch 68/2000, Step 8, d_loss: 0.40340253710746765, g_loss: 2.814197063446045\n","Epoch 68/2000, Step 9, d_loss: 0.43248090147972107, g_loss: 3.01115083694458\n","Epoch 68/2000, Step 10, d_loss: 0.5310535430908203, g_loss: 2.6448657512664795\n","Epoch 68/2000, Step 11, d_loss: 0.382513165473938, g_loss: 3.8879668712615967\n","Epoch 68/2000, Step 12, d_loss: 0.3914768397808075, g_loss: 4.52681827545166\n","Epoch 68/2000, Step 13, d_loss: 0.35957571864128113, g_loss: 4.123754501342773\n","Epoch 68/2000, Step 14, d_loss: 0.4280591607093811, g_loss: 5.2829413414001465\n","Epoch 68/2000, Step 15, d_loss: 0.3516777753829956, g_loss: 4.380985260009766\n","Epoch 68/2000, Step 16, d_loss: 0.37969884276390076, g_loss: 4.353626251220703\n","Epoch 68/2000, Step 17, d_loss: 0.3957791030406952, g_loss: 3.44453763961792\n","Epoch 68/2000, Step 18, d_loss: 0.41175377368927, g_loss: 4.052390098571777\n","Epoch 68/2000, Step 19, d_loss: 0.35851043462753296, g_loss: 3.982656478881836\n","Epoch 68/2000, Step 20, d_loss: 0.420226126909256, g_loss: 3.428828716278076\n","Epoch 68/2000, Step 21, d_loss: 0.4332239627838135, g_loss: 2.7969040870666504\n","Epoch 68/2000, Step 22, d_loss: 0.3873160183429718, g_loss: 2.6440212726593018\n","Epoch 68/2000, Step 23, d_loss: 0.3942720890045166, g_loss: 3.9975340366363525\n","Epoch 68/2000, Step 24, d_loss: 0.39785444736480713, g_loss: 3.803401231765747\n","Epoch 68/2000, Step 25, d_loss: 0.40232667326927185, g_loss: 5.257720947265625\n","Epoch 68/2000, Step 26, d_loss: 0.3580496311187744, g_loss: 4.060166358947754\n","Epoch 68/2000, Step 27, d_loss: 0.36004072427749634, g_loss: 4.193136215209961\n","Epoch 68/2000, Step 28, d_loss: 0.372226357460022, g_loss: 4.233109474182129\n","Epoch 68/2000, Step 29, d_loss: 0.47652024030685425, g_loss: 3.773707628250122\n","Epoch 68/2000, Step 30, d_loss: 0.3652719259262085, g_loss: 4.254786968231201\n","Epoch 68/2000, Step 31, d_loss: 0.36166030168533325, g_loss: 3.291398525238037\n","Epoch 68/2000, Step 32, d_loss: 0.37483906745910645, g_loss: 3.9618303775787354\n","Epoch 68/2000, Step 33, d_loss: 0.7274200916290283, g_loss: 4.050638198852539\n","Epoch 68/2000, Step 34, d_loss: 0.41431790590286255, g_loss: 3.6300058364868164\n","Epoch 68/2000, Step 35, d_loss: 0.4549950957298279, g_loss: 3.1974997520446777\n","Epoch 68/2000, Step 36, d_loss: 0.4047423005104065, g_loss: 3.669494867324829\n","Epoch 68/2000, Step 37, d_loss: 0.46264559030532837, g_loss: 3.5265257358551025\n","Epoch 68/2000, Step 38, d_loss: 0.45836126804351807, g_loss: 3.989468812942505\n","Epoch 68/2000, Step 39, d_loss: 0.4074590802192688, g_loss: 4.092050552368164\n","Epoch 68/2000, Step 40, d_loss: 0.39918023347854614, g_loss: 3.634464740753174\n","Epoch 68/2000, Step 41, d_loss: 0.3722192943096161, g_loss: 4.482335090637207\n","Epoch 68/2000, Step 42, d_loss: 0.4296708106994629, g_loss: 3.995565176010132\n","Epoch 68/2000, Step 43, d_loss: 0.43960118293762207, g_loss: 4.4298224449157715\n","Epoch 68/2000, Step 44, d_loss: 0.38758957386016846, g_loss: 3.4287912845611572\n","Epoch 68/2000, Step 45, d_loss: 0.3619826138019562, g_loss: 4.182528495788574\n","Epoch 68/2000, Step 46, d_loss: 0.5534366965293884, g_loss: 3.447923421859741\n","Epoch 68/2000, Step 47, d_loss: 0.3941326439380646, g_loss: 3.5279929637908936\n","Epoch 68/2000, Step 48, d_loss: 0.4006076753139496, g_loss: 4.2591094970703125\n","Epoch 68/2000, Step 49, d_loss: 0.44030889868736267, g_loss: 2.99902081489563\n","Epoch 68/2000, Step 50, d_loss: 0.46718981862068176, g_loss: 2.922367811203003\n","Epoch 68/2000, Step 51, d_loss: 0.4108006954193115, g_loss: 3.630310297012329\n","Epoch 68/2000, Step 52, d_loss: 0.3619479537010193, g_loss: 3.5032005310058594\n","Epoch 68/2000, Step 53, d_loss: 0.3940313160419464, g_loss: 4.935977935791016\n","Epoch 68/2000, Step 54, d_loss: 0.369203120470047, g_loss: 3.8666505813598633\n","Epoch 68/2000, Step 55, d_loss: 0.4773174822330475, g_loss: 4.6028618812561035\n","Epoch 68/2000, Step 56, d_loss: 0.41076165437698364, g_loss: 3.3929805755615234\n","Epoch 68/2000, Step 57, d_loss: 0.3762359619140625, g_loss: 3.8689703941345215\n","Epoch 68/2000, Step 58, d_loss: 0.4097982347011566, g_loss: 3.6207950115203857\n","Epoch 68/2000, Step 59, d_loss: 0.3670576512813568, g_loss: 3.078688383102417\n","Epoch 68/2000, Step 60, d_loss: 0.427122563123703, g_loss: 2.7704081535339355\n","Epoch 68/2000, Step 61, d_loss: 0.44563716650009155, g_loss: 2.6741530895233154\n","Epoch 68/2000, Step 62, d_loss: 0.4431355893611908, g_loss: 3.4163877964019775\n","Epoch 68/2000, Step 63, d_loss: 0.37984099984169006, g_loss: 3.8006296157836914\n","Epoch 68/2000, Step 64, d_loss: 0.40115129947662354, g_loss: 4.406539440155029\n","Epoch 68/2000, Step 65, d_loss: 0.3753943145275116, g_loss: 4.755366802215576\n","Epoch 68/2000, Step 66, d_loss: 0.41041088104248047, g_loss: 5.6852946281433105\n","Epoch 68/2000, Step 67, d_loss: 0.35932716727256775, g_loss: 4.49900484085083\n","Epoch 68/2000, Step 68, d_loss: 0.3570183515548706, g_loss: 4.183667182922363\n","Epoch 68/2000, Step 69, d_loss: 0.39729398488998413, g_loss: 4.245534896850586\n","Epoch 68/2000, Step 70, d_loss: 0.40957942605018616, g_loss: 4.12757682800293\n","Epoch 68/2000, Step 71, d_loss: 0.414883017539978, g_loss: 3.137368679046631\n","Epoch 68/2000, Step 72, d_loss: 0.3830201327800751, g_loss: 3.2184979915618896\n","Epoch 68/2000, Step 73, d_loss: 0.37696200609207153, g_loss: 3.027740240097046\n","Epoch 68/2000, Step 74, d_loss: 0.4248524308204651, g_loss: 4.230286598205566\n","Epoch 68/2000, Step 75, d_loss: 0.39120861887931824, g_loss: 3.410947561264038\n","Epoch 68/2000, Step 76, d_loss: 0.3963143825531006, g_loss: 4.320071697235107\n","Epoch 68/2000, Step 77, d_loss: 0.34926822781562805, g_loss: 4.588331699371338\n","Epoch 68/2000, Step 78, d_loss: 0.35240721702575684, g_loss: 4.435494422912598\n","Epoch 68/2000, Step 79, d_loss: 0.3419342041015625, g_loss: 5.172937393188477\n","Epoch 68/2000, Step 80, d_loss: 0.40116697549819946, g_loss: 5.701586723327637\n","Epoch 68/2000, Step 81, d_loss: 0.5225251913070679, g_loss: 3.899735689163208\n","Epoch 68/2000, Step 82, d_loss: 0.37539374828338623, g_loss: 3.37636399269104\n","Epoch 68/2000, Step 83, d_loss: 0.3719504475593567, g_loss: 3.358834743499756\n","Epoch 68/2000, Step 84, d_loss: 0.37140095233917236, g_loss: 2.5207276344299316\n","Epoch 68/2000, Step 85, d_loss: 0.43987515568733215, g_loss: 3.663301467895508\n","Epoch 68/2000, Step 86, d_loss: 0.43654343485832214, g_loss: 3.8759877681732178\n","Epoch 68/2000, Step 87, d_loss: 0.423263281583786, g_loss: 4.193873405456543\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 69/2000, Step 1, d_loss: 0.3543161451816559, g_loss: 4.669961929321289\n","Epoch 69/2000, Step 2, d_loss: 0.3702591061592102, g_loss: 5.084573745727539\n","Epoch 69/2000, Step 3, d_loss: 0.3643878400325775, g_loss: 4.54215669631958\n","Epoch 69/2000, Step 4, d_loss: 0.44803115725517273, g_loss: 3.307565689086914\n","Epoch 69/2000, Step 5, d_loss: 0.6432415843009949, g_loss: 4.38420295715332\n","Epoch 69/2000, Step 6, d_loss: 0.3865474760532379, g_loss: 2.6509151458740234\n","Epoch 69/2000, Step 7, d_loss: 0.5248555541038513, g_loss: 2.6888670921325684\n","Epoch 69/2000, Step 8, d_loss: 0.49136993288993835, g_loss: 2.571145534515381\n","Epoch 69/2000, Step 9, d_loss: 0.5097910165786743, g_loss: 2.805088758468628\n","Epoch 69/2000, Step 10, d_loss: 0.3978809714317322, g_loss: 4.382513046264648\n","Epoch 69/2000, Step 11, d_loss: 0.3688068985939026, g_loss: 4.7274627685546875\n","Epoch 69/2000, Step 12, d_loss: 0.38439053297042847, g_loss: 5.6501054763793945\n","Epoch 69/2000, Step 13, d_loss: 0.5631685256958008, g_loss: 5.656496524810791\n","Epoch 69/2000, Step 14, d_loss: 0.48536747694015503, g_loss: 4.9518141746521\n","Epoch 69/2000, Step 15, d_loss: 0.37627702951431274, g_loss: 3.0790774822235107\n","Epoch 69/2000, Step 16, d_loss: 0.4399017095565796, g_loss: 3.5185132026672363\n","Epoch 69/2000, Step 17, d_loss: 0.5114989280700684, g_loss: 3.258852481842041\n","Epoch 69/2000, Step 18, d_loss: 0.482946515083313, g_loss: 2.0309412479400635\n","Epoch 69/2000, Step 19, d_loss: 0.5089196562767029, g_loss: 5.909229278564453\n","Epoch 69/2000, Step 20, d_loss: 0.3713585138320923, g_loss: 3.786898136138916\n","Epoch 69/2000, Step 21, d_loss: 0.3931981921195984, g_loss: 4.840847015380859\n","Epoch 69/2000, Step 22, d_loss: 0.40868401527404785, g_loss: 4.557406902313232\n","Epoch 69/2000, Step 23, d_loss: 0.38257700204849243, g_loss: 4.897093772888184\n","Epoch 69/2000, Step 24, d_loss: 0.38588854670524597, g_loss: 4.3176984786987305\n","Epoch 69/2000, Step 25, d_loss: 0.3845229744911194, g_loss: 3.9414162635803223\n","Epoch 69/2000, Step 26, d_loss: 0.43640151619911194, g_loss: 4.003758430480957\n","Epoch 69/2000, Step 27, d_loss: 0.4036235511302948, g_loss: 3.444932699203491\n","Epoch 69/2000, Step 28, d_loss: 0.36568549275398254, g_loss: 3.021308422088623\n","Epoch 69/2000, Step 29, d_loss: 0.3708159327507019, g_loss: 3.409383773803711\n","Epoch 69/2000, Step 30, d_loss: 0.4089939594268799, g_loss: 3.8296258449554443\n","Epoch 69/2000, Step 31, d_loss: 0.5473499298095703, g_loss: 4.106335639953613\n","Epoch 69/2000, Step 32, d_loss: 0.46073225140571594, g_loss: 3.432528257369995\n","Epoch 69/2000, Step 33, d_loss: 0.4456370770931244, g_loss: 3.3517518043518066\n","Epoch 69/2000, Step 34, d_loss: 0.5162579417228699, g_loss: 3.787280559539795\n","Epoch 69/2000, Step 35, d_loss: 0.43362751603126526, g_loss: 2.955035924911499\n","Epoch 69/2000, Step 36, d_loss: 0.3830481469631195, g_loss: 5.182063102722168\n","Epoch 69/2000, Step 37, d_loss: 0.39899536967277527, g_loss: 4.044102191925049\n","Epoch 69/2000, Step 38, d_loss: 0.39674437046051025, g_loss: 5.135511875152588\n","Epoch 69/2000, Step 39, d_loss: 0.37364301085472107, g_loss: 3.8708765506744385\n","Epoch 69/2000, Step 40, d_loss: 0.3520262837409973, g_loss: 5.301575183868408\n","Epoch 69/2000, Step 41, d_loss: 0.36341536045074463, g_loss: 5.4316253662109375\n","Epoch 69/2000, Step 42, d_loss: 0.4616023302078247, g_loss: 4.72243595123291\n","Epoch 69/2000, Step 43, d_loss: 0.4213312268257141, g_loss: 3.653667688369751\n","Epoch 69/2000, Step 44, d_loss: 0.37308794260025024, g_loss: 5.948119163513184\n","Epoch 69/2000, Step 45, d_loss: 0.3824210464954376, g_loss: 3.695075273513794\n","Epoch 69/2000, Step 46, d_loss: 0.399094820022583, g_loss: 3.4187095165252686\n","Epoch 69/2000, Step 47, d_loss: 0.43645426630973816, g_loss: 3.3962011337280273\n","Epoch 69/2000, Step 48, d_loss: 0.38769203424453735, g_loss: 3.648230791091919\n","Epoch 69/2000, Step 49, d_loss: 0.3923293650150299, g_loss: 3.934239149093628\n","Epoch 69/2000, Step 50, d_loss: 0.40399137139320374, g_loss: 4.199729919433594\n","Epoch 69/2000, Step 51, d_loss: 0.46738937497138977, g_loss: 4.389323711395264\n","Epoch 69/2000, Step 52, d_loss: 0.36001092195510864, g_loss: 3.72784686088562\n","Epoch 69/2000, Step 53, d_loss: 0.3558296859264374, g_loss: 6.228546619415283\n","Epoch 69/2000, Step 54, d_loss: 0.37870433926582336, g_loss: 4.233977317810059\n","Epoch 69/2000, Step 55, d_loss: 0.42296770215034485, g_loss: 5.164352893829346\n","Epoch 69/2000, Step 56, d_loss: 0.5817406177520752, g_loss: 4.125588893890381\n","Epoch 69/2000, Step 57, d_loss: 0.409054696559906, g_loss: 4.515468597412109\n","Epoch 69/2000, Step 58, d_loss: 0.44814613461494446, g_loss: 3.6700472831726074\n","Epoch 69/2000, Step 59, d_loss: 0.4236508309841156, g_loss: 4.383084774017334\n","Epoch 69/2000, Step 60, d_loss: 0.39909327030181885, g_loss: 3.295769214630127\n","Epoch 69/2000, Step 61, d_loss: 0.41268613934516907, g_loss: 3.7642064094543457\n","Epoch 69/2000, Step 62, d_loss: 0.41616106033325195, g_loss: 3.6505894660949707\n","Epoch 69/2000, Step 63, d_loss: 0.3919852674007416, g_loss: 4.600436210632324\n","Epoch 69/2000, Step 64, d_loss: 0.41629326343536377, g_loss: 5.30842924118042\n","Epoch 69/2000, Step 65, d_loss: 0.375735878944397, g_loss: 4.455532073974609\n","Epoch 69/2000, Step 66, d_loss: 0.4108533263206482, g_loss: 4.254423141479492\n","Epoch 69/2000, Step 67, d_loss: 0.4241429567337036, g_loss: 3.9883804321289062\n","Epoch 69/2000, Step 68, d_loss: 0.3636085093021393, g_loss: 4.2200398445129395\n","Epoch 69/2000, Step 69, d_loss: 0.41780516505241394, g_loss: 5.117424964904785\n","Epoch 69/2000, Step 70, d_loss: 0.458415687084198, g_loss: 2.654323101043701\n","Epoch 69/2000, Step 71, d_loss: 0.3830198645591736, g_loss: 3.0612339973449707\n","Epoch 69/2000, Step 72, d_loss: 0.41150471568107605, g_loss: 3.7345800399780273\n","Epoch 69/2000, Step 73, d_loss: 0.3775917887687683, g_loss: 3.526668071746826\n","Epoch 69/2000, Step 74, d_loss: 0.4435332119464874, g_loss: 3.9527807235717773\n","Epoch 69/2000, Step 75, d_loss: 0.5530413389205933, g_loss: 4.1004462242126465\n","Epoch 69/2000, Step 76, d_loss: 0.38504624366760254, g_loss: 3.9812960624694824\n","Epoch 69/2000, Step 77, d_loss: 0.4674077033996582, g_loss: 2.839717149734497\n","Epoch 69/2000, Step 78, d_loss: 0.46772465109825134, g_loss: 3.425767183303833\n","Epoch 69/2000, Step 79, d_loss: 0.4514191150665283, g_loss: 2.622572183609009\n","Epoch 69/2000, Step 80, d_loss: 0.4456356167793274, g_loss: 2.925288677215576\n","Epoch 69/2000, Step 81, d_loss: 0.4005286693572998, g_loss: 4.230503559112549\n","Epoch 69/2000, Step 82, d_loss: 0.3824256360530853, g_loss: 4.897922039031982\n","Epoch 69/2000, Step 83, d_loss: 0.3458230793476105, g_loss: 5.475301265716553\n","Epoch 69/2000, Step 84, d_loss: 0.662540853023529, g_loss: 5.298304080963135\n","Epoch 69/2000, Step 85, d_loss: 0.36540675163269043, g_loss: 4.529821872711182\n","Epoch 69/2000, Step 86, d_loss: 0.42016366124153137, g_loss: 3.6019833087921143\n","Epoch 69/2000, Step 87, d_loss: 0.3865811824798584, g_loss: 3.5967538356781006\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 70/2000, Step 1, d_loss: 0.47806262969970703, g_loss: 3.2460503578186035\n","Epoch 70/2000, Step 2, d_loss: 0.46947717666625977, g_loss: 2.3802883625030518\n","Epoch 70/2000, Step 3, d_loss: 0.5017054677009583, g_loss: 2.9097900390625\n","Epoch 70/2000, Step 4, d_loss: 0.44495147466659546, g_loss: 3.358795404434204\n","Epoch 70/2000, Step 5, d_loss: 0.3851310908794403, g_loss: 4.23569917678833\n","Epoch 70/2000, Step 6, d_loss: 0.43389299511909485, g_loss: 4.292234420776367\n","Epoch 70/2000, Step 7, d_loss: 0.37103602290153503, g_loss: 5.3017449378967285\n","Epoch 70/2000, Step 8, d_loss: 0.4121364653110504, g_loss: 5.061969757080078\n","Epoch 70/2000, Step 9, d_loss: 0.45472392439842224, g_loss: 4.1486053466796875\n","Epoch 70/2000, Step 10, d_loss: 0.4353867173194885, g_loss: 3.8951516151428223\n","Epoch 70/2000, Step 11, d_loss: 0.388357549905777, g_loss: 2.769963264465332\n","Epoch 70/2000, Step 12, d_loss: 0.44919243454933167, g_loss: 2.7999508380889893\n","Epoch 70/2000, Step 13, d_loss: 0.3999806344509125, g_loss: 2.5505149364471436\n","Epoch 70/2000, Step 14, d_loss: 0.4734615683555603, g_loss: 4.235317707061768\n","Epoch 70/2000, Step 15, d_loss: 0.39092859625816345, g_loss: 3.593019962310791\n","Epoch 70/2000, Step 16, d_loss: 0.39496558904647827, g_loss: 4.392861843109131\n","Epoch 70/2000, Step 17, d_loss: 0.3645835220813751, g_loss: 4.425843238830566\n","Epoch 70/2000, Step 18, d_loss: 0.4090369939804077, g_loss: 4.752324104309082\n","Epoch 70/2000, Step 19, d_loss: 0.3563994765281677, g_loss: 5.090575695037842\n","Epoch 70/2000, Step 20, d_loss: 0.3596115708351135, g_loss: 4.9754252433776855\n","Epoch 70/2000, Step 21, d_loss: 0.48265910148620605, g_loss: 4.485230445861816\n","Epoch 70/2000, Step 22, d_loss: 0.36447155475616455, g_loss: 4.152749061584473\n","Epoch 70/2000, Step 23, d_loss: 0.38913214206695557, g_loss: 3.3618547916412354\n","Epoch 70/2000, Step 24, d_loss: 0.41990095376968384, g_loss: 2.908062696456909\n","Epoch 70/2000, Step 25, d_loss: 0.46158668398857117, g_loss: 3.279170036315918\n","Epoch 70/2000, Step 26, d_loss: 0.42075109481811523, g_loss: 3.687612533569336\n","Epoch 70/2000, Step 27, d_loss: 0.391032874584198, g_loss: 4.540590286254883\n","Epoch 70/2000, Step 28, d_loss: 0.4049948751926422, g_loss: 4.593899726867676\n","Epoch 70/2000, Step 29, d_loss: 0.4156673550605774, g_loss: 4.881756782531738\n","Epoch 70/2000, Step 30, d_loss: 0.4956199526786804, g_loss: 4.526110649108887\n","Epoch 70/2000, Step 31, d_loss: 0.35136324167251587, g_loss: 4.411281585693359\n","Epoch 70/2000, Step 32, d_loss: 0.39226505160331726, g_loss: 3.8477442264556885\n","Epoch 70/2000, Step 33, d_loss: 0.36928293108940125, g_loss: 6.275218963623047\n","Epoch 70/2000, Step 34, d_loss: 0.35570332407951355, g_loss: 4.747696399688721\n","Epoch 70/2000, Step 35, d_loss: 0.3633876442909241, g_loss: 4.606415748596191\n","Epoch 70/2000, Step 36, d_loss: 0.3940926492214203, g_loss: 4.372211933135986\n","Epoch 70/2000, Step 37, d_loss: 0.3509899973869324, g_loss: 3.8961174488067627\n","Epoch 70/2000, Step 38, d_loss: 0.366166353225708, g_loss: 3.895463466644287\n","Epoch 70/2000, Step 39, d_loss: 0.4120580554008484, g_loss: 3.997812271118164\n","Epoch 70/2000, Step 40, d_loss: 0.49217891693115234, g_loss: 3.4941165447235107\n","Epoch 70/2000, Step 41, d_loss: 0.36465010046958923, g_loss: 3.5841243267059326\n","Epoch 70/2000, Step 42, d_loss: 0.42704057693481445, g_loss: 3.4731125831604004\n","Epoch 70/2000, Step 43, d_loss: 0.45402586460113525, g_loss: 3.9736459255218506\n","Epoch 70/2000, Step 44, d_loss: 0.44879287481307983, g_loss: 3.75203013420105\n","Epoch 70/2000, Step 45, d_loss: 0.37867480516433716, g_loss: 3.626716136932373\n","Epoch 70/2000, Step 46, d_loss: 0.4045814573764801, g_loss: 3.821251392364502\n","Epoch 70/2000, Step 47, d_loss: 0.3864104747772217, g_loss: 4.346133708953857\n","Epoch 70/2000, Step 48, d_loss: 0.4321296215057373, g_loss: 4.203696250915527\n","Epoch 70/2000, Step 49, d_loss: 0.36316442489624023, g_loss: 6.0806169509887695\n","Epoch 70/2000, Step 50, d_loss: 0.4124796986579895, g_loss: 5.257565498352051\n","Epoch 70/2000, Step 51, d_loss: 0.38342100381851196, g_loss: 4.955740928649902\n","Epoch 70/2000, Step 52, d_loss: 0.37220263481140137, g_loss: 4.867702007293701\n","Epoch 70/2000, Step 53, d_loss: 0.41109582781791687, g_loss: 4.517210960388184\n","Epoch 70/2000, Step 54, d_loss: 0.37000572681427, g_loss: 3.973555326461792\n","Epoch 70/2000, Step 55, d_loss: 0.356700599193573, g_loss: 4.473999977111816\n","Epoch 70/2000, Step 56, d_loss: 0.36278682947158813, g_loss: 3.089369297027588\n","Epoch 70/2000, Step 57, d_loss: 0.3740035593509674, g_loss: 3.4122366905212402\n","Epoch 70/2000, Step 58, d_loss: 0.4062730073928833, g_loss: 3.392353057861328\n","Epoch 70/2000, Step 59, d_loss: 0.3800939619541168, g_loss: 3.4398462772369385\n","Epoch 70/2000, Step 60, d_loss: 0.40848878026008606, g_loss: 5.698503494262695\n","Epoch 70/2000, Step 61, d_loss: 0.37742817401885986, g_loss: 4.086653709411621\n","Epoch 70/2000, Step 62, d_loss: 0.3727200925350189, g_loss: 4.567309856414795\n","Epoch 70/2000, Step 63, d_loss: 0.39407262206077576, g_loss: 5.351802349090576\n","Epoch 70/2000, Step 64, d_loss: 0.3993905186653137, g_loss: 5.493918418884277\n","Epoch 70/2000, Step 65, d_loss: 0.4375930428504944, g_loss: 5.177062034606934\n","Epoch 70/2000, Step 66, d_loss: 0.359500914812088, g_loss: 4.577382564544678\n","Epoch 70/2000, Step 67, d_loss: 0.37324485182762146, g_loss: 4.202040672302246\n","Epoch 70/2000, Step 68, d_loss: 0.37930190563201904, g_loss: 3.645134210586548\n","Epoch 70/2000, Step 69, d_loss: 0.39127790927886963, g_loss: 3.4524953365325928\n","Epoch 70/2000, Step 70, d_loss: 0.4079092741012573, g_loss: 4.151664733886719\n","Epoch 70/2000, Step 71, d_loss: 0.3770751953125, g_loss: 3.7581825256347656\n","Epoch 70/2000, Step 72, d_loss: 0.38871338963508606, g_loss: 3.289942979812622\n","Epoch 70/2000, Step 73, d_loss: 0.45216092467308044, g_loss: 5.184446334838867\n","Epoch 70/2000, Step 74, d_loss: 0.40187889337539673, g_loss: 3.629729986190796\n","Epoch 70/2000, Step 75, d_loss: 0.41991376876831055, g_loss: 4.258213996887207\n","Epoch 70/2000, Step 76, d_loss: 0.409235417842865, g_loss: 6.760375022888184\n","Epoch 70/2000, Step 77, d_loss: 0.39854902029037476, g_loss: 4.112753868103027\n","Epoch 70/2000, Step 78, d_loss: 0.4449000656604767, g_loss: 3.6826906204223633\n","Epoch 70/2000, Step 79, d_loss: 0.41134464740753174, g_loss: 3.3209686279296875\n","Epoch 70/2000, Step 80, d_loss: 0.3984231650829315, g_loss: 2.9140872955322266\n","Epoch 70/2000, Step 81, d_loss: 0.4207894206047058, g_loss: 3.2630224227905273\n","Epoch 70/2000, Step 82, d_loss: 0.4020341634750366, g_loss: 3.561516284942627\n","Epoch 70/2000, Step 83, d_loss: 0.38027122616767883, g_loss: 4.4429216384887695\n","Epoch 70/2000, Step 84, d_loss: 0.3991398215293884, g_loss: 4.000460624694824\n","Epoch 70/2000, Step 85, d_loss: 0.433156281709671, g_loss: 4.242255210876465\n","Epoch 70/2000, Step 86, d_loss: 0.42547428607940674, g_loss: 4.295244216918945\n","Epoch 70/2000, Step 87, d_loss: 0.3794451057910919, g_loss: 4.065002918243408\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 71/2000, Step 1, d_loss: 0.4144919812679291, g_loss: 3.86938738822937\n","Epoch 71/2000, Step 2, d_loss: 0.3757021725177765, g_loss: 3.5910861492156982\n","Epoch 71/2000, Step 3, d_loss: 0.39579594135284424, g_loss: 4.241152763366699\n","Epoch 71/2000, Step 4, d_loss: 0.37592241168022156, g_loss: 4.38037633895874\n","Epoch 71/2000, Step 5, d_loss: 0.3855423927307129, g_loss: 4.1132493019104\n","Epoch 71/2000, Step 6, d_loss: 0.4563310444355011, g_loss: 4.240107536315918\n","Epoch 71/2000, Step 7, d_loss: 0.3561384975910187, g_loss: 4.885284423828125\n","Epoch 71/2000, Step 8, d_loss: 0.3650607764720917, g_loss: 4.313445568084717\n","Epoch 71/2000, Step 9, d_loss: 0.3733398914337158, g_loss: 5.61618709564209\n","Epoch 71/2000, Step 10, d_loss: 0.39232534170150757, g_loss: 4.441553115844727\n","Epoch 71/2000, Step 11, d_loss: 0.37219586968421936, g_loss: 5.653506755828857\n","Epoch 71/2000, Step 12, d_loss: 0.36137738823890686, g_loss: 4.855249881744385\n","Epoch 71/2000, Step 13, d_loss: 0.3899204730987549, g_loss: 4.026015758514404\n","Epoch 71/2000, Step 14, d_loss: 0.3771620988845825, g_loss: 4.311990737915039\n","Epoch 71/2000, Step 15, d_loss: 0.3649422526359558, g_loss: 4.560761451721191\n","Epoch 71/2000, Step 16, d_loss: 0.371330589056015, g_loss: 3.9526097774505615\n","Epoch 71/2000, Step 17, d_loss: 0.4666973948478699, g_loss: 3.7344470024108887\n","Epoch 71/2000, Step 18, d_loss: 0.35603103041648865, g_loss: 4.109732151031494\n","Epoch 71/2000, Step 19, d_loss: 0.39100512862205505, g_loss: 4.913386821746826\n","Epoch 71/2000, Step 20, d_loss: 0.36141330003738403, g_loss: 4.793532848358154\n","Epoch 71/2000, Step 21, d_loss: 0.3771269917488098, g_loss: 5.239465713500977\n","Epoch 71/2000, Step 22, d_loss: 0.38763996958732605, g_loss: 4.300576686859131\n","Epoch 71/2000, Step 23, d_loss: 0.44892096519470215, g_loss: 4.891230583190918\n","Epoch 71/2000, Step 24, d_loss: 0.3790457844734192, g_loss: 4.9783616065979\n","Epoch 71/2000, Step 25, d_loss: 0.3570425808429718, g_loss: 3.7653799057006836\n","Epoch 71/2000, Step 26, d_loss: 0.3976686894893646, g_loss: 4.197704792022705\n","Epoch 71/2000, Step 27, d_loss: 0.3742394745349884, g_loss: 3.9343433380126953\n","Epoch 71/2000, Step 28, d_loss: 0.37383517622947693, g_loss: 4.408624649047852\n","Epoch 71/2000, Step 29, d_loss: 0.3796226680278778, g_loss: 3.984818935394287\n","Epoch 71/2000, Step 30, d_loss: 0.3741697371006012, g_loss: 4.499242305755615\n","Epoch 71/2000, Step 31, d_loss: 0.35373759269714355, g_loss: 5.059493541717529\n","Epoch 71/2000, Step 32, d_loss: 0.3683820068836212, g_loss: 4.587181568145752\n","Epoch 71/2000, Step 33, d_loss: 0.408274382352829, g_loss: 4.401666641235352\n","Epoch 71/2000, Step 34, d_loss: 0.38735106587409973, g_loss: 4.0181427001953125\n","Epoch 71/2000, Step 35, d_loss: 0.425335168838501, g_loss: 3.787766933441162\n","Epoch 71/2000, Step 36, d_loss: 0.3757246136665344, g_loss: 4.18617582321167\n","Epoch 71/2000, Step 37, d_loss: 0.43731430172920227, g_loss: 2.874098539352417\n","Epoch 71/2000, Step 38, d_loss: 0.4731115996837616, g_loss: 3.1219122409820557\n","Epoch 71/2000, Step 39, d_loss: 0.44507452845573425, g_loss: 4.121563911437988\n","Epoch 71/2000, Step 40, d_loss: 0.41137856245040894, g_loss: 4.8839545249938965\n","Epoch 71/2000, Step 41, d_loss: 0.38838180899620056, g_loss: 4.9505295753479\n","Epoch 71/2000, Step 42, d_loss: 0.4029856026172638, g_loss: 5.735511302947998\n","Epoch 71/2000, Step 43, d_loss: 0.366688996553421, g_loss: 4.345099925994873\n","Epoch 71/2000, Step 44, d_loss: 0.4105084538459778, g_loss: 5.102143287658691\n","Epoch 71/2000, Step 45, d_loss: 0.4423312842845917, g_loss: 4.82629919052124\n","Epoch 71/2000, Step 46, d_loss: 0.35252466797828674, g_loss: 4.077193737030029\n","Epoch 71/2000, Step 47, d_loss: 0.3838959038257599, g_loss: 3.4330971240997314\n","Epoch 71/2000, Step 48, d_loss: 0.4115562438964844, g_loss: 3.7870914936065674\n","Epoch 71/2000, Step 49, d_loss: 0.4722338914871216, g_loss: 5.04017448425293\n","Epoch 71/2000, Step 50, d_loss: 0.3771320581436157, g_loss: 4.569166660308838\n","Epoch 71/2000, Step 51, d_loss: 0.37066760659217834, g_loss: 4.702869415283203\n","Epoch 71/2000, Step 52, d_loss: 0.3660324513912201, g_loss: 4.345228672027588\n","Epoch 71/2000, Step 53, d_loss: 0.4028378129005432, g_loss: 5.216838359832764\n","Epoch 71/2000, Step 54, d_loss: 0.3701761066913605, g_loss: 4.969182968139648\n","Epoch 71/2000, Step 55, d_loss: 0.43416059017181396, g_loss: 3.721283435821533\n","Epoch 71/2000, Step 56, d_loss: 0.3792346715927124, g_loss: 3.523852586746216\n","Epoch 71/2000, Step 57, d_loss: 0.41510680317878723, g_loss: 3.5224874019622803\n","Epoch 71/2000, Step 58, d_loss: 0.41258665919303894, g_loss: 3.969632863998413\n","Epoch 71/2000, Step 59, d_loss: 0.4069543778896332, g_loss: 3.002322196960449\n","Epoch 71/2000, Step 60, d_loss: 0.41604918241500854, g_loss: 3.6073782444000244\n","Epoch 71/2000, Step 61, d_loss: 0.4675403833389282, g_loss: 3.647430658340454\n","Epoch 71/2000, Step 62, d_loss: 0.3557283580303192, g_loss: 3.4661619663238525\n","Epoch 71/2000, Step 63, d_loss: 0.37586328387260437, g_loss: 4.384253978729248\n","Epoch 71/2000, Step 64, d_loss: 0.3646257817745209, g_loss: 4.830657005310059\n","Epoch 71/2000, Step 65, d_loss: 0.34707731008529663, g_loss: 4.643753528594971\n","Epoch 71/2000, Step 66, d_loss: 0.35421425104141235, g_loss: 4.4225172996521\n","Epoch 71/2000, Step 67, d_loss: 0.3864406943321228, g_loss: 4.53872537612915\n","Epoch 71/2000, Step 68, d_loss: 0.3556444048881531, g_loss: 4.184586048126221\n","Epoch 71/2000, Step 69, d_loss: 0.39418405294418335, g_loss: 4.85921573638916\n","Epoch 71/2000, Step 70, d_loss: 0.3715129494667053, g_loss: 4.69216251373291\n","Epoch 71/2000, Step 71, d_loss: 0.35287973284721375, g_loss: 4.1142897605896\n","Epoch 71/2000, Step 72, d_loss: 0.35483741760253906, g_loss: 3.868806838989258\n","Epoch 71/2000, Step 73, d_loss: 0.3886158764362335, g_loss: 4.006775856018066\n","Epoch 71/2000, Step 74, d_loss: 0.3949863612651825, g_loss: 3.807725667953491\n","Epoch 71/2000, Step 75, d_loss: 0.36996161937713623, g_loss: 3.919130325317383\n","Epoch 71/2000, Step 76, d_loss: 0.37036916613578796, g_loss: 4.004361152648926\n","Epoch 71/2000, Step 77, d_loss: 0.3863776624202728, g_loss: 4.285781383514404\n","Epoch 71/2000, Step 78, d_loss: 0.35551753640174866, g_loss: 3.911613941192627\n","Epoch 71/2000, Step 79, d_loss: 0.37036237120628357, g_loss: 4.737850189208984\n","Epoch 71/2000, Step 80, d_loss: 0.4738669991493225, g_loss: 5.29986047744751\n","Epoch 71/2000, Step 81, d_loss: 0.3347161114215851, g_loss: 4.785566329956055\n","Epoch 71/2000, Step 82, d_loss: 0.46811795234680176, g_loss: 3.904571056365967\n","Epoch 71/2000, Step 83, d_loss: 0.3948701322078705, g_loss: 3.6928491592407227\n","Epoch 71/2000, Step 84, d_loss: 0.3739223778247833, g_loss: 4.112086772918701\n","Epoch 71/2000, Step 85, d_loss: 0.3723505735397339, g_loss: 3.8574113845825195\n","Epoch 71/2000, Step 86, d_loss: 0.3920675218105316, g_loss: 3.990609645843506\n","Epoch 71/2000, Step 87, d_loss: 0.36431261897087097, g_loss: 3.402742385864258\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 72/2000, Step 1, d_loss: 0.3695089519023895, g_loss: 4.526829719543457\n","Epoch 72/2000, Step 2, d_loss: 0.3518187701702118, g_loss: 5.266927242279053\n","Epoch 72/2000, Step 3, d_loss: 0.3600434958934784, g_loss: 4.546036720275879\n","Epoch 72/2000, Step 4, d_loss: 0.3512099087238312, g_loss: 4.191250801086426\n","Epoch 72/2000, Step 5, d_loss: 0.40548163652420044, g_loss: 4.200627326965332\n","Epoch 72/2000, Step 6, d_loss: 0.37032487988471985, g_loss: 4.146511077880859\n","Epoch 72/2000, Step 7, d_loss: 0.3494100570678711, g_loss: 4.507066249847412\n","Epoch 72/2000, Step 8, d_loss: 0.3783527612686157, g_loss: 3.833483934402466\n","Epoch 72/2000, Step 9, d_loss: 0.36641985177993774, g_loss: 4.325735092163086\n","Epoch 72/2000, Step 10, d_loss: 0.3878946006298065, g_loss: 3.4694037437438965\n","Epoch 72/2000, Step 11, d_loss: 0.3808605670928955, g_loss: 4.13370418548584\n","Epoch 72/2000, Step 12, d_loss: 0.3491934537887573, g_loss: 4.081541061401367\n","Epoch 72/2000, Step 13, d_loss: 0.3528424799442291, g_loss: 4.250917911529541\n","Epoch 72/2000, Step 14, d_loss: 0.45893561840057373, g_loss: 4.644195556640625\n","Epoch 72/2000, Step 15, d_loss: 0.3598743677139282, g_loss: 4.225823879241943\n","Epoch 72/2000, Step 16, d_loss: 0.38106560707092285, g_loss: 4.412965297698975\n","Epoch 72/2000, Step 17, d_loss: 0.3631925880908966, g_loss: 3.0609514713287354\n","Epoch 72/2000, Step 18, d_loss: 0.3535836935043335, g_loss: 3.868875741958618\n","Epoch 72/2000, Step 19, d_loss: 0.417294979095459, g_loss: 3.1298935413360596\n","Epoch 72/2000, Step 20, d_loss: 0.3963128626346588, g_loss: 4.33774995803833\n","Epoch 72/2000, Step 21, d_loss: 0.3545103669166565, g_loss: 4.53658390045166\n","Epoch 72/2000, Step 22, d_loss: 0.4044041931629181, g_loss: 4.126260757446289\n","Epoch 72/2000, Step 23, d_loss: 0.36694157123565674, g_loss: 5.622744560241699\n","Epoch 72/2000, Step 24, d_loss: 0.34506186842918396, g_loss: 4.002951622009277\n","Epoch 72/2000, Step 25, d_loss: 0.36254021525382996, g_loss: 5.402364730834961\n","Epoch 72/2000, Step 26, d_loss: 0.34869059920310974, g_loss: 4.194008827209473\n","Epoch 72/2000, Step 27, d_loss: 0.3811650574207306, g_loss: 4.203210353851318\n","Epoch 72/2000, Step 28, d_loss: 0.3480835556983948, g_loss: 3.9806509017944336\n","Epoch 72/2000, Step 29, d_loss: 0.35736268758773804, g_loss: 4.340564250946045\n","Epoch 72/2000, Step 30, d_loss: 0.4080338478088379, g_loss: 3.7759647369384766\n","Epoch 72/2000, Step 31, d_loss: 0.3735899031162262, g_loss: 3.452157974243164\n","Epoch 72/2000, Step 32, d_loss: 0.37004736065864563, g_loss: 3.2584216594696045\n","Epoch 72/2000, Step 33, d_loss: 0.35384583473205566, g_loss: 2.966303586959839\n","Epoch 72/2000, Step 34, d_loss: 0.4210018813610077, g_loss: 3.7651734352111816\n","Epoch 72/2000, Step 35, d_loss: 0.3771842122077942, g_loss: 3.9958488941192627\n","Epoch 72/2000, Step 36, d_loss: 0.40487149357795715, g_loss: 3.9828343391418457\n","Epoch 72/2000, Step 37, d_loss: 0.35971423983573914, g_loss: 4.219324588775635\n","Epoch 72/2000, Step 38, d_loss: 0.3616619408130646, g_loss: 4.724348068237305\n","Epoch 72/2000, Step 39, d_loss: 0.41486212611198425, g_loss: 4.2493720054626465\n","Epoch 72/2000, Step 40, d_loss: 0.36870402097702026, g_loss: 4.460806846618652\n","Epoch 72/2000, Step 41, d_loss: 0.3478204607963562, g_loss: 4.516316890716553\n","Epoch 72/2000, Step 42, d_loss: 0.37020567059516907, g_loss: 4.102867126464844\n","Epoch 72/2000, Step 43, d_loss: 0.34992077946662903, g_loss: 3.979268789291382\n","Epoch 72/2000, Step 44, d_loss: 0.3958417475223541, g_loss: 4.149899959564209\n","Epoch 72/2000, Step 45, d_loss: 0.3868478238582611, g_loss: 4.7298054695129395\n","Epoch 72/2000, Step 46, d_loss: 0.36952531337738037, g_loss: 4.087068557739258\n","Epoch 72/2000, Step 47, d_loss: 0.37867069244384766, g_loss: 4.096329212188721\n","Epoch 72/2000, Step 48, d_loss: 0.3599807918071747, g_loss: 4.952974319458008\n","Epoch 72/2000, Step 49, d_loss: 0.3867171108722687, g_loss: 4.148746967315674\n","Epoch 72/2000, Step 50, d_loss: 0.38115718960762024, g_loss: 4.469356060028076\n","Epoch 72/2000, Step 51, d_loss: 0.39302685856819153, g_loss: 4.582790851593018\n","Epoch 72/2000, Step 52, d_loss: 0.3676724433898926, g_loss: 3.37691330909729\n","Epoch 72/2000, Step 53, d_loss: 0.3738233745098114, g_loss: 4.2425384521484375\n","Epoch 72/2000, Step 54, d_loss: 0.3609906733036041, g_loss: 3.5001180171966553\n","Epoch 72/2000, Step 55, d_loss: 0.34805575013160706, g_loss: 4.529760360717773\n","Epoch 72/2000, Step 56, d_loss: 0.3683409094810486, g_loss: 4.096755027770996\n","Epoch 72/2000, Step 57, d_loss: 0.3851653039455414, g_loss: 4.0366621017456055\n","Epoch 72/2000, Step 58, d_loss: 0.3656926453113556, g_loss: 3.98278546333313\n","Epoch 72/2000, Step 59, d_loss: 0.38564154505729675, g_loss: 4.204794406890869\n","Epoch 72/2000, Step 60, d_loss: 0.3583396077156067, g_loss: 4.025513648986816\n","Epoch 72/2000, Step 61, d_loss: 0.3786306083202362, g_loss: 4.113893985748291\n","Epoch 72/2000, Step 62, d_loss: 0.41971009969711304, g_loss: 4.328917503356934\n","Epoch 72/2000, Step 63, d_loss: 0.3777483105659485, g_loss: 4.407510280609131\n","Epoch 72/2000, Step 64, d_loss: 0.39561814069747925, g_loss: 3.6503939628601074\n","Epoch 72/2000, Step 65, d_loss: 0.38560131192207336, g_loss: 4.103975772857666\n","Epoch 72/2000, Step 66, d_loss: 0.3998629152774811, g_loss: 4.635420322418213\n","Epoch 72/2000, Step 67, d_loss: 0.4025728106498718, g_loss: 3.631268262863159\n","Epoch 72/2000, Step 68, d_loss: 0.3810621500015259, g_loss: 4.062798976898193\n","Epoch 72/2000, Step 69, d_loss: 0.37644025683403015, g_loss: 4.452230930328369\n","Epoch 72/2000, Step 70, d_loss: 0.3843217194080353, g_loss: 5.259296417236328\n","Epoch 72/2000, Step 71, d_loss: 0.373094767332077, g_loss: 4.844768524169922\n","Epoch 72/2000, Step 72, d_loss: 0.36315909028053284, g_loss: 4.390511512756348\n","Epoch 72/2000, Step 73, d_loss: 0.3654860258102417, g_loss: 4.270287036895752\n","Epoch 72/2000, Step 74, d_loss: 0.39341214299201965, g_loss: 4.038567543029785\n","Epoch 72/2000, Step 75, d_loss: 0.4102475643157959, g_loss: 3.5712697505950928\n","Epoch 72/2000, Step 76, d_loss: 0.377320796251297, g_loss: 3.595066547393799\n","Epoch 72/2000, Step 77, d_loss: 0.3814221918582916, g_loss: 3.89905047416687\n","Epoch 72/2000, Step 78, d_loss: 0.4060547351837158, g_loss: 3.990515947341919\n","Epoch 72/2000, Step 79, d_loss: 0.3604680299758911, g_loss: 3.4774913787841797\n","Epoch 72/2000, Step 80, d_loss: 0.3668668866157532, g_loss: 4.400301456451416\n","Epoch 72/2000, Step 81, d_loss: 0.404390424489975, g_loss: 4.070257663726807\n","Epoch 72/2000, Step 82, d_loss: 0.3458036482334137, g_loss: 5.067302703857422\n","Epoch 72/2000, Step 83, d_loss: 0.3621769845485687, g_loss: 5.496966361999512\n","Epoch 72/2000, Step 84, d_loss: 0.42292672395706177, g_loss: 4.1174139976501465\n","Epoch 72/2000, Step 85, d_loss: 0.37275782227516174, g_loss: 4.049311637878418\n","Epoch 72/2000, Step 86, d_loss: 0.353363037109375, g_loss: 4.325283527374268\n","Epoch 72/2000, Step 87, d_loss: 0.3614875078201294, g_loss: 3.9284489154815674\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 73/2000, Step 1, d_loss: 0.38385677337646484, g_loss: 3.4929168224334717\n","Epoch 73/2000, Step 2, d_loss: 0.3660643994808197, g_loss: 4.029524326324463\n","Epoch 73/2000, Step 3, d_loss: 0.35988613963127136, g_loss: 5.690134525299072\n","Epoch 73/2000, Step 4, d_loss: 0.3561241626739502, g_loss: 4.874870300292969\n","Epoch 73/2000, Step 5, d_loss: 0.3508816361427307, g_loss: 5.439877510070801\n","Epoch 73/2000, Step 6, d_loss: 0.3612189292907715, g_loss: 5.65289831161499\n","Epoch 73/2000, Step 7, d_loss: 0.38567182421684265, g_loss: 5.007486820220947\n","Epoch 73/2000, Step 8, d_loss: 0.36280256509780884, g_loss: 4.138586521148682\n","Epoch 73/2000, Step 9, d_loss: 0.36498820781707764, g_loss: 4.857203006744385\n","Epoch 73/2000, Step 10, d_loss: 0.38267356157302856, g_loss: 4.84107780456543\n","Epoch 73/2000, Step 11, d_loss: 0.37920066714286804, g_loss: 3.609933614730835\n","Epoch 73/2000, Step 12, d_loss: 0.37741899490356445, g_loss: 3.738030195236206\n","Epoch 73/2000, Step 13, d_loss: 0.3599645793437958, g_loss: 3.258662462234497\n","Epoch 73/2000, Step 14, d_loss: 0.42823636531829834, g_loss: 3.403578996658325\n","Epoch 73/2000, Step 15, d_loss: 0.37860530614852905, g_loss: 3.631930351257324\n","Epoch 73/2000, Step 16, d_loss: 0.3750007450580597, g_loss: 4.151845455169678\n","Epoch 73/2000, Step 17, d_loss: 0.4144545793533325, g_loss: 4.038654327392578\n","Epoch 73/2000, Step 18, d_loss: 0.3628520965576172, g_loss: 5.165323257446289\n","Epoch 73/2000, Step 19, d_loss: 0.36480316519737244, g_loss: 4.848240852355957\n","Epoch 73/2000, Step 20, d_loss: 0.39765045046806335, g_loss: 5.564427852630615\n","Epoch 73/2000, Step 21, d_loss: 0.33562570810317993, g_loss: 4.8423752784729\n","Epoch 73/2000, Step 22, d_loss: 0.3641311526298523, g_loss: 4.792187690734863\n","Epoch 73/2000, Step 23, d_loss: 0.37774884700775146, g_loss: 4.501420497894287\n","Epoch 73/2000, Step 24, d_loss: 0.36160337924957275, g_loss: 4.31528377532959\n","Epoch 73/2000, Step 25, d_loss: 0.3556891977787018, g_loss: 4.174423694610596\n","Epoch 73/2000, Step 26, d_loss: 0.33940696716308594, g_loss: 4.001067161560059\n","Epoch 73/2000, Step 27, d_loss: 0.3717414140701294, g_loss: 4.2118000984191895\n","Epoch 73/2000, Step 28, d_loss: 0.36022642254829407, g_loss: 4.719812393188477\n","Epoch 73/2000, Step 29, d_loss: 0.3712764382362366, g_loss: 4.051207065582275\n","Epoch 73/2000, Step 30, d_loss: 0.3626282215118408, g_loss: 3.892220973968506\n","Epoch 73/2000, Step 31, d_loss: 0.4079434871673584, g_loss: 3.446540355682373\n","Epoch 73/2000, Step 32, d_loss: 0.40106070041656494, g_loss: 3.7651255130767822\n","Epoch 73/2000, Step 33, d_loss: 0.3666936457157135, g_loss: 3.750140905380249\n","Epoch 73/2000, Step 34, d_loss: 0.4307333827018738, g_loss: 3.9201672077178955\n","Epoch 73/2000, Step 35, d_loss: 0.38620221614837646, g_loss: 3.996152877807617\n","Epoch 73/2000, Step 36, d_loss: 0.3798009753227234, g_loss: 4.251288890838623\n","Epoch 73/2000, Step 37, d_loss: 0.36921587586402893, g_loss: 4.522944927215576\n","Epoch 73/2000, Step 38, d_loss: 0.3777684271335602, g_loss: 4.647339820861816\n","Epoch 73/2000, Step 39, d_loss: 0.45647096633911133, g_loss: 4.322006702423096\n","Epoch 73/2000, Step 40, d_loss: 0.3703688085079193, g_loss: 5.203500270843506\n","Epoch 73/2000, Step 41, d_loss: 0.4411528706550598, g_loss: 3.800623655319214\n","Epoch 73/2000, Step 42, d_loss: 0.4470667839050293, g_loss: 3.7343664169311523\n","Epoch 73/2000, Step 43, d_loss: 0.4357546269893646, g_loss: 3.155076265335083\n","Epoch 73/2000, Step 44, d_loss: 0.37461355328559875, g_loss: 4.641233921051025\n","Epoch 73/2000, Step 45, d_loss: 0.41555917263031006, g_loss: 4.187591552734375\n","Epoch 73/2000, Step 46, d_loss: 0.39975202083587646, g_loss: 3.4509339332580566\n","Epoch 73/2000, Step 47, d_loss: 0.4297845661640167, g_loss: 4.3769211769104\n","Epoch 73/2000, Step 48, d_loss: 0.3944513201713562, g_loss: 4.263426303863525\n","Epoch 73/2000, Step 49, d_loss: 0.39048686623573303, g_loss: 4.038699150085449\n","Epoch 73/2000, Step 50, d_loss: 0.3555735945701599, g_loss: 3.4765219688415527\n","Epoch 73/2000, Step 51, d_loss: 0.37176936864852905, g_loss: 3.3713901042938232\n","Epoch 73/2000, Step 52, d_loss: 0.39786434173583984, g_loss: 4.514611721038818\n","Epoch 73/2000, Step 53, d_loss: 0.41733425855636597, g_loss: 3.6844167709350586\n","Epoch 73/2000, Step 54, d_loss: 0.40567445755004883, g_loss: 5.780848503112793\n","Epoch 73/2000, Step 55, d_loss: 0.35366544127464294, g_loss: 4.025091171264648\n","Epoch 73/2000, Step 56, d_loss: 0.39409899711608887, g_loss: 3.688101053237915\n","Epoch 73/2000, Step 57, d_loss: 0.37295210361480713, g_loss: 4.3266777992248535\n","Epoch 73/2000, Step 58, d_loss: 0.36797645688056946, g_loss: 4.076542854309082\n","Epoch 73/2000, Step 59, d_loss: 0.39742642641067505, g_loss: 5.384074687957764\n","Epoch 73/2000, Step 60, d_loss: 0.3907824754714966, g_loss: 3.683907985687256\n","Epoch 73/2000, Step 61, d_loss: 0.3766615390777588, g_loss: 5.6016435623168945\n","Epoch 73/2000, Step 62, d_loss: 0.3827892541885376, g_loss: 3.547691583633423\n","Epoch 73/2000, Step 63, d_loss: 0.35910025238990784, g_loss: 3.926805019378662\n","Epoch 73/2000, Step 64, d_loss: 0.366795152425766, g_loss: 4.479226112365723\n","Epoch 73/2000, Step 65, d_loss: 0.4679246246814728, g_loss: 5.440661430358887\n","Epoch 73/2000, Step 66, d_loss: 0.37805497646331787, g_loss: 5.403330326080322\n","Epoch 73/2000, Step 67, d_loss: 0.36365827918052673, g_loss: 3.965986728668213\n","Epoch 73/2000, Step 68, d_loss: 0.3547392785549164, g_loss: 5.4956135749816895\n","Epoch 73/2000, Step 69, d_loss: 0.3751899003982544, g_loss: 3.572392463684082\n","Epoch 73/2000, Step 70, d_loss: 0.3570709228515625, g_loss: 3.596967935562134\n","Epoch 73/2000, Step 71, d_loss: 0.40810438990592957, g_loss: 4.432507514953613\n","Epoch 73/2000, Step 72, d_loss: 0.3854356110095978, g_loss: 4.4405646324157715\n","Epoch 73/2000, Step 73, d_loss: 0.40074437856674194, g_loss: 2.6559293270111084\n","Epoch 73/2000, Step 74, d_loss: 0.3585529923439026, g_loss: 4.430191993713379\n","Epoch 73/2000, Step 75, d_loss: 0.3578152656555176, g_loss: 5.097700595855713\n","Epoch 73/2000, Step 76, d_loss: 0.3531687259674072, g_loss: 4.51827335357666\n","Epoch 73/2000, Step 77, d_loss: 0.3614513576030731, g_loss: 4.810522556304932\n","Epoch 73/2000, Step 78, d_loss: 0.414517343044281, g_loss: 5.185468673706055\n","Epoch 73/2000, Step 79, d_loss: 0.38179412484169006, g_loss: 4.439365863800049\n","Epoch 73/2000, Step 80, d_loss: 0.35710328817367554, g_loss: 4.009407043457031\n","Epoch 73/2000, Step 81, d_loss: 0.3427138328552246, g_loss: 4.267495632171631\n","Epoch 73/2000, Step 82, d_loss: 0.3887687921524048, g_loss: 3.535892963409424\n","Epoch 73/2000, Step 83, d_loss: 0.3727525770664215, g_loss: 3.9058239459991455\n","Epoch 73/2000, Step 84, d_loss: 0.35483622550964355, g_loss: 4.014981746673584\n","Epoch 73/2000, Step 85, d_loss: 0.3549277186393738, g_loss: 4.555904388427734\n","Epoch 73/2000, Step 86, d_loss: 0.3528277575969696, g_loss: 4.395599842071533\n","Epoch 73/2000, Step 87, d_loss: 0.40502381324768066, g_loss: 4.990039825439453\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 74/2000, Step 1, d_loss: 0.36546802520751953, g_loss: 4.804312229156494\n","Epoch 74/2000, Step 2, d_loss: 0.3713913559913635, g_loss: 6.238539695739746\n","Epoch 74/2000, Step 3, d_loss: 0.3805493712425232, g_loss: 5.175169944763184\n","Epoch 74/2000, Step 4, d_loss: 0.3678022027015686, g_loss: 4.8580002784729\n","Epoch 74/2000, Step 5, d_loss: 0.4252988398075104, g_loss: 4.494689464569092\n","Epoch 74/2000, Step 6, d_loss: 0.34912556409835815, g_loss: 4.296876907348633\n","Epoch 74/2000, Step 7, d_loss: 0.36917805671691895, g_loss: 2.8968193531036377\n","Epoch 74/2000, Step 8, d_loss: 0.40360113978385925, g_loss: 4.335654258728027\n","Epoch 74/2000, Step 9, d_loss: 0.40577206015586853, g_loss: 5.370553016662598\n","Epoch 74/2000, Step 10, d_loss: 0.3804696798324585, g_loss: 3.861023426055908\n","Epoch 74/2000, Step 11, d_loss: 0.37339070439338684, g_loss: 3.1889922618865967\n","Epoch 74/2000, Step 12, d_loss: 0.41516298055648804, g_loss: 4.411993503570557\n","Epoch 74/2000, Step 13, d_loss: 0.36473968625068665, g_loss: 3.8931801319122314\n","Epoch 74/2000, Step 14, d_loss: 0.3899562954902649, g_loss: 3.5065035820007324\n","Epoch 74/2000, Step 15, d_loss: 0.3472892642021179, g_loss: 4.676147937774658\n","Epoch 74/2000, Step 16, d_loss: 0.38201677799224854, g_loss: 4.227495193481445\n","Epoch 74/2000, Step 17, d_loss: 0.3685009181499481, g_loss: 5.108809947967529\n","Epoch 74/2000, Step 18, d_loss: 0.37874123454093933, g_loss: 6.533167839050293\n","Epoch 74/2000, Step 19, d_loss: 0.4000048041343689, g_loss: 4.372739791870117\n","Epoch 74/2000, Step 20, d_loss: 0.3836210072040558, g_loss: 3.4981672763824463\n","Epoch 74/2000, Step 21, d_loss: 0.3547430634498596, g_loss: 3.1692240238189697\n","Epoch 74/2000, Step 22, d_loss: 0.3754325807094574, g_loss: 3.937113046646118\n","Epoch 74/2000, Step 23, d_loss: 0.38548219203948975, g_loss: 4.1641998291015625\n","Epoch 74/2000, Step 24, d_loss: 0.43331676721572876, g_loss: 2.823179006576538\n","Epoch 74/2000, Step 25, d_loss: 0.3988450765609741, g_loss: 3.673652410507202\n","Epoch 74/2000, Step 26, d_loss: 0.3978689908981323, g_loss: 4.438090801239014\n","Epoch 74/2000, Step 27, d_loss: 0.3979916274547577, g_loss: 4.317458629608154\n","Epoch 74/2000, Step 28, d_loss: 0.36989688873291016, g_loss: 4.112140655517578\n","Epoch 74/2000, Step 29, d_loss: 0.4514982998371124, g_loss: 4.896883964538574\n","Epoch 74/2000, Step 30, d_loss: 0.38248759508132935, g_loss: 3.4567711353302\n","Epoch 74/2000, Step 31, d_loss: 0.35176774859428406, g_loss: 3.710094451904297\n","Epoch 74/2000, Step 32, d_loss: 0.36605650186538696, g_loss: 6.015767574310303\n","Epoch 74/2000, Step 33, d_loss: 0.38956665992736816, g_loss: 4.931546211242676\n","Epoch 74/2000, Step 34, d_loss: 0.36543163657188416, g_loss: 5.1355719566345215\n","Epoch 74/2000, Step 35, d_loss: 0.3586040735244751, g_loss: 4.260331630706787\n","Epoch 74/2000, Step 36, d_loss: 0.3812519311904907, g_loss: 4.480348587036133\n","Epoch 74/2000, Step 37, d_loss: 0.3649943768978119, g_loss: 6.382386207580566\n","Epoch 74/2000, Step 38, d_loss: 0.48792463541030884, g_loss: 4.063472270965576\n","Epoch 74/2000, Step 39, d_loss: 0.3621380031108856, g_loss: 3.306196928024292\n","Epoch 74/2000, Step 40, d_loss: 0.413733571767807, g_loss: 3.741575241088867\n","Epoch 74/2000, Step 41, d_loss: 0.5667110681533813, g_loss: 3.5656557083129883\n","Epoch 74/2000, Step 42, d_loss: 0.4180494546890259, g_loss: 2.356353759765625\n","Epoch 74/2000, Step 43, d_loss: 0.43625152111053467, g_loss: 3.808859348297119\n","Epoch 74/2000, Step 44, d_loss: 0.3564113676548004, g_loss: 3.8789877891540527\n","Epoch 74/2000, Step 45, d_loss: 0.38815590739250183, g_loss: 4.926051616668701\n","Epoch 74/2000, Step 46, d_loss: 0.36619749665260315, g_loss: 6.141946315765381\n","Epoch 74/2000, Step 47, d_loss: 0.3689795732498169, g_loss: 6.182949066162109\n","Epoch 74/2000, Step 48, d_loss: 0.46408820152282715, g_loss: 6.304599761962891\n","Epoch 74/2000, Step 49, d_loss: 0.3427671492099762, g_loss: 3.853905200958252\n","Epoch 74/2000, Step 50, d_loss: 0.4492223560810089, g_loss: 4.59150505065918\n","Epoch 74/2000, Step 51, d_loss: 0.35465580224990845, g_loss: 3.5188839435577393\n","Epoch 74/2000, Step 52, d_loss: 0.35508182644844055, g_loss: 3.9275519847869873\n","Epoch 74/2000, Step 53, d_loss: 0.4393887221813202, g_loss: 3.9038918018341064\n","Epoch 74/2000, Step 54, d_loss: 0.3716542422771454, g_loss: 3.015658140182495\n","Epoch 74/2000, Step 55, d_loss: 0.3711831867694855, g_loss: 4.513401985168457\n","Epoch 74/2000, Step 56, d_loss: 0.35560140013694763, g_loss: 3.925870418548584\n","Epoch 74/2000, Step 57, d_loss: 0.3761546015739441, g_loss: 4.425787448883057\n","Epoch 74/2000, Step 58, d_loss: 0.4198819100856781, g_loss: 3.6740756034851074\n","Epoch 74/2000, Step 59, d_loss: 0.3617670238018036, g_loss: 4.1018571853637695\n","Epoch 74/2000, Step 60, d_loss: 0.36281102895736694, g_loss: 4.589542388916016\n","Epoch 74/2000, Step 61, d_loss: 0.37646931409835815, g_loss: 5.8312273025512695\n","Epoch 74/2000, Step 62, d_loss: 0.3631330728530884, g_loss: 3.856614589691162\n","Epoch 74/2000, Step 63, d_loss: 0.5071702003479004, g_loss: 4.057343006134033\n","Epoch 74/2000, Step 64, d_loss: 0.3893239498138428, g_loss: 3.1484694480895996\n","Epoch 74/2000, Step 65, d_loss: 0.39577820897102356, g_loss: 2.713142156600952\n","Epoch 74/2000, Step 66, d_loss: 0.4357941746711731, g_loss: 2.923029899597168\n","Epoch 74/2000, Step 67, d_loss: 0.4411906599998474, g_loss: 3.4458417892456055\n","Epoch 74/2000, Step 68, d_loss: 0.46343010663986206, g_loss: 3.7820520401000977\n","Epoch 74/2000, Step 69, d_loss: 0.37401536107063293, g_loss: 4.331021785736084\n","Epoch 74/2000, Step 70, d_loss: 0.41895177960395813, g_loss: 4.280747890472412\n","Epoch 74/2000, Step 71, d_loss: 0.38914158940315247, g_loss: 4.7965593338012695\n","Epoch 74/2000, Step 72, d_loss: 0.4049518406391144, g_loss: 4.4850969314575195\n","Epoch 74/2000, Step 73, d_loss: 0.5831254124641418, g_loss: 5.084349632263184\n","Epoch 74/2000, Step 74, d_loss: 0.49232059717178345, g_loss: 4.314968585968018\n","Epoch 74/2000, Step 75, d_loss: 0.4885683059692383, g_loss: 2.610680341720581\n","Epoch 74/2000, Step 76, d_loss: 0.40841779112815857, g_loss: 2.451460123062134\n","Epoch 74/2000, Step 77, d_loss: 0.5450659394264221, g_loss: 1.8105427026748657\n","Epoch 74/2000, Step 78, d_loss: 0.5080539584159851, g_loss: 1.7416025400161743\n","Epoch 74/2000, Step 79, d_loss: 0.5073047876358032, g_loss: 3.398306131362915\n","Epoch 74/2000, Step 80, d_loss: 0.42580991983413696, g_loss: 4.6091132164001465\n","Epoch 74/2000, Step 81, d_loss: 0.38706937432289124, g_loss: 3.722715377807617\n","Epoch 74/2000, Step 82, d_loss: 0.4382566809654236, g_loss: 4.792780876159668\n","Epoch 74/2000, Step 83, d_loss: 0.5171324610710144, g_loss: 4.087727069854736\n","Epoch 74/2000, Step 84, d_loss: 0.4792150855064392, g_loss: 3.2367749214172363\n","Epoch 74/2000, Step 85, d_loss: 0.4655247628688812, g_loss: 2.757950782775879\n","Epoch 74/2000, Step 86, d_loss: 0.5004445910453796, g_loss: 3.694941520690918\n","Epoch 74/2000, Step 87, d_loss: 0.4443407654762268, g_loss: 2.8810200691223145\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 75/2000, Step 1, d_loss: 0.5408742427825928, g_loss: 4.65229606628418\n","Epoch 75/2000, Step 2, d_loss: 0.4199383556842804, g_loss: 3.274454355239868\n","Epoch 75/2000, Step 3, d_loss: 0.3897375464439392, g_loss: 4.2302470207214355\n","Epoch 75/2000, Step 4, d_loss: 0.3848502039909363, g_loss: 4.358423709869385\n","Epoch 75/2000, Step 5, d_loss: 0.4622970223426819, g_loss: 4.16426420211792\n","Epoch 75/2000, Step 6, d_loss: 0.4057653546333313, g_loss: 3.476994514465332\n","Epoch 75/2000, Step 7, d_loss: 0.396472692489624, g_loss: 3.6223249435424805\n","Epoch 75/2000, Step 8, d_loss: 0.37678858637809753, g_loss: 3.9349985122680664\n","Epoch 75/2000, Step 9, d_loss: 0.4131978750228882, g_loss: 2.9791808128356934\n","Epoch 75/2000, Step 10, d_loss: 0.4437994658946991, g_loss: 4.844590663909912\n","Epoch 75/2000, Step 11, d_loss: 0.40227851271629333, g_loss: 3.5584123134613037\n","Epoch 75/2000, Step 12, d_loss: 0.43156880140304565, g_loss: 4.374281883239746\n","Epoch 75/2000, Step 13, d_loss: 0.3590027689933777, g_loss: 4.223379135131836\n","Epoch 75/2000, Step 14, d_loss: 0.3750070631504059, g_loss: 4.799307346343994\n","Epoch 75/2000, Step 15, d_loss: 0.36549806594848633, g_loss: 5.300324440002441\n","Epoch 75/2000, Step 16, d_loss: 0.42396974563598633, g_loss: 4.579977989196777\n","Epoch 75/2000, Step 17, d_loss: 0.36530837416648865, g_loss: 4.515809059143066\n","Epoch 75/2000, Step 18, d_loss: 0.42408835887908936, g_loss: 3.5147221088409424\n","Epoch 75/2000, Step 19, d_loss: 0.43810710310935974, g_loss: 3.1998291015625\n","Epoch 75/2000, Step 20, d_loss: 0.42852193117141724, g_loss: 3.248051166534424\n","Epoch 75/2000, Step 21, d_loss: 0.36026617884635925, g_loss: 3.024057388305664\n","Epoch 75/2000, Step 22, d_loss: 0.36757808923721313, g_loss: 3.3443193435668945\n","Epoch 75/2000, Step 23, d_loss: 0.4198246896266937, g_loss: 3.774888277053833\n","Epoch 75/2000, Step 24, d_loss: 0.43716758489608765, g_loss: 5.04074239730835\n","Epoch 75/2000, Step 25, d_loss: 0.364605575799942, g_loss: 4.4441375732421875\n","Epoch 75/2000, Step 26, d_loss: 0.34678736329078674, g_loss: 5.376064300537109\n","Epoch 75/2000, Step 27, d_loss: 0.34792590141296387, g_loss: 5.006566524505615\n","Epoch 75/2000, Step 28, d_loss: 0.7048414945602417, g_loss: 4.831882476806641\n","Epoch 75/2000, Step 29, d_loss: 0.35585224628448486, g_loss: 4.2363667488098145\n","Epoch 75/2000, Step 30, d_loss: 0.42280858755111694, g_loss: 3.165117025375366\n","Epoch 75/2000, Step 31, d_loss: 0.45320528745651245, g_loss: 3.082982301712036\n","Epoch 75/2000, Step 32, d_loss: 0.4590493440628052, g_loss: 3.619101047515869\n","Epoch 75/2000, Step 33, d_loss: 0.38441482186317444, g_loss: 3.2838246822357178\n","Epoch 75/2000, Step 34, d_loss: 0.38625726103782654, g_loss: 3.3526060581207275\n","Epoch 75/2000, Step 35, d_loss: 0.3617725372314453, g_loss: 3.5933752059936523\n","Epoch 75/2000, Step 36, d_loss: 0.45766589045524597, g_loss: 4.469780921936035\n","Epoch 75/2000, Step 37, d_loss: 0.37387362122535706, g_loss: 4.728199481964111\n","Epoch 75/2000, Step 38, d_loss: 0.3754265606403351, g_loss: 4.662472248077393\n","Epoch 75/2000, Step 39, d_loss: 0.381903737783432, g_loss: 4.410984992980957\n","Epoch 75/2000, Step 40, d_loss: 0.49215927720069885, g_loss: 6.065262317657471\n","Epoch 75/2000, Step 41, d_loss: 0.3814907968044281, g_loss: 4.8671746253967285\n","Epoch 75/2000, Step 42, d_loss: 0.36939483880996704, g_loss: 3.5985822677612305\n","Epoch 75/2000, Step 43, d_loss: 0.409284383058548, g_loss: 3.252000331878662\n","Epoch 75/2000, Step 44, d_loss: 0.4655643105506897, g_loss: 2.674412965774536\n","Epoch 75/2000, Step 45, d_loss: 0.4185444712638855, g_loss: 3.23795747756958\n","Epoch 75/2000, Step 46, d_loss: 0.3715928792953491, g_loss: 3.7720115184783936\n","Epoch 75/2000, Step 47, d_loss: 0.41249483823776245, g_loss: 4.135743618011475\n","Epoch 75/2000, Step 48, d_loss: 0.39124929904937744, g_loss: 5.0068583488464355\n","Epoch 75/2000, Step 49, d_loss: 0.823703408241272, g_loss: 3.9692349433898926\n","Epoch 75/2000, Step 50, d_loss: 0.3729976713657379, g_loss: 3.6762733459472656\n","Epoch 75/2000, Step 51, d_loss: 0.4357232451438904, g_loss: 3.820831298828125\n","Epoch 75/2000, Step 52, d_loss: 0.4082154631614685, g_loss: 3.282874822616577\n","Epoch 75/2000, Step 53, d_loss: 0.6542831063270569, g_loss: 3.6143648624420166\n","Epoch 75/2000, Step 54, d_loss: 0.4122934639453888, g_loss: 2.4950990676879883\n","Epoch 75/2000, Step 55, d_loss: 0.40171629190444946, g_loss: 2.9328761100769043\n","Epoch 75/2000, Step 56, d_loss: 0.43499723076820374, g_loss: 4.065551280975342\n","Epoch 75/2000, Step 57, d_loss: 0.4578372538089752, g_loss: 4.0359601974487305\n","Epoch 75/2000, Step 58, d_loss: 0.46995335817337036, g_loss: 3.9087133407592773\n","Epoch 75/2000, Step 59, d_loss: 0.6316180229187012, g_loss: 4.588160037994385\n","Epoch 75/2000, Step 60, d_loss: 0.6408108472824097, g_loss: 4.364045143127441\n","Epoch 75/2000, Step 61, d_loss: 0.39804789423942566, g_loss: 2.573371648788452\n","Epoch 75/2000, Step 62, d_loss: 0.42449015378952026, g_loss: 3.434068202972412\n","Epoch 75/2000, Step 63, d_loss: 0.5256491899490356, g_loss: 2.9905433654785156\n","Epoch 75/2000, Step 64, d_loss: 0.586548388004303, g_loss: 5.098156452178955\n","Epoch 75/2000, Step 65, d_loss: 0.6221619844436646, g_loss: 4.868745803833008\n","Epoch 75/2000, Step 66, d_loss: 0.7178586721420288, g_loss: 2.9381263256073\n","Epoch 75/2000, Step 67, d_loss: 0.48425307869911194, g_loss: 3.6330678462982178\n","Epoch 75/2000, Step 68, d_loss: 0.474212646484375, g_loss: 2.941901206970215\n","Epoch 75/2000, Step 69, d_loss: 0.49609512090682983, g_loss: 4.105743408203125\n","Epoch 75/2000, Step 70, d_loss: 0.5145915746688843, g_loss: 3.90964937210083\n","Epoch 75/2000, Step 71, d_loss: 0.4525337517261505, g_loss: 4.753992080688477\n","Epoch 75/2000, Step 72, d_loss: 0.5682756304740906, g_loss: 4.9708356857299805\n","Epoch 75/2000, Step 73, d_loss: 0.4928331673145294, g_loss: 4.012119293212891\n","Epoch 75/2000, Step 74, d_loss: 0.5548056364059448, g_loss: 3.7391607761383057\n","Epoch 75/2000, Step 75, d_loss: 0.46607568860054016, g_loss: 3.5145130157470703\n","Epoch 75/2000, Step 76, d_loss: 0.4224545359611511, g_loss: 4.481853485107422\n","Epoch 75/2000, Step 77, d_loss: 0.44314807653427124, g_loss: 4.855147838592529\n","Epoch 75/2000, Step 78, d_loss: 0.5488327741622925, g_loss: 4.5056047439575195\n","Epoch 75/2000, Step 79, d_loss: 0.5739107131958008, g_loss: 3.1575613021850586\n","Epoch 75/2000, Step 80, d_loss: 0.46119970083236694, g_loss: 2.916513442993164\n","Epoch 75/2000, Step 81, d_loss: 0.4837145507335663, g_loss: 4.464331150054932\n","Epoch 75/2000, Step 82, d_loss: 0.4598294794559479, g_loss: 3.888427495956421\n","Epoch 75/2000, Step 83, d_loss: 0.4786970615386963, g_loss: 4.355071067810059\n","Epoch 75/2000, Step 84, d_loss: 0.5055451393127441, g_loss: 3.798253059387207\n","Epoch 75/2000, Step 85, d_loss: 0.48985639214515686, g_loss: 4.48136043548584\n","Epoch 75/2000, Step 86, d_loss: 0.5330871343612671, g_loss: 5.553081035614014\n","Epoch 75/2000, Step 87, d_loss: 0.4158842861652374, g_loss: 5.606076717376709\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 76/2000, Step 1, d_loss: 0.4118242561817169, g_loss: 5.647035121917725\n","Epoch 76/2000, Step 2, d_loss: 0.35868993401527405, g_loss: 4.749587059020996\n","Epoch 76/2000, Step 3, d_loss: 0.4168136417865753, g_loss: 4.928929328918457\n","Epoch 76/2000, Step 4, d_loss: 0.44059956073760986, g_loss: 4.644856929779053\n","Epoch 76/2000, Step 5, d_loss: 0.5539102554321289, g_loss: 2.9630215167999268\n","Epoch 76/2000, Step 6, d_loss: 0.4490368366241455, g_loss: 3.8188912868499756\n","Epoch 76/2000, Step 7, d_loss: 0.4810030460357666, g_loss: 3.2920734882354736\n","Epoch 76/2000, Step 8, d_loss: 0.44188302755355835, g_loss: 2.7440242767333984\n","Epoch 76/2000, Step 9, d_loss: 0.5147019028663635, g_loss: 3.155111312866211\n","Epoch 76/2000, Step 10, d_loss: 0.49874383211135864, g_loss: 3.7706944942474365\n","Epoch 76/2000, Step 11, d_loss: 0.6615274548530579, g_loss: 3.866786003112793\n","Epoch 76/2000, Step 12, d_loss: 0.4786774516105652, g_loss: 4.155865669250488\n","Epoch 76/2000, Step 13, d_loss: 0.4320664405822754, g_loss: 4.4979047775268555\n","Epoch 76/2000, Step 14, d_loss: 0.3683561384677887, g_loss: 3.7443325519561768\n","Epoch 76/2000, Step 15, d_loss: 0.46924731135368347, g_loss: 5.490736961364746\n","Epoch 76/2000, Step 16, d_loss: 0.3660403788089752, g_loss: 4.329355239868164\n","Epoch 76/2000, Step 17, d_loss: 0.4331827759742737, g_loss: 4.17760705947876\n","Epoch 76/2000, Step 18, d_loss: 0.37981075048446655, g_loss: 4.0183892250061035\n","Epoch 76/2000, Step 19, d_loss: 0.5284956097602844, g_loss: 3.6375062465667725\n","Epoch 76/2000, Step 20, d_loss: 0.38470834493637085, g_loss: 3.090479612350464\n","Epoch 76/2000, Step 21, d_loss: 0.36197546124458313, g_loss: 4.689389228820801\n","Epoch 76/2000, Step 22, d_loss: 0.4042091369628906, g_loss: 4.874192714691162\n","Epoch 76/2000, Step 23, d_loss: 0.40866559743881226, g_loss: 4.690185070037842\n","Epoch 76/2000, Step 24, d_loss: 0.3687696158885956, g_loss: 3.463117837905884\n","Epoch 76/2000, Step 25, d_loss: 0.3937062919139862, g_loss: 2.7758829593658447\n","Epoch 76/2000, Step 26, d_loss: 0.4010588824748993, g_loss: 3.714168071746826\n","Epoch 76/2000, Step 27, d_loss: 0.41192591190338135, g_loss: 3.9002339839935303\n","Epoch 76/2000, Step 28, d_loss: 0.36639007925987244, g_loss: 4.606369972229004\n","Epoch 76/2000, Step 29, d_loss: 0.42180198431015015, g_loss: 3.922153949737549\n","Epoch 76/2000, Step 30, d_loss: 0.3885146677494049, g_loss: 4.325033187866211\n","Epoch 76/2000, Step 31, d_loss: 0.3789593279361725, g_loss: 4.318447589874268\n","Epoch 76/2000, Step 32, d_loss: 0.3692481517791748, g_loss: 6.277477264404297\n","Epoch 76/2000, Step 33, d_loss: 0.4294663071632385, g_loss: 3.831005811691284\n","Epoch 76/2000, Step 34, d_loss: 0.3566046357154846, g_loss: 4.099976062774658\n","Epoch 76/2000, Step 35, d_loss: 0.45224595069885254, g_loss: 4.107425212860107\n","Epoch 76/2000, Step 36, d_loss: 0.3732258677482605, g_loss: 3.7531509399414062\n","Epoch 76/2000, Step 37, d_loss: 0.4898098409175873, g_loss: 2.453352928161621\n","Epoch 76/2000, Step 38, d_loss: 0.4505903720855713, g_loss: 4.310039520263672\n","Epoch 76/2000, Step 39, d_loss: 0.39046940207481384, g_loss: 4.690943241119385\n","Epoch 76/2000, Step 40, d_loss: 0.38788434863090515, g_loss: 4.617422103881836\n","Epoch 76/2000, Step 41, d_loss: 0.37243807315826416, g_loss: 4.880523204803467\n","Epoch 76/2000, Step 42, d_loss: 0.3759247660636902, g_loss: 5.356889724731445\n","Epoch 76/2000, Step 43, d_loss: 0.36511775851249695, g_loss: 4.327816486358643\n","Epoch 76/2000, Step 44, d_loss: 0.38287976384162903, g_loss: 4.457327365875244\n","Epoch 76/2000, Step 45, d_loss: 0.35785552859306335, g_loss: 5.906805992126465\n","Epoch 76/2000, Step 46, d_loss: 0.3604249954223633, g_loss: 4.2565412521362305\n","Epoch 76/2000, Step 47, d_loss: 0.3645048439502716, g_loss: 3.875382900238037\n","Epoch 76/2000, Step 48, d_loss: 0.40762829780578613, g_loss: 3.7626633644104004\n","Epoch 76/2000, Step 49, d_loss: 0.3893016576766968, g_loss: 3.0274288654327393\n","Epoch 76/2000, Step 50, d_loss: 0.3639889657497406, g_loss: 3.8618228435516357\n","Epoch 76/2000, Step 51, d_loss: 0.39653560519218445, g_loss: 3.538059949874878\n","Epoch 76/2000, Step 52, d_loss: 0.4326985478401184, g_loss: 3.451148748397827\n","Epoch 76/2000, Step 53, d_loss: 0.3638480305671692, g_loss: 4.521848678588867\n","Epoch 76/2000, Step 54, d_loss: 0.4937809109687805, g_loss: 2.9524834156036377\n","Epoch 76/2000, Step 55, d_loss: 0.3597516417503357, g_loss: 3.9358816146850586\n","Epoch 76/2000, Step 56, d_loss: 0.36845162510871887, g_loss: 3.4383928775787354\n","Epoch 76/2000, Step 57, d_loss: 0.39022937417030334, g_loss: 5.052892684936523\n","Epoch 76/2000, Step 58, d_loss: 0.3576684892177582, g_loss: 4.3581061363220215\n","Epoch 76/2000, Step 59, d_loss: 0.43338924646377563, g_loss: 4.549361228942871\n","Epoch 76/2000, Step 60, d_loss: 0.40530452132225037, g_loss: 3.746981143951416\n","Epoch 76/2000, Step 61, d_loss: 0.4239484965801239, g_loss: 4.346404552459717\n","Epoch 76/2000, Step 62, d_loss: 0.4093475937843323, g_loss: 4.392271041870117\n","Epoch 76/2000, Step 63, d_loss: 0.3454468846321106, g_loss: 4.015291213989258\n","Epoch 76/2000, Step 64, d_loss: 0.38012564182281494, g_loss: 3.6995065212249756\n","Epoch 76/2000, Step 65, d_loss: 0.4098973572254181, g_loss: 3.793640613555908\n","Epoch 76/2000, Step 66, d_loss: 0.45233097672462463, g_loss: 3.9773993492126465\n","Epoch 76/2000, Step 67, d_loss: 0.41020089387893677, g_loss: 4.124653339385986\n","Epoch 76/2000, Step 68, d_loss: 0.39899924397468567, g_loss: 4.334590435028076\n","Epoch 76/2000, Step 69, d_loss: 0.36496201157569885, g_loss: 3.5997698307037354\n","Epoch 76/2000, Step 70, d_loss: 0.40693750977516174, g_loss: 4.29683780670166\n","Epoch 76/2000, Step 71, d_loss: 0.3926084339618683, g_loss: 5.72492790222168\n","Epoch 76/2000, Step 72, d_loss: 0.3876986801624298, g_loss: 6.178826808929443\n","Epoch 76/2000, Step 73, d_loss: 0.5840274095535278, g_loss: 5.089198589324951\n","Epoch 76/2000, Step 74, d_loss: 0.3816133439540863, g_loss: 5.0075273513793945\n","Epoch 76/2000, Step 75, d_loss: 0.376592755317688, g_loss: 4.005772590637207\n","Epoch 76/2000, Step 76, d_loss: 0.35174867510795593, g_loss: 3.8131303787231445\n","Epoch 76/2000, Step 77, d_loss: 0.4223453998565674, g_loss: 4.0529093742370605\n","Epoch 76/2000, Step 78, d_loss: 0.3731478750705719, g_loss: 3.4422860145568848\n","Epoch 76/2000, Step 79, d_loss: 0.3908922076225281, g_loss: 3.896286964416504\n","Epoch 76/2000, Step 80, d_loss: 0.47133496403694153, g_loss: 4.133012771606445\n","Epoch 76/2000, Step 81, d_loss: 0.36702218651771545, g_loss: 4.512389659881592\n","Epoch 76/2000, Step 82, d_loss: 0.4534139931201935, g_loss: 4.765408039093018\n","Epoch 76/2000, Step 83, d_loss: 0.38757091760635376, g_loss: 3.2230472564697266\n","Epoch 76/2000, Step 84, d_loss: 0.3859005570411682, g_loss: 3.382187843322754\n","Epoch 76/2000, Step 85, d_loss: 0.4122597277164459, g_loss: 3.189578056335449\n","Epoch 76/2000, Step 86, d_loss: 0.4193047285079956, g_loss: 3.734222888946533\n","Epoch 76/2000, Step 87, d_loss: 0.41158050298690796, g_loss: 3.302985429763794\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 77/2000, Step 1, d_loss: 0.3760059177875519, g_loss: 4.863114356994629\n","Epoch 77/2000, Step 2, d_loss: 0.38783907890319824, g_loss: 4.49406099319458\n","Epoch 77/2000, Step 3, d_loss: 0.43619152903556824, g_loss: 4.298805236816406\n","Epoch 77/2000, Step 4, d_loss: 0.37386927008628845, g_loss: 4.308177471160889\n","Epoch 77/2000, Step 5, d_loss: 0.4533991515636444, g_loss: 3.2116622924804688\n","Epoch 77/2000, Step 6, d_loss: 0.4154926538467407, g_loss: 3.665785551071167\n","Epoch 77/2000, Step 7, d_loss: 0.49546152353286743, g_loss: 2.9932031631469727\n","Epoch 77/2000, Step 8, d_loss: 0.6703194379806519, g_loss: 2.6644206047058105\n","Epoch 77/2000, Step 9, d_loss: 0.4292796552181244, g_loss: 4.091194152832031\n","Epoch 77/2000, Step 10, d_loss: 0.44301679730415344, g_loss: 4.935239315032959\n","Epoch 77/2000, Step 11, d_loss: 0.5020760893821716, g_loss: 5.786765098571777\n","Epoch 77/2000, Step 12, d_loss: 0.35971829295158386, g_loss: 4.475579261779785\n","Epoch 77/2000, Step 13, d_loss: 0.3820013105869293, g_loss: 4.316935062408447\n","Epoch 77/2000, Step 14, d_loss: 0.5139566659927368, g_loss: 4.391468524932861\n","Epoch 77/2000, Step 15, d_loss: 0.4142538607120514, g_loss: 4.24122953414917\n","Epoch 77/2000, Step 16, d_loss: 0.4421125650405884, g_loss: 4.248648166656494\n","Epoch 77/2000, Step 17, d_loss: 0.39836618304252625, g_loss: 3.745992660522461\n","Epoch 77/2000, Step 18, d_loss: 0.3590831458568573, g_loss: 2.971064567565918\n","Epoch 77/2000, Step 19, d_loss: 0.45428988337516785, g_loss: 3.253028631210327\n","Epoch 77/2000, Step 20, d_loss: 0.41422006487846375, g_loss: 3.124547004699707\n","Epoch 77/2000, Step 21, d_loss: 0.4415874481201172, g_loss: 4.716341972351074\n","Epoch 77/2000, Step 22, d_loss: 0.4813145101070404, g_loss: 3.728048324584961\n","Epoch 77/2000, Step 23, d_loss: 0.3767623007297516, g_loss: 3.9753613471984863\n","Epoch 77/2000, Step 24, d_loss: 0.38836491107940674, g_loss: 4.415896415710449\n","Epoch 77/2000, Step 25, d_loss: 0.414559930562973, g_loss: 3.7630350589752197\n","Epoch 77/2000, Step 26, d_loss: 0.38941487669944763, g_loss: 4.452104091644287\n","Epoch 77/2000, Step 27, d_loss: 0.39616823196411133, g_loss: 3.8135876655578613\n","Epoch 77/2000, Step 28, d_loss: 0.4460349380970001, g_loss: 3.713172435760498\n","Epoch 77/2000, Step 29, d_loss: 0.438105046749115, g_loss: 3.949035882949829\n","Epoch 77/2000, Step 30, d_loss: 0.3711615800857544, g_loss: 3.9258718490600586\n","Epoch 77/2000, Step 31, d_loss: 0.44809669256210327, g_loss: 3.0619468688964844\n","Epoch 77/2000, Step 32, d_loss: 0.4013153314590454, g_loss: 3.9738337993621826\n","Epoch 77/2000, Step 33, d_loss: 0.405596524477005, g_loss: 3.964663505554199\n","Epoch 77/2000, Step 34, d_loss: 0.3716224730014801, g_loss: 4.005551815032959\n","Epoch 77/2000, Step 35, d_loss: 0.36749914288520813, g_loss: 4.03523063659668\n","Epoch 77/2000, Step 36, d_loss: 0.42800259590148926, g_loss: 4.045766353607178\n","Epoch 77/2000, Step 37, d_loss: 0.5161577463150024, g_loss: 4.220515727996826\n","Epoch 77/2000, Step 38, d_loss: 0.3507482409477234, g_loss: 3.9977355003356934\n","Epoch 77/2000, Step 39, d_loss: 0.35791879892349243, g_loss: 4.646265029907227\n","Epoch 77/2000, Step 40, d_loss: 0.3803728222846985, g_loss: 4.146826267242432\n","Epoch 77/2000, Step 41, d_loss: 0.3729495406150818, g_loss: 4.650481224060059\n","Epoch 77/2000, Step 42, d_loss: 0.4103998839855194, g_loss: 4.916507720947266\n","Epoch 77/2000, Step 43, d_loss: 0.35180234909057617, g_loss: 4.023426532745361\n","Epoch 77/2000, Step 44, d_loss: 0.3590146601200104, g_loss: 4.373376369476318\n","Epoch 77/2000, Step 45, d_loss: 0.38512295484542847, g_loss: 4.768026351928711\n","Epoch 77/2000, Step 46, d_loss: 0.4024665057659149, g_loss: 4.778999328613281\n","Epoch 77/2000, Step 47, d_loss: 0.3679259717464447, g_loss: 3.311811685562134\n","Epoch 77/2000, Step 48, d_loss: 0.3848670721054077, g_loss: 3.0045292377471924\n","Epoch 77/2000, Step 49, d_loss: 0.36730724573135376, g_loss: 4.129724502563477\n","Epoch 77/2000, Step 50, d_loss: 0.3960515260696411, g_loss: 5.039629936218262\n","Epoch 77/2000, Step 51, d_loss: 0.3691836893558502, g_loss: 5.703579902648926\n","Epoch 77/2000, Step 52, d_loss: 0.36454257369041443, g_loss: 5.1970534324646\n","Epoch 77/2000, Step 53, d_loss: 0.3808923065662384, g_loss: 5.179781436920166\n","Epoch 77/2000, Step 54, d_loss: 0.36752253770828247, g_loss: 5.407445907592773\n","Epoch 77/2000, Step 55, d_loss: 0.6407361626625061, g_loss: 4.171968936920166\n","Epoch 77/2000, Step 56, d_loss: 0.3651493787765503, g_loss: 3.659390926361084\n","Epoch 77/2000, Step 57, d_loss: 0.37779244780540466, g_loss: 3.750727653503418\n","Epoch 77/2000, Step 58, d_loss: 0.4533131718635559, g_loss: 3.256173610687256\n","Epoch 77/2000, Step 59, d_loss: 0.5047021508216858, g_loss: 4.229809284210205\n","Epoch 77/2000, Step 60, d_loss: 0.40477097034454346, g_loss: 3.7779762744903564\n","Epoch 77/2000, Step 61, d_loss: 0.46187126636505127, g_loss: 3.5427839756011963\n","Epoch 77/2000, Step 62, d_loss: 0.459810733795166, g_loss: 3.872126340866089\n","Epoch 77/2000, Step 63, d_loss: 0.4134290814399719, g_loss: 4.097072124481201\n","Epoch 77/2000, Step 64, d_loss: 0.36477717757225037, g_loss: 5.770215034484863\n","Epoch 77/2000, Step 65, d_loss: 0.4303637742996216, g_loss: 5.782991409301758\n","Epoch 77/2000, Step 66, d_loss: 0.3616754114627838, g_loss: 4.90495491027832\n","Epoch 77/2000, Step 67, d_loss: 0.3903714716434479, g_loss: 4.418440341949463\n","Epoch 77/2000, Step 68, d_loss: 0.41159671545028687, g_loss: 4.146900653839111\n","Epoch 77/2000, Step 69, d_loss: 0.3872202932834625, g_loss: 4.248558044433594\n","Epoch 77/2000, Step 70, d_loss: 0.3669479191303253, g_loss: 4.609190940856934\n","Epoch 77/2000, Step 71, d_loss: 0.40564823150634766, g_loss: 3.8902676105499268\n","Epoch 77/2000, Step 72, d_loss: 0.3880561590194702, g_loss: 3.731135845184326\n","Epoch 77/2000, Step 73, d_loss: 0.37995976209640503, g_loss: 3.276735305786133\n","Epoch 77/2000, Step 74, d_loss: 0.3812970519065857, g_loss: 4.013361930847168\n","Epoch 77/2000, Step 75, d_loss: 0.41522642970085144, g_loss: 4.5878682136535645\n","Epoch 77/2000, Step 76, d_loss: 0.37348416447639465, g_loss: 4.40324592590332\n","Epoch 77/2000, Step 77, d_loss: 0.35499390959739685, g_loss: 5.387907981872559\n","Epoch 77/2000, Step 78, d_loss: 0.40706709027290344, g_loss: 5.135733604431152\n","Epoch 77/2000, Step 79, d_loss: 0.4116220474243164, g_loss: 3.898679256439209\n","Epoch 77/2000, Step 80, d_loss: 0.43996164202690125, g_loss: 3.858494281768799\n","Epoch 77/2000, Step 81, d_loss: 0.36244913935661316, g_loss: 4.859430313110352\n","Epoch 77/2000, Step 82, d_loss: 0.392391562461853, g_loss: 3.2390894889831543\n","Epoch 77/2000, Step 83, d_loss: 0.4687253534793854, g_loss: 2.9023549556732178\n","Epoch 77/2000, Step 84, d_loss: 0.4366821050643921, g_loss: 2.995199203491211\n","Epoch 77/2000, Step 85, d_loss: 0.40350887179374695, g_loss: 2.8460137844085693\n","Epoch 77/2000, Step 86, d_loss: 0.4243026375770569, g_loss: 3.362064838409424\n","Epoch 77/2000, Step 87, d_loss: 0.44254887104034424, g_loss: 3.431541919708252\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 78/2000, Step 1, d_loss: 0.41316425800323486, g_loss: 4.001922130584717\n","Epoch 78/2000, Step 2, d_loss: 0.37454986572265625, g_loss: 5.965081214904785\n","Epoch 78/2000, Step 3, d_loss: 0.3885369896888733, g_loss: 4.747200965881348\n","Epoch 78/2000, Step 4, d_loss: 0.6142075657844543, g_loss: 6.169323921203613\n","Epoch 78/2000, Step 5, d_loss: 0.39810386300086975, g_loss: 3.130289077758789\n","Epoch 78/2000, Step 6, d_loss: 0.37371692061424255, g_loss: 3.065690279006958\n","Epoch 78/2000, Step 7, d_loss: 0.41131359338760376, g_loss: 2.7534501552581787\n","Epoch 78/2000, Step 8, d_loss: 0.4115588366985321, g_loss: 3.0434353351593018\n","Epoch 78/2000, Step 9, d_loss: 0.4115423560142517, g_loss: 3.7346177101135254\n","Epoch 78/2000, Step 10, d_loss: 0.3993114233016968, g_loss: 3.3783929347991943\n","Epoch 78/2000, Step 11, d_loss: 0.42458605766296387, g_loss: 4.308347702026367\n","Epoch 78/2000, Step 12, d_loss: 0.4225854277610779, g_loss: 3.704495906829834\n","Epoch 78/2000, Step 13, d_loss: 0.36107996106147766, g_loss: 3.9582161903381348\n","Epoch 78/2000, Step 14, d_loss: 0.3562402129173279, g_loss: 5.584057331085205\n","Epoch 78/2000, Step 15, d_loss: 0.37936124205589294, g_loss: 4.958834171295166\n","Epoch 78/2000, Step 16, d_loss: 0.5569858551025391, g_loss: 5.953835964202881\n","Epoch 78/2000, Step 17, d_loss: 0.36189520359039307, g_loss: 3.8000190258026123\n","Epoch 78/2000, Step 18, d_loss: 0.3929598331451416, g_loss: 3.9537787437438965\n","Epoch 78/2000, Step 19, d_loss: 0.43193113803863525, g_loss: 3.8768997192382812\n","Epoch 78/2000, Step 20, d_loss: 0.44788506627082825, g_loss: 3.107686996459961\n","Epoch 78/2000, Step 21, d_loss: 0.39948779344558716, g_loss: 3.5268633365631104\n","Epoch 78/2000, Step 22, d_loss: 0.41618722677230835, g_loss: 3.924042224884033\n","Epoch 78/2000, Step 23, d_loss: 0.3860984146595001, g_loss: 4.788130283355713\n","Epoch 78/2000, Step 24, d_loss: 0.3825756311416626, g_loss: 3.897125005722046\n","Epoch 78/2000, Step 25, d_loss: 0.4010462462902069, g_loss: 4.417539119720459\n","Epoch 78/2000, Step 26, d_loss: 0.511832594871521, g_loss: 3.654863119125366\n","Epoch 78/2000, Step 27, d_loss: 0.39118167757987976, g_loss: 4.427828311920166\n","Epoch 78/2000, Step 28, d_loss: 0.35082513093948364, g_loss: 5.622507095336914\n","Epoch 78/2000, Step 29, d_loss: 0.36235177516937256, g_loss: 4.158996105194092\n","Epoch 78/2000, Step 30, d_loss: 0.38912174105644226, g_loss: 4.088780403137207\n","Epoch 78/2000, Step 31, d_loss: 0.42827537655830383, g_loss: 4.0336480140686035\n","Epoch 78/2000, Step 32, d_loss: 0.36824047565460205, g_loss: 4.428323268890381\n","Epoch 78/2000, Step 33, d_loss: 0.3907249867916107, g_loss: 3.638888359069824\n","Epoch 78/2000, Step 34, d_loss: 0.4039570093154907, g_loss: 3.751654624938965\n","Epoch 78/2000, Step 35, d_loss: 0.4086756706237793, g_loss: 2.968836545944214\n","Epoch 78/2000, Step 36, d_loss: 0.3790687322616577, g_loss: 4.49611759185791\n","Epoch 78/2000, Step 37, d_loss: 0.40477681159973145, g_loss: 4.263032913208008\n","Epoch 78/2000, Step 38, d_loss: 0.38741931319236755, g_loss: 4.763347148895264\n","Epoch 78/2000, Step 39, d_loss: 0.36384859681129456, g_loss: 4.669864177703857\n","Epoch 78/2000, Step 40, d_loss: 0.3650498688220978, g_loss: 5.597329616546631\n","Epoch 78/2000, Step 41, d_loss: 0.5419716238975525, g_loss: 4.720864772796631\n","Epoch 78/2000, Step 42, d_loss: 0.5994682312011719, g_loss: 3.475318431854248\n","Epoch 78/2000, Step 43, d_loss: 0.3944329619407654, g_loss: 2.8713839054107666\n","Epoch 78/2000, Step 44, d_loss: 0.4908435344696045, g_loss: 2.1392250061035156\n","Epoch 78/2000, Step 45, d_loss: 0.4530790448188782, g_loss: 2.0319106578826904\n","Epoch 78/2000, Step 46, d_loss: 0.7649097442626953, g_loss: 1.8735765218734741\n","Epoch 78/2000, Step 47, d_loss: 0.5502983331680298, g_loss: 4.436063766479492\n","Epoch 78/2000, Step 48, d_loss: 0.38541170954704285, g_loss: 4.551521301269531\n","Epoch 78/2000, Step 49, d_loss: 0.3587753176689148, g_loss: 5.458279132843018\n","Epoch 78/2000, Step 50, d_loss: 0.37563827633857727, g_loss: 5.46615743637085\n","Epoch 78/2000, Step 51, d_loss: 0.48297569155693054, g_loss: 6.153721809387207\n","Epoch 78/2000, Step 52, d_loss: 1.0384379625320435, g_loss: 4.927122592926025\n","Epoch 78/2000, Step 53, d_loss: 0.3546285629272461, g_loss: 3.4025042057037354\n","Epoch 78/2000, Step 54, d_loss: 0.38146600127220154, g_loss: 1.696600317955017\n","Epoch 78/2000, Step 55, d_loss: 0.38861364126205444, g_loss: 1.3053715229034424\n","Epoch 78/2000, Step 56, d_loss: 0.6422731876373291, g_loss: 2.2273852825164795\n","Epoch 78/2000, Step 57, d_loss: 0.4434272050857544, g_loss: 2.9395930767059326\n","Epoch 78/2000, Step 58, d_loss: 0.3884952664375305, g_loss: 4.019079685211182\n","Epoch 78/2000, Step 59, d_loss: 0.4014352262020111, g_loss: 4.0604567527771\n","Epoch 78/2000, Step 60, d_loss: 0.41729941964149475, g_loss: 4.131396293640137\n","Epoch 78/2000, Step 61, d_loss: 0.4210021197795868, g_loss: 4.349018573760986\n","Epoch 78/2000, Step 62, d_loss: 0.4144565165042877, g_loss: 5.246323585510254\n","Epoch 78/2000, Step 63, d_loss: 0.3560636639595032, g_loss: 3.1279165744781494\n","Epoch 78/2000, Step 64, d_loss: 0.3854045569896698, g_loss: 2.40921688079834\n","Epoch 78/2000, Step 65, d_loss: 0.4559090733528137, g_loss: 3.1270995140075684\n","Epoch 78/2000, Step 66, d_loss: 0.4233459532260895, g_loss: 4.255084037780762\n","Epoch 78/2000, Step 67, d_loss: 0.41312792897224426, g_loss: 5.055047988891602\n","Epoch 78/2000, Step 68, d_loss: 0.3605198860168457, g_loss: 3.5932788848876953\n","Epoch 78/2000, Step 69, d_loss: 0.41653236746788025, g_loss: 3.933485746383667\n","Epoch 78/2000, Step 70, d_loss: 0.3780301809310913, g_loss: 5.155675411224365\n","Epoch 78/2000, Step 71, d_loss: 0.35408738255500793, g_loss: 5.194765090942383\n","Epoch 78/2000, Step 72, d_loss: 0.5849108099937439, g_loss: 4.119493007659912\n","Epoch 78/2000, Step 73, d_loss: 0.3760300278663635, g_loss: 3.151775360107422\n","Epoch 78/2000, Step 74, d_loss: 0.3954765796661377, g_loss: 3.224102735519409\n","Epoch 78/2000, Step 75, d_loss: 0.45261350274086, g_loss: 3.0222620964050293\n","Epoch 78/2000, Step 76, d_loss: 0.39999502897262573, g_loss: 3.1398792266845703\n","Epoch 78/2000, Step 77, d_loss: 0.4041859209537506, g_loss: 3.4931509494781494\n","Epoch 78/2000, Step 78, d_loss: 0.39730069041252136, g_loss: 3.692826509475708\n","Epoch 78/2000, Step 79, d_loss: 0.38179463148117065, g_loss: 4.258309364318848\n","Epoch 78/2000, Step 80, d_loss: 0.3639959692955017, g_loss: 4.796350002288818\n","Epoch 78/2000, Step 81, d_loss: 0.4431316554546356, g_loss: 4.111677646636963\n","Epoch 78/2000, Step 82, d_loss: 0.365875780582428, g_loss: 4.40891170501709\n","Epoch 78/2000, Step 83, d_loss: 0.35876497626304626, g_loss: 3.708789825439453\n","Epoch 78/2000, Step 84, d_loss: 0.35841405391693115, g_loss: 4.072661399841309\n","Epoch 78/2000, Step 85, d_loss: 0.3824382722377777, g_loss: 3.7543704509735107\n","Epoch 78/2000, Step 86, d_loss: 0.34657299518585205, g_loss: 3.7483553886413574\n","Epoch 78/2000, Step 87, d_loss: 0.40847206115722656, g_loss: 4.1110005378723145\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 79/2000, Step 1, d_loss: 0.380459189414978, g_loss: 4.422468185424805\n","Epoch 79/2000, Step 2, d_loss: 0.3730674088001251, g_loss: 3.51501727104187\n","Epoch 79/2000, Step 3, d_loss: 0.3691668212413788, g_loss: 5.0114030838012695\n","Epoch 79/2000, Step 4, d_loss: 0.35849571228027344, g_loss: 4.677670955657959\n","Epoch 79/2000, Step 5, d_loss: 0.41252467036247253, g_loss: 4.161799430847168\n","Epoch 79/2000, Step 6, d_loss: 0.39373302459716797, g_loss: 5.1735711097717285\n","Epoch 79/2000, Step 7, d_loss: 0.37999990582466125, g_loss: 3.9045443534851074\n","Epoch 79/2000, Step 8, d_loss: 0.3596373200416565, g_loss: 3.1603927612304688\n","Epoch 79/2000, Step 9, d_loss: 0.3967946767807007, g_loss: 2.226830005645752\n","Epoch 79/2000, Step 10, d_loss: 0.46933919191360474, g_loss: 4.665524482727051\n","Epoch 79/2000, Step 11, d_loss: 0.40598535537719727, g_loss: 3.9814419746398926\n","Epoch 79/2000, Step 12, d_loss: 0.4264508783817291, g_loss: 4.252978801727295\n","Epoch 79/2000, Step 13, d_loss: 0.3933190703392029, g_loss: 5.895537376403809\n","Epoch 79/2000, Step 14, d_loss: 0.4402325451374054, g_loss: 4.48579740524292\n","Epoch 79/2000, Step 15, d_loss: 0.3508455753326416, g_loss: 4.221501350402832\n","Epoch 79/2000, Step 16, d_loss: 0.3846058249473572, g_loss: 4.7136359214782715\n","Epoch 79/2000, Step 17, d_loss: 0.3986262381076813, g_loss: 5.011478424072266\n","Epoch 79/2000, Step 18, d_loss: 0.34747710824012756, g_loss: 5.0703959465026855\n","Epoch 79/2000, Step 19, d_loss: 0.39603981375694275, g_loss: 4.497286796569824\n","Epoch 79/2000, Step 20, d_loss: 0.361955851316452, g_loss: 4.331127166748047\n","Epoch 79/2000, Step 21, d_loss: 0.375498503446579, g_loss: 4.006411075592041\n","Epoch 79/2000, Step 22, d_loss: 0.35663068294525146, g_loss: 4.36917781829834\n","Epoch 79/2000, Step 23, d_loss: 0.35654234886169434, g_loss: 4.076517105102539\n","Epoch 79/2000, Step 24, d_loss: 0.38675186038017273, g_loss: 4.694440841674805\n","Epoch 79/2000, Step 25, d_loss: 0.36118918657302856, g_loss: 4.147278308868408\n","Epoch 79/2000, Step 26, d_loss: 0.36521482467651367, g_loss: 4.215248107910156\n","Epoch 79/2000, Step 27, d_loss: 0.3785601854324341, g_loss: 4.621181011199951\n","Epoch 79/2000, Step 28, d_loss: 0.3558765649795532, g_loss: 5.568745136260986\n","Epoch 79/2000, Step 29, d_loss: 0.35046225786209106, g_loss: 4.038937568664551\n","Epoch 79/2000, Step 30, d_loss: 0.3473491370677948, g_loss: 5.413966655731201\n","Epoch 79/2000, Step 31, d_loss: 0.34872376918792725, g_loss: 5.011234760284424\n","Epoch 79/2000, Step 32, d_loss: 0.45247992873191833, g_loss: 4.562349796295166\n","Epoch 79/2000, Step 33, d_loss: 0.4401755928993225, g_loss: 3.552170515060425\n","Epoch 79/2000, Step 34, d_loss: 0.4317779541015625, g_loss: 4.046986103057861\n","Epoch 79/2000, Step 35, d_loss: 0.4068215787410736, g_loss: 3.6617913246154785\n","Epoch 79/2000, Step 36, d_loss: 0.41563838720321655, g_loss: 3.119157075881958\n","Epoch 79/2000, Step 37, d_loss: 0.419347882270813, g_loss: 3.4288241863250732\n","Epoch 79/2000, Step 38, d_loss: 0.4348432719707489, g_loss: 4.175220489501953\n","Epoch 79/2000, Step 39, d_loss: 0.3569510579109192, g_loss: 4.507657051086426\n","Epoch 79/2000, Step 40, d_loss: 0.468627393245697, g_loss: 3.1875507831573486\n","Epoch 79/2000, Step 41, d_loss: 0.3770347535610199, g_loss: 4.822539329528809\n","Epoch 79/2000, Step 42, d_loss: 0.3451337516307831, g_loss: 5.229555130004883\n","Epoch 79/2000, Step 43, d_loss: 0.36480945348739624, g_loss: 3.8232553005218506\n","Epoch 79/2000, Step 44, d_loss: 0.3900463879108429, g_loss: 4.021252632141113\n","Epoch 79/2000, Step 45, d_loss: 0.3781018853187561, g_loss: 3.9716153144836426\n","Epoch 79/2000, Step 46, d_loss: 0.41997525095939636, g_loss: 4.24033784866333\n","Epoch 79/2000, Step 47, d_loss: 0.36206862330436707, g_loss: 3.8442800045013428\n","Epoch 79/2000, Step 48, d_loss: 0.3602027893066406, g_loss: 4.308282852172852\n","Epoch 79/2000, Step 49, d_loss: 0.35148724913597107, g_loss: 4.608646869659424\n","Epoch 79/2000, Step 50, d_loss: 0.41710418462753296, g_loss: 3.163447618484497\n","Epoch 79/2000, Step 51, d_loss: 0.5483128428459167, g_loss: 4.80270528793335\n","Epoch 79/2000, Step 52, d_loss: 0.3745359480381012, g_loss: 2.6996994018554688\n","Epoch 79/2000, Step 53, d_loss: 0.4164142310619354, g_loss: 3.118478298187256\n","Epoch 79/2000, Step 54, d_loss: 0.47425544261932373, g_loss: 2.75394344329834\n","Epoch 79/2000, Step 55, d_loss: 0.48607170581817627, g_loss: 3.4269134998321533\n","Epoch 79/2000, Step 56, d_loss: 0.4421502649784088, g_loss: 4.126028537750244\n","Epoch 79/2000, Step 57, d_loss: 0.44337043166160583, g_loss: 3.8873870372772217\n","Epoch 79/2000, Step 58, d_loss: 0.38594698905944824, g_loss: 4.707399368286133\n","Epoch 79/2000, Step 59, d_loss: 0.3642794191837311, g_loss: 4.853317737579346\n","Epoch 79/2000, Step 60, d_loss: 0.3701571226119995, g_loss: 4.935731887817383\n","Epoch 79/2000, Step 61, d_loss: 0.5402576327323914, g_loss: 5.026033878326416\n","Epoch 79/2000, Step 62, d_loss: 0.39060211181640625, g_loss: 4.542104721069336\n","Epoch 79/2000, Step 63, d_loss: 0.35398510098457336, g_loss: 4.590257167816162\n","Epoch 79/2000, Step 64, d_loss: 0.3804401159286499, g_loss: 3.1811327934265137\n","Epoch 79/2000, Step 65, d_loss: 0.4164577126502991, g_loss: 3.0070576667785645\n","Epoch 79/2000, Step 66, d_loss: 0.48908570408821106, g_loss: 3.427079677581787\n","Epoch 79/2000, Step 67, d_loss: 0.4239804744720459, g_loss: 3.444071054458618\n","Epoch 79/2000, Step 68, d_loss: 0.39064517617225647, g_loss: 4.018136501312256\n","Epoch 79/2000, Step 69, d_loss: 0.36748915910720825, g_loss: 4.0375847816467285\n","Epoch 79/2000, Step 70, d_loss: 0.37050366401672363, g_loss: 4.675156116485596\n","Epoch 79/2000, Step 71, d_loss: 0.39013683795928955, g_loss: 4.3056230545043945\n","Epoch 79/2000, Step 72, d_loss: 0.43363744020462036, g_loss: 4.581348419189453\n","Epoch 79/2000, Step 73, d_loss: 0.3859969973564148, g_loss: 4.899667739868164\n","Epoch 79/2000, Step 74, d_loss: 0.40588074922561646, g_loss: 4.458049297332764\n","Epoch 79/2000, Step 75, d_loss: 0.39489877223968506, g_loss: 4.8269548416137695\n","Epoch 79/2000, Step 76, d_loss: 0.38089361786842346, g_loss: 4.169153213500977\n","Epoch 79/2000, Step 77, d_loss: 0.3929157853126526, g_loss: 2.802173614501953\n","Epoch 79/2000, Step 78, d_loss: 0.5236015915870667, g_loss: 4.430607318878174\n","Epoch 79/2000, Step 79, d_loss: 0.5406644344329834, g_loss: 3.9236395359039307\n","Epoch 79/2000, Step 80, d_loss: 0.3793191909790039, g_loss: 3.9274351596832275\n","Epoch 79/2000, Step 81, d_loss: 0.35886698961257935, g_loss: 4.53856897354126\n","Epoch 79/2000, Step 82, d_loss: 0.3989711403846741, g_loss: 4.52344274520874\n","Epoch 79/2000, Step 83, d_loss: 0.4306415021419525, g_loss: 4.282227993011475\n","Epoch 79/2000, Step 84, d_loss: 0.40029850602149963, g_loss: 4.380730628967285\n","Epoch 79/2000, Step 85, d_loss: 0.3755667805671692, g_loss: 3.458805561065674\n","Epoch 79/2000, Step 86, d_loss: 0.5430929660797119, g_loss: 4.380532741546631\n","Epoch 79/2000, Step 87, d_loss: 0.39399924874305725, g_loss: 4.339570999145508\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 80/2000, Step 1, d_loss: 0.35569286346435547, g_loss: 4.306985378265381\n","Epoch 80/2000, Step 2, d_loss: 0.4689171314239502, g_loss: 3.3950181007385254\n","Epoch 80/2000, Step 3, d_loss: 0.39130473136901855, g_loss: 3.9479379653930664\n","Epoch 80/2000, Step 4, d_loss: 0.38914671540260315, g_loss: 3.4720468521118164\n","Epoch 80/2000, Step 5, d_loss: 0.39585286378860474, g_loss: 3.2170121669769287\n","Epoch 80/2000, Step 6, d_loss: 0.41798698902130127, g_loss: 5.012609481811523\n","Epoch 80/2000, Step 7, d_loss: 0.36096838116645813, g_loss: 3.986166477203369\n","Epoch 80/2000, Step 8, d_loss: 0.4581116735935211, g_loss: 4.406924724578857\n","Epoch 80/2000, Step 9, d_loss: 0.4043034613132477, g_loss: 4.612816333770752\n","Epoch 80/2000, Step 10, d_loss: 0.39764460921287537, g_loss: 3.7422919273376465\n","Epoch 80/2000, Step 11, d_loss: 0.39697954058647156, g_loss: 4.380103588104248\n","Epoch 80/2000, Step 12, d_loss: 0.38650619983673096, g_loss: 4.749518394470215\n","Epoch 80/2000, Step 13, d_loss: 0.4941786825656891, g_loss: 4.713109016418457\n","Epoch 80/2000, Step 14, d_loss: 0.45355698466300964, g_loss: 5.08751916885376\n","Epoch 80/2000, Step 15, d_loss: 0.4527590572834015, g_loss: 2.8395769596099854\n","Epoch 80/2000, Step 16, d_loss: 0.4294775426387787, g_loss: 3.526745319366455\n","Epoch 80/2000, Step 17, d_loss: 0.5193677544593811, g_loss: 2.8608336448669434\n","Epoch 80/2000, Step 18, d_loss: 0.4804728329181671, g_loss: 3.838691473007202\n","Epoch 80/2000, Step 19, d_loss: 0.4627208113670349, g_loss: 3.2239654064178467\n","Epoch 80/2000, Step 20, d_loss: 0.4341122806072235, g_loss: 4.407203197479248\n","Epoch 80/2000, Step 21, d_loss: 0.3907283544540405, g_loss: 3.9107375144958496\n","Epoch 80/2000, Step 22, d_loss: 0.38034483790397644, g_loss: 5.442697048187256\n","Epoch 80/2000, Step 23, d_loss: 0.36446845531463623, g_loss: 4.988592624664307\n","Epoch 80/2000, Step 24, d_loss: 0.4313819706439972, g_loss: 5.550321102142334\n","Epoch 80/2000, Step 25, d_loss: 0.3866153359413147, g_loss: 4.445520877838135\n","Epoch 80/2000, Step 26, d_loss: 0.3853703439235687, g_loss: 4.384490966796875\n","Epoch 80/2000, Step 27, d_loss: 0.3530113101005554, g_loss: 3.973330497741699\n","Epoch 80/2000, Step 28, d_loss: 0.37627995014190674, g_loss: 3.9874517917633057\n","Epoch 80/2000, Step 29, d_loss: 0.4049008786678314, g_loss: 3.8616881370544434\n","Epoch 80/2000, Step 30, d_loss: 0.5134538412094116, g_loss: 3.6311757564544678\n","Epoch 80/2000, Step 31, d_loss: 0.40524235367774963, g_loss: 3.318920135498047\n","Epoch 80/2000, Step 32, d_loss: 0.35994747281074524, g_loss: 4.428317546844482\n","Epoch 80/2000, Step 33, d_loss: 0.34565669298171997, g_loss: 5.0033369064331055\n","Epoch 80/2000, Step 34, d_loss: 0.3566921353340149, g_loss: 5.576420783996582\n","Epoch 80/2000, Step 35, d_loss: 0.3468725085258484, g_loss: 5.641881465911865\n","Epoch 80/2000, Step 36, d_loss: 0.37374868988990784, g_loss: 5.467214107513428\n","Epoch 80/2000, Step 37, d_loss: 0.3548129200935364, g_loss: 5.557527542114258\n","Epoch 80/2000, Step 38, d_loss: 0.4171428382396698, g_loss: 4.508635520935059\n","Epoch 80/2000, Step 39, d_loss: 0.3516255021095276, g_loss: 4.84581184387207\n","Epoch 80/2000, Step 40, d_loss: 0.45859843492507935, g_loss: 4.499780654907227\n","Epoch 80/2000, Step 41, d_loss: 0.39531150460243225, g_loss: 3.3192176818847656\n","Epoch 80/2000, Step 42, d_loss: 0.35262957215309143, g_loss: 2.928713321685791\n","Epoch 80/2000, Step 43, d_loss: 0.4959154427051544, g_loss: 2.5260024070739746\n","Epoch 80/2000, Step 44, d_loss: 0.41781342029571533, g_loss: 2.580453395843506\n","Epoch 80/2000, Step 45, d_loss: 0.3853130042552948, g_loss: 3.720018148422241\n","Epoch 80/2000, Step 46, d_loss: 0.40961456298828125, g_loss: 2.7390801906585693\n","Epoch 80/2000, Step 47, d_loss: 0.347180038690567, g_loss: 3.8688650131225586\n","Epoch 80/2000, Step 48, d_loss: 0.3530118763446808, g_loss: 4.296797752380371\n","Epoch 80/2000, Step 49, d_loss: 0.3538886606693268, g_loss: 5.551062107086182\n","Epoch 80/2000, Step 50, d_loss: 0.39398491382598877, g_loss: 4.454004764556885\n","Epoch 80/2000, Step 51, d_loss: 0.607084333896637, g_loss: 4.46285343170166\n","Epoch 80/2000, Step 52, d_loss: 0.4822045862674713, g_loss: 3.98016619682312\n","Epoch 80/2000, Step 53, d_loss: 0.37889784574508667, g_loss: 2.584367275238037\n","Epoch 80/2000, Step 54, d_loss: 0.5101021528244019, g_loss: 2.3129079341888428\n","Epoch 80/2000, Step 55, d_loss: 0.5663794279098511, g_loss: 2.543450355529785\n","Epoch 80/2000, Step 56, d_loss: 0.5183945298194885, g_loss: 3.7217702865600586\n","Epoch 80/2000, Step 57, d_loss: 0.44967514276504517, g_loss: 3.0980448722839355\n","Epoch 80/2000, Step 58, d_loss: 0.42355966567993164, g_loss: 4.746546745300293\n","Epoch 80/2000, Step 59, d_loss: 0.40149882435798645, g_loss: 5.548141956329346\n","Epoch 80/2000, Step 60, d_loss: 0.4383847713470459, g_loss: 3.928697109222412\n","Epoch 80/2000, Step 61, d_loss: 0.4864708483219147, g_loss: 5.159161567687988\n","Epoch 80/2000, Step 62, d_loss: 0.3828495442867279, g_loss: 3.6612069606781006\n","Epoch 80/2000, Step 63, d_loss: 0.4101918637752533, g_loss: 3.0589680671691895\n","Epoch 80/2000, Step 64, d_loss: 0.3773369789123535, g_loss: 3.253269910812378\n","Epoch 80/2000, Step 65, d_loss: 0.41017407178878784, g_loss: 3.503399133682251\n","Epoch 80/2000, Step 66, d_loss: 0.40553784370422363, g_loss: 3.394455671310425\n","Epoch 80/2000, Step 67, d_loss: 0.46001601219177246, g_loss: 2.677061080932617\n","Epoch 80/2000, Step 68, d_loss: 0.40608876943588257, g_loss: 5.002882480621338\n","Epoch 80/2000, Step 69, d_loss: 0.3876509368419647, g_loss: 4.836126327514648\n","Epoch 80/2000, Step 70, d_loss: 0.40651869773864746, g_loss: 4.006226062774658\n","Epoch 80/2000, Step 71, d_loss: 0.43900999426841736, g_loss: 4.3685784339904785\n","Epoch 80/2000, Step 72, d_loss: 0.3975299894809723, g_loss: 3.7445504665374756\n","Epoch 80/2000, Step 73, d_loss: 0.384245902299881, g_loss: 3.50272798538208\n","Epoch 80/2000, Step 74, d_loss: 0.3761441111564636, g_loss: 4.2384843826293945\n","Epoch 80/2000, Step 75, d_loss: 0.4030053913593292, g_loss: 4.932089328765869\n","Epoch 80/2000, Step 76, d_loss: 0.4057742655277252, g_loss: 3.355532169342041\n","Epoch 80/2000, Step 77, d_loss: 0.3876815140247345, g_loss: 3.311224937438965\n","Epoch 80/2000, Step 78, d_loss: 0.4530819356441498, g_loss: 4.560119152069092\n","Epoch 80/2000, Step 79, d_loss: 0.410293310880661, g_loss: 3.161735773086548\n","Epoch 80/2000, Step 80, d_loss: 0.4564076066017151, g_loss: 1.942074179649353\n","Epoch 80/2000, Step 81, d_loss: 0.3769128620624542, g_loss: 4.450683116912842\n","Epoch 80/2000, Step 82, d_loss: 0.4427545368671417, g_loss: 3.119906187057495\n","Epoch 80/2000, Step 83, d_loss: 0.4049617648124695, g_loss: 3.3980658054351807\n","Epoch 80/2000, Step 84, d_loss: 0.3854101002216339, g_loss: 4.931197643280029\n","Epoch 80/2000, Step 85, d_loss: 0.43361416459083557, g_loss: 4.894039630889893\n","Epoch 80/2000, Step 86, d_loss: 0.36229485273361206, g_loss: 6.00700044631958\n","Epoch 80/2000, Step 87, d_loss: 0.3669253885746002, g_loss: 4.269161701202393\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 81/2000, Step 1, d_loss: 0.4106289744377136, g_loss: 3.478346586227417\n","Epoch 81/2000, Step 2, d_loss: 0.3586794137954712, g_loss: 3.5310397148132324\n","Epoch 81/2000, Step 3, d_loss: 0.3746664524078369, g_loss: 3.0277881622314453\n","Epoch 81/2000, Step 4, d_loss: 0.42499515414237976, g_loss: 4.510685443878174\n","Epoch 81/2000, Step 5, d_loss: 0.41478440165519714, g_loss: 2.8825159072875977\n","Epoch 81/2000, Step 6, d_loss: 0.3749110996723175, g_loss: 5.475612163543701\n","Epoch 81/2000, Step 7, d_loss: 0.36536064743995667, g_loss: 3.811725616455078\n","Epoch 81/2000, Step 8, d_loss: 0.379391074180603, g_loss: 4.903995513916016\n","Epoch 81/2000, Step 9, d_loss: 0.3748816251754761, g_loss: 3.966701030731201\n","Epoch 81/2000, Step 10, d_loss: 0.3769457936286926, g_loss: 5.248500347137451\n","Epoch 81/2000, Step 11, d_loss: 0.38793569803237915, g_loss: 4.45569372177124\n","Epoch 81/2000, Step 12, d_loss: 0.38379785418510437, g_loss: 4.31567907333374\n","Epoch 81/2000, Step 13, d_loss: 0.42107707262039185, g_loss: 3.9568419456481934\n","Epoch 81/2000, Step 14, d_loss: 0.37383759021759033, g_loss: 4.262607574462891\n","Epoch 81/2000, Step 15, d_loss: 0.39607927203178406, g_loss: 3.953756093978882\n","Epoch 81/2000, Step 16, d_loss: 0.4046536386013031, g_loss: 4.074883460998535\n","Epoch 81/2000, Step 17, d_loss: 0.39267587661743164, g_loss: 4.442501068115234\n","Epoch 81/2000, Step 18, d_loss: 0.36233964562416077, g_loss: 4.0087738037109375\n","Epoch 81/2000, Step 19, d_loss: 0.3952990770339966, g_loss: 3.749762773513794\n","Epoch 81/2000, Step 20, d_loss: 0.36586230993270874, g_loss: 4.249239921569824\n","Epoch 81/2000, Step 21, d_loss: 0.3899770677089691, g_loss: 4.444787979125977\n","Epoch 81/2000, Step 22, d_loss: 0.35073715448379517, g_loss: 4.9866766929626465\n","Epoch 81/2000, Step 23, d_loss: 0.37969884276390076, g_loss: 4.106067180633545\n","Epoch 81/2000, Step 24, d_loss: 0.3687402009963989, g_loss: 3.547224521636963\n","Epoch 81/2000, Step 25, d_loss: 0.35678157210350037, g_loss: 4.1521992683410645\n","Epoch 81/2000, Step 26, d_loss: 0.3633502125740051, g_loss: 4.8988261222839355\n","Epoch 81/2000, Step 27, d_loss: 0.37671133875846863, g_loss: 3.881485939025879\n","Epoch 81/2000, Step 28, d_loss: 0.36432379484176636, g_loss: 3.3719496726989746\n","Epoch 81/2000, Step 29, d_loss: 0.37326696515083313, g_loss: 4.789880275726318\n","Epoch 81/2000, Step 30, d_loss: 0.3452807366847992, g_loss: 4.446201801300049\n","Epoch 81/2000, Step 31, d_loss: 0.3420027494430542, g_loss: 3.061046838760376\n","Epoch 81/2000, Step 32, d_loss: 0.3977419435977936, g_loss: 4.009566783905029\n","Epoch 81/2000, Step 33, d_loss: 0.3564186096191406, g_loss: 4.105062484741211\n","Epoch 81/2000, Step 34, d_loss: 0.3698136508464813, g_loss: 3.824664831161499\n","Epoch 81/2000, Step 35, d_loss: 0.3519132733345032, g_loss: 4.45373010635376\n","Epoch 81/2000, Step 36, d_loss: 0.35368096828460693, g_loss: 5.344061374664307\n","Epoch 81/2000, Step 37, d_loss: 0.34780368208885193, g_loss: 4.986270427703857\n","Epoch 81/2000, Step 38, d_loss: 0.3799784183502197, g_loss: 5.342912197113037\n","Epoch 81/2000, Step 39, d_loss: 0.39104562997817993, g_loss: 5.858270168304443\n","Epoch 81/2000, Step 40, d_loss: 0.34695813059806824, g_loss: 4.955258846282959\n","Epoch 81/2000, Step 41, d_loss: 0.45823121070861816, g_loss: 6.529616355895996\n","Epoch 81/2000, Step 42, d_loss: 0.35141757130622864, g_loss: 4.051745891571045\n","Epoch 81/2000, Step 43, d_loss: 0.38773876428604126, g_loss: 3.3689730167388916\n","Epoch 81/2000, Step 44, d_loss: 0.4058803617954254, g_loss: 2.9869608879089355\n","Epoch 81/2000, Step 45, d_loss: 0.4217119514942169, g_loss: 2.762483835220337\n","Epoch 81/2000, Step 46, d_loss: 0.3779958486557007, g_loss: 3.100705862045288\n","Epoch 81/2000, Step 47, d_loss: 0.43781545758247375, g_loss: 3.9170725345611572\n","Epoch 81/2000, Step 48, d_loss: 0.3736952543258667, g_loss: 3.9590344429016113\n","Epoch 81/2000, Step 49, d_loss: 0.3570614457130432, g_loss: 4.556838512420654\n","Epoch 81/2000, Step 50, d_loss: 0.3756598234176636, g_loss: 4.1068034172058105\n","Epoch 81/2000, Step 51, d_loss: 0.34170249104499817, g_loss: 4.667701721191406\n","Epoch 81/2000, Step 52, d_loss: 0.33894655108451843, g_loss: 6.537346839904785\n","Epoch 81/2000, Step 53, d_loss: 0.3535992503166199, g_loss: 5.44521427154541\n","Epoch 81/2000, Step 54, d_loss: 0.37269070744514465, g_loss: 5.551040172576904\n","Epoch 81/2000, Step 55, d_loss: 0.3625508248806, g_loss: 5.279415130615234\n","Epoch 81/2000, Step 56, d_loss: 0.36142000555992126, g_loss: 4.8740057945251465\n","Epoch 81/2000, Step 57, d_loss: 0.35019731521606445, g_loss: 4.176151752471924\n","Epoch 81/2000, Step 58, d_loss: 0.3935815095901489, g_loss: 4.076164245605469\n","Epoch 81/2000, Step 59, d_loss: 0.34922438859939575, g_loss: 3.247978925704956\n","Epoch 81/2000, Step 60, d_loss: 0.4340490698814392, g_loss: 2.937056541442871\n","Epoch 81/2000, Step 61, d_loss: 0.37421736121177673, g_loss: 2.5380148887634277\n","Epoch 81/2000, Step 62, d_loss: 0.4112072288990021, g_loss: 3.7273149490356445\n","Epoch 81/2000, Step 63, d_loss: 0.384536474943161, g_loss: 4.767848968505859\n","Epoch 81/2000, Step 64, d_loss: 0.3438016176223755, g_loss: 4.131870746612549\n","Epoch 81/2000, Step 65, d_loss: 0.3689804971218109, g_loss: 4.570478439331055\n","Epoch 81/2000, Step 66, d_loss: 0.3769534230232239, g_loss: 5.153672218322754\n","Epoch 81/2000, Step 67, d_loss: 0.3413478136062622, g_loss: 7.279284477233887\n","Epoch 81/2000, Step 68, d_loss: 0.3417525291442871, g_loss: 3.8844122886657715\n","Epoch 81/2000, Step 69, d_loss: 0.36935552954673767, g_loss: 4.59221076965332\n","Epoch 81/2000, Step 70, d_loss: 0.3584570586681366, g_loss: 4.299365997314453\n","Epoch 81/2000, Step 71, d_loss: 0.35088208317756653, g_loss: 4.128993511199951\n","Epoch 81/2000, Step 72, d_loss: 0.37343093752861023, g_loss: 3.976392984390259\n","Epoch 81/2000, Step 73, d_loss: 0.37901878356933594, g_loss: 4.513392925262451\n","Epoch 81/2000, Step 74, d_loss: 0.3444325029850006, g_loss: 4.793112277984619\n","Epoch 81/2000, Step 75, d_loss: 0.35129207372665405, g_loss: 5.065322399139404\n","Epoch 81/2000, Step 76, d_loss: 0.35086044669151306, g_loss: 4.8875579833984375\n","Epoch 81/2000, Step 77, d_loss: 0.3745986819267273, g_loss: 5.754689693450928\n","Epoch 81/2000, Step 78, d_loss: 0.35542696714401245, g_loss: 5.283478736877441\n","Epoch 81/2000, Step 79, d_loss: 0.4243834614753723, g_loss: 3.8971433639526367\n","Epoch 81/2000, Step 80, d_loss: 0.3510434627532959, g_loss: 4.515876293182373\n","Epoch 81/2000, Step 81, d_loss: 0.37277951836586, g_loss: 3.46108078956604\n","Epoch 81/2000, Step 82, d_loss: 0.3797101676464081, g_loss: 5.1118083000183105\n","Epoch 81/2000, Step 83, d_loss: 0.3558945059776306, g_loss: 4.306137561798096\n","Epoch 81/2000, Step 84, d_loss: 0.3720356523990631, g_loss: 4.489864349365234\n","Epoch 81/2000, Step 85, d_loss: 0.35775113105773926, g_loss: 4.362054347991943\n","Epoch 81/2000, Step 86, d_loss: 0.3717162311077118, g_loss: 3.5536880493164062\n","Epoch 81/2000, Step 87, d_loss: 0.36612939834594727, g_loss: 3.5575592517852783\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 82/2000, Step 1, d_loss: 0.3721049129962921, g_loss: 6.020643711090088\n","Epoch 82/2000, Step 2, d_loss: 0.3506288528442383, g_loss: 4.869832515716553\n","Epoch 82/2000, Step 3, d_loss: 0.352741539478302, g_loss: 5.497128009796143\n","Epoch 82/2000, Step 4, d_loss: 0.3864031434059143, g_loss: 5.318116664886475\n","Epoch 82/2000, Step 5, d_loss: 0.4421961307525635, g_loss: 5.48551607131958\n","Epoch 82/2000, Step 6, d_loss: 0.3893001079559326, g_loss: 4.343871593475342\n","Epoch 82/2000, Step 7, d_loss: 0.455737829208374, g_loss: 3.6886866092681885\n","Epoch 82/2000, Step 8, d_loss: 0.3846864700317383, g_loss: 2.7744526863098145\n","Epoch 82/2000, Step 9, d_loss: 0.42138025164604187, g_loss: 2.1606619358062744\n","Epoch 82/2000, Step 10, d_loss: 0.5363609790802002, g_loss: 2.410834550857544\n","Epoch 82/2000, Step 11, d_loss: 0.5666840076446533, g_loss: 3.667532444000244\n","Epoch 82/2000, Step 12, d_loss: 0.3973304331302643, g_loss: 3.0074708461761475\n","Epoch 82/2000, Step 13, d_loss: 0.4100480377674103, g_loss: 4.152174472808838\n","Epoch 82/2000, Step 14, d_loss: 0.38218390941619873, g_loss: 5.6312031745910645\n","Epoch 82/2000, Step 15, d_loss: 0.4203101098537445, g_loss: 5.29777717590332\n","Epoch 82/2000, Step 16, d_loss: 0.363525390625, g_loss: 6.0394792556762695\n","Epoch 82/2000, Step 17, d_loss: 0.4780665338039398, g_loss: 4.804581642150879\n","Epoch 82/2000, Step 18, d_loss: 0.405215322971344, g_loss: 4.214749336242676\n","Epoch 82/2000, Step 19, d_loss: 0.4526083469390869, g_loss: 4.053789138793945\n","Epoch 82/2000, Step 20, d_loss: 0.36994898319244385, g_loss: 4.2868332862854\n","Epoch 82/2000, Step 21, d_loss: 0.39737239480018616, g_loss: 3.6182425022125244\n","Epoch 82/2000, Step 22, d_loss: 0.4167875051498413, g_loss: 3.8368258476257324\n","Epoch 82/2000, Step 23, d_loss: 0.3821374177932739, g_loss: 4.282224178314209\n","Epoch 82/2000, Step 24, d_loss: 0.4087134003639221, g_loss: 4.128519535064697\n","Epoch 82/2000, Step 25, d_loss: 0.3937568664550781, g_loss: 2.7563414573669434\n","Epoch 82/2000, Step 26, d_loss: 0.3726959526538849, g_loss: 4.213400840759277\n","Epoch 82/2000, Step 27, d_loss: 0.39827242493629456, g_loss: 4.214016914367676\n","Epoch 82/2000, Step 28, d_loss: 0.35126280784606934, g_loss: 4.486563205718994\n","Epoch 82/2000, Step 29, d_loss: 0.39528265595436096, g_loss: 5.141048431396484\n","Epoch 82/2000, Step 30, d_loss: 0.40518254041671753, g_loss: 4.243190288543701\n","Epoch 82/2000, Step 31, d_loss: 0.3604128062725067, g_loss: 4.1162848472595215\n","Epoch 82/2000, Step 32, d_loss: 0.4046708345413208, g_loss: 4.083169937133789\n","Epoch 82/2000, Step 33, d_loss: 0.40768712759017944, g_loss: 3.1045074462890625\n","Epoch 82/2000, Step 34, d_loss: 0.3697425425052643, g_loss: 3.6875925064086914\n","Epoch 82/2000, Step 35, d_loss: 0.38625767827033997, g_loss: 3.247239112854004\n","Epoch 82/2000, Step 36, d_loss: 0.37773799896240234, g_loss: 3.186951160430908\n","Epoch 82/2000, Step 37, d_loss: 0.38565507531166077, g_loss: 3.7448947429656982\n","Epoch 82/2000, Step 38, d_loss: 0.3657034933567047, g_loss: 5.88153076171875\n","Epoch 82/2000, Step 39, d_loss: 0.35841238498687744, g_loss: 3.3958985805511475\n","Epoch 82/2000, Step 40, d_loss: 0.3576810657978058, g_loss: 3.689389705657959\n","Epoch 82/2000, Step 41, d_loss: 0.36531099677085876, g_loss: 4.1897358894348145\n","Epoch 82/2000, Step 42, d_loss: 0.3705267906188965, g_loss: 3.270886182785034\n","Epoch 82/2000, Step 43, d_loss: 0.43266499042510986, g_loss: 4.546432018280029\n","Epoch 82/2000, Step 44, d_loss: 0.3664136826992035, g_loss: 4.258981227874756\n","Epoch 82/2000, Step 45, d_loss: 0.3559456765651703, g_loss: 4.42059850692749\n","Epoch 82/2000, Step 46, d_loss: 0.49276477098464966, g_loss: 4.613330841064453\n","Epoch 82/2000, Step 47, d_loss: 0.38850003480911255, g_loss: 4.7014851570129395\n","Epoch 82/2000, Step 48, d_loss: 0.42623329162597656, g_loss: 4.281686305999756\n","Epoch 82/2000, Step 49, d_loss: 0.400363028049469, g_loss: 3.7560930252075195\n","Epoch 82/2000, Step 50, d_loss: 0.39191311597824097, g_loss: 3.9319329261779785\n","Epoch 82/2000, Step 51, d_loss: 0.36319297552108765, g_loss: 4.182186603546143\n","Epoch 82/2000, Step 52, d_loss: 0.40543222427368164, g_loss: 4.4304327964782715\n","Epoch 82/2000, Step 53, d_loss: 0.3732224404811859, g_loss: 4.904188632965088\n","Epoch 82/2000, Step 54, d_loss: 0.40191078186035156, g_loss: 4.523733139038086\n","Epoch 82/2000, Step 55, d_loss: 0.34098541736602783, g_loss: 5.1972126960754395\n","Epoch 82/2000, Step 56, d_loss: 0.3548351228237152, g_loss: 4.3848443031311035\n","Epoch 82/2000, Step 57, d_loss: 0.37210187315940857, g_loss: 5.606647968292236\n","Epoch 82/2000, Step 58, d_loss: 0.38029247522354126, g_loss: 4.256368637084961\n","Epoch 82/2000, Step 59, d_loss: 0.40488284826278687, g_loss: 5.429391860961914\n","Epoch 82/2000, Step 60, d_loss: 0.35245269536972046, g_loss: 4.361326694488525\n","Epoch 82/2000, Step 61, d_loss: 0.4039726257324219, g_loss: 4.295497894287109\n","Epoch 82/2000, Step 62, d_loss: 0.355176717042923, g_loss: 3.058776378631592\n","Epoch 82/2000, Step 63, d_loss: 0.3598126769065857, g_loss: 3.024385690689087\n","Epoch 82/2000, Step 64, d_loss: 0.4670187830924988, g_loss: 3.760366678237915\n","Epoch 82/2000, Step 65, d_loss: 0.3747217655181885, g_loss: 3.6874520778656006\n","Epoch 82/2000, Step 66, d_loss: 0.3464726209640503, g_loss: 4.1812052726745605\n","Epoch 82/2000, Step 67, d_loss: 0.3488149344921112, g_loss: 5.062355995178223\n","Epoch 82/2000, Step 68, d_loss: 0.35074764490127563, g_loss: 4.383313179016113\n","Epoch 82/2000, Step 69, d_loss: 0.35089877247810364, g_loss: 3.9895877838134766\n","Epoch 82/2000, Step 70, d_loss: 0.36649152636528015, g_loss: 6.2275285720825195\n","Epoch 82/2000, Step 71, d_loss: 0.4091649055480957, g_loss: 3.627507209777832\n","Epoch 82/2000, Step 72, d_loss: 0.40931883454322815, g_loss: 2.6364262104034424\n","Epoch 82/2000, Step 73, d_loss: 0.36112073063850403, g_loss: 4.56396484375\n","Epoch 82/2000, Step 74, d_loss: 0.35765165090560913, g_loss: 5.7462005615234375\n","Epoch 82/2000, Step 75, d_loss: 0.3683461844921112, g_loss: 4.543653964996338\n","Epoch 82/2000, Step 76, d_loss: 0.37816789746284485, g_loss: 6.590446472167969\n","Epoch 82/2000, Step 77, d_loss: 0.3728286027908325, g_loss: 6.020094394683838\n","Epoch 82/2000, Step 78, d_loss: 0.3658868074417114, g_loss: 5.344338893890381\n","Epoch 82/2000, Step 79, d_loss: 0.3758983910083771, g_loss: 5.326696395874023\n","Epoch 82/2000, Step 80, d_loss: 0.541323184967041, g_loss: 4.014176845550537\n","Epoch 82/2000, Step 81, d_loss: 0.35381484031677246, g_loss: 3.621380090713501\n","Epoch 82/2000, Step 82, d_loss: 0.4433629512786865, g_loss: 2.1895225048065186\n","Epoch 82/2000, Step 83, d_loss: 0.4135062098503113, g_loss: 3.0868237018585205\n","Epoch 82/2000, Step 84, d_loss: 0.43572691082954407, g_loss: 2.7747137546539307\n","Epoch 82/2000, Step 85, d_loss: 0.45586222410202026, g_loss: 3.2069756984710693\n","Epoch 82/2000, Step 86, d_loss: 0.4636157155036926, g_loss: 3.594304084777832\n","Epoch 82/2000, Step 87, d_loss: 0.3728421628475189, g_loss: 3.6055829524993896\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 83/2000, Step 1, d_loss: 0.3654460310935974, g_loss: 5.08736515045166\n","Epoch 83/2000, Step 2, d_loss: 0.37538501620292664, g_loss: 4.556643962860107\n","Epoch 83/2000, Step 3, d_loss: 0.4136170446872711, g_loss: 4.936042308807373\n","Epoch 83/2000, Step 4, d_loss: 0.34949991106987, g_loss: 5.042909145355225\n","Epoch 83/2000, Step 5, d_loss: 0.39171677827835083, g_loss: 4.904215335845947\n","Epoch 83/2000, Step 6, d_loss: 0.3544926643371582, g_loss: 4.474217891693115\n","Epoch 83/2000, Step 7, d_loss: 0.3557656407356262, g_loss: 3.463658094406128\n","Epoch 83/2000, Step 8, d_loss: 0.358132004737854, g_loss: 4.193117141723633\n","Epoch 83/2000, Step 9, d_loss: 0.37156254053115845, g_loss: 4.117941379547119\n","Epoch 83/2000, Step 10, d_loss: 0.3403105139732361, g_loss: 3.730093002319336\n","Epoch 83/2000, Step 11, d_loss: 0.36832141876220703, g_loss: 4.364113807678223\n","Epoch 83/2000, Step 12, d_loss: 0.38928720355033875, g_loss: 5.2969069480896\n","Epoch 83/2000, Step 13, d_loss: 0.3561210036277771, g_loss: 5.531780242919922\n","Epoch 83/2000, Step 14, d_loss: 0.3506692945957184, g_loss: 4.735342502593994\n","Epoch 83/2000, Step 15, d_loss: 0.35947152972221375, g_loss: 4.2035956382751465\n","Epoch 83/2000, Step 16, d_loss: 0.3921751379966736, g_loss: 3.6115918159484863\n","Epoch 83/2000, Step 17, d_loss: 0.359496533870697, g_loss: 4.347714424133301\n","Epoch 83/2000, Step 18, d_loss: 0.35343873500823975, g_loss: 3.832536220550537\n","Epoch 83/2000, Step 19, d_loss: 0.4171541929244995, g_loss: 3.819805860519409\n","Epoch 83/2000, Step 20, d_loss: 0.3860417604446411, g_loss: 3.716212272644043\n","Epoch 83/2000, Step 21, d_loss: 0.4344938099384308, g_loss: 3.88828182220459\n","Epoch 83/2000, Step 22, d_loss: 0.3652205169200897, g_loss: 5.153757572174072\n","Epoch 83/2000, Step 23, d_loss: 0.37671342492103577, g_loss: 5.996082305908203\n","Epoch 83/2000, Step 24, d_loss: 0.373509019613266, g_loss: 4.890373706817627\n","Epoch 83/2000, Step 25, d_loss: 0.4124419093132019, g_loss: 5.285060882568359\n","Epoch 83/2000, Step 26, d_loss: 0.3476405739784241, g_loss: 5.406091213226318\n","Epoch 83/2000, Step 27, d_loss: 0.3857748806476593, g_loss: 3.2022695541381836\n","Epoch 83/2000, Step 28, d_loss: 0.3654545247554779, g_loss: 4.016871929168701\n","Epoch 83/2000, Step 29, d_loss: 0.3764694929122925, g_loss: 3.388212203979492\n","Epoch 83/2000, Step 30, d_loss: 0.35082361102104187, g_loss: 3.172664165496826\n","Epoch 83/2000, Step 31, d_loss: 0.361445814371109, g_loss: 4.128028869628906\n","Epoch 83/2000, Step 32, d_loss: 0.379866361618042, g_loss: 2.917489767074585\n","Epoch 83/2000, Step 33, d_loss: 0.42406150698661804, g_loss: 4.017703056335449\n","Epoch 83/2000, Step 34, d_loss: 0.3863238990306854, g_loss: 4.475648880004883\n","Epoch 83/2000, Step 35, d_loss: 0.35249847173690796, g_loss: 5.755005836486816\n","Epoch 83/2000, Step 36, d_loss: 0.4072725772857666, g_loss: 4.566080570220947\n","Epoch 83/2000, Step 37, d_loss: 0.3847481310367584, g_loss: 4.648213863372803\n","Epoch 83/2000, Step 38, d_loss: 0.3714050352573395, g_loss: 4.464743614196777\n","Epoch 83/2000, Step 39, d_loss: 0.3505667448043823, g_loss: 3.4334535598754883\n","Epoch 83/2000, Step 40, d_loss: 0.41000115871429443, g_loss: 4.455448627471924\n","Epoch 83/2000, Step 41, d_loss: 0.41140449047088623, g_loss: 3.1897692680358887\n","Epoch 83/2000, Step 42, d_loss: 0.3729372024536133, g_loss: 3.5997884273529053\n","Epoch 83/2000, Step 43, d_loss: 0.35796377062797546, g_loss: 4.4326252937316895\n","Epoch 83/2000, Step 44, d_loss: 0.3846985995769501, g_loss: 3.821995496749878\n","Epoch 83/2000, Step 45, d_loss: 0.45211875438690186, g_loss: 3.1699321269989014\n","Epoch 83/2000, Step 46, d_loss: 0.4473169147968292, g_loss: 3.3890111446380615\n","Epoch 83/2000, Step 47, d_loss: 0.40853622555732727, g_loss: 4.0861382484436035\n","Epoch 83/2000, Step 48, d_loss: 0.3603469729423523, g_loss: 4.707049369812012\n","Epoch 83/2000, Step 49, d_loss: 0.3773816227912903, g_loss: 4.786980628967285\n","Epoch 83/2000, Step 50, d_loss: 0.3685772716999054, g_loss: 4.062747478485107\n","Epoch 83/2000, Step 51, d_loss: 0.4549053907394409, g_loss: 3.3742337226867676\n","Epoch 83/2000, Step 52, d_loss: 0.38535964488983154, g_loss: 4.3796234130859375\n","Epoch 83/2000, Step 53, d_loss: 0.3893634080886841, g_loss: 3.402737855911255\n","Epoch 83/2000, Step 54, d_loss: 0.3898419141769409, g_loss: 4.369336128234863\n","Epoch 83/2000, Step 55, d_loss: 0.39981189370155334, g_loss: 3.568103075027466\n","Epoch 83/2000, Step 56, d_loss: 0.3826729357242584, g_loss: 3.536869525909424\n","Epoch 83/2000, Step 57, d_loss: 0.41407546401023865, g_loss: 4.4970831871032715\n","Epoch 83/2000, Step 58, d_loss: 0.37051284313201904, g_loss: 4.9335455894470215\n","Epoch 83/2000, Step 59, d_loss: 0.3780282735824585, g_loss: 3.2672653198242188\n","Epoch 83/2000, Step 60, d_loss: 0.5437065958976746, g_loss: 4.1132636070251465\n","Epoch 83/2000, Step 61, d_loss: 0.3911106288433075, g_loss: 3.230772018432617\n","Epoch 83/2000, Step 62, d_loss: 0.424043744802475, g_loss: 3.885952949523926\n","Epoch 83/2000, Step 63, d_loss: 0.4195743203163147, g_loss: 2.8619496822357178\n","Epoch 83/2000, Step 64, d_loss: 0.378844290971756, g_loss: 3.1829147338867188\n","Epoch 83/2000, Step 65, d_loss: 0.37231671810150146, g_loss: 3.5116946697235107\n","Epoch 83/2000, Step 66, d_loss: 0.4292546510696411, g_loss: 3.5313820838928223\n","Epoch 83/2000, Step 67, d_loss: 0.4750204086303711, g_loss: 4.202144622802734\n","Epoch 83/2000, Step 68, d_loss: 0.35757386684417725, g_loss: 3.644707679748535\n","Epoch 83/2000, Step 69, d_loss: 0.3615732192993164, g_loss: 5.097627639770508\n","Epoch 83/2000, Step 70, d_loss: 0.5267378091812134, g_loss: 4.469857215881348\n","Epoch 83/2000, Step 71, d_loss: 0.3932773172855377, g_loss: 3.9353485107421875\n","Epoch 83/2000, Step 72, d_loss: 0.3567337393760681, g_loss: 3.835763454437256\n","Epoch 83/2000, Step 73, d_loss: 0.38240960240364075, g_loss: 3.774258852005005\n","Epoch 83/2000, Step 74, d_loss: 0.4932422637939453, g_loss: 2.4915783405303955\n","Epoch 83/2000, Step 75, d_loss: 0.3665130138397217, g_loss: 2.977971076965332\n","Epoch 83/2000, Step 76, d_loss: 0.4064881205558777, g_loss: 4.114318370819092\n","Epoch 83/2000, Step 77, d_loss: 0.4473413825035095, g_loss: 3.830918550491333\n","Epoch 83/2000, Step 78, d_loss: 0.3482576012611389, g_loss: 4.166706562042236\n","Epoch 83/2000, Step 79, d_loss: 0.3633960485458374, g_loss: 4.029907703399658\n","Epoch 83/2000, Step 80, d_loss: 0.3747320771217346, g_loss: 5.109273433685303\n","Epoch 83/2000, Step 81, d_loss: 0.39494258165359497, g_loss: 5.224967002868652\n","Epoch 83/2000, Step 82, d_loss: 0.3637107014656067, g_loss: 4.767841815948486\n","Epoch 83/2000, Step 83, d_loss: 0.3620069622993469, g_loss: 4.56588077545166\n","Epoch 83/2000, Step 84, d_loss: 0.3995187282562256, g_loss: 4.054431438446045\n","Epoch 83/2000, Step 85, d_loss: 0.41497015953063965, g_loss: 3.264627456665039\n","Epoch 83/2000, Step 86, d_loss: 0.48091158270835876, g_loss: 4.435104846954346\n","Epoch 83/2000, Step 87, d_loss: 0.36813855171203613, g_loss: 4.124113082885742\n","Going\n","Generating Samples...\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Saved\n","Epoch 84/2000, Step 1, d_loss: 0.37196624279022217, g_loss: 4.988560199737549\n","Epoch 84/2000, Step 2, d_loss: 0.4160672426223755, g_loss: 4.422641754150391\n","Epoch 84/2000, Step 3, d_loss: 0.36962974071502686, g_loss: 4.780967712402344\n","Epoch 84/2000, Step 4, d_loss: 0.3788871467113495, g_loss: 5.623023986816406\n","Epoch 84/2000, Step 5, d_loss: 0.35287055373191833, g_loss: 5.178558826446533\n","Epoch 84/2000, Step 6, d_loss: 0.34815678000450134, g_loss: 4.877829551696777\n","Epoch 84/2000, Step 7, d_loss: 0.3949132263660431, g_loss: 5.180516242980957\n","Epoch 84/2000, Step 8, d_loss: 0.36756446957588196, g_loss: 4.266006946563721\n","Epoch 84/2000, Step 9, d_loss: 0.4058116376399994, g_loss: 3.7948105335235596\n","Epoch 84/2000, Step 10, d_loss: 0.3398430049419403, g_loss: 3.923567295074463\n","Epoch 84/2000, Step 11, d_loss: 0.3510541021823883, g_loss: 6.675710201263428\n","Epoch 84/2000, Step 12, d_loss: 0.3880550265312195, g_loss: 3.685112237930298\n","Epoch 84/2000, Step 13, d_loss: 0.3638138175010681, g_loss: 3.0090646743774414\n","Epoch 84/2000, Step 14, d_loss: 0.387004554271698, g_loss: 4.807968616485596\n","Epoch 84/2000, Step 15, d_loss: 0.362161785364151, g_loss: 5.042383193969727\n","Epoch 84/2000, Step 16, d_loss: 0.377398818731308, g_loss: 4.427103519439697\n","Epoch 84/2000, Step 17, d_loss: 0.3912130296230316, g_loss: 4.589003562927246\n","Epoch 84/2000, Step 18, d_loss: 0.3510758876800537, g_loss: 4.7681121826171875\n","Epoch 84/2000, Step 19, d_loss: 0.37845656275749207, g_loss: 5.5049543380737305\n","Epoch 84/2000, Step 20, d_loss: 0.38005903363227844, g_loss: 5.124782085418701\n","Epoch 84/2000, Step 21, d_loss: 0.42018917202949524, g_loss: 4.544521331787109\n","Epoch 84/2000, Step 22, d_loss: 0.3878236413002014, g_loss: 4.640888690948486\n","Epoch 84/2000, Step 23, d_loss: 0.3586232364177704, g_loss: 3.9888973236083984\n","Epoch 84/2000, Step 24, d_loss: 0.4104284346103668, g_loss: 3.2425460815429688\n","Epoch 84/2000, Step 25, d_loss: 0.40520840883255005, g_loss: 3.488781213760376\n","Epoch 84/2000, Step 26, d_loss: 0.4662589430809021, g_loss: 4.788722991943359\n","Epoch 84/2000, Step 27, d_loss: 0.41738659143447876, g_loss: 5.655567169189453\n","Epoch 84/2000, Step 28, d_loss: 0.35879626870155334, g_loss: 4.108936309814453\n","Epoch 84/2000, Step 29, d_loss: 0.3920443058013916, g_loss: 4.439587116241455\n","Epoch 84/2000, Step 30, d_loss: 0.3931189477443695, g_loss: 4.438149929046631\n","Epoch 84/2000, Step 31, d_loss: 0.41835159063339233, g_loss: 5.464229106903076\n","Epoch 84/2000, Step 32, d_loss: 0.4012286067008972, g_loss: 4.8193817138671875\n","Epoch 84/2000, Step 33, d_loss: 0.5219016075134277, g_loss: 4.311236381530762\n","Epoch 84/2000, Step 34, d_loss: 0.3564614951610565, g_loss: 3.866506814956665\n","Epoch 84/2000, Step 35, d_loss: 0.41787275671958923, g_loss: 3.6771585941314697\n"]}],"source":["from PIL import Image\n","import torch.nn as nn\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","#All images follow this format Abstract_image_155\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print (device.type)\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","bs = 32\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 128\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 1\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","\n","# Size of feature maps in discriminator\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers\n","beta1 = 0.99\n","\n","\n","###############\n","dataset = datasets.ImageFolder(root='G:/My Drive/Colab Notebooks/art77/Abstract_gallery',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/animals/',\n","#dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/art77/ag2/',\n","                                   transform=transforms.Compose([\n","                                   transforms.Grayscale(num_output_channels=1), # Add this line\n","                                   transforms.Resize(image_size),\n","                                   transforms.CenterCrop(image_size),\n","                                   transforms.ToTensor(),\n","                                   transforms.Normalize((0.5,), (0.5,))\n","                               ]))\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n","                                         shuffle=True, num_workers=workers)\n","#################################\n","\n","\n","#not used\n","img_size = 200\n","\n","print('test')\n","def noise(bs, nz):\n","\n","    #Generate random Gaussian noise.\n","\n","    return Variable(torch.randn(bs, nz, 1, 1))\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input is noise size\n","            #switched from using ReLU to LeakyReLu\n","            nn.ConvTranspose2d( 100, ngf * 8, 3, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, ngf * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d( ngf * 2, nc, 3, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # input is ``(nc) x 64 x 64``\n","            nn.Conv2d(1, ndf, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.1),\n","            nn.Conv2d(ndf , ndf * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. ``(ndf*8) x 4 x 4``\n","            nn.Conv2d(ndf * 4, 1, 3, 1, 0, bias=False),\n","            nn.AdaptiveAvgPool2d(1),\n","\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input).squeeze()\n","\n","class GAN:\n","    def __init__(self, discriminator, generator, batch_size=1):\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.batch_size = batch_size\n","        self.g_losses = []\n","        self.d_losses = []\n","        # Define binary cross entropy loss\n","        self.loss = nn.BCELoss()\n","\n","        # Define separate optimizers for discriminator and generator\n","        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0015)\n","        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n","\n","    def train(self, num_epochs, dataloader, resume=False, checkpoint_path='checkpointA.pth'):\n","      # Training Loop for each epoch\n","      start_epoch = 0\n","      if resume:\n","        if os.path.isfile(checkpoint_path):\n","          print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","          checkpoint = torch.load(checkpoint_path)\n","          start_epoch = checkpoint['epoch']\n","          self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","          self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","          self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","          self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","          print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n","        else:\n","            print(f\"=> no checkpoint found at '{checkpoint_path}'\")\n","\n","      for epoch in range(start_epoch, num_epochs):\n","        print ('Going')\n","\n","        if epoch % 1 == 0:\n","          print('Generating Samples...')\n","          # Generate images from noise, using the generator network.\n","          count = 6\n","          for i in range(count):\n","\n","            sample_vectors = noise(bs,100)\n","            samples = self.generator(sample_vectors)\n","            save_image(samples, f'G:/My Drive/Colab Notebooks/dandies/new_dandies/Arcwride_{epoch}_{i}.png', normalize=True)\n","            #save_image(samples, f'/content/drive/MyDrive/Colab Notebooks/dandies/new_dandies/Odlud_{epoch}_{i}.png', normalize=True)\n","\n","            print ('Saved')\n","            # Batch Loop for each set of images and labels\n","          for n, (images, _) in enumerate(dataloader):\n","                current_batch_size = images.size(0)\n","\n","                real_images = Variable(images)\n","                #Switched from 1 to using .9 as the target\n","                real_labels = Variable(torch.full((current_batch_size,), 0.9))\n","\n","\n","\n","\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images)\n","\n","                d_loss_real = self.loss(real_outputs.squeeze(0), real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(current_batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on real images\n","                self.d_optimizer.zero_grad()\n","                real_outputs = self.discriminator(real_images).view(-1).squeeze()\n","                real_outputs = real_outputs.squeeze()\n","\n","                d_loss_real = self.loss(real_outputs, real_labels)\n","                d_loss_real.backward()\n","\n","                # Prepare fake images and fake labels\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                fake_labels = Variable(torch.zeros(self.batch_size, 1)).view(-1)\n","\n","                # Train Discriminator on fake images\n","                fake_outputs = self.discriminator(fake_images.detach()).view(-1)\n","                d_loss_fake = self.loss(fake_outputs, fake_labels)\n","                d_loss_fake.backward()\n","\n","                # Update Discriminator weights\n","                self.d_optimizer.step()\n","                #self.d_losses.append(d_loss_real+d_loss_fake.item())\n","\n","                # Train Generator to fool the Discriminator\n","                self.g_optimizer.zero_grad()\n","                noise_vectors = noise(self.batch_size, nz)\n","                fake_images = self.generator(noise_vectors)\n","                outputs = self.discriminator(fake_images).view(-1)\n","\n","                # We train the generator to generate images that the discriminator will think are real\n","                g_loss = self.loss(outputs, Variable(torch.ones(self.batch_size)).view(-1))\n","                g_loss.backward()\n","\n","                # Update Generator weights\n","                self.g_optimizer.step()\n","                self.g_losses.append(g_loss.item())\n","\n","                if (n+1) % 1 == 0:\n","                    print(f'Epoch {epoch+1}/{num_epochs}, Step {n+1}, d_loss: {d_loss_real+d_loss_fake}, g_loss: {g_loss}')\n","                    torch.save({\n","                    'epoch': epoch,\n","                    'generator_state_dict': gan.generator.state_dict(),\n","                    'discriminator_state_dict': gan.discriminator.state_dict(),\n","                    'g_optimizer_state_dict': gan.g_optimizer.state_dict(),\n","                    'd_optimizer_state_dict': gan.d_optimizer.state_dict(),\n","                    'g_loss': g_loss,\n","                    'd_loss': d_loss_fake\n","                    }, 'checkpointA.pth')\n","\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","gan = GAN(discriminator, generator)\n","gan.train(2000, dataloader, resume=False)"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7TZvDVQTtN6","executionInfo":{"status":"ok","timestamp":1690605787603,"user_tz":240,"elapsed":19,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"}},"outputId":"5af1965c-5a51-4b47-cc3e-9ee2f70c1b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":4234,"status":"error","timestamp":1690520592505,"user":{"displayName":"Travis Joldersma","userId":"05017183511704664878"},"user_tz":240},"id":"k-Ceo4kqb0n7","outputId":"0548b42f-16ae-4cb0-96aa-0eb7a805d992"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbd95867bba1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gan' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["\n","#Visualize the generator and discriminator losses over epochs\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","sns.lineplot(data=gan.g_losses, label=\"G\")\n","sns.lineplot(data=gan.d_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1GN2L9UPYR6TCzDsqMBaHwwc6PkLlmsyg","authorship_tag":"ABX9TyOiOqxu3Su2UOd8WWSAq1Kd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}